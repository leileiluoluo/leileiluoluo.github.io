[{"categories":["计算机"],"contents":"继承是来自于面向对象数据库的概念，其为数据库设计带来了新的可能性。\n先看一个例子：假定我们正在构建一个数据模型来存储所有的城市，而城市中有的是普通城市，有的是省会城市，如何进行表设计呢？继承特性就能很巧妙的表示这种关系。\n下面为建表语句：\n-- 城市表 CREATE TABLE cities ( name varchar(100) PRIMARY KEY, -- 名称  population float8, -- 人口，单位为百万  elevation int, -- 平均海拔，单位为米  province varchar(100) -- 省份 ); -- 省会表 CREATE TABLE capitals () INHERITS (cities); 上面的capitals表继承了cities表的所有列。\n下面插入一些示例数据：\n-- 城市表插入数据 INSERT INTO cities VALUES (\u0026#39;大连\u0026#39;, 7.51, 40, \u0026#39;辽宁\u0026#39;); INSERT INTO cities VALUES (\u0026#39;盘锦\u0026#39;, 1.39, 4, \u0026#39;辽宁\u0026#39;); INSERT INTO cities VALUES (\u0026#39;朝阳\u0026#39;, 2.79, 160, \u0026#39;辽宁\u0026#39;); -- 省会表插入数据 INSERT INTO capitals VALUES (\u0026#39;沈阳\u0026#39;, 9.14, 50, \u0026#39;辽宁\u0026#39;); 对于上面的插入语句需要注意：针对两张表，INSERT命令得分别插入，哪怕两张表字段不一样，INSERT命令也不支持只插入父表，而自动寻找对应的子表。（还有COPY命令也一样）\n在 PostgreSQL 中，一个表可以继承 0 个或多个表，查询时，可以只查询一个表的数据，也可以查询一个表与其所有继承表的数据。\n下面的查询会返回包括capitals在内的所有数据：\ntest=# SELECT * FROM cities; name | population | elevation | province ------+------------+-----------+---------- 大连 | 7.51 | 40 | 辽宁 盘锦 | 1.39 | 4 | 辽宁 朝阳 | 2.79 | 160 | 辽宁 沈阳 | 9.14 | 50 | 辽宁 (4 rows) 而下面的查询仅会返回cities的所有数据：\ntest=# SELECT * FROM ONLY cities; name | population | elevation | province ------+------------+-----------+---------- 大连 | 7.51 | 40 | 辽宁 盘锦 | 1.39 | 4 | 辽宁 朝阳 | 2.79 | 160 | 辽宁 (3 rows) 这里，ONLY关键字的意思是仅查询cities表的数据，不包括从cities表继承的表的数据。除SELECT外，UPDATE与DELETE也支持ONLY关键字（如执行DELETE FROM ONLY cities;时，只会删除cities表的数据）。\n想知道每一行具体来自于哪张表时，可使用如下查询：\ntest=# SELECT p.relname as table, c.* test-# FROM cities c, pg_class p test-# WHERE c.tableoid = p.oid; table | name | population | elevation | province ----------+------+------------+-----------+---------- cities | 大连 | 7.51 | 40 | 辽宁 cities | 盘锦 | 1.39 | 4 | 辽宁 cities | 朝阳 | 2.79 | 160 | 辽宁 capitals | 沈阳 | 9.14 | 50 | 辽宁 (4 rows) 默认情况下，子表会继承父表上的所有检查约束和非空约束（除非使用NO INHERIT子句明确指定不继承哪些）；而不会继承唯一约束、主键约束和外键约束。\n所以，如下两条重复语句是可以执行成功的：\n-- 因主键约束未继承过来，省会表可以插入两条重复数据 INSERT INTO capitals VALUES (\u0026#39;沈阳\u0026#39;, 9.14, 50, \u0026#39;辽宁\u0026#39;); INSERT INTO capitals VALUES (\u0026#39;沈阳\u0026#39;, 9.14, 50, \u0026#39;辽宁\u0026#39;); 因一张表可以继承多个父表，这样子表拥有的列就是：所有父表拥有的列的并集再加上自定义的列。若父表间拥有同名的列，或子表与父表拥有同名的列，则这些列将会被合并（同名的列必须具有相同的类型，否则会报错）；同名检查约束与非空约束的合并规则也是一样的。\n就像前面的示例一样，表继承一般是在初期创建子表时（CREATE TABLE ... INHERITS ...）建立的。此外，还可以使用ALTER TABLE ... INHERIT ...来对现有表建立父子关系。这时，新子表必须包含父表所有的列，且类型必须与父表一致，而且检查约束的名称与检查表达式也必须与父表一致。也可以使用ALTER TABLE ... NO INHERIT ...来从子级中删除某个继承链。当继承关系用于表分区时，像这样动态添加和删除继承链的特性会很有用。\n创建兼容表（稍后将成为新子表）的一种便捷方法是在CREATE TABLE中使用LIKE子句。这将创建一个与源表具有相同列的表。如果在源表上定义了任何检查约束，则应指定LIKE的INCLUDING CONSTRAINTS选项，因为新子表必须具有与父表匹配的约束才能被视为兼容。\n当父表有子表存在时，父表不能被直接删除。如果子表的列或检查约束是从任意父表继承的，则也不能删除或更改它们。若希望删除父表及其所有的继承表，则可以使用CASCADE选项。\nDROP TABLE cities CASCADE;  参考资料\n[1] 5.10 Inheritance - Data Definition | PostgreSQL 16 Documentation - www.postgresql.org\n[2] 3.6 Inheritance - Advanced Features | PostgreSQL 16 Documentation - www.postgresql.org\n[3] When to use inherited tables in PostgreSQL? | Stack Overflow - stackoverflow.com\n ","permalink":"https://olzhy.github.io/posts/postgres-table-inheritance.html","tags":["PostgreSQL"],"title":"PostgreSQL 表继承使用详解"},{"categories":["计算机"],"contents":"Moshi 是一个可用于 Java 与 Kotlin 的 JSON 序列化与反序列化库，其主要使用 Kotlin 编写。本文以样例代码的方式来演示该库在 Java 中的使用。\n示例项目使用 Maven 管理，下面列出写作本文时用到的 JDK、Maven 及 Moshi 的版本：\nJDK：Amazon Corretto 17.0.8 Maven：3.9.2 Moshi：1.18.30 开始前，需要在pom.xml的\u0026lt;dependencies\u0026gt;下引入 Moshi 依赖（moshi为核心模块，moshi-adapters模块包含诸如Date类型处理等实用的 Adapter）：\n\u0026lt;!-- pom.xml --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.squareup.moshi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;moshi\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.squareup.moshi\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;moshi-adapters\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 为了省去编写繁琐的Setters与Getters，该示例项目还使用了 Lombok，依赖如下：\n\u0026lt;!-- pom.xml --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.30\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 此外，我们以编写 JUnit 单元测试的方式来演示 Moshi 的使用，所以还需引入junit-jupiter依赖：\n\u0026lt;!-- pom.xml --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.10.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 准备好后，即可以开始使用了。\n1 基础使用 首先，看一下最基础的使用，即如何使用 Moshi 进行序列化与反序列化（即 Java 对象转换为 JSON，以及 JSON 转换为 Java 对象）。\n先新建一个 Model 类，本文以User为例，该类有三个字段：name、roles和createdAt，包括了String、List、Date以及枚举类型。\n// src/main/java/com/example/demo/model/User.java package com.example.demo.model; import lombok.Getter; import lombok.NoArgsConstructor; import lombok.Setter; import lombok.ToString; import java.util.Date; import java.util.List; @ToString @Getter @Setter @NoArgsConstructor public class User { private String name; private List\u0026lt;Role\u0026gt; roles; private Date createdAt; public enum Role { ADMIN, EDITOR, VIEWER } } 下面即编写一个测试用例来演示 User 对象与 JSON 的互转：\n// src/test/java/com/example/demo/MoshiTest#testBasicUsage @Test public void testBasicUsage() { // 构造 Moshi 实例  Moshi moshi = new Moshi.Builder() .add(Date.class, new Rfc3339DateJsonAdapter()) .build(); // 获取 User 的 JsonAdapter  JsonAdapter\u0026lt;User\u0026gt; jsonAdapter = moshi.adapter(User.class); // 构造 User 对象  User user = new User(); user.setName(\u0026#34;Larry\u0026#34;); user.setRoles(List.of(User.Role.ADMIN, User.Role.EDITOR)); user.setCreatedAt(new Date()); // 序列化  String json = jsonAdapter.toJson(user); System.out.println(json); // 反序列化  try { User userParsed = jsonAdapter.fromJson(json); System.out.println(userParsed); } catch (IOException e) { e.printStackTrace(); } } 需要注意的是，如上代码在构造 Moshi 实例时指定了Date类型对应的 JSON Adapter Rfc3339DateJsonAdapter，否则在解析该类型字段时会报错。\n如上代码运行结果如下：\n{\u0026#34;createdAt\u0026#34;:\u0026#34;2023-10-14T09:47:37.763Z\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;roles\u0026#34;:[\u0026#34;ADMIN\u0026#34;,\u0026#34;EDITOR\u0026#34;]} User(name=Larry, roles=[ADMIN, EDITOR], createdAt=Sat Oct 14 17:47:37 CST 2023) 可以看到，User 对象序列化为的 JSON、JSON 反序列化为的 User 对象都是正确的。\n2 使用 @Json 自定义字段名 上面的例子中，JSON 里的字段名与 Java 类里的属性名是完全一致的。\n如果某个字段名需要自定义，该怎么做呢？下面即进行了演示：\n// src/main/java/com/example/demo/model/User.java package com.example.demo.model; @ToString @Getter @Setter @NoArgsConstructor public class User { // ...  @Json(name = \u0026#34;created_at\u0026#34;) private Date createdAt; // ... } 可以看到，只需在 Java 类对应的属性上加上@Json注解，然后自定义其名称就可以了。\n再次运行一下上面的测试用例（src/test/java/com/example/demo/MoshiTest#testBasicUsage），得到了如下结果：\n{\u0026#34;created_at\u0026#34;:\u0026#34;2023-10-14T09:55:48.793Z\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;roles\u0026#34;:[\u0026#34;ADMIN\u0026#34;,\u0026#34;EDITOR\u0026#34;]} User(name=Larry, roles=[ADMIN, EDITOR], createdAt=Sat Oct 14 17:55:48 CST 2023) 可以看到，User 类的createdAt属性在序列化为 JSON 时变为了created_at；该 JSON 再次反序列化为 User 对象时，createdAt属性的赋值也是正常的。\n3 自定义 Adapter 前面的例子中，我们为Date类型指定了 Moshi 自带的 Adapter Rfc3339DateJsonAdapter 来支持其解析。如果觉得这个 Adapter 解析后的日期格式不是想要的，有办法自己指定吗？当然是可以的，Moshi 支持我们针对某种类型使用自定义 Adapter 来实现其序列化与反序列化逻辑。\n下面即对 User 类中的枚举类型Role编写一个自定义 Adapter 来实现其自定义解析：\n// src/main/java/com/example/demo/adapter/RoleAdapter.java package com.example.demo.adapter; import com.example.demo.model.User; import com.squareup.moshi.FromJson; import com.squareup.moshi.ToJson; public class RoleAdapter { @ToJson public String toJson(User.Role role) { return role.name().substring(0, 1); } @FromJson public User.Role fromJson(String role) { switch (role.charAt(0)) { case \u0026#39;A\u0026#39;: return User.Role.ADMIN; case \u0026#39;E\u0026#39;: return User.Role.EDITOR; case \u0026#39;V\u0026#39;: return User.Role.VIEWER; } return null; } } 可以看到，我们为Role类型编写一个自定义 Adapter RoleAdapter。该类中有两个方法toJson与fromJson，且分别被加上了注解@ToJson与@FromJson，这两个方法分别用于Role类型由 Java 属性转为 JSON 字段以及由 JSON 字段转换为 Java 属性时的逻辑（本示例仅取第一个字母来表示 User 拥有的Role）。\n下面编写一个测试用例来演示该自定义 Adapter 的使用：\n// src/test/java/com/example/demo/MoshiTest#testCustomTypeAdapter @Test public void testCustomTypeAdapter() { // 构造 Moshi 实例  Moshi moshi = new Moshi.Builder() .add(Date.class, new Rfc3339DateJsonAdapter()) .add(new RoleAdapter()) .build(); // 获取 User 的 JsonAdapter  JsonAdapter\u0026lt;User\u0026gt; jsonAdapter = moshi.adapter(User.class); // 构造 User 对象  User user = new User(); user.setName(\u0026#34;Larry\u0026#34;); user.setRoles(List.of(User.Role.ADMIN, User.Role.EDITOR)); user.setCreatedAt(new Date()); // 序列化  String json = jsonAdapter.toJson(user); System.out.println(json); // 反序列化  try { User userParsed = jsonAdapter.fromJson(json); System.out.println(userParsed); } catch (IOException e) { e.printStackTrace(); } } 可以看到，只需在构造 Moshi 实例时，添加上该 Adapter 即可。\n运行一下该测试用例，得到的结果如下：\n{\u0026#34;created_at\u0026#34;:\u0026#34;2023-10-14T10:39:04.174Z\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;roles\u0026#34;:[\u0026#34;A\u0026#34;,\u0026#34;E\u0026#34;]} User(name=Larry, roles=[ADMIN, EDITOR], createdAt=Sat Oct 14 18:39:04 CST 2023) 可以看到，User 对象序列化为的 JSON 中，roles字段的值使用了我们的自定义逻辑（仅取第一个字母来表示 User 的 Role）；该 JSON 再次反序列化为的 User 对象也使用了我们的自定义逻辑，反序列化结果也是正确的。\n4 JSON 数组如何处理？ 上面演示的均为 Java 对象与 JSON 对象的互相转换，Java List 如何与 JSON 数组互相转换呢？\n下面的测试用例作了演示：\n// src/test/java/com/example/demo/MoshiTest#testJSONArrayParsing @Test public void testJSONArrayParsing() { // 新建一个类型  Type type = Types.newParameterizedType(List.class, User.class); // 构造 Moshi 实例  Moshi moshi = new Moshi.Builder() .add(Date.class, new Rfc3339DateJsonAdapter()) .add(new RoleAdapter()) .build(); // 获取 User 的 JsonAdapter  JsonAdapter\u0026lt;List\u0026lt;User\u0026gt;\u0026gt; jsonAdapter = moshi.adapter(type); // 构造 User 对象  User user = new User(); user.setName(\u0026#34;Larry\u0026#34;); user.setCreatedAt(new Date()); user.setRoles(List.of(User.Role.ADMIN, User.Role.EDITOR)); // 序列化  String json = jsonAdapter.toJson(List.of(user)); System.out.println(json); // 反序列化  try { List\u0026lt;User\u0026gt; usersParsed = jsonAdapter.fromJson(json); System.out.println(usersParsed); } catch (IOException e) { e.printStackTrace(); } } 可以看到，只需新建一个 Moshi Type 即可，使用起来也一样简单。\n如上测试用例的运行结果如下：\n[{\u0026#34;created_at\u0026#34;:\u0026#34;2023-10-14T11:22:27.153Z\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;roles\u0026#34;:[\u0026#34;A\u0026#34;,\u0026#34;E\u0026#34;]}] [User(name=Larry, roles=[ADMIN, EDITOR], createdAt=Sat Oct 14 19:22:27 CST 2023)] 可以看到，User List 与 JSON Array 的互相转换结果均是正确的。\n综上，本文探索了在 Java 中使用 Moshi 进行 JSON 序列化和反序列化的各种常见用法。文中所涉及的全部代码已托管至本人 GitHub，欢迎关注或 Fork。\n 参考资料\n[1] Moshi - A modern JSON library for Kotlin and Java | GitHub - github.com\n ","permalink":"https://olzhy.github.io/posts/try-using-moshi-json-library-in-java.html","tags":["Java"],"title":"尝试在 Java 中使用 Moshi JSON 库"},{"categories":["随笔"],"contents":"今天在群裏看到一個朋友分享了一篇文章，說「中國迅速成爲科技大國，漢語在背後發揮了很重要的優勢」。\n乍一看這個標題可能覺得有一點牽强，但仔細想想，的確有一定的道理。\n我從事計算機行業，關於專業知識的一手資料基本都是英文的，所以平日裏接觸英文也比較多。不知是西方人的思維方式還是文字本身的緣由，英文資料給我的感覺是，不喜歡抽象而喜歡具體，知識之間的邊界比較明顯。爲了講清楚一個稍微大一點的概念會細分出很多相對隔離的枝節，每個部分講的事無巨細，生怕哪個點有歧義，簡直就是單一領域專研者的圭臬之選。但各個類別之間缺乏連通，少了一些匯總與歸納，當涉及的問題大一點的時候，需要翻閲大量相對獨立的枝葉，無法從最開始建立整體觀。\n中文資料給人的感覺正好與前一種模式相反，喜歡抽象卻不喜歡具體，特別擅長做提綱挈領，容易一開始就能抓住事情的主幹，但到細節部分卻非常惜墨。這可能是我們的思維習慣造成的。對於一些細枝末節，中文思維可能會默認別人已心領神會，不屑於將其描述的明明白白，但到具體落地時，往往理解的千差萬別，容易出現問題。\n這是我對英文與中文這兩種語言在使用現象上感受到的不同。因兩者一個代表表音文字，一個代表表意文字，這些語言使用者的習慣與思維模式的不同是否源於文字本身呢？哪種文字更能映射真實的世界呢？下面便作淺析。\n文字是描述事物的符號，源於人們對現實世界的直觀理解以及對事物閒關係的解構。創立文字的首要目的就是用來傳遞信息，傳遞信息的宗旨就是要最大限度地降低失真程度。文字抵達眼前時，需由大腦或内心來解析為人們之間的共識或情感。英語等表音文字進入大腦的感覺是一條條長長短短的綫，呈現的畫像是靜態的、條狀的；而漢字這種由象形而來的文字，就是事物形象的直觀映射，進入大腦的感覺是一張張跳動的圖，呈現的畫像是動態的、立體的。所以，漢字似乎以更接近現實的方式映射了這個世界。\n漢字的象形屬性與表意能力讓每個字具有了特殊的情景與氣氛，且漢語的造詞非常謹慎細緻，讓本是相關的事物從内核態就建立了聯係，且出現新事物時，不用造新字，用恰當的漢字組合將其情景或氣氛描述出來就好，這個是漢字非常擅長的；而對於表音文字，從一開始就丟失了事物的形態，其承載的信息也就變得非常單薄，且一個個本應相關的單詞從造詞層面就沒有將其聯係起來，讓事物變成一個個不相關的個體，衍生能力就相對較弱，出現新事物時常有逼不得已造新詞的情況，造成詞典越來越臃腫，體現了其天生設計上的不足。\n此外，漢語的最小表意單元是字，粒度小，組合能力強，且多數字還有偏旁部首等輔助信息，面對一個陌生字或者是陌生詞，人們常常能做到「見詞猜意」；而表音文字的最小表意單位卻是詞，粒度大，組合能力弱，雖然有的詞也有詞根，但感覺沒那麽準確，面對一個陌生詞，人們常常無法做到「見詞猜意」。\n這樣，相對於表音文字，漢語的「熵」以及汎化能力就會高很多，同樣長度的段落，漢語承載的信息量會比表音文字高出很多；表達相同的場景或現象，漢語使用詞語的長度也會比表音文字少出很多。\n英語等走演繹路綫的表音文字適合做細分學科，能解決很多局部問題。但如今科技發展，面對的問題越來越複雜，涉及的面也越來越廣，需要很多交叉學科的綜合知識來解決，這樣，天生愛歸納且「熵」值高的漢語就有了很大的優勢。\n分析到這裏也不禁感嘆，世界上那麽多語言，除漢語外，均爲表音文字；而似乎更高級一些的表意文字，卻僅此一家，真的贊嘆祖先的偉大！\n癸卯年八月廿六於大連\n","permalink":"https://olzhy.github.io/posts/about-chinese-character.html","tags":["随笔"],"title":"關於漢字"},{"categories":["随笔"],"contents":"9 月 29 號中秋節，我、太太和孩子一起乘坐大巴往岳父家趕，開始了我們的假期模式。大巴車上坐滿了人，幾乎不剩一個空餘的座位，果然是過節，路上非常的堵，大巴行駛極其緩慢。因早上沒吃主食，又喝了一大碗粥，上車沒多久就開始尿急了，痛苦難忍，趕緊上前告訴了司機師傅。恨不得車馬上就停下來，想著路邊只一顆能遮躰的樹就好，也理解了高速路上那些路邊小便的不文明的行爲，不經歷不知道，有的時候真的是不得已爲之。好不容易遇到了一個加油站，車停了下來，我便徑直飛向那裏邊的厠所，一撒爾儘，那叫一個酣暢淋漓，人生幾大樂事應該有一條「久憋逢厠所」。\n到家後，和岳父收割了幾天玉米，做了些家務，帶了幾天孩子，看望了太太的親戚，還趕了農村的大集，吃了很甜的秋桃。\n\n\n\n假期快結束時，還去參加了朋友的婚禮，和太太跑前跑後幫了一些忙，匆忙又有意義。\n\n\n\n此外，還寫了兩篇技術文章。總的來説，這個假期過得還不錯，沒有虛度，也沒有花銷太多，簡簡單單也挺好。\n","permalink":"https://olzhy.github.io/posts/the-national-day-holiday-2023.html","tags":["随笔"],"title":"2023 國慶假期回顧"},{"categories":["计算机"],"contents":"VS Code 中有一个非常易用的、用于 API 测试的扩展，名为 REST Client。可以在 VS Code 中使用该扩展来发送 HTTP 请求及接收响应，其语法比 cURL 命令更简单，是我们开发人员在测试 API 时的一个不错的选择。\n本文将结合 GitHub REST API 来演示该扩展的使用，全文共有五个部分：基础使用、将文件内容载入为请求体、一个文件内编写多个请求、系统变量与环境变量的使用，以及多环境配置与选择环境执行。\n开始前，请确保已在 VS Code 中安装了 REST Client 扩展（安装非常简单，在 VS Code 的 Extensions 中搜索「REST Client」进行安装即可）。\n1 基础使用 下面以调用 GitHub REST API 查询一个仓库（本文使用本人的一个公开仓库 olzhy.github.io）的 Issues 为例，来演示 REST Client 的基础使用。\n欲在 VS Code 中使用 REST Client，只需新建一个文件，并将其以.http（或.rest）为扩展名即可。\n如下即为使用 REST Client 获取olzhy.github.io仓库前 10 条 Issues 的写法：\nGET https://api.github.com/repos/olzhy/olzhy.github.io/issues ?page=1 \u0026amp;per_page=10 Accept: application/vnd.github+json 将如上内容保存为一个.http文件后，REST Client 扩展会自动检测到如上内容，并在 GET 上面显示「Send Request」按钮；点击该按钮即可发送请求，稍后会看到右侧弹出一个窗口，显示返回的状态码、 Header，以及 Body 的完整内容。\n效果如下：\n2 将文件内容载入为请求体 上面查询一个 GitHub 仓库的 Issues API 是一个 GET 请求，而对于诸如 POST 等需要 Body 的请求，可以采用如下写法：\nPOST https://api.github.com/repos/olzhy/olzhy.github.io/issues Authorization: Bearer ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx { \u0026#34;title\u0026#34;: \u0026#34;发现一个 Bug\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;请尽快解决！\u0026#34; } 可以看到，上面演示的是一个针对仓库olzhy.github.io新建 Issue 的样例。相较于前面的 GET 请求，只需在 URL 和请求头下空出一行，填入请求体即可。\n请求体太长的话，也可以将其抽取到一个文件中，然后使用如下写法将文件内容载入为请求体即可：\nPOST https://api.github.com/repos/olzhy/olzhy.github.io/issues Authorization: Bearer ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u0026lt;@ ./body.json 3 一个文件内编写多个请求 上面的两个场景均非常简单，而实际使用中，我们常会想在一个.http文件中编写多个请求，且可能会存在后一个请求依赖前一个请求的情况。对于这种情况，REST Client 也是支持的。\n下面的示例即在一个文件中编写了三个请求（分别为：新建 Issue、获取刚刚新建的 Issue 和更新刚刚新建的 Issue），且后面的请求依赖前面的返回结果。\n@baseUrl = https://api.github.com/repos/olzhy/olzhy.github.io @accessToken = ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx # @name createIssue POST {{baseUrl}}/issues Authorization: Bearer {{accessToken}} { \u0026#34;title\u0026#34;: \u0026#34;发现一个 Bug\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;请尽快解决！\u0026#34; } ### @newCreatedIssueNumber = {{createIssue.response.body.$.number}} # @name getIssueByNumber GET {{baseUrl}}/issues/{{newCreatedIssueNumber}} ### # @name updateIssueByNumber PATCH {{baseUrl}}/issues/{{newCreatedIssueNumber}} Authorization: Bearer {{accessToken}} { \u0026#34;title\u0026#34;: \u0026#34;紧急，发现一个 Bug\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;请尽快解决！！\u0026#34; } 可以看到，三个请求以内容为###的行进行分割，每个请求都起了一个名字（如：# @name createIssue），后面的请求可以根据前面请求的名字来获取其返回的内容（如：@newCreatedIssueNumber = {{createIssue.response.body.$.number}}），此外我们还看到文件头部使用@key = value方式声明了一些公用变量。\n4 系统变量与环境变量的使用 REST Client 支持读取系统环境变量以及从.env文件读取环境变量，此外还提供日期与 UUID 等实用变量，以支持我们在组织请求头与请求体时使用。\n请看下面的一个样例：\nPOST https://api.example.com/v2/comments HTTP/1.1 Content-Type: application/json { \u0026#34;user_name\u0026#34;: \u0026#34;{{$dotenv USERNAME}}\u0026#34;, // 读取与 .http 文件同一目录下的 .env 文件中的环境变量 \u0026#34;request_id\u0026#34;: \u0026#34;{{$guid}}\u0026#34;, // 生成一个 36 位的 UUID，如：f63ebc18-216b-4c8e-8d51-9ab66bbe39fc \u0026#34;updated_at\u0026#34;: \u0026#34;{{$timestamp}}\u0026#34;, // 生成一个时间戳，表示从 1970-01-01 至今的秒数，如：1696475647 \u0026#34;created_at\u0026#34;: \u0026#34;{{$timestamp -1 d}}\u0026#34;, // 指定偏移量生成一个时间戳，如：1696389771 \u0026#34;custom_date\u0026#34;: \u0026#34;{{$datetime \u0026#39;YYYY-MM-DD\u0026#39;}}\u0026#34;, // 格式化日期，如：2023-10-05 \u0026#34;secret\u0026#34;: \u0026#34;{{$processEnv SECRET}}\u0026#34; // 读取系统环境变量 } 可以看到，如上请求体中的字段有的是从系统环境变量读取的，有的是从.env文件中读取的，还有的是使用实用变量（时间戳与 UUID）来生成的。\n5 多环境配置与选择环境执行 REST Client 还支持定义多个环境，然后执行.http中的请求时，可以选择环境来分别执行。\n下面即是一个在 VS Code 的配置文件setting.json中添加 REST Client 环境信息的样例：\n// settings.json \u0026#34;rest-client.environmentVariables\u0026#34;: { \u0026#34;$shared\u0026#34;: { \u0026#34;strict\u0026#34;: true }, \u0026#34;dev\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;https://dev-api.example.com/v2\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;xxxxxx\u0026#34; }, \u0026#34;qa\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;https://qa-api.example.com/v2\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;xxxxxx\u0026#34; }, \u0026#34;production\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;https://api.example.com/v2\u0026#34;, \u0026#34;token\u0026#34;: \u0026#34;xxxxxx\u0026#34; } } 下面是.http文件的内容：\nGET {{address}}/comments/1 HTTP/1.1 Authorization: Bearer {{token}} 执行请求时，在 VS Code 的右下角会有一个选择环境的 Button，点击后，选择不同环境进行执行即可。\n效果如下：\n综上，本文对 VS Code 扩展 REST Client 进行了探索，发现日常的一些简单的 API 测试场景使用它来做还是很适合的。\n 参考资料\n[1] REST Client | Visual Studio Marketplace - marketplace.visualstudio.com\n[2] REST API to view and manage issues | GitHub Docs - docs.github.com\n ","permalink":"https://olzhy.github.io/posts/getting-started-with-vscode-extension-rest-client.html","tags":["工具使用","自动化测试"],"title":"在 VS Code 中使用 REST Client 扩展做 API 测试"},{"categories":["计算机"],"contents":"前面两篇文章「如何使用 Spring Boot 和 Kotlin 构建 RESTful API 服务？」、「如何使用 Kotlin HTTP 工具包 http4k 构建 RESTful API 服务？」分别介绍了 Kotlin 使用 Spring Boot，以及 Kotlin 使用 http4k 开发 RESTful API 的方法。本文则关注如何使用 Kotlin 官方主推的 Web 框架 Ktor 来开发 RESTful API？\n本文将以开发 User 的增、删、改、查 API 为例，来学习 Ktor 的使用。示例项目使用 Gradle 管理，项目结构依然采用业界最通用 MVC 三层架构；为了突出重点，本文不涉及数据库和 DAO 层，而在 Service 层使用一个 List 作数据存储；为了接近真实项目的情景，该示例项目的依赖注入使用 Kodein 来实现。\n全文共有三个部分：项目结构介绍、项目代码浅析，以及 API 测试与验证。以期阅读本文后，我们对如何使用 Ktor 开发 API 会有一个基本的了解。\n开始前，列出本文用到的依赖软件或框架的版本：\nGradle：8.3 Kotlin：1.9.10 JDK：Amazon Corretto 17.0.8 Ktor：2.3.4 1 项目结构介绍 该项目使用 Gradle 管理，项目结构如下：\nktor-restful-service-demo |--- src/main/ | |--- resources/ | | |--- application.conf | | \\--- logback.xml | \\--- kotlin/ | \\--- com.example.demo/ | |--- route/ | | \\--- UserRoute.kt | |--- service/ | | |--- UserService.kt | |--- code/ | | \\--- ErrorCodes.kt | |--- model/ | | |--- ErrorResponse.kt | | \\--- User.kt | |--- plugin/ | | |--- Routing.kt | | \\--- Serialization.kt | |--- conf/ | | \\--- KodeinConf.kt | \\--- DemoApplication.kt ... |--- gradle/ |--- gradlew \\--- build.gradle.kts 可以看到，项目根目录下是 Gradle 配置文件build.gradle.kts、Gradle 命令gradlew和 Gradle Wrapper 文件夹gradle；然后是配置文件目录src/main/resources和源码目录src/main/kotlin。\nsrc/main/resources下有两个文件：application.conf和logback.xml，分别为 Ktor Server 配置文件和 Logback 日志配置文件。\n下面看一下src/main/kotlin包下的几个目录：\n  route\n类似于其它框架的 Controller 层，用于 Ktor 路由配置。\n  service\nService 层，主要业务逻辑都在这里编写。\n  code\nErrorCodes.kt枚举类所在目录，本示例项目使用该枚举类存放所有错误响应信息。\n  model\n数据模型类所在目录。\n  plugin\nKtor 插件所在目录，用于配置根路由和序列化方式等。\n  conf\n配置类所在目录，本项目的用于依赖注入的框架 Kodein 的配置类KodeinConf.kt即位于此。\n  除了这些包，src/main/kotlin下还有一个文件DemoApplication.kt，为程序的总入口。\n2 项目代码浅析 前面介绍了示例项目的目录结构与包的含义，接下来浅析一下 Gradle 配置文件和各个包下的代码。\n2.1 Gradle 配置文件 该示例项目使用 Gradle 管理，配置文件build.gradle.kts内容如下：\n// build.gradle.kts plugins { kotlin(\u0026#34;jvm\u0026#34;) version \u0026#34;1.9.10\u0026#34; id(\u0026#34;io.ktor.plugin\u0026#34;) version \u0026#34;2.3.4\u0026#34; } application { mainClass.set(\u0026#34;com.example.demo.DemoApplicationKt\u0026#34;) } repositories { mavenCentral() } dependencies { implementation(\u0026#34;io.ktor:ktor-server-core\u0026#34;) implementation(\u0026#34;io.ktor:ktor-server-netty\u0026#34;) implementation(\u0026#34;io.ktor:ktor-server-content-negotiation\u0026#34;) implementation(\u0026#34;io.ktor:ktor-serialization-jackson\u0026#34;) implementation(\u0026#34;org.kodein.di:kodein-di:7.20.2\u0026#34;) implementation(\u0026#34;ch.qos.logback:logback-classic:1.4.11\u0026#34;) } 可以看到，该文件指定了 Kotlin 的版本为1.9.10，Ktor 的版本为2.3.4；程序入口为DemoApplication.kt；仓库为 Maven Repository，依赖有io.ktor:ktor-server-core（Ktor 核心组件）、io.ktor:ktor-server-netty（所使用的 Netty Server 引擎）、io.ktor:ktor-server-content-negotiation（用于 Kotlin 对象与 JSON 等格式的序列化与反序列化转换）、io.ktor:ktor-serialization-jackson（本项目所使用的 JSON 序列化实现 Jackson）、org.kodein.di:kodein-di:7.20.2（Kodein 依赖注入包），以及ch.qos.logback:logback-classic:1.4.11（Logback 日志打印包）。\n2.2 plugin 包下的代码 plugin 包下有两个文件：Routing.kt和Serialization.kt，分别用于根路由配置和序列化方式配置。\nRouting.kt的代码如下：\n// src/main/kotlin/com/example/demo/plugin/Routing.kt package com.example.demo.plugin import com.example.demo.route.userRouting import io.ktor.server.application.* import io.ktor.server.routing.* fun Application.configureRouting() { routing { userRouting() } } 可以看到，如上代码负责配置项目的根路由，本项目配置的路由只有一个：userRouting()，位于route包下，为 User 的路由规则，稍后会看一下具体的代码。\nSerialization.kt的代码如下：\n// src/main/kotlin/com/example/demo/plugin/Serialization.kt package com.example.demo.plugin import io.ktor.serialization.jackson.* import io.ktor.server.application.* import io.ktor.server.plugins.contentnegotiation.* fun Application.configureSerialization() { install(ContentNegotiation) { jackson() } } 可以看到，如上代码将 Jackson 配置为内容的序列化与反序列化实现。\n2.3 route 包下的代码 Route 相当于 Controller，负责接收请求，调用 Service 进行处理，最后返回响应。\n本示例项目的 route 包下只有一个文件UserRoute.kt，其源码如下：\n// src/main/kotlin/com/example/demo/route/UserRoute.kt package com.example.demo.route import com.example.demo.code.ErrorCodes import com.example.demo.conf.kodein import com.example.demo.model.User import com.example.demo.service.UserService import io.ktor.http.* import io.ktor.server.application.* import io.ktor.server.request.* import io.ktor.server.response.* import io.ktor.server.routing.* import org.kodein.di.instance fun Route.userRouting() { val userService: UserService by kodein.instance() route(\u0026#34;/users\u0026#34;) { // list all  get { val users = userService.listAll() call.respond(users) } // get user by id  get(Regex(\u0026#34;/(?\u0026lt;id\u0026gt;\\\\d+)\u0026#34;)) { val id = call.parameters[\u0026#34;id\u0026#34;]!!.toLong() val user = userService.getById(id) ?: return@get call.respond( ErrorCodes.USER_NOT_FOUND.status, ErrorCodes.USER_NOT_FOUND.toErrorResponse() ) call.respond(user) } // update  patch { val user = call.receive\u0026lt;User\u0026gt;() userService.getById(user.id) ?: return@patch call.respond( ErrorCodes.USER_NOT_FOUND.status, ErrorCodes.USER_NOT_FOUND.toErrorResponse() ) userService.update(user) call.respond(HttpStatusCode.NoContent) } // save  post { val user = call.receive\u0026lt;User\u0026gt;() userService.getById(user.id)?.let { return@post call.respond( ErrorCodes.USER_ALREADY_EXISTS.status, ErrorCodes.USER_ALREADY_EXISTS.toErrorResponse() ) } userService.save(user) call.respond(HttpStatusCode.Created) } // delete by id  delete(Regex(\u0026#34;/(?\u0026lt;id\u0026gt;\\\\d+)\u0026#34;)) { val id = call.parameters[\u0026#34;id\u0026#34;]!!.toLong() userService.getById(id) ?: return@delete call.respond( ErrorCodes.USER_NOT_FOUND.status, ErrorCodes.USER_NOT_FOUND.toErrorResponse() ) userService.deleteById(id) call.respond(HttpStatusCode.NoContent) } } } 可以看到，如上代码中userRouting为Route的扩展函数，使用 Kodein 注入方式拿到了UserService的实例；其中有五个 API，分别为：获取全部 User、获取单个 User、更新 User、新建 User，以及删除 User；内部均调用了UserService来进行实现，对于错误信息的响应，均使用了统一的枚举类ErrorCodes.kt。\n2.4 code 包下的代码 code 包下只有一个文件ErrorCodes.kt，为全局统一的错误信息枚举类，其源码如下：\n// src/main/kotlin/com/example/demo/code/ErrorCodes.kt package com.example.demo.code import com.example.demo.model.ErrorResponse import io.ktor.http.* enum class ErrorCodes(val status: HttpStatusCode, private val code: String, private val description: String) { USER_NOT_FOUND(HttpStatusCode.NotFound, \u0026#34;user_not_found\u0026#34;, \u0026#34;user not found\u0026#34;), USER_ALREADY_EXISTS(HttpStatusCode.BadRequest, \u0026#34;user_already_exists\u0026#34;, \u0026#34;user already exists\u0026#34;); fun toErrorResponse(): ErrorResponse = ErrorResponse(code, description) } 可以看到，如上代码定义了两个错误信息：用户不存在与用于已存在。刚刚在UserRoute.kt代码中，已看到了这些错误信息的使用。\n2.5 service 包下的代码 Service 承载具体的业务逻辑实现，该示例项目只有一个 Service：UserService.kt，其负责具体的 User 增、删、改、查逻辑处理。\n其代码如下：\n// src/main/kotlin/com/example/demo/service/UserService.kt package com.example.demo.service import com.example.demo.model.User interface UserService { fun listAll(): List\u0026lt;User\u0026gt; fun getById(id: Long): User? fun update(user: User) fun save(user: User) fun deleteById(id: Long) } class DefaultUserServiceImpl : UserService { private val fakeUsers = mutableListOf( User(id = 1L, name = \u0026#34;Larry\u0026#34;, age = 28), User(id = 2L, name = \u0026#34;Stephen\u0026#34;, age = 19), User(id = 3L, name = \u0026#34;Jacky\u0026#34;, age = 24) ) override fun listAll(): List\u0026lt;User\u0026gt; { return fakeUsers } override fun getById(id: Long): User? { return fakeUsers.find { it.id == id } } override fun update(user: User) { fakeUsers.filter { it.id == user.id }.forEach { it.name = user.name it.age = user.age } } override fun save(user: User) { getById(user.id) ?: fakeUsers.add(user) } override fun deleteById(id: Long) { fakeUsers.removeIf { it.id == id } } } 可以看到，如上代码中包含一个接口和一个实现类，负责具体的 User 增、删、改、查实现，其使用一个mutableList来充当存储功能，初始化时预置了三条数据。\n2.6 model 包下的代码 Model 用于承载与传递数据，该示例项目的 model 包下有两个文件：User.kt与ErrorResponse.kt，分别为 User 数据类与错误信息数据类。\nUser.kt的代码如下：\n// src/main/kotlin/com/example/demo/model/User.kt package com.example.demo.model data class User(val id: Long, var name: String, var age: Int) ErrorResponse.kt的代码如下：\n// src/main/kotlin/com/example/demo/model/ErrorResponse.kt package com.example.demo.model data class ErrorResponse(val code: String, val description: String) 可以看到，User 有三个字段：id、name 和 age；ErrorResponse 有两个字段：code 和 description。\n2.7 conf 包下的代码 该示例项目的 conf 包主要用于存放除了 Ktor 配置之外的其它配置信息，其下只有一个文件：KodeinConf.kt，为 Kodein 依赖注入相关的配置。\n其源码如下：\n// src/main/kotlin/com/example/demo/conf/KodeinConf.kt package com.example.demo.conf import com.example.demo.service.DefaultUserServiceImpl import com.example.demo.service.UserService import org.kodein.di.DI import org.kodein.di.bind import org.kodein.di.singleton val kodein = DI { bind\u0026lt;UserService\u0026gt;() with singleton { DefaultUserServiceImpl() } } 可以看到，如上代码声明了kodein变量，并指定了 Service 的接口与实现。\n2.8 程序入口代码 DemoApplication.kt为程序的入口，其代码如下：\n// src/main/kotlin/com/example/demo/DemoApplication.kt package com.example.demo import com.example.demo.plugin.configureRouting import com.example.demo.plugin.configureSerialization import io.ktor.server.application.* fun main(args: Array\u0026lt;String\u0026gt;): Unit = io.ktor.server.netty.EngineMain.main(args) fun Application.module() { configureRouting() configureSerialization() } 可以看到，该代码指定了 Server 引擎为 Netty；还为 KtorApplication编写了一个扩展函数module，该module调用了plugin下分别用于配置根路由与序列化的两个扩展函数configureRouting与configureSerialization；module的调用则在 Ktor 配置文件application.conf中作了指定。\n2.9 配置文件内容 该示例项目src/main/resources目录下有两个配置文件：application.conf和logback.xml，分别用于 Ktor Server 的配置与 Logback 日志输出的配置。\napplication.conf内容如下：\n# src/main/resources/application.conf ktor { deployment { port = 8080 port = ${?PORT} } application { modules = [ com.example.demo.DemoApplicationKt.module ] } } 可以看到，该文件采用 HOCON 格式，指定了服务的端口以及 module 的位置。\nlogback.xml内容如下：\n\u0026lt;!-- src/main/resources/logback.xml --\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;appender name=\u0026#34;STDOUT\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;pattern\u0026gt;%d{YYYY-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;STDOUT\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;logger name=\u0026#34;io.netty\u0026#34; level=\u0026#34;INFO\u0026#34;/\u0026gt; \u0026lt;/configuration\u0026gt; 可以看到该文件指定了日志的级别与输出格式。\n3 API 测试与验证 概览了项目的整体结构和各个包下的源码，下面就将其启动，并使用 CURL 命名对各个 API 进行测试。\n项目启动命令如下：\n./gradlew run 启动完成后，即可以开始 API 验证了。\n3.1 查询所有 User 首先查询一下所有 User，CURL 命令如下：\ncurl -X GET http://localhost:8080/users [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28},{\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Stephen\u0026#34;,\u0026#34;age\u0026#34;:19},{\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Jacky\u0026#34;,\u0026#34;age\u0026#34;:24}] 可以看到三条预置数据正确返回。\n3.2 查询单个 User 下面查询一下 ID 为 1 的 User，CURL 命名如下：\ncurl -X GET http://localhost:8080/users/1 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28} 可以看到返回正确。\n再尝试查询一个不存在的 User：\ncurl -X GET http://localhost:8080/users/100 {\u0026#34;code\u0026#34;:\u0026#34;user_not_found\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user not found\u0026#34;} 可以看到，返回了我们在ErrorCodes.kt枚举类中定义的错误信息。\n3.3 更新 User 再尝试更新一下 ID 为 1 的 User，CURL 命令如下：\ncurl -X PATCH -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Larry2\u0026#34;, \u0026#34;age\u0026#34;: 19}\u0026#39; http://localhost:8080/users 更新完成后，再次查询，发现更新成功：\ncurl -X GET http://localhost:8080/users/1 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry2\u0026#34;,\u0026#34;age\u0026#34;:19} 再尝试对一个不存在的 User 进行更新，CURL 命令如下：\ncurl -X PATCH -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 100, \u0026#34;name\u0026#34;: \u0026#34;Larry2\u0026#34;, \u0026#34;age\u0026#34;: 19}\u0026#39; http://localhost:8080/users {\u0026#34;code\u0026#34;:\u0026#34;user_not_found\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user not found\u0026#34;} 发现返回了我们设定的错误信息。\n3.4 新建 User 下面，尝试一下新建 User，CURL 命令如下：\ncurl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 4, \u0026#34;name\u0026#34;: \u0026#34;Lucy\u0026#34;, \u0026#34;age\u0026#34;: 16}\u0026#39; http://localhost:8080/users 然后，再次查询一下所有 User：\ncurl -X GET http://localhost:8080/users [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry2\u0026#34;,\u0026#34;age\u0026#34;:19},{\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Stephen\u0026#34;,\u0026#34;age\u0026#34;:19},{\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Jacky\u0026#34;,\u0026#34;age\u0026#34;:24},{\u0026#34;id\u0026#34;:4,\u0026#34;name\u0026#34;:\u0026#34;Lucy\u0026#34;,\u0026#34;age\u0026#34;:16}] 发现返回结果已包含刚刚新建的 User。\n再尝试新建一个 ID 已存在的 User：\ncurl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Lucy\u0026#34;, \u0026#34;age\u0026#34;: 16}\u0026#39; http://localhost:8080/users {\u0026#34;code\u0026#34;:\u0026#34;user_already_exists\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user already exists\u0026#34;} 发现返回了设定的错误信息。\n3.5 删除单个 User 最后试一下删除 User，CURL 命令如下：\n# 删除已有 User curl -X DELETE http://localhost:8080/users/1 # 删除不存在的 User curl -X DELETE http://localhost:8080/users/100 {\u0026#34;code\u0026#34;:\u0026#34;user_not_found\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user not found\u0026#34;} 返回也是正确的。\n综上，本文使用 Ktor 开发了一个针对 User 增、删、改、查的示例项目，并对项目结构和源码进行了分析，最后进行了 API 测试与验证，发现功能均是正常的。最后的结论是，使用 Ktor 开发 API 还是比较顺滑的。\n本文整个示例项目的代码已托管至本人 GitHub，欢迎关注或 Fork。\n 参考资料\n[1] Creating HTTP APIs | Ktor Documentation - ktor.io\n[2] A sample Ktor project showing how to create HTTP APIs using Ktor | GitHub - github.com\n[3] Building a REST API with Ktor | Medium - medium.com\n[4] Using Kodein Dependency Injection framework with Ktor | GitHub - github.com\n[5] Kotlin Dependency Injection with Kodein | Techkluster - techkluster.com\n[6] Generate Ktor Project | Ktor Project Generator - start.ktor.io\n ","permalink":"https://olzhy.github.io/posts/building-restful-api-with-ktor.html","tags":["Kotlin","Gradle"],"title":"如何使用 Kotlin Web 框架 Ktor 构建 RESTful API 服务？"},{"categories":["计算机"],"contents":"上文「如何使用 Spring Boot 和 Kotlin 构建 RESTful API 服务？」介绍了 Kotlin 可以无缝借用现有 Java Web 框架来开发 API 服务。除此之外，还有一些 Web 工具包是直接使用 Kotlin 开发的，如 Ktor、http4k 等，用这些原生 Kotlin 工具包开发 API 服务则可以充分使用 Kotlin 的语法和函数式编程的特性。本文即专门探索一下如何使用 http4k 来开发 RESTful API 服务。\n先看一下 http4k 是什么？\nhttp4k 是一个使用纯 Kotlin 编写的非常轻巧但功能齐全的函数式 HTTP 工具包，支持使用统一的方式来编写 HTTP 服务端、客户端，以及测试代码。\n本文会以开发一个真实的 API 服务（User 的增、删、改、查）为例，演示如何使用 http4k 开发 RESTful API。该项目使用 Gradle 作依赖管理，采用传统的 MVC 三层架构，使用 http4k 作 Controller 层的逻辑处理，无 DAO 层，无数据库操作，Service 层使用一个 List 来模拟数据的存储，支持 Swagger UI 的自动生成。全文主要有三个部分：模板项目搭建、业务代码编写，以及 API 测试与验证。\n下面列出写作本文时用到的依赖项及其版本：\nGradle：8.3 Kotlin：1.9.10 JDK：Amazon Corretto 17.0.8 http4k：5.8.1.0 1 模板项目搭建 本文使用 Gradle 作为项目的构建与依赖管理工具，由其搭建的空项目的整体目录结构如下：\nhttp4k-restful-service-demo |--- gradle/ |--- src/main/ | |--- resources/ | \\--- kotlin/ | \\--- com.example.demo.DemoApplication.kt |--- src/test/kotlin/ | \\--- com.example.demo.DemoApplicationTest.kt |--- gradlew |--- settings.gradle.kts \\--- build.gradle.kts 可以看到这是一个标准的 Gradle 模板工程。\n下面主要看一下 Gralde 描述文件的内容：\nimport org.jetbrains.kotlin.gradle.tasks.KotlinCompile plugins { kotlin(\u0026#34;jvm\u0026#34;) version \u0026#34;1.9.10\u0026#34; application } java { sourceCompatibility = JavaVersion.VERSION_17 } repositories { mavenCentral() } dependencies { implementation(platform(\u0026#34;org.http4k:http4k-bom:5.8.1.0\u0026#34;)) implementation(\u0026#34;org.http4k:http4k-core\u0026#34;) implementation(\u0026#34;org.http4k:http4k-contract\u0026#34;) implementation(\u0026#34;org.http4k:http4k-format-jackson\u0026#34;) implementation(\u0026#34;org.http4k:http4k-contract-ui-swagger\u0026#34;) implementation(\u0026#34;com.google.inject:guice:7.0.0\u0026#34;) } application { mainClass.set(\u0026#34;com.example.demo.DemoApplicationKt\u0026#34;) } tasks.withType\u0026lt;KotlinCompile\u0026gt; { kotlinOptions { freeCompilerArgs += \u0026#34;-Xjvm-default=all\u0026#34; jvmTarget = \u0026#34;17\u0026#34; } } tasks.withType\u0026lt;Test\u0026gt; { useJUnitPlatform() } 可以看到，我们使用的 Kotlin 版本为 1.9.10，指定程序启动类为com.example.demo.DemoApplication.kt。\n用到的 http4k 模块有：\n  http4k-core\nhttp4k 核心模块，诸如 HttpHandler、Filter 等基础功能都在里头了。\n  http4k-contract\n支持更完善的参数配置，支持 OpenAPI 元信息描述、Swagger 配置等特性。\n  http4k-format-jackson\n支持 JSON 和数据类的相互转换。\n  http4k-contract-ui-swagger\n支持 Swagger UI 的生成以及 Swagger 静态资源的本地化。\n  此外，我们还使用了 Guice 来做依赖注入（因其比较轻量，适合示例工程使用）。\n2 业务代码编写 该部分主要编写针对 User 增、删、改、查的业务代码。整个项目主要由 Controller 层、Service 层、Model 类、Error Codes 枚举类、程序入口类几个部分组成。\n看一下开发完成后的项目结构：\nhttp4k-restful-service-demo |--- src/main/ | |--- resources/ | \\--- kotlin/ | \\--- com.example.demo/ | |--- controller/ | | \\--- UserController.kt | |--- service/ | | |--- UserService.kt | |--- code/ | | \\--- ErrorCodes.kt | |--- model/ | | |--- ErrorResponse.kt | | \\--- User.kt | \\--- DemoApplication.kt ... |--- gradlew \\--- build.gradle.kts 下面逐一看一下各层的代码。\n2.1 Controller 层代码 Controller 层负责请求接收与响应返回，而具体的处理逻辑则在 Service 层内。\n该项目的 Controller 层只有一个类UserController.kt，用于定义路由以及具体处理请求的Handler函数，Handler函数内部则调用UserService来处理业务。\n// src/main/kotlin/com/example/demo/controller/UserController.kt package com.example.demo.controller import com.example.demo.code.ErrorCodes import com.example.demo.model.User import com.example.demo.service.UserService import jakarta.inject.Inject import org.http4k.contract.ContractRoute import org.http4k.contract.div import org.http4k.contract.meta import org.http4k.core.Body import org.http4k.core.Method.* import org.http4k.core.Request import org.http4k.core.Response import org.http4k.core.Status.Companion.CREATED import org.http4k.core.Status.Companion.NO_CONTENT import org.http4k.core.Status.Companion.OK import org.http4k.core.with import org.http4k.format.Jackson.auto import org.http4k.lens.Path import org.http4k.lens.long class UserController @Inject constructor( private val userService: UserService ) { companion object { private val usersLens = Body.auto\u0026lt;List\u0026lt;User\u0026gt;\u0026gt;().toLens() private val userLens = Body.auto\u0026lt;User\u0026gt;().toLens() } val routes: List\u0026lt;ContractRoute\u0026gt; = listOf( // listAll  \u0026#34;/users\u0026#34; meta { summary = \u0026#34;list all users\u0026#34; returning(OK, usersLens to listOf(User(1, \u0026#34;Larry\u0026#34;, 28))) } bindContract GET to ::listAll, // getById  \u0026#34;/users\u0026#34; / Path.long().of(\u0026#34;id\u0026#34;) meta { summary = \u0026#34;get user by id\u0026#34; returning(OK, userLens to User(1, \u0026#34;Larry\u0026#34;, 28)) returning(ErrorCodes.USER_NOT_FOUND.status, ErrorCodes.USER_NOT_FOUND.toSampleResponse()) } bindContract GET to { id -\u0026gt; { req -\u0026gt; getById(req, id) } }, // update  \u0026#34;/users\u0026#34; meta { summary = \u0026#34;update user\u0026#34; receiving(userLens to User(1, \u0026#34;Larry\u0026#34;, 28)) returning(NO_CONTENT) returning(ErrorCodes.USER_NOT_FOUND.status, ErrorCodes.USER_NOT_FOUND.toSampleResponse()) } bindContract PATCH to { req -\u0026gt; update(req, userLens(req)) }, // save  \u0026#34;/users\u0026#34; meta { summary = \u0026#34;save user\u0026#34; receiving(userLens to User(1, \u0026#34;Larry\u0026#34;, 28)) returning(CREATED) returning(ErrorCodes.USER_ALREADY_EXISTS.status, ErrorCodes.USER_ALREADY_EXISTS.toSampleResponse()) } bindContract POST to { req -\u0026gt; save(req, userLens(req)) }, // deleteById  \u0026#34;/users\u0026#34; / Path.long().of(\u0026#34;id\u0026#34;) meta { summary = \u0026#34;delete user by id\u0026#34; returning(NO_CONTENT) returning(ErrorCodes.USER_NOT_FOUND.status, ErrorCodes.USER_NOT_FOUND.toSampleResponse()) } bindContract DELETE to { id -\u0026gt; { req -\u0026gt; deleteById(req, id) } } ) private fun listAll(req: Request): Response { val users = userService.listAll() return Response(OK).with(usersLens of users) } private fun getById(req: Request, id: Long): Response { val user = userService.getById(id) return user?.let { Response(OK).with(userLens of it) } ?: ErrorCodes.USER_NOT_FOUND.toResponse() } private fun update(req: Request, user: User): Response { // exists?  userService.getById(user.id) ?: return ErrorCodes.USER_NOT_FOUND.toResponse() // update  userService.update(user) return Response(NO_CONTENT) } private fun save(req: Request, user: User): Response { // exists?  val userStored = userService.getById(user.id) if (null != userStored) { return ErrorCodes.USER_ALREADY_EXISTS.toResponse() } // save  userService.save(user) return Response(CREATED) } private fun deleteById(req: Request, id: Long): Response { // exists?  userService.getById(id) ?: return ErrorCodes.USER_NOT_FOUND.toResponse() // delete  userService.deleteById(id) return Response(NO_CONTENT) } } 下面浅析一下这段代码：\n  UserController依赖UserService，使用 Guice 来自动注入依赖；\n  http4k 使用透镜（Lens，如代码中的Body.auto\u0026lt;User\u0026gt;().toLens()）来做 JSON 和 Model 的相互映射和转换；\n  http4k 可以定义一组路由（ContractRoute，如代码中的\u0026quot;/users\u0026quot; meta {} bindContract GET to ::xxxHandler）来指定请求路径、OpenAPI 元数据（用于生成 Swagger 文档）、HTTP 方法，以及处理请求的 Handler 函数；\n  http4k 中的 Handler 函数就是一个输入为Request，输出为Response的普通函数，业务逻辑都可以在这里边完成（本文的 Handler 函数做了自定义设计，将业务上用到的参数也放到了参数列表里，如：fun getById(req: Request, id: Long): Response）。\n  2.2 Service 层代码 Service 层包含业务处理的主要部分，该项目 Service 层仅有一个类UserService.kt。\n本文为了方便且专注于 http4k 的使用，没有引入 DAO 层和数据库，仅使用一个MutableList（val fakeUsers = mutableListOf(...)）来存储数据。\n// src/main/kotlin/com/example/demo/service/UserService.kt package com.example.demo.service import com.example.demo.model.User interface UserService { fun listAll(): List\u0026lt;User\u0026gt; fun getById(id: Long): User? fun update(user: User) fun save(user: User) fun deleteById(id: Long) } class DefaultUserServiceImpl : UserService { private val fakeUsers = mutableListOf( User(id = 1L, name = \u0026#34;Larry\u0026#34;, age = 28), User(id = 2L, name = \u0026#34;Stephen\u0026#34;, age = 19), User(id = 3L, name = \u0026#34;Jacky\u0026#34;, age = 24) ) override fun listAll(): List\u0026lt;User\u0026gt; { return fakeUsers } override fun getById(id: Long): User? { return fakeUsers.find { it.id == id } } override fun update(user: User) { fakeUsers.filter { it.id == user.id }.forEach { it.name = user.name it.age = user.age } } override fun save(user: User) { getById(user.id) ?: fakeUsers.add(user) } override fun deleteById(id: Long) { fakeUsers.removeIf { it.id == id } } } 如上代码中包含一个接口类和一个实现类，负责 User 的增、删、改、查。\n2.3 Model 类代码 该项目包含两个 Model 类：User.kt和ErrorResponse.kt，前一个是 User 的模型类，后一个是标准的错误响应模型类。\n// src/main/kotlin/com/example/demo/model/User.kt package com.example.demo.model data class User(val id: Long, var name: String, var age: Int) // src/main/kotlin/com/example/demo/model/ErrorResponse.kt package com.example.demo.model data class ErrorResponse(val code: String, val description: String) 2.4 Error Codes 枚举类代码 该项目特别设计了一个枚举类（ErrorCodes.kt）来存放所有的错误响应信息。\n// src/main/kotlin/com/example/demo/code/ErrorCodes.kt package com.example.demo.code import com.example.demo.model.ErrorResponse import org.http4k.core.Body import org.http4k.core.Response import org.http4k.core.Status import org.http4k.core.with import org.http4k.format.Jackson.auto import org.http4k.lens.BiDiBodyLens enum class ErrorCodes(val status: Status, private val code: String, private val description: String) { USER_NOT_FOUND(Status.NOT_FOUND, \u0026#34;user_not_found\u0026#34;, \u0026#34;user not found\u0026#34;), USER_ALREADY_EXISTS(Status.BAD_REQUEST, \u0026#34;user_already_exists\u0026#34;, \u0026#34;user already exists\u0026#34;); fun toResponse(): Response = Response(status).with(Body.auto\u0026lt;ErrorResponse\u0026gt;().toLens() of ErrorResponse(code, description)) fun toSampleResponse(): Pair\u0026lt;BiDiBodyLens\u0026lt;ErrorResponse\u0026gt;, ErrorResponse\u0026gt; = Body.auto\u0026lt;ErrorResponse\u0026gt;().toLens() to ErrorResponse(code, description) } 2.5 程序入口类代码 下面看一下程序入口类DemoApplication.kt的代码：\n// src/main/kotlin/com/example/demo/DemoApplication.kt package com.example.demo import com.example.demo.controller.UserController import com.example.demo.service.DefaultUserServiceImpl import com.example.demo.service.UserService import com.google.inject.AbstractModule import com.google.inject.Guice import org.http4k.contract.ContractRoute import org.http4k.contract.ContractRoutingHttpHandler import org.http4k.contract.contract import org.http4k.contract.openapi.ApiInfo import org.http4k.contract.openapi.v3.ApiServer import org.http4k.contract.openapi.v3.OpenApi3 import org.http4k.contract.ui.swagger.swaggerUiWebjar import org.http4k.core.* import org.http4k.filter.CachingFilters import org.http4k.format.Jackson import org.http4k.routing.bind import org.http4k.routing.routes import org.http4k.server.SunHttp import org.http4k.server.asServer class MainGuiceModule : AbstractModule() { override fun configure() { bind(UserService::class.java).to(DefaultUserServiceImpl::class.java) } } fun createContractHandler(routes: List\u0026lt;ContractRoute\u0026gt;, descriptionPath: String): ContractRoutingHttpHandler { return contract { this.routes += routes renderer = OpenApi3( ApiInfo(\u0026#34;User API\u0026#34;, \u0026#34;v1.0\u0026#34;), Jackson, servers = listOf(ApiServer(Uri.of(\u0026#34;http://localhost:8080/\u0026#34;), \u0026#34;local server\u0026#34;)) ) this.descriptionPath = descriptionPath } } val timingFilter = Filter { next: HttpHandler -\u0026gt; { req: Request -\u0026gt; val start = System.currentTimeMillis() val resp: Response = next(req) val timeElapsed = System.currentTimeMillis() - start println(\u0026#34;[timing filter] request to ${req.uri}took ${timeElapsed}ms\u0026#34;) resp } } fun main() { // guice  val injector = Guice.createInjector(MainGuiceModule()) val userController = injector.getInstance(UserController::class.java) // app  val app: HttpHandler = routes( createContractHandler(userController.routes, \u0026#34;/openapi.json\u0026#34;), \u0026#34;/swagger\u0026#34; bind swaggerUiWebjar { url = \u0026#34;/openapi.json\u0026#34; } ) // start  val filteredApp: HttpHandler = CachingFilters.Response.NoCache().then(timingFilter).then(app) filteredApp.asServer(SunHttp(8080)).start().block() } 下面浅析一下这段代码：\n MainGuiceModule类用于 Guice 的相关配置，这里指明了接口和实现； createContractHandler函数用于配置总路由和 OpenAPI 信息； timingFilter用于演示 http4k 中自定义Filter的写法，该Filter负责对每个请求打印请求路径和耗时信息； main函数首先配置了一下 Guice；然后配置了一下根 app 路由和 Swagger；最后在 app 上加了自定义 Filter 后将其启动（Server 类型使用了默认的SunHttp，http4k 还支持Jetty、Undertow等其它服务类型）。  4 API 测试与验证 项目开发完成后，即可以启动和验证了。\n启动命令如下：\n./gradlew run 启动后，浏览器打开http://localhost:8080/swagger，即可以看到自动生成的 Swagger UI。\n4.1 查询所有 User 下面验证一下各个 API。\n首先查询一下所有 User，CURL 命令如下：\ncurl -X GET http://localhost:8080/users [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28},{\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Stephen\u0026#34;,\u0026#34;age\u0026#34;:19},{\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Jacky\u0026#34;,\u0026#34;age\u0026#34;:24}] 可以看到三条预置数据正确返回。\n查询服务控制台，可以看到 Filter 打印了该请求的耗时信息：\n[timing filter] GET /users took 6ms 4.2 查询单个 User 下面查询一下 ID 为 1 的 User，CURL 命名如下：\ncurl -X GET http://localhost:8080/users/1 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28} 服务控制台同样打印了该请求的耗时信息：\n[timing filter] GET /users/1 took 4ms 尝试查询一个不存在的 User：\ncurl -X GET http://localhost:8080/users/100 {\u0026#34;code\u0026#34;:\u0026#34;user_not_found\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user not found\u0026#34;} 可以看到，返回了我们在ErrorCodes.kt枚举类中定义的错误信息。\n4.3 更新 User 再尝试更新一下 ID 为 1 的 User，CURL 命令如下：\ncurl -X PATCH -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Larry2\u0026#34;, \u0026#34;age\u0026#34;: 19}\u0026#39; http://localhost:8080/users 更新完成后，再次查询，发现更新成功：\ncurl -X GET http://localhost:8080/users/1 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry2\u0026#34;,\u0026#34;age\u0026#34;:19} 服务端控制台同样打印了该请求的耗时信息：\n[timing filter] PATCH /users took 31ms 再尝试对一个不存在的 User 进行更新，CURL 命令如下：\ncurl -X PATCH -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 100, \u0026#34;name\u0026#34;: \u0026#34;Larry2\u0026#34;, \u0026#34;age\u0026#34;: 19}\u0026#39; http://localhost:8080/users {\u0026#34;code\u0026#34;:\u0026#34;user_not_found\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user not found\u0026#34;} 发现返回了我们设定的错误信息。\n4.4 新建 User 下面，尝试一下新建 User，CURL 命令如下：\ncurl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 4, \u0026#34;name\u0026#34;: \u0026#34;Lucy\u0026#34;, \u0026#34;age\u0026#34;: 16}\u0026#39; http://localhost:8080/users 再尝试新建一个 ID 已存在的 User：\ncurl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Lucy\u0026#34;, \u0026#34;age\u0026#34;: 16}\u0026#39; http://localhost:8080/users {\u0026#34;code\u0026#34;:\u0026#34;user_already_exists\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user already exists\u0026#34;} 返回均是正确的。\n4.5 删除单个 User 最后试一下删除 User，CURL 命令如下：\n# 删除已有 User curl -X DELETE http://localhost:8080/users/1 # 删除不存在的 User curl -X DELETE http://localhost:8080/users/100 {\u0026#34;code\u0026#34;:\u0026#34;user_not_found\u0026#34;,\u0026#34;description\u0026#34;:\u0026#34;user not found\u0026#34;} 返回也是正确的。\n综上，本文尝试使用 http4k 工具包开发了针对 User 增、删、改、查的通用 RESTful API，并进行了简单的测试，总体来看该包还是比较轻量，比较易于使用的。\n本文涉及的整个样例项目代码已托管至本人 GitHub，欢迎关注或 Fork。\n 参考资料\n[1] Introduction | http4k - www.http4k.org\n[2] http4k Examples | GitHub - github.com\n[3] Kotlin Guice Examples | GitHub - GitHub.com\n ","permalink":"https://olzhy.github.io/posts/building-restful-api-with-http4k.html","tags":["Kotlin","Gradle"],"title":"如何使用 Kotlin HTTP 工具包 http4k 构建 RESTful API 服务？"},{"categories":["计算机"],"contents":"本文将探索「如何使用 Spring Boot 和 Kotlin 构建 RESTful API 服务？」。本文将以搭建一个真实项目的方式来演示使用 Kotlin 构建 RESTful API 服务的整个过程，除了整体框架采用 Spring Boot 外，该项目的依赖管理采用的是 Gradle、数据库访问采用的是 MyBatis，数据库使用的是本地搭建的 MySQL。\n本文主要有三个部分，即：模板项目创建、编写业务代码，以及 API 测试与验证。\n下面列出该项目用到的软件或框架版本：\nJDK：Amazon Corretto 17.0.8 Kotlin：1.9.10 Gradle：8.3 Spring Boot：3.1.3 MySQL：8.1.0 1 模板项目创建 首先，使用「Spring Initializr」创建一个空的模板项目。\n选项如下：\nProject：Gradle - Kotlin Language：Kotlin Spring Boot：3.1.3 Packing：Jar Java：17 Dependencies：Spring Web、MyBatis Framework 和 MySQL Driver 然后，点击 GENERATE 会生成一个模板工程并下载到本地，解压后导入 IDE 即可看到这个模板工程的全貌了。\n生成的 Demo 项目目录结构如下：\ndemo |--- gradle/ |--- src/main/ | |--- resources/ | \\--- kotlin/ | \\--- com.example.demo.DemoApplication.kt |--- src/test/kotlin/ | \\--- com.example.demo.DemoApplicationTests.kt |--- gradlew |--- settings.gradle.kts \\--- build.gradle.kts 下面重点看一下该工程的 Gradle 描述文件和程序入口类DemoApplication.kt。\n1.1 Gradle 描述文件 可以看到这是一个标准的 Gradle 工程，我们将 Gradle 描述文件build.gradle.kts里边的 Kotlin 版本改成最新的 1.9.10（kotlin(\u0026quot;jvm\u0026quot;) version \u0026quot;1.9.10\u0026quot;），删去不需要的 Dependency 后，完整文件内容如下：\n// build.gradle.kts import org.jetbrains.kotlin.gradle.tasks.KotlinCompile plugins { id(\u0026#34;org.springframework.boot\u0026#34;) version \u0026#34;3.1.3\u0026#34; id(\u0026#34;io.spring.dependency-management\u0026#34;) version \u0026#34;1.1.3\u0026#34; kotlin(\u0026#34;jvm\u0026#34;) version \u0026#34;1.9.10\u0026#34; kotlin(\u0026#34;plugin.spring\u0026#34;) version \u0026#34;1.9.10\u0026#34; } group = \u0026#34;com.example\u0026#34; version = \u0026#34;0.0.1-SNAPSHOT\u0026#34; java { sourceCompatibility = JavaVersion.VERSION_17 } repositories { mavenCentral() } dependencies { implementation(\u0026#34;org.springframework.boot:spring-boot-starter-web\u0026#34;) implementation(\u0026#34;org.mybatis.spring.boot:mybatis-spring-boot-starter:3.0.2\u0026#34;) runtimeOnly(\u0026#34;com.mysql:mysql-connector-j\u0026#34;) testImplementation(\u0026#34;org.springframework.boot:spring-boot-starter-test\u0026#34;) } tasks.withType\u0026lt;KotlinCompile\u0026gt; { kotlinOptions { freeCompilerArgs += \u0026#34;-Xjsr305=strict\u0026#34; jvmTarget = \u0026#34;17\u0026#34; } } tasks.withType\u0026lt;Test\u0026gt; { useJUnitPlatform() } 对于这个文件，需要特殊说明的是：\n  使用了插件kotlin(\u0026quot;plugin.spring\u0026quot;)\n这是因为在 Kotlin 中，类默认是final的，即无法被继承。使用该插件则可将使用了 Spring 注解的类变为open的，这样 Spring 才能正常工作。\n  MySQL Driver 包的引用方式是runtimeOnly\n在 dependencies 中，可以看到mysql-connector-j的引用方式为runtimeOnly，即仅在运行时需要，在编译期是不需要的。\n  Kotlin 编译器参数为-Xjsr305=strict\n使用该编译器参数的目的是开启JSR-305严格检查模式，以充分利用 Kotlin 的空安全检查。\n  1.2 程序入口类 DemoApplication.kt 分析完 Gradle 描述文件，下面看一下程序入口类DemoApplication.kt：\n// src/main/kotlin/com/example/demo/DemoApplication.kt package com.example.demo import org.springframework.boot.autoconfigure.SpringBootApplication import org.springframework.boot.runApplication @SpringBootApplication class DemoApplication fun main(args: Array\u0026lt;String\u0026gt;) { runApplication\u0026lt;DemoApplication\u0026gt;(*args) } 需要特殊说明的是：\n  class DemoApplication是一个空类，除了被添加@SpringBootApplication注解外，没有任何属性与方法，所以无需加花括号；\n  程序入口函数main是一个顶层函数，不属于DemoApplication类。\n  2 编写业务代码 模板工程准备就绪，现在可以开始编写业务代码了。业务场景为提供 User 的增、删、改、查 API。\n项目采用传统的 MVC 三层架构，代码目录结构如下：\ndemo |--- src/main/ | |--- resources/ | | |--- application.yaml | | \\--- shema.sql | \\--- kotlin/ | \\--- com.example.demo/ | |--- controller/ | | \\--- UserController.kt | |--- service/ | | |--- UserService.kt | | \\--- impl/ | | \\--- UserServiceImpl.kt | |--- dao/ | | \\--- UserMapper.kt | |--- model/ | | \\--- User.kt | \\--- DemoApplication.kt ... |--- gradlew \\--- build.gradle.kts 下面逐一看下 Controller 层、Service 层、DAO 层、Model 类的代码，以及配置文件和数据库脚本。\n2.1 Controller 层代码 Controller 层只有一个类UserController.kt，用于实现 User 的增、删、改、查。\n完整代码如下：\n// src/main/kotlin/com/example/demo/controller/UserController.kt package com.example.demo.controller import com.example.demo.model.User import com.example.demo.service.UserService import org.springframework.http.HttpStatus import org.springframework.web.bind.annotation.* @RestController @RequestMapping(\u0026#34;/users\u0026#34;) class UserController(val userService: UserService) { @GetMapping(\u0026#34;/\u0026#34;) fun listAll() = userService.listAll() @GetMapping(\u0026#34;/{id}\u0026#34;) fun getById(@PathVariable id: Long) = userService.getById(id) @PatchMapping(\u0026#34;/\u0026#34;) @ResponseStatus(HttpStatus.NO_CONTENT) fun update(@RequestBody user: User) { user.id?.let { userService.update(user) } } @PostMapping(\u0026#34;/\u0026#34;) @ResponseStatus(HttpStatus.CREATED) fun save(@RequestBody user: User) = userService.save(user) @DeleteMapping(\u0026#34;/{id}\u0026#34;) @ResponseStatus(HttpStatus.NO_CONTENT) fun deleteById(@PathVariable(\u0026#34;id\u0026#34;) id: Long) = userService.deleteById(id) } 对于这段代码，熟悉 Java 的同学对这种写法应该非常熟悉了，就是标准的 Controller 写法，只是使用了 Kotlin 的语法。\n2.2 Service 层代码 Service 层为 Controller 层提供服务，包含接口和实现类。\n其下面的两个文件UserService.kt和UserServiceImpl.kt代码如下：\n// src/main/kotlin/com/example/demo/service/UserService.kt package com.example.demo.service import com.example.demo.model.User interface UserService { fun listAll(): List\u0026lt;User\u0026gt; fun getById(id: Long): User? fun update(user: User) fun save(user: User) fun deleteById(id: Long) } // src/main/kotlin/com/example/demo/service/impl/UserServiceImpl.kt package com.example.demo.service.impl import com.example.demo.dao.UserMapper import com.example.demo.model.User import com.example.demo.service.UserService import org.springframework.stereotype.Service @Service class UserServiceImpl(val userMapper: UserMapper) : UserService { override fun listAll(): List\u0026lt;User\u0026gt; = userMapper.listAll() override fun getById(id: Long): User? = userMapper.getById(id) override fun update(user: User) = userMapper.update(user) override fun save(user: User) = userMapper.save(user) override fun deleteById(id: Long) = userMapper.deleteById(id) } 可以看到，Service 层的逻辑也比较简洁，只是调用 MyBatis Mapper 来实现对应的功能。\n2.3 DAO 层代码 我们的 DAO 层使用的是 MyBatis 来实现的，没有配置繁琐的 Mapper.xml 文件，使用的是注解的方式来操作数据库。\n文件UserMapper.kt的源码如下：\n// src/main/kotlin/com/example/demo/dao/UserMapper.kt package com.example.demo.dao import com.example.demo.model.User import org.apache.ibatis.annotations.* @Mapper interface UserMapper { @Select(\u0026#34;SELECT id, name, age FROM user\u0026#34;) fun listAll(): List\u0026lt;User\u0026gt; @Select(\u0026#34;SELECT id, name, age FROM user WHERE id = #{id}\u0026#34;) fun getById(id: Long): User? @Update(\u0026#34;UPDATE user SET name = #{name}, age = #{age} WHERE id = #{id}\u0026#34;) fun update(user: User) @Insert(\u0026#34;INSERT INTO user(name, age) VALUES(#{name}, #{age})\u0026#34;) fun save(user: User) @Delete(\u0026#34;DELETE FROM user WHERE id = #{id}\u0026#34;) fun deleteById(id: Long) } 2.4 Model 代码 Model 用于数据的传递，即接收数据库查询数据，并最终序列化为 JSON 来返回给 API 调用者；也用于将 API 调用者发出的 JSON 请求体转换为 Kotlin 对象。\n本项目只有一个 ModelUser.kt，其源码如下：\n// src/main/kotlin/com/example/demo/model/User.kt package com.example.demo.model data class User(val id: Long?, val name: String, val age: Int) 2.5 配置文件信息 我们 Spring 配置文件采用的是 YAML 格式，主要配置了数据库连接信息并指定了初始化 SQL 脚本的位置。\n# src/main/resources/application.yaml spring: datasource: url: jdbc:mysql://localhost:3306/test username: root password: root driver-class-name: com.mysql.cj.jdbc.Driver sql: init: schema-locations: classpath:schema.sql mode: always 连接信息指向的是在本地搭建的 MySQL 数据库，每次项目启动后都会重新执行resources下的schema.sql脚本。\n2.6 数据库脚本 建表语句如下（需要手动执行）：\n-- src/main/resources/database.sql CREATE DATABASE `test` DEFAULT CHARSET utf8 COLLATE utf8_general_ci; 包含建表语句的 SQL 文件schema.sql内容如下（自动执行）：\n-- src/main/resources/schema.sql DROP TABLE IF EXISTS user; CREATE TABLE user ( id BIGINT AUTO_INCREMENT, name VARCHAR(100) NOT NULL, age INT, PRIMARY KEY (id) ); 至此，支持 User 增、删、改、查的业务代码就基本写好了。\n3 API 测试与验证 下面准备启动项目并做一些 API 测试与验证。\n3.1 项目启动 在项目根目录执行如下 Gradle 命令即可启动项目：\n./gradlew bootRun 3.2 API 测试与验证 下面，使用 CURL 命令对 API 进行测试。\n首先新建一个 User：\ncurl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;name\u0026#34;: \u0026#34;Larry\u0026#34;, \u0026#34;age\u0026#34;: 28}\u0026#39; http://localhost:8080/users/ 然后查询所有 User，发现刚刚新建的 User 已建好，ID 为 1：\ncurl -X GET http://localhost:8080/users/ [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28}] 接着更新一下 ID 为 1 的 User 信息：\ncurl -X PATCH -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Larry2\u0026#34;, \u0026#34;age\u0026#34;: 29}\u0026#39; http://localhost:8080/users/ 查询 ID 为 1 的 User，发现信息已更新成功：\ncurl -X GET http://localhost:8080/users/1 {\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry2\u0026#34;,\u0026#34;age\u0026#34;:29} 最后删除 ID 为 1 的 User，然后再次查询所有 User，发现返回为空的 List：\ncurl -X DELETE http://localhost:8080/users/1 curl -X GET http://localhost:8080/users/ [] 如上测试说明我们编写的针对 User 增、删、改、查的 API 都是好用的。\n综上，我们使用 Kotlin + Gradle + Spring Boot + MyBatis 搭建了一个样例 API 项目，并编写了业务代码，最后进行了测试，发现使用 Spring Boot 和 Kotlin 构建 RESTful API 服务还是比较简单可行的。\n本文涉及的整个样例项目代码已托管至本人 GitHub，欢迎关注或 Fork。\n 参考资料\n[1] Get started with Spring Boot and Kotlin | Kotlin Documentation - kotlinlang.org\n[2] Building web applications with Spring Boot and Kotlin | Spring - spring.io\n[3] Build REST API with Spring Boot and Kotlin | Anirban\u0026rsquo;s Tech Blog - theanirban.dev\n[4] Examples for Using MyBatis with Kotlin | GitHub - github.com\n ","permalink":"https://olzhy.github.io/posts/building-restful-api-with-spring-boot-and-kotlin.html","tags":["Kotlin","Spring","Gradle"],"title":"如何使用 Spring Boot 和 Kotlin 构建 RESTful API 服务？"},{"categories":["计算机"],"contents":"jOOQ 是一个轻量级的 Java ORM（对象关系映射）框架，可用来构建复杂的 SQL 查询。jOOQ 可以根据数据库表自动生成对应的 Java 类，且字段类型与数据库一一对应，减少了 SQL 注入的风险。\n本文即是对 jOOQ 的初探，包括四个部分：准备数据库和测试数据、jOOQ Java 代码生成、jOOQ 初步使用，以及 jOOQ 与 Spring Boot 的集成。\n开始各个部分前，列出本文涉及的各软件版本：\nJava：20（BellSoft LibericaJDK） Maven：3.9.2 MySQL：8.1.0 jOOQ：3.18.6 Spring Boot：3.1.3 1 准备数据库、表和测试数据 探索 jOOQ 的使用之前，需要有一个数据库和几张表。学生课程系统就是一个不错的业务场景，既接近实际又涉及连表等复杂查询，很适合用来作演示学习。\n本文为学生课程系统创建了一个 school 数据库，并在其下创建了三张表 student（学生表）、course（课程表）和 score（成绩表）。\n如下为完整的建库、建表和数据插入语句：\n-- 创建数据库 school DROP DATABASE IF EXISTS school; CREATE DATABASE school DEFAULT CHARSET utf8 COLLATE utf8_general_ci; -- 使用数据库 school USE school; -- 创建学生表 DROP TABLE IF EXISTS student; CREATE TABLE student ( no INT NOT NULL, -- 编号  name VARCHAR(20) NOT NULL, -- 姓名  gender ENUM(\u0026#39;男\u0026#39;, \u0026#39;女\u0026#39;) NOT NULL, -- 性别  birthday DATETIME, -- 出生日期  CONSTRAINT PRIMARY KEY (no) -- 编号为主键 ); -- 为学生表插入数据 INSERT INTO student VALUES (1, \u0026#39;闫浩然\u0026#39;, \u0026#39;男\u0026#39;, \u0026#39;1999-09-01\u0026#39;), (2, \u0026#39;肖雪\u0026#39;, \u0026#39;女\u0026#39;, \u0026#39;2000-03-21\u0026#39;), (3, \u0026#39;张如意\u0026#39;, \u0026#39;女\u0026#39;, \u0026#39;2001-08-08\u0026#39;); -- 创建课程表 DROP TABLE IF EXISTS course; CREATE TABLE course ( no INT NOT NULL, -- 编号  name VARCHAR(20) NOT NULL, -- 名称  CONSTRAINT PRIMARY KEY (no) -- 编号为主键 ); -- 为课程表插入数据 INSERT INTO course VALUES (1, \u0026#39;语文\u0026#39;), (2, \u0026#39;数学\u0026#39;), (3, \u0026#39;英语\u0026#39;); -- 创建成绩表 DROP TABLE IF EXISTS score; CREATE TABLE score ( student_no INT NOT NULL, -- 学生编号  course_no INT NOT NULL, -- 课程编号  degree DECIMAL(4, 1) NOT NULL, -- 分数  CONSTRAINT PRIMARY KEY (student_no, course_no), -- 学生编号与课程编号为联合主键  CONSTRAINT FOREIGN KEY (student_no) REFERENCES student(no), -- 学生编号为外键  CONSTRAINT FOREIGN KEY (course_no) REFERENCES course(no) -- 课程编号为外键 ); -- 为成绩表插入数据 INSERT INTO score VALUES (1, 1, 90.5), (1, 2, 88.0), (1, 3, 98.0), (2, 1, 78.5), (2, 2, 68.0), (2, 3, 93.0), (3, 1, 83.0), (3, 2, 94.5), (3, 3, 73.0); 2 jOOQ Java 代码生成 该部分尝试用 jOOQ Maven 插件（jooq-codegen-maven）的方式来生成 Java 代码。\n本文使用的是在本地搭建的 MySQL 数据库，将第一部分的 SQL 语句在数据库执行后，即可以尝试使用 jOOQ Maven 插件来生成 Java 代码了（主要是表相关的 Java 类和 POJO 类）。\n插件jooq-codegen-maven在 Maven 配置文件pom.xml中的配置信息如下：\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.jooq\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jooq-codegen-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${jooq.version}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;generate\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;jdbc\u0026gt; \u0026lt;driver\u0026gt;com.mysql.cj.jdbc.Driver\u0026lt;/driver\u0026gt; \u0026lt;url\u0026gt;jdbc:mysql://localhost:3306/school\u0026lt;/url\u0026gt; \u0026lt;user\u0026gt;root\u0026lt;/user\u0026gt; \u0026lt;password\u0026gt;root\u0026lt;/password\u0026gt; \u0026lt;/jdbc\u0026gt; \u0026lt;generator\u0026gt; \u0026lt;generate\u0026gt; \u0026lt;pojos\u0026gt;true\u0026lt;/pojos\u0026gt; \u0026lt;/generate\u0026gt; \u0026lt;database\u0026gt; \u0026lt;includes\u0026gt;.*\u0026lt;/includes\u0026gt; \u0026lt;inputSchema\u0026gt;school\u0026lt;/inputSchema\u0026gt; \u0026lt;/database\u0026gt; \u0026lt;target\u0026gt; \u0026lt;packageName\u0026gt;com.leileiluoluo.jooq.model.generated\u0026lt;/packageName\u0026gt; \u0026lt;directory\u0026gt;src/main/java\u0026lt;/directory\u0026gt; \u0026lt;/target\u0026gt; \u0026lt;/generator\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; 然后，使用如下命令生成 Java 代码：\nmvn clean generate-sources 可以看到，代码被生成到了src/main/java文件夹下的com.leileiluoluo.jooq.model.generated包下。\n3 jOOQ 初步使用 使用 jOOQ 的一个主要目的可能是想借力其丰富的 SQL 构造能力。\n下面即会使用 jOOQ 以及在第二部分生成的 Java 代码（主要是表相关的类和 POJO 类）来实现一些常用的查询。\n如下即是使用 jOOQ 来查询所有 Student 的一段示例代码：\nimport com.leileiluoluo.jooq.model.generated.tables.pojos.Student; import org.jooq.DSLContext; import org.jooq.SQLDialect; import org.jooq.impl.DSL; import java.sql.Connection; import java.sql.DriverManager; import java.util.List; import static com.leileiluoluo.jooq.model.generated.Tables.STUDENT; public class JOOQSimpleQueryTest { public static void main(String[] args) { String username = \u0026#34;root\u0026#34;; String password = \u0026#34;root\u0026#34;; String url = \u0026#34;jdbc:mysql://localhost:3306/school\u0026#34;; try (Connection conn = DriverManager.getConnection(url, username, password)) { DSLContext context = DSL.using(conn, SQLDialect.MYSQL); List\u0026lt;Student\u0026gt; students = context.selectFrom(STUDENT) .fetchInto(Student.class); students.forEach(student -\u0026gt; { System.out.printf(\u0026#34;no: %s, name: %s, gender: %s, birthday: %s\\n\u0026#34;, student.getNo(), student.getName(), student.getGender(), student.getBirthday()); }); } catch (Exception e) { e.printStackTrace(); } } } 可以看到，上面这段代码首先使用DriverManager.getConnection(url, username, password);来创建了一个数据库连接；然后使用DSL.using(conn, SQLDialect.MYSQL);来创建了DSLContext对象；然后即可以用DSLContext来像写 SQL 语句一样（context.selectFrom(STUDENT).fetchInto(Student.class);）来拼装查询语句了，查询结果会自动转换为 POJO 类的类型，非常方便快捷。\n程序运行结果如下：\nno: 1, name: 闫浩然, gender: 男, birthday: 1999-09-01T00:00 no: 2, name: 肖雪, gender: 女, birthday: 2000-03-21T00:00 no: 3, name: 张如意, gender: 女, birthday: 2001-08-08T00:00 上面的示例针对的是单表查询的情形，下面再看一下复杂查询的拼装：\nDSLContext context = DSL.using(conn, SQLDialect.MYSQL); List\u0026lt;Record3\u0026lt;String, String, BigDecimal\u0026gt;\u0026gt; studentCourseScores = context.select( STUDENT.NAME, COURSE.NAME, SCORE.DEGREE ).from(SCORE) .join(STUDENT).on(SCORE.STUDENT_NO.eq(STUDENT.NO)) .join(COURSE).on(SCORE.COURSE_NO.eq(COURSE.NO)) .fetch(); studentCourseScores.forEach(record -\u0026gt; { String studentName = record.getValue(STUDENT.NAME); String courseName = record.getValue(COURSE.NAME); BigDecimal degree = record.getValue(SCORE.DEGREE); System.out.printf(\u0026#34;student: %s, course: %s, degree: %s\\n\u0026#34;, studentName, courseName, degree); }); 上面的查询涉及三个表的连接，依然可以像写 SQL 一样来进行构造。\n程序运行结果如下：\nstudent: 张如意, course: 语文, degree: 83.0 student: 肖雪, course: 语文, degree: 78.5 student: 闫浩然, course: 语文, degree: 90.5 student: 张如意, course: 数学, degree: 94.5 student: 肖雪, course: 数学, degree: 68.0 student: 闫浩然, course: 数学, degree: 88.0 student: 张如意, course: 英语, degree: 73.0 student: 肖雪, course: 英语, degree: 93.0 student: 闫浩然, course: 英语, degree: 98.0 其对应的 SQL 语句如下：\nSELECT s.name, c.name, sc.degree FROM score sc JOIN student s ON sc.student_no=s.no JOIN course c ON sc.course_no=c.no; 通过这两段示例程序，即可以看到 jOOQ 的使用非常的简单。针对单表的查询，可以直接将结果映射到 POJO 类；对于多表连接等复杂查询，拼装起来也并不复杂，且结果可以转换为一个多值的类RecordN\u0026lt;?, ?, ?, ...\u0026gt;。\n4 jOOQ 与 Spring Boot 的集成 第三部分的示例仅适用于本地测试的情形，对于实际的项目，还需要考虑其如何与框架进行集成。\n该部分即会探索 jOOQ 与 Spring Boot 的集成，主要会探索两个方面：DSLContext的自动创建、DAO 层的封装。\n4.1 DSLContext 的自动创建 在 Spring Boot 中使用 jOOQ 时，DSLContext如何进行创建，这些交给spring-boot-starter-jooq就可以了，我们依然在application.xml采用通用的数据库配置即可，DSLContext会由 Spring 容器自动创建，我们只需在需要的地方进行自动注入就可以了。\n# application.yaml spring: datasource: url: jdbc:mysql://localhost:3306/school username: root password: root driver-class-name: com.mysql.cj.jdbc.Driver // StudentDao.java @Service public class StudentDaoImpl implements StudentDao { @Autowired private DSLContext context; } 4.2 DAO 层的封装 虽然 jOOQ 也支持自动生成 DAO 层，但其生成的 DAO 层代码比较泛化，有很多方法可能根本就用不着。所以，经过调研后，本人决定仅使用其构建 SQL 的能力（以及自动生成的表相关的类和 POJO 类），DAO 层还是根据业务情形自己来实现比较好一些。\n如下即是为 Student 查询设计的 StudentDao 的示例代码：\n// StudentDao.java package com.leileiluoluo.jooq.dao; import com.leileiluoluo.jooq.model.generated.tables.pojos.Student; import java.util.List; import java.util.Optional; public interface StudentDao { Integer countAll(); List\u0026lt;Student\u0026gt; listAll(); List\u0026lt;Student\u0026gt; listWithPagination(int offset, int limit); Optional\u0026lt;Student\u0026gt; getByNo(Integer no); void save(Student record); void update(Student record); void deleteByNo(Integer no); } // StudentDaoImpl.java package com.leileiluoluo.jooq.dao.impl; import com.leileiluoluo.jooq.dao.StudentDao; import com.leileiluoluo.jooq.model.generated.tables.pojos.Student; import org.jooq.DSLContext; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import java.util.List; import java.util.Optional; import static com.leileiluoluo.jooq.model.generated.Tables.STUDENT; @Service public class StudentDaoImpl implements StudentDao { @Autowired private DSLContext context; @Override public Integer countAll() { return context.fetchCount(STUDENT); } @Override public List\u0026lt;Student\u0026gt; listAll() { return context.selectFrom(STUDENT) .fetchInto(Student.class); } @Override public List\u0026lt;Student\u0026gt; listWithPagination(int offset, int limit) { return context.selectFrom(STUDENT) .offset(offset) .limit(limit) .fetchInto(Student.class); } @Override public Optional\u0026lt;Student\u0026gt; getByNo(Integer no) { Student student = context.select() .from(STUDENT) .where(STUDENT.NO.eq(no)) .fetchOneInto(Student.class); return Optional.ofNullable(student); } @Override public void save(Student student) { context.insertInto(STUDENT) .set(STUDENT.NO, student.getNo()) .set(STUDENT.NAME, student.getName()) .set(STUDENT.GENDER, student.getGender()) .set(STUDENT.BIRTHDAY, student.getBirthday()) .execute(); } @Override public void update(Student student) { context.update(STUDENT) .set(STUDENT.NAME, student.getName()) .set(STUDENT.GENDER, student.getGender()) .set(STUDENT.BIRTHDAY, student.getBirthday()) .where( STUDENT.NO.eq(student.getNo()) ) .execute(); } @Override public void deleteByNo(Integer no) { context.deleteFrom(STUDENT) .where( STUDENT.NO.eq(no) ).execute(); } } 可以看到，增、删、改、查都有了，基本满足了实际业务中的需要；在其上设计 Service 和 Controller 即可以实现真实的 REST 业务需求了。\n综上，本文准备了一些测试数据，探索了 jOOQ 的代码生成和 SQL 构建能力，最后还思考了其与 Spring Boot 的集成。总体来看，jOOQ 还是比较易用的，是一个不错的 MyBatis 或 Hibernate 替代方案。\n此外，本文涉及的所有代码均已提交至本人 GitHub，有兴趣的同学可以关注或 Fork。\n 参考资料\n[1] The jOOQ User Manual | jOOQ - www.jooq.org\n[2] jOOQ 系列教程 | Diamond - jooq.diamondfsd.com\n[3] Configure jOOQ with Spring Boot and PostgreSQL | Medium - medium.com\n[4] Spring Boot with jOOQ and PostgreSQL | Medium - medium.com\n ","permalink":"https://olzhy.github.io/posts/jooq-getting-started.html","tags":["Java"],"title":"Java 数据库操作工具包 jOOQ 初探"},{"categories":["计算机"],"contents":"本文将会以对比 Java 的方式来学习 Kotlin 中的一些惯用写法与最佳实践，以便对 Java 转 Kotlin 的同学能有一些帮助。\n1 能使用表达式就不要使用函数块 先看一段 Java 代码：\npublic static String ageGroup(int age) { if (age \u0026gt;= 0 \u0026amp;\u0026amp; age \u0026lt; 18) { return \u0026#34;未成年\u0026#34;; } else if (age \u0026lt; 45) { return \u0026#34;青年\u0026#34;; } else if (age \u0026lt; 60) { return \u0026#34;中年\u0026#34;; } else { return \u0026#34;老年\u0026#34;; } } 上面这段 Java 代码，是对年龄段进行分组。\n如果使用 Kotlin 来改写的话，会是下面这个样子：\n// 不推荐的写法 fun ageGroup(age: Int): String { return if (age in 0..\u0026lt;18) { \u0026#34;未成年\u0026#34; } else if (age \u0026lt; 45) { \u0026#34;青年\u0026#34; } else if (age \u0026lt; 60) { \u0026#34;中年\u0026#34; } else { \u0026#34;老年\u0026#34; } } 但如上方式是不推荐的，有两点改进建议，就是：能使用表达式就不要使用函数块；能使用when就不要使用if。\n所以，如上 Kotlin 代码改写为表达式结合when语句的写法如下：\n// 推荐的写法 fun ageGroup(age: Int): String = when { age in 0..\u0026lt;18 -\u0026gt; \u0026#34;未成年\u0026#34; age \u0026lt; 45 -\u0026gt; \u0026#34;青年\u0026#34; age \u0026lt; 60 -\u0026gt; \u0026#34;中年\u0026#34; else -\u0026gt; \u0026#34;老年\u0026#34; } 2 使用扩展函数充当工具包的场景 先看一段 Java 代码：\nimport java.text.SimpleDateFormat; import java.util.Date; public class DatesUtil { public static String formatDate(Date date) { return new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;).format(date); } public static void main(String[] args) { System.out.println(formatDate(new Date())); } } 上面这段代码是 Java 中比较常用的静态工具类的写法。\n如果把它直接转化为 Kotlin 的写法，代码是下面这个样子：\n// 不推荐的写法 import java.text.SimpleDateFormat import java.util.* object DateUtil { fun formatDate(date: Date): String = SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;).format(date) } fun main() { println(DateUtil.formatDate(Date())) } 对于这种工具包的场景，上面这种从 Java 延续过来的写法在 Kotlin 中是不推荐的，Kotlin 更推荐使用扩展函数来实现这类功能，这样显得代码更具可读性。\n使用扩展函数充当工具包的代码如下：\n// 推荐的写法 import java.text.SimpleDateFormat import java.util.* fun Date.format(): String = SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;).format(this) fun main() { println(Date().format()) } 3 使用命名参数代替一串 Set 先看一段 Java 代码：\nclass DatabaseConfig { private String host; private Integer port; private String charset = \u0026#34;utf-8\u0026#34;; private String timezone = \u0026#34;Asia/Beijing\u0026#34;; public void setHost(String host) { this.host = host; } public void setPost(Integer port) { this.port = port; } public void setCharset(String charset) { this.charset = charset; } public void setTimezone(String timezone) { this.timezone = timezone; } } // 使用一串 Set 来设定必须的值 DatabaseConfig databaseConfig = new DatabaseConfig(); databaseConfig.setHost(\u0026#34;localhost\u0026#34;); databaseConfig.setPost(3306); 如上代码定义了一个配置类，其中一些是有默认值的字段，而另一些是初始化时必须设定值的字段。\n在 Kotlin 中，原生支持命名参数和默认值，所以如上代码改造为 Kotlin 的写法如下：\ndata class DatabaseConfig(val host: String, val post: Int, val charset: String = \u0026#34;utf-8\u0026#34;, val timezone: String = \u0026#34;Asia/Beijing\u0026#34;) // 使用命名参数设定必须的值 val databaseConfig = DatabaseConfig( host = \u0026#34;localhost\u0026#34;, post = 3306 ) 这样在初始化对象时，省去一串 Set 调用，会更简洁可读一些。\n4 使用 apply 为对象作一组初始化操作 先看一段 Java 代码：\npublic static void main(String[] args) { File file = new File(\u0026#34;test.txt\u0026#34;); file.setExecutable(false); file.setReadable(true); file.setWritable(false); } 上面这段代码是 Java 中比较常见对象初始化的写法。\n直接将其转化为 Kotlin 的写法，代码会是下面这个样子：\n// 不推荐的写法 fun main() { val file = File(\u0026#34;test.txt\u0026#34;) file.setExecutable(false) file.setReadable(true) file.setWritable(false) } 对于这种对一个对象作一组初始化操作的场景应使用apply扩展函数，这样不必每条语句都携带对象的变量名，看起来会更精简一些。\n使用apply后的 Kotlin 代码如下：\n// 推荐的写法 fun main() { val file = File(\u0026#34;test.txt\u0026#34;) file.apply { setExecutable(false) setReadable(true) setWritable(false) } } 5 不要为了实现参数默认值而使用函数重载 先看一段 Java 代码：\npublic class TestOverload { public static void main(String[] args) { greet(); } private static void greet() { greet(\u0026#34;World\u0026#34;); } private static void greet(String name) { System.out.println(\u0026#34;Hello \u0026#34; + name); } } 如上代码中，TestOverload类中的greet方法是一个重载方法，无参数的greet方法的目的是满足默认值填充的功能。\n将如上 Java 代码直接转换为 Kotlin 写法的代码如下：\n// 不推荐的写法 fun main() { fun greet(name: String) { println(\u0026#34;Hello $name\u0026#34;) } fun greet() { greet(\u0026#34;World\u0026#34;) } greet() } 而这种写法是不推荐的，Kotlin 中可以直接使用带默认值的函数。\n改造后的代码如下：\n// 推荐的写法 fun main() { fun greet(name: String = \u0026#34;World\u0026#34;) { println(\u0026#34;Hello $name\u0026#34;) } greet() } 6 要合理利用 Kotlin 的 Null 安全 先看一段 Java 代码：\nif (null == order || null == order.getCustomer() || null == order.getCustomer().getAddress()) { throw new IllegalArgumentException(\u0026#34;Invalid Order\u0026#34;); } String city = order.getCustomer().getAddress().getCity(); 这段代码展示了在 Java 中对嵌套对象取值时需要逐层判空的问题。\n而在 Kotlin 中无需这么繁琐，只要结合使用空安全检查（?.）与 Elvis 表达式（?:）即可。\n用 Kotlin 改写后的代码如下：\n// 推荐的写法 val city = order?.customer?.address?.city ?: throw IllegalArgumentException(\u0026#34;Invalid Order\u0026#34;) 此外，需要注意，下面这种绕过空安全校验直接强制取值的写法是不推荐的：\n// 不推荐的写法 val city = order!!.customer!!.address!!.city 7 将 let 用起来 先看一段 Java 代码：\nOrder order = getOrderById(orderId); if (null != order) { boolean valid = isCustomerValid(order.getCustomer()); // ... } 该代码中，首先查询了 Order，判断不为空时再对 Order 下面的 Customer 做有效性检查。\n而在 Kotlin 中，可以使用let来取代这类if-not-null检查。\n使用 Kotlin 改写后的代码如下：\n// 推荐的写法 val order: Order? = getOrderById(orderId) order?.let { val valid = isCustomerValid(it.customer) // ... } 8 将值对象（Value Object）用起来 Kotlin 中可以使用数据类（data class）来定义一个不可变对象，非常适用于值对象（Java 中叫 VO，只用于传值的不可变对象）的使用场景。\n下面这段 Kotlin 代码定义了一个Email数据类，用于邮件发送：\n// 推荐的写法 data class Email(val to: String, val subject: String, val content: String) interface EmailService { fun send(email: Email) } Java 14 中也借鉴了 Kotlin 的data class，引入了record关键字来定义不可变数据类。\n如上代码对应 Java 中的写法如下：\npublic record Email(String to, String subject, String content) {} public interface EmailService { void send(Email email); } 9 做字段映射时尝试使用单表达式函数 先看一段 Kotlin 代码：\ndata class User(val name: String, val age: Int, val gender: String) // 不推荐的写法 fun parseMapToUser(userMap: Map\u0026lt;String, Any\u0026gt;): User { return User( name = userMap[\u0026#34;name\u0026#34;] as String, age = userMap[\u0026#34;age\u0026#34;] as Int, gender = userMap[\u0026#34;gender\u0026#34;] as String) } 这段代码在提取Map中的字段信息，从而组装成具体的对象。若一个函数仅做诸如此类字段映射和对象转换时，如上的这种写法是不推荐的。\n使用单表达式来改写如上写法会显得更精简且更有可读性。\n代码如下：\n// 推荐的写法 fun parseMapToUser(userMap: Map\u0026lt;String, Any\u0026gt;) = User( name = userMap[\u0026#34;name\u0026#34;] as String, age = userMap[\u0026#34;age\u0026#34;] as Int, gender = userMap[\u0026#34;gender\u0026#34;] as String) 此外，也可以使用扩展函数来实现此类功能。\n// 推荐的写法 fun Map\u0026lt;String, Any\u0026gt;.toUser() = User( name = this[\u0026#34;name\u0026#34;] as String, age = this[\u0026#34;age\u0026#34;] as Int, gender = this[\u0026#34;gender\u0026#34;] as String) 10 不建议将属性的初始化工作放在 init 块内进行 先看一段 Java 代码：\npublic class UserClient { private String baseUrl; private String usersUrl; private CloseableHttpClient httpClient; public UserClient(String baseUrl) { this.baseUrl = baseUrl; // 初始化 usersUrl  userUrl = baseUrl + \u0026#34;/users\u0026#34;; // 初始化 httpClient  HttpClientBuilder builder = HttpClientBuilder.create(); builder.setUserAgent(\u0026#34;UserClient\u0026#34;); builder.setConnectionManagerShared(true); httpClient = builder.build(); } public List\u0026lt;User\u0026gt; getUsers() { // ...  } } 如上代码中，有一些属性是构造器参数，需要调用时传入的；另一些属性是需要根据构造器参数进行拼接或需要在构造方法内部进行初始化的。\n将如上代码转换为 Kotlin 的写法可能会是下面这个样子：\n// 不推荐的写法 class UserClient(baseUrl: String) { private val usersUrl = \u0026#34;$baseUrl/users\u0026#34; private val httpClient: CloseableHttpClient // 在 init 块内初始化 httpClient  init { val builder = HttpClientBuilder.create() builder.setUserAgent(\u0026#34;UserClient\u0026#34;) builder.setConnectionManagerShared(true) httpClient = builder.build() } fun getUsers() { // ...  } } 即把非构造器参数的初始化工作放在init块内进行。但这种方式是不推荐的，因为在 Kotlin 中可以直接在定义参数的时候使用单表达式对其进行初始化。\n推荐的写法如下：\n// 推荐的写法 class UserClient(baseUrl: String) { private val usersUrl = \u0026#34;$baseUrl/users\u0026#34; // 定义时就可以直接使用单表达式初始化 httpClient  private val httpClient = HttpClientBuilder.create().apply { setUserAgent(\u0026#34;UserClient\u0026#34;) setConnectionManagerShared(true) }.build() fun getUsers() { // ...  } } 11 使用 object 声明无状态的接口实现 若一个类没有状态，仅用来做诸如接口的实现工作，则非常适合将其声明为object。\n使用object声明的示例代码如下：\nobject DefaultListener : MouseAdapter() { override fun mouseClicked(e: MouseEvent?) { // ...  } override fun mouseReleased(e: MouseEvent?) { // ...  } } 如上代码中，MouseAdapter是 Java awt 中的一个抽象类，DefaultListener实现了其中的两个方法。\n12 在需要的时候使用解构 Java 中不支持一个方法返回多个值，也不支持多个值在变量的携带，这在实际使用中非常的不方便，多于一个值的返回就得考虑新建一个类。\nKotlin 虽然也没有多值返回这个功能，但 Kotlin 支持解构以及内置二值（Pair）和三值（Triple）数据类，也可以达到多值返回和使用的效果。\n看一段 Kotlin 代码：\nfun getStudents(): List\u0026lt;Pair\u0026lt;String, Int\u0026gt;\u0026gt; = listOf(Pair(\u0026#34;Larry\u0026#34;, 28), Pair(\u0026#34;Lucy\u0026#34;, 26)) fun main() { for ((name, age) in getStudents()) { println(\u0026#34;$name, $age\u0026#34;) } } 如上代码即是使用内置的Pair类来支持二值的返回，并且该类本身支持解构，所以支持使用（(name, age) = xxx）的方式一次性将多个值取出来。\n三个值的返回可以使用Triple类，而对于多于三个值的情形，则可以定义数据类来实现，其也支持解构。\n自定义数据类及使用解构的示例 Kotlin 代码如下：\ndata class Student(val name: String, val age: Int, val gender: String, val grade: Int) fun getStudents(): List\u0026lt;Student\u0026gt; = listOf( Student(\u0026#34;Larry\u0026#34;, 28, \u0026#34;Male\u0026#34;, 3), Student(\u0026#34;Lucy\u0026#34;, 26, \u0026#34;Female\u0026#34;, 2) ) fun main() { for ((name, age, gender, grade) in getStudents()) { println(\u0026#34;$name, $age, $gender, $grade\u0026#34;) } } 13 巧用密封类取代异常的使用场景 先看一段 Kotlin 代码：\n// 不推荐的写法 data class User(val id: Long, val avatarUrl: String, val name: String, val email: String) @Throws(UserException::class) fun requestUser(id: Long): User = try { restTemplate.getForObject\u0026lt;User\u0026gt;(\u0026#34;https://api.some-domain.com/api/users/$id\u0026#34;) } catch (ex: IOException) { throw UserException( message = \u0026#34;parse_failed\u0026#34;, cause = ex ) } catch (ex: RestClientException) { throw UserException( message = \u0026#34;request_failed\u0026#34;, cause = ex ) } fun main() { // 获取用户头像  val avatarUrl = try { requestUser(id).avatarUrl } catch (ex: UserException) { \u0026#34;https://www.some-domain.com/images/default-avatar.png\u0026#34; } } 如上这段代码的requestUser函数使用restTemplate调用 REST API 来获取单个用户的信息，若调用中过程出现了异常会统一封装为UserException抛出。\n其实这段代码可以通过使用 Kotlin 中的密封类（sealed class）来进行改写。\n改写后的代码如下：\n// 推荐的写法 data class User(val id: Long, val avatarUrl: String, val name: String, val email: String) sealed class UserResponse { data class Success(val user: User) : UserResponse() data class Error(val code: String, val description: String) : UserResponse() } fun requestUser(id: Long): UserResponse = try { val user = restTemplate.getForObject\u0026lt;User\u0026gt;(\u0026#34;https://api.some-domain.com/api/users/$id\u0026#34;) UserResponse.Success(user = user) } catch (ex: IOException) { UserResponse.Error(\u0026#34;parse_failed\u0026#34;, \u0026#34;${ex.message}\u0026#34;) } catch (ex: RestClientException) { UserResponse.Error(\u0026#34;request_failed\u0026#34;, \u0026#34;${ex.message}\u0026#34;) } fun main() { val avatarUrl = when (val userResp = requestUser(1)) { is UserResponse.Success -\u0026gt; userResp.user.avatarUrl is UserResponse.Error -\u0026gt; \u0026#34;https://www.some-domain.com/images/default-avatar.png\u0026#34; } } 可以看到，使用密封类进行改写后的代码比使用异常更具可读性，除了可读性外，因为异常检查是在运行时做的，而对密封类进行when判断时，Kotlin 会在编译期检查when里边的分支是否覆盖了密封类的所有子结果，这一点对代码的健壮性来说也是很有益的。\n14 if-else 嵌套不要超过 3 层 Java 中要求if-else嵌套不要超过 3 层。\n如《阿里巴巴 Java 开发手册 · 黄山版》第一部分编程规约的控制语句部分就讲：如果非使用if()...else if()...else...方式表达逻辑，避免后续代码维护困难，请勿超过 3 层；超过 3 层的if-else的逻辑判断代码可以使用卫语句等方式实现。\n下面先看一段 Java 代码：\n// 不推荐的写法 public Long getPriceTotalByOrderId(Long orderId) throws BusinessException { long priceTotal = 0L; // 查询订单  Order order = orderService.getOrderById(orderId); if (null != order) { // 查询订单中的商品  List\u0026lt;Product\u0026gt; products = order.getProducts(); if (!products.isEmpty()) { // 计算商品总价  for (Product product : products) { if (!product.isGift()) { // 非赠品才计入总价  priceTotal += product.getPrice(); } } } else { throw new BusinessException(\u0026#34;该订单未包含任何商品\u0026#34;); } } else { throw new BusinessException(\u0026#34;未找到相应的订单\u0026#34;); } return priceTotal; } 这段代码使用的if-else嵌套为 3 层，这类实现在我们日常接触的代码中很常见。\n若使用卫语句改造一下，如上代码会变成下面这个样子：\n// 推荐的写法 public Long getPriceTotalByOrderId(Long orderId) throws BusinessException { long priceTotal = 0L; // 查询订单  Order order = orderService.getOrderById(orderId); if (null == order) { throw new BusinessException(\u0026#34;未找到相应的订单\u0026#34;); } // 查询订单中的商品  List\u0026lt;Product\u0026gt; products = order.getProducts(); if (products.isEmpty()) { throw new BusinessException(\u0026#34;该订单未包含任何商品\u0026#34;); } // 计算商品总价  for (Product product : products) { if (!product.isGift()) { // 非赠品才计入总价  priceTotal += product.getPrice(); } } return priceTotal; } 可以看到，改造后的代码，将 3 层if-else嵌套变为扁平的一层，代码逻辑变得更加清晰，减少了出 Bug 的可能。\n在 Kotlin 中也是一样的，要尽量避免 3 层或 3 层以上的if-else嵌套：\n// 不推荐的写法 @Throws(BusinessException::class) fun getPriceTotalByOrderId(orderId: Long): Long { var priceTotal = 0L // 查询订单  val order: Order? = DefaultOrderService().getOrderById(orderId) return if (null != order) { // 查询订单中的商品  val products: List\u0026lt;Product\u0026gt; = order.products if (products.isNotEmpty()) { // 计算商品总价  for (product in products) { if (!product.isGift()) { // 非赠品才计入总价  priceTotal += product.price } } priceTotal } else { throw BusinessException(\u0026#34;该订单未包含任何商品\u0026#34;) } } else { throw BusinessException(\u0026#34;未找到相应的订单\u0026#34;) } } 而应尽量将if-else嵌套变得扁平化：\n// 推荐的写法 @Throws(BusinessException::class) fun getPriceTotalByOrderId(orderId: Long): Long { // 查询订单  val order: Order = DefaultOrderService().getOrderById(orderId) ?: throw BusinessException(\u0026#34;未找到相应的订单\u0026#34;) // 查询订单中的商品  val products: List\u0026lt;Product\u0026gt; = order.products if (products.isEmpty()) { throw BusinessException(\u0026#34;该订单未包含任何商品\u0026#34;) } // 计算商品总价  return products.filterNot { it.isGift() } .sumOf { it.price } } 15 不要在生产环境使用 System.out.println() 打印日志 应避免在生产环境直接使用System.out.println()等标准输出打印日志，而应使用日志包来完成。\n如《阿里巴巴 Java 开发手册 · 黄山版》第三部分异常日志的日志规约部分就讲：生产环境禁止使用System.out、System.err或e.printStackTrace()打印日志或异常堆栈。\n如下为使用 Slf4j 打印日志的 Java 代码：\n// 推荐的写法 public class UserServiceImpl implements UserService { private static final Logger logger = LoggerFactory.getLogger(UserServiceImpl.class); @Override public void save(User user) { try { // ...  } catch (Exception e) { logger.error(\u0026#34;user save failed\u0026#34;, e); } } } 如下为使用 Slf4j 打印日志的 Kotlin 代码：\n// 推荐的写法 class UserServiceImpl : UserService { companion object { private val logger = LoggerFactory.getLogger(UserServiceImpl::class.java) } override fun save(user: User) { try { // ...  } catch (e: Exception) { logger.error(\u0026#34;user save failed\u0026#34;, e) } } } \n综上，本文以对比 Java 的方式总结了 Kotlin 中的一些惯用写法与最佳实践，希望对初学 Kotlin 的 Javaer 有一些帮助。\n 参考资料\n[1] Idioms | Kotlin Documentation - kotlinlang.org\n[2] Idiomatic Kotlin Best Practices | Philipp Hauer\u0026rsquo;s Blog - phauer.com\n[3] 阿里巴巴 Java 开发手册黄山版 | Alibaba P3C - github.com\n ","permalink":"https://olzhy.github.io/posts/kotlin-idioms-and-best-practices.html","tags":["Kotlin","Java"],"title":"对比 Java 学习 Kotlin 中的惯用写法与最佳实践"},{"categories":["计算机"],"contents":"本文关注 Java 并发编程基础，介绍并发的基本概念、进程和线程的概念、Java 多线程的初步使用、共享资源访问控制等基础知识。\n开始探索 Java 并发编程之前，我们需要知道：什么是并发？以及，并发与并行有什么不同？\n1 什么是并发？并发与并行有什么不同？ 并发（Concurrency）指的是在一个重叠的时间段内执行多个任务。即一个任务可以在前一个任务未完成时开始执行，CPU 会对每个任务分配时间片并切换上下文，但同一时刻依然最多只有一个任务在执行。\n并行（Parallelism）指的是在同一时刻执行多个任务。与并发任务在同一个处理器内核或同一个处理器上执行不同，并行任务是在不同的处理器内核或者不同的处理器上执行的。\n下图展示了并发与并行在处理任务上的不同：\n（并发与并行对比 - 引用自 Baeldung） 可以看到，在该图中有两个处理器内核（Core 1 和 Core 2）和两个任务（Task 1 与 Task 2）。并发执行的话，是在一个内核上将两个任务按时间交替切换执行；而并行执行的话，是在两个不同的内核上将两个任务同时分别独立执行。\n更通俗一点，假如一个人是一个处理器内核的话，并发与并行做的事情可以用一句话来比喻：一个人一边吃饭一边看书是并发，多个人同时吃饭是并行。\n为什么要使用并发呢？因为并发可以带来诸多的好处：\n  并发可以更充分的利用处理器内核\n比如一个任务在等待 IO 的时候，处理器内核是闲置的，这时完全可以启动另一个任务做一些其它的事情。\n  并发可以更公平的获取执行权\n比如一个 Web 服务，假设请求都是顺序处理的话，只有等上一个用户请求处理完成了，才能处理下一个请求。那上一个用户请求非常耗时的话，后面的用户请求会长时间得不到处理。如果采用并发，直观感觉上，所有的用户请求都在同时处理，各个请求任务得到更公平的执行权，用户体验得到极大的改善。\n  不过，凡事有利必有弊，并发也会带来诸多的问题：\n  共享数据访问控制让编码变得复杂\n并发任务会涉及同一块内存区域的访问问题。比如一个任务在对一个内存位置进行读的时候，另一个任务正在对这个位置进行写，那这个任务读到的值应该是另一个任务写之前的旧值，还是写之后的新值？还有，两个任务同时对一个内存位置进行写的时候，写进去的应当是哪个任务的值？这都是问题。所以需要复杂的编码来做控制。\n  CPU 上下文切换带来新的开销\nCPU 通过对并发任务分配时间片来让各个任务得以执行，而切换任务的时候会带来上下文切换。即将一个任务切换到下一个任务的时候，CPU 会将当前任务的状态保存下来，再去加载下一个任务的状态，这就是一次上下文切换，而这个切换带来的开销并不小。\n  了解了并发以后，接着就有一个问题：那怎么来实现并发呢？这里，就不得不介绍线程的概念，介绍线程又不得不先介绍进程。\n2 什么是进程？什么是线程？ 进程（Process）是一个执行的程序，一个进程由多个线程（Thread）组成，线程是轻量级的进程，线程无法脱离进程单独存在。进程是资源分配的最小单位，进程间相互独立，不共享数据；线程是 CPU 调度的最小单位（即操作系统分配处理器时间的基本单元），线程间共享进程的数据。\n所以，做个比喻：进程就像火车，线程就像车厢。线程无法脱离进程单独运行，就像车厢无法脱离火车单独行进；进程之间相互独立不共享数据而线程之间共享进程的数据，就像不同火车间相互独立不能跨火车共享餐车而同一火车内车厢间可以轻松穿过并共享餐车。\nJava 里边的并发编程其实就是多线程编程。从 Java 应用程序的角度来看，入口 Main 方法启动的就是一个 main 线程，我们可以在 main 线程创建其它的线程，多个线程一起做一些事情，就是并发编程。\n基础概念就介绍到这里，下面就看一下 Java 里边如何使用多线程吧。\n3 Java 线程基础 3.1 创建线程任务的方式 创建 Java 线程任务有三种方式：实现 Runnable 接口、继承 Thread 类，以及实现 Callable 接口。\n实现 Runnable 接口 Java 中最通用的描述线程任务的方法是实现 Runnable 接口并重写其run方法。而线程的启动则需要将任务对象传入Thread对象并调用其start方法来实现。\n如下为使用该方式描述任务并启动线程的示例程序：\npublic class HelloRunnable implements Runnable { public static void main(String[] args) { new Thread(new HelloRunnable()).start(); } @Override public void run() { System.out.println(\u0026#34;Hello from a thread!\u0026#34;); } } 如上代码中，HelloRunnable类实现了Runnable接口，并重写了其run方法。在main方法新建Thread对象，并将HelloRunnable对象作为参数传入，最后调用Thread对象的start方法来启动线程。\n继承 Thread 类 为了方便线程的使用，Thread类本身实现了Runnable接口，所以继承Thread类并重写其run方法也是一种描述任务的方法。而线程的启动则变为直接调用对象的start方法即可。\n如下为使用该方式描述任务并启动线程的示例程序：\npublic class HelloThread extends Thread { public static void main(String[] args) { new HelloThread().start(); } @Override public void run() { System.out.println(\u0026#34;Hello from a thread!\u0026#34;); } } 如上代码中，HelloThread类继承了Thread类，并重写了其run方法。在main方法直接新建HelloThread对象并调用其父类start方法即可启动线程。\n实现 Callable 接口 前两种方法，任务处理完均无法生成返回值。而实现 Callable 接口这种方法就是专为生成返回值设计的一种任务创建方法。使用该方法描述任务时，需要实现Callable接口并重写其call方法，而任务的启动同样需要使用Thread来实现，而为了获取执行结果，中间需要借用一下FutureTask对象，等待结果返回的过程是阻塞的。\n如下为使用该方式描述任务并启动线程的示例程序：\nimport java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.FutureTask; public class HelloCallable implements Callable\u0026lt;String\u0026gt; { public static void main(String[] args) throws ExecutionException, InterruptedException { FutureTask\u0026lt;String\u0026gt; task = new FutureTask\u0026lt;\u0026gt;(new HelloCallable()); new Thread(task).start(); String result = task.get(); System.out.println(result); } @Override public String call() throws Exception { System.out.println(\u0026#34;Hello from a thread!\u0026#34;); return \u0026#34;OK\u0026#34;; } } 如上代码中，HelloCallable类实现了Callable接口，并实现了其call方法。在main方法新建FutureTask对象，并将HelloCallable对象作为参数传入；然后新建Thread对象，将FutureTask对象作为参数传入；最后调用Thread对象的start方法来启动线程。\n了解了线程的创建方法，下面看看Thread类自带的几个线程控制相关的方法。\n3.2 线程控制基础方法    方法 方法类型 功用     yield() Thread 类方法 告诉调度器，当前线程可以让渡 CPU 给其它线程使用了。   sleep() Thread 类方法 让当前线程休眠指定时间   join() Thread 实例方法 等待线程执行完成   interrupt() Thread 实例方法 打断线程的执行   setDaemon() Thread 实例方法 设置是否为守护线程   setName() Thread 实例方法 设置线程名    yield() Thread的类方法，表示当前线程的主要任务已经完成，告诉线程调度器，可以让渡 CPU 给其它的线程使用一会了。\n如下为使用Thread.yield()的一个示例程序：\npublic class HelloYield implements Runnable { public static void main(String[] args) { // 启动两个线程  for (int i = 0; i \u0026lt; 2; i++) { new Thread(new HelloYield()).start(); } } @Override public void run() { for (int i = 0; i \u0026lt; 5; i++) { System.out.println(Thread.currentThread().getName() + \u0026#34;#\u0026#34; + i); // 每循环打印一次，即让渡 CPU 给别的线程  Thread.yield(); } } } 该示例程序的运行结果如下：\nThread-0#0 Thread-1#0 Thread-0#1 Thread-1#1 Thread-0#2 Thread-1#2 Thread-1#3 Thread-0#3 Thread-0#4 Thread-1#4 该示例代码中，HelloYield是一个实现了Runnable接口的线程任务，该任务是一个迭代次数为 5 的循环，每次循环会打印线程名和当前循环编号，并调用一次Thread的yield方法。我们在main线程启动了两个HelloYield线程任务。\n从运行结果可以看到，两个HelloYield线程任务交替打印信息直至执行完毕。\nsleep() Thread的类方法，表示当前线程要休眠一段指定的时间，这段时间不占用 CPU 处理时间，从而别的线程可能会抢占到执行权。休眠的过程中可能被别的线程打断，从而抛出InterruptedException。\n如下为使用Thread.sleep()的一个示例程序：\nimport java.util.concurrent.TimeUnit; public class HelloSleep implements Runnable { public static void main(String[] args) { // 启动两个线程  for (int i = 0; i \u0026lt; 2; i++) { new Thread(new HelloSleep()).start(); } } @Override public void run() { for (int i = 0; i \u0026lt; 5; i++) { System.out.println(Thread.currentThread().getName() + \u0026#34;#\u0026#34; + i); // 休眠 100 毫秒  try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } } } 该示例程序的运行结果如下：\nThread-0#0 Thread-1#0 Thread-0#1 Thread-1#1 Thread-1#2 Thread-0#2 Thread-1#3 Thread-0#3 Thread-1#4 Thread-0#4 该示例代码中，HelloSleep是一个实现了Runnable接口的线程任务，该任务是一个迭代次数为 5 的循环，每次循环会打印线程名和当前循环编号，然后休眠 100 毫秒。我们在main线程启动了两个HelloSleep线程任务。\n从运行结果可以看到，两个HelloSleep线程任务交替打印信息直至执行完毕。\njoin() Thread的实例方法，一个线程可以调用另一个线程的join方法，表示调用join方法的这个线程会被阻塞执行，一直等待被调用join方法的另一个线程执行完毕后再继续执行当前线程。\n如下为使用Thread.join()的一个示例程序：\nimport java.util.concurrent.TimeUnit; public class HelloJoin implements Runnable { public static void main(String[] args) { // 启动线程 t  Thread t = new Thread(new HelloJoin()); t.start(); // 等待 t 线程执行完毕  try { t.join(); } catch (InterruptedException e) { e.printStackTrace(); } // 打印 main 线程信息  System.out.println(\u0026#34;Hello from main Thread!\u0026#34;); } @Override public void run() { for (int i = 0; i \u0026lt; 5; i++) { System.out.println(Thread.currentThread().getName() + \u0026#34;#\u0026#34; + i); // 休眠 100 毫秒  try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } } } 该示例程序的运行结果如下：\nThread-0#0 Thread-0#1 Thread-0#2 Thread-0#3 Thread-0#4 Hello from main Thread! 该示例代码中，HelloJoin是一个实现了Runnable接口的线程任务，该任务是一个迭代次数为 5 的循环，每次循环会打印线程名和当前循环编号，然后休眠 100 毫秒。我们在main线程将HelloJoin线程任务启动后，接着调用其join方法，然后main线程打印一句 Hello 信息。\n从运行结果可以看到，该子线程运行完毕后才打印了main线程的 Hello 信息。\ninterrupt() Thread的实例方法，可以调用其来中断一个线程，但被中断线程并未消亡，只是收到一个提醒。\n如下为使用Thread.interrupt()的一个示例程序：\nimport java.util.concurrent.TimeUnit; public class HelloInterrupt implements Runnable { public static void main(String[] args) { // 启动线程 t  Thread t = new Thread(new HelloInterrupt()); t.start(); // 打断线程 t  t.interrupt(); } @Override public void run() { for (int i = 0; i \u0026lt; 5; i++) { System.out.println(Thread.currentThread().getName() + \u0026#34;#\u0026#34; + i); // 休眠 100 毫秒  try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { System.out.println(\u0026#34;Interrupted by other Thread!\u0026#34;); } } } } 该示例程序的运行结果如下：\nThread-0#0 Interrupted by other Thread! Thread-0#1 Thread-0#2 Thread-0#3 Thread-0#4 该示例代码中，HelloInterrupt是一个实现了Runnable接口的线程任务，该任务是一个迭代次数为 5 的循环，每次循环会打印线程名和当前循环编号，然后休眠 100 毫秒；休眠中若捕获到InterruptedException，则打印一句被中断的信息。我们在main线程将HelloInterrupt线程任务启动后，接着调用其interrupt方法将其打断。\n从运行结果可以看到，该子线程运行过程中捕获到了InterruptedException并打印了被中断信息，但未中止，直至任务完毕才退出执行。\nsetDaemon() Thread的实例方法。Java 线程有用户线程和守护线程两种类型，如果设置为守护线程，那当用户线程结束时，守护线程会跟着结束。需要注意main线程是用户线程。\n如下为使用Thread.setDaemon()的一个示例程序：\nimport java.util.concurrent.TimeUnit; public class HelloDaemon implements Runnable { public static void main(String[] args) { // 启动线程  Thread t = new Thread(new HelloDaemon()); t.setDaemon(true); t.start(); } @Override public void run() { for (int i = 0; i \u0026lt; 5; i++) { System.out.println(Thread.currentThread().getName() + \u0026#34;#\u0026#34; + i); // 休眠 100 毫秒  try { TimeUnit.MILLISECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } } } 该示例代码只是对前面的示例代码稍稍作了一点修改，只是启动时将 Daemon 设置为了 true。可以看到运行该程序将不会打印任何内容，这是因为main线程退出时，Daemon 线程也跟着退出了，没有得到执行。\nsetName() Thread的实例方法，用于给线程设置一个名称。\n如下为使用Thread.setName()的一个示例程序：\npublic class HelloThreadWithName implements Runnable { public static void main(String[] args) { // 启动线程  Thread t = new Thread(new HelloThreadWithName()); t.setName(\u0026#34;HelloThread\u0026#34;); t.start(); } @Override public void run() { for (int i = 0; i \u0026lt; 5; i++) { System.out.println(Thread.currentThread().getName() + \u0026#34;#\u0026#34; + i); } } } 该示例程序的运行结果如下：\nHelloThread#0 HelloThread#1 HelloThread#2 HelloThread#3 HelloThread#4 可以看到，打印的线程名不再是之前的Thread-0这种默认名称了，而变成了我们启动前给线程设置的名称。\n了解了线程的创建方式和基础操作方法后，下面介绍一下多线程下共享资源的争夺问题和解决办法。\n4 共享资源访问控制 4.1 共享资源争夺问题 当多个线程对同一块数据进行操作的时候，可能产生「竞争条件」，出现该现象的根本原因是对数据的操作是非「原子化」的，即前一个线程对数据的操作还未结束，而后一个线程就开始对同样的数据进行操作，这就可能造成数据的结果出现未知的情况。\n下面看一个示例程序：\npublic class EvenGenerator implements Runnable { private int counter = 0; private volatile boolean canceled = false; public static void main(String[] args) { EvenGenerator generator = new EvenGenerator(); // 同时启动 5 个 EvenGenerator 线程任务  for (int i = 0; i \u0026lt; 5; i++) { new Thread(generator).start(); } } // 生成一个偶数  private int generate() { counter++; counter++; return counter; } @Override public void run() { // 无限循环调用 generate 生成 num，若生成的 num 不是偶数，则打印错误信息并退出循环  while (!isCanceled()) { int num = generate(); if (num % 2 != 0) { System.out.printf(\u0026#34;Error occurred, a bad number %d generated!\\n\u0026#34;, num); setCanceled(true); return; } } } public boolean isCanceled() { return canceled; } public void setCanceled(boolean canceled) { this.canceled = canceled; } } 该示例程序中，EvenGenerator是一个偶数生成器，其实现了Runnable接口。EvenGenerator有一个成员变量counter，初始值为 0；EvenGenerator有一个方法generate，每次通过对counter自增两次来生成一个偶数（期望的序列：0，2，4，\u0026hellip;）；EvenGenerator还有一个成员变量canceled，其是一个标记，用于停止所有线程任务的执行，其getters和setters方法分别为isCanceled和setCanceled；EvenGenerator重写了Runnable的run方法，在该方法中，只要canceled为false，就会调用generate方法生成一个数值，如果该数值不是偶数，就会打印错误消息，然后设置canceled为true，并退出while循环。最后在main方法中，实例化了EvenGenerator对象，并启动 5 个线程来同时执行任务，我们期望generate方法生成的数值永远是偶数（0，2，4，\u0026hellip;），该程序永远不会退出，下面就看看运行结果是否跟我们预想的一样？\n该示例程序的运行结果如下：\nError occurred, a bad number 762519 generated! Error occurred, a bad number 1084511 generated! Error occurred, a bad number 1084509 generated! Error occurred, a bad number 1084507 generated! Error occurred, a bad number 807973 generated! 可以看到，该程序运行中生成了奇数，然后退出了。这是为什么呢？这个程序在单线程的情况下是没问题的，而在多线程情况下就会发生共享资源访问问题。因generate方法内对counter变量的两次自增操作并非是一个原子操作，在多个线程对其同时进行调用时，就可能会出现一个线程只进行了一次自增，而正要进行第二次自增时，另一个线程进入，又进行一次自增，这样前一个线程接着进行第二次自增并返回时就会是奇数。\n此外，还需要说明下canceled变量声明时为什么使用了volatile关键字？该关键字是用于解决变量可见性问题的，其能够保证canceled变量被修改后，对各个线程都是可见的。为啥有变量可见性问题呢？这是因为，为了性能考量，每个线程在处理时会将变量从主存拷贝到 CPU 高速缓存，然后进行计算，而当中间某个线程对共享变量值进行更改并写入主存时，其它线程对该更新「不可见」，读取的还是 CPU 高速缓存内的旧值，而volatile关键字就是保证变量的读取与写入都在主存而不是 CPU 缓存，这样各个线程看到的就会是变量的最新值。此外，volatile关键字还可以禁止指令重排，避免多线程下的不一致问题。\n该部分说明了共享资源争夺问题，那么如何进行防范呢？防止「竞争条件」出现的办法就是进行线程同步，即将关键的代码进行加锁，让多个线程排队顺序执行操作。\n那么如何进行加锁以进行线程同步呢？下面就分别介绍一下 synchronized 关键字和 Lock 对象两种常用的线程同步办法。\n4.2 使用 synchronized 关键字进行线程同步 Java 中可使用 synchronized 关键字进行线程同步，即使用 synchronized 关键字修饰的方法或代码块可以保证在同一时刻最多只有一个线程在访问。\nsynchronized 关键字可以修饰实例方法、代码块和静态方法。若修饰的是实例方法，相当于是synchronized(this)，即锁对象为当前实例对象；若修饰的是代码块，锁对象为括号内的对象或类（.class）；若修饰的是静态方法，锁对象为类对象。\n此外，若使用 synchronized 关键字修饰实例方法，锁会作用在整个实例对象。因此，一个类中若有多个实例方法被 synchronized 修饰，调用其中一个被 synchronized 修饰的方法时，整个实例对象会被锁定，此时实例对象的其它 synchronized 方法亦不可被调用，直至前一个调用完成并释放掉锁。\n而使用 synchronized 关键字修饰静态方法时，锁会作用在整个类，所以由该类衍生的所有实例都会使用这同一把锁。\n下面使用 synchronized 关键字对上一步产生「竞争条件」的EvenGenerator代码改造一下，使其满足线程安全的要求。\n改造后的程序代码如下：\npublic class SynchronizedEvenGenerator implements Runnable { private int counter = 0; private volatile boolean canceled = false; public static void main(String[] args) { SynchronizedEvenGenerator generator = new SynchronizedEvenGenerator(); // 同时启动 5 个 EvenGenerator 线程任务  for (int i = 0; i \u0026lt; 5; i++) { new Thread(generator).start(); } } // 生成一个偶数  private synchronized int generate() { counter++; counter++; return counter; } @Override public void run() { // 无限循环调用 generate 生成 num，若生成的 num 不是偶数，则打印错误信息并退出循环  while (!isCanceled()) { int num = generate(); if (num % 2 != 0) { System.out.printf(\u0026#34;Error occurred, a bad number %d generated!\\n\u0026#34;, num); setCanceled(true); return; } } } public boolean isCanceled() { return canceled; } public void setCanceled(boolean canceled) { this.canceled = canceled; } } 运行如上程序，发现不会因并发访问而生成非偶数而自动退出了。\nJava 中除了使用 synchronized 关键字进行线程同步外，还可以使用 Lock 对象进行线程同步。\n4.3 使用 Lock 对象进行线程同步 除了使用 synchronized 关键字进行线程同步外，还可以使用 Lock 对象来进行线程同步。这种方式较 synchronized 方式更加灵活。\n相较于 synchronized 关键字不需要用户去手动释放锁（发生异常或者调用完成后会自动释放锁）而言，Lock 则必须要用户去手动释放锁，如果没有主动释放锁，就有可能出现死锁的情况。\n使用 Lock 对象进行线程同步的通用模式是使用try {} finally {}语句块：\n// 新建锁对象 Lock lock = ...; // 加锁 lock.lock(); try { // 处理任务 } finally { // 解锁  lock.unlock(); } 需要记住的是，加锁后要记得解锁，而且解锁语句需要放在finally {}语句块内，这样不管是正常结束还是发生异常都会保证锁的释放。\n有return语句的话，也建议将其放在try {}语句块内，这样即可保证锁释放前不会将数据暴露给别的任务。\n下面使用 Lock 对象的方式对EvenGenerator代码进行改造，以使其满足线程安全的要求。\n改造后的程序代码如下：\nimport java.util.concurrent.locks.Lock; import java.util.concurrent.locks.ReentrantLock; public class LockedEvenGenerator implements Runnable { private int counter = 0; private volatile boolean canceled = false; private final Lock lock = new ReentrantLock(); public static void main(String[] args) { LockedEvenGenerator generator = new LockedEvenGenerator(); // 同时启动 5 个 EvenGenerator 线程任务  for (int i = 0; i \u0026lt; 5; i++) { new Thread(generator).start(); } } // 生成一个偶数  private int generate() { lock.lock(); try { counter++; counter++; return counter; } finally { lock.unlock(); } } @Override public void run() { // 无限循环调用 generate 生成 num，若生成的 num 不是偶数，则打印错误信息并退出循环  while (!isCanceled()) { int num = generate(); if (num % 2 != 0) { System.out.printf(\u0026#34;Error occurred, a bad number %d generated!\\n\u0026#34;, num); setCanceled(true); return; } } } public boolean isCanceled() { return canceled; } public void setCanceled(boolean canceled) { this.canceled = canceled; } } 运行如上程序，同样不会出现因并发访问而生成非偶数然后自动退出的情况了。\n合理的使用锁机制会保证线程安全，从而解决多线程下共享资源的竞争问题。但锁的使用不当也会造成死锁等问题，下面看一下死锁造成的原因及规避办法。\n综上，本文完成了 Java 并发编程基础知识的整理。\n 参考资料\n[1] Thinking in Java, 4th Edition, Bruce Eckel | Amazon - www.amazon.com\n[2] Core Java Volume I - Fundamentals, 11th Edition, Cay Horstmann | Amazon - www.amazon.com\n[3] Lesson: Concurrency | Java Documentation - docs.oracle.com\n[4] Java Concurrency and Multithreading Tutorial | Jenkov - jenkov.com\n[5] Java 多线程（超详细）| CSDN 博客 - blog.csdn.net\n[6] 并发的基础概念以及优缺点 | CSDN 博客 - blog.csdn.net\n[7] Difference between Concurrency and Parallelism | GeeksforGeeks - www.geeksforgeeks.org\n[8] Concurrency vs Parallelism | Baeldung - www.baeldung.com\n[9] Process vs Thread | Baeldung - www.baeldung.com\n ","permalink":"https://olzhy.github.io/posts/concurrent-programming-with-java.html","tags":["Java"],"title":"Java 并发编程基础"},{"categories":["计算机"],"contents":"上文「如何快速搭建一个 Spring Boot 项目」介绍了使用 Spring Initializr 搭建 Spring Boot 模板项目的方法。本文接着介绍如何使用 Spring Boot 构建一个 RESTful Web 服务，主要关注项目的结构、注解的使用和单元测试代码的编写，并由此探索 Spring Boot 的设计理念与使用方法。\n本文实现的 RESTful Web 服务提供用户（User）的增删改查功能，内部使用 Java ArrayList 实现数据的存储。\n写作本文时，所使用的 JDK 版本、Maven 版本和 Spring Boot 版本分别为：\n JDK 版本：BellSoft Liberica JDK 17 Maven 版本：3.9.2 Spring Boot 版本：3.1.0  1 项目结构 本文使用三层架构代码结构，src/main/java下主要分三个目录：controller、model和service。处理请求和返回响应的逻辑控制器代码需要放在controller目录下；表示数据对象的 POJO 类需要方在model目录下；主要的业务逻辑代码需要抽取到服务中，然后放在service目录下（service目录下是接口类，impl目录下是实现类）。\nspring-boot-restful-service-demo ├─ src/main/java │ └─ com.example.demo │ ├─ controller │ │ └─ UserController.java │ ├─ model │ │ └─ User.java │ ├─ service │ │ ├─ UserService.java │ │ └─ impl │ │ └─ UserServiceImpl.java │ └─ DemoApplication.java ├─ src/test/java │ └─ com.example.demo │ └─ controller │ └─ UserControllerTest.java └─ pom.xml 此外，src/test/java用于存放测试代码，测试代码应与被测试代码使用相同的包名。\n2 源码分析 下面分析下该项目的源码，以期对 Spring Boot 的使用有一个基本的了解。\n2.1 pom.xml 代码 Spring Boot 提供各类封装好的 Starter（以spring-boot-starter-*格式命名）供我们去使用，当需要某项依赖时，直接在pom.xml引用对应的 Starter 即可。\n本文使用 Maven 管理依赖，pom.xml源码如下：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;demo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;demo\u0026lt;/name\u0026gt; \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;17\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-validation\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 可以看到，本文的示例项目使用了三个 Starter：spring-boot-starter-web、spring-boot-starter-validation和spring-boot-starter-test。\n  spring-boot-starter-web包含了编写 Spring Web 程序相关的所有依赖，如编写 RESTful 接口相关的依赖、Spring MVC 相关的依赖、程序的运行时服务器（默认为 Apache Tomcat）相关的依赖等；\n  spring-boot-starter-validation包含了请求参数校验相关的所有依赖；\n  spring-boot-starter-test包含了测试 Spring Boot 程序的所有依赖，如 JUnit Jupiter、Hamcrest 和 Mockito 等。\n  此外，还使用了一个插件spring-boot-maven-plugin，提供了对程序打包和运行的支持。\n2.2 启动类代码 程序入口类src/main/java/com/example/demo/DemoApplication.java的代码如下：\npackage com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 从启动类即可以看到，Spring Boot 应用程序无须web.xml等冗长的配置文件，使用纯 Java 注解的方式即可进行配置。\n可以看到，该类只使用了一个注解：@SpringBootApplication，该注解是一个便捷注解，其包含了如下三个注解：\n @Configuration：用于定义配置类； @EnableAutoConfiguration：用于自动装入应用程序所需的所有 Bean； @ComponentScan：扫描指定路径，将类装配到 Spring 容器中。  2.3 Controller 代码 控制器类src/main/java/com/example/demo/controller/UserController.java的代码如下：\npackage com.example.demo.controller; import com.example.demo.model.User; import com.example.demo.service.UserService; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.http.HttpStatus; import org.springframework.validation.annotation.Validated; import org.springframework.web.bind.annotation.*; import java.util.List; @RestController @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @Autowired private UserService userService; @GetMapping(\u0026#34;/\u0026#34;) public List\u0026lt;User\u0026gt; getUsers() { return userService.getUsers(); } @GetMapping(\u0026#34;/{id}\u0026#34;) public User getUser(@PathVariable(\u0026#34;id\u0026#34;) Integer id) { return userService.getUser(id); } @PostMapping(\u0026#34;/\u0026#34;) @ResponseStatus(HttpStatus.CREATED) public void addUser(@RequestBody @Validated User user) { userService.addUser(user); } @PatchMapping(\u0026#34;/\u0026#34;) @ResponseStatus(HttpStatus.NO_CONTENT) public void updateUser(@RequestBody @Validated User user) { userService.updateUser(user); } @DeleteMapping(\u0026#34;/{id}\u0026#34;) @ResponseStatus(HttpStatus.NO_CONTENT) public void deleteUser(@PathVariable(\u0026#34;id\u0026#34;) Integer id) { userService.deleteUser(id); } } 该类中：\n 注解@RestController相当于是@Controller和@ResponseBody两个注解的结合，表示该类提供 Web 接口，该类中的方法会处理 HTTP 请求并以 JSON 响应客户端； 注解@GetMapping表示方法接收 HTTP GET 请求，而@PostMapping、@PatchMapping和@DeleteMapping分别表示方法接收 POST、PATCH 和 DELETE 请求； 注解@ResponseStatus用于指定方法返回的 HTTP 状态码； 注解@RequestBody用于指定接收 JSON 请求体的对象，@PathVariable用于获取 URL 路径中对应的参数值。  2.4 Service 代码 服务接口类src/main/java/com/example/demo/service/UserService.java的代码如下：\npackage com.example.demo.service; import com.example.demo.model.User; import java.util.List; public interface UserService { List\u0026lt;User\u0026gt; getUsers(); User getUser(Integer id); void addUser(User user); void updateUser(User user); void deleteUser(Integer id); } 服务实现类src/main/java/com/example/demo/service/impl/UserServiceImpl.java的代码如下：\npackage com.example.demo.service.impl; import com.example.demo.model.User; import com.example.demo.service.UserService; import org.springframework.stereotype.Service; import java.util.ArrayList; import java.util.List; @Service public class UserServiceImpl implements UserService { private static final List\u0026lt;User\u0026gt; USERS = new ArrayList\u0026lt;\u0026gt;(); @Override public List\u0026lt;User\u0026gt; getUsers() { return USERS; } @Override public User getUser(Integer id) { int i = findUserIndex(id); if (i \u0026lt; USERS.size()) { return USERS.get(i); } return null; } @Override public void addUser(User user) { USERS.add(user); } @Override public void updateUser(User user) { int i = findUserIndex(user.id()); // update  if (i \u0026lt; USERS.size()) { USERS.set(i, user); } } @Override public void deleteUser(Integer id) { int i = findUserIndex(id); // update  if (i \u0026lt; USERS.size()) { USERS.remove(i); } } private int findUserIndex(Integer userId) { int i = 0; for (; i \u0026lt; USERS.size(); i++) { if (USERS.get(i).id().equals(userId)) { break; } } return i; } } 可以看到，Service 使用ArrayList来存储数据，并提供对 User 增、删、改、查的支持。\n3 程序运行与测试 打开命令行，在程序根目录执行如下 Maven 命令启动应用程序：\nmvn spring-boot:run 程序启动后，命令行执行如下 CURL 命令新建三个 User：\ncurl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;id\u0026#34;: 1, \u0026#34;name\u0026#34;: \u0026#34;Larry\u0026#34;, \u0026#34;age\u0026#34;: 28}\u0026#39; http://localhost:8080/users/ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;id\u0026#34;: 2, \u0026#34;name\u0026#34;: \u0026#34;Lucy\u0026#34;, \u0026#34;age\u0026#34;: 18}\u0026#39; http://localhost:8080/users/ curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;id\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;Jacky\u0026#34;, \u0026#34;age\u0026#34;: 30}\u0026#39; http://localhost:8080/users/ 执行如下 CURL 命令 查询全部 User：\ncurl -X GET http://localhost:8080/users/ [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28},{\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Lucy\u0026#34;,\u0026#34;age\u0026#34;:18},{\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Jacky\u0026#34;,\u0026#34;age\u0026#34;:30}] 执行如下 CURL 命令更新 ID 为 3 的 User：\ncurl -X PATCH -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;id\u0026#34;: 3, \u0026#34;name\u0026#34;: \u0026#34;Alan\u0026#34;, \u0026#34;age\u0026#34;: 29}\u0026#39; http://localhost:8080/users/ 执行如下 CURL 命令查询 ID 为 3 的 User，发现信息已被更新：\ncurl -X GET http://localhost:8080/users/3 {\u0026#34;id\u0026#34;:3,\u0026#34;name\u0026#34;:\u0026#34;Alan\u0026#34;,\u0026#34;age\u0026#34;:29} 执行如下 CURL 命令删除 ID 为 3 的 User：\ncurl -X DELETE http://localhost:8080/users/3 再次查询所有 User，发现 ID 为 3 的 User 已被删除：\ncurl -X GET http://localhost:8080/users/ [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;Larry\u0026#34;,\u0026#34;age\u0026#34;:28},{\u0026#34;id\u0026#34;:2,\u0026#34;name\u0026#34;:\u0026#34;Lucy\u0026#34;,\u0026#34;age\u0026#34;:18}] 4 添加单元测试代码 控制器测试类src/test/java/com/example/demo/controller/UserControllerTest.java的代码如下：\npackage com.example.demo.controller; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.http.MediaType; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.request.MockMvcRequestBuilders; import org.springframework.test.web.servlet.result.MockMvcResultMatchers; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; @SpringBootTest @AutoConfigureMockMvc public class UserControllerTest { @Autowired private MockMvc mvc; @BeforeEach public void createUser() throws Exception { String body = \u0026#34;{\\\u0026#34;id\\\u0026#34;: 1, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;Larry\\\u0026#34;, \\\u0026#34;age\\\u0026#34;: 28}\u0026#34;; mvc.perform(MockMvcRequestBuilders.post(\u0026#34;/users/\u0026#34;).contentType(MediaType.APPLICATION_JSON).content(body)).andExpect(status().isCreated()); } @Test public void testGetUsers() throws Exception { mvc.perform(MockMvcRequestBuilders.get(\u0026#34;/users/\u0026#34;)).andExpect(status().isOk()).andExpect(MockMvcResultMatchers.jsonPath(\u0026#34;$\u0026#34;).isArray()); } @Test public void testGetUser() throws Exception { mvc.perform(MockMvcRequestBuilders.get(\u0026#34;/users/{id}\u0026#34;, 1)).andExpect(status().isOk()).andExpect(MockMvcResultMatchers.jsonPath(\u0026#34;$.name\u0026#34;).value(\u0026#34;Larry\u0026#34;)); } @Test public void testAddUser() throws Exception { String body = \u0026#34;{\\\u0026#34;id\\\u0026#34;: 2, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;Lucy\\\u0026#34;, \\\u0026#34;age\\\u0026#34;: 18}\u0026#34;; mvc.perform(MockMvcRequestBuilders.post(\u0026#34;/users/\u0026#34;).contentType(MediaType.APPLICATION_JSON).content(body)).andExpect(status().isCreated()); } @Test public void testUpdateUser() throws Exception { String body = \u0026#34;{\\\u0026#34;id\\\u0026#34;: 1, \\\u0026#34;name\\\u0026#34;: \\\u0026#34;Larry\\\u0026#34;, \\\u0026#34;age\\\u0026#34;: 29}\u0026#34;; mvc.perform(MockMvcRequestBuilders.patch(\u0026#34;/users/\u0026#34;).contentType(MediaType.APPLICATION_JSON).content(body)).andExpect(status().isNoContent()); } @Test public void testDeleteUser() throws Exception { mvc.perform(MockMvcRequestBuilders.delete(\u0026#34;/users/{id}\u0026#34;, 1)).andExpect(status().isNoContent()); } } 可以看到，如上代码使用MockMvc实现了对UserController的单元测试。\n综上，本文完成了对 Spring Boot RESTful 服务的搭建，了解了 Spring Boot 常用注解的含义和使用方式。本文涉及的完整项目代码已托管至「GitHub」，欢迎关注或 Fork。\n 参考资料\n[1] Building a RESTful Web Service | Spring - spring.io\n[2] Building an Application with Spring Boot | Spring - spring.io\n[3] Spring Initializr | Spring - spring.io\n[4] Spring Boot | Spring - spring.io\n ","permalink":"https://olzhy.github.io/posts/building-a-restful-web-service-with-spring-boot.html","tags":["Java","Spring","Maven"],"title":"如何使用 Spring Boot 构建一个 RESTful Web 服务"},{"categories":["计算机"],"contents":"Spring Boot 可以用最少的配置来快速创建一个独立的、生产级的 Spring 应用程序。\n本文介绍如何快速搭建一个 Spring Boot「Hello World！」项目。\n本文使用的操作系统为 MacOS。另外，写作本文时，所使用的 JDK 版本、Maven 版本和 Spring Boot 版本分别为：\n JDK 版本：BellSoft Liberica JDK 17 Maven 版本：3.9.2 Spring Boot 版本：3.1.0  关于「JDK 的下载与安装」和「Maven 的下载与安装」均非常简单，本文不再赘述。\n1 创建模板项目 浏览器访问「start.spring.io」，使用 Spring Initializr 来创建一个 Spring Boot Web 项目。\n本文的选项如下：\n Project 选择 Maven Language 选择 Java Spring Boot 选择 3.1.0 Packaging 选择 Jar Java 选择 17 Dependencies 勾选 Spring Web  选好以后，点击「Generate」按钮即可以生成项目模板，将 zip 包下载到本地，解压以后即可以使用 IDE 打开了。\n打开以后，可以看到该模板工程的项目结构：\ndemo ├─ src/main/java │ └─ com.example.demo │ └─ DemoApplication.java └─ pom.xml 2 添加代码 下面，将src/main/java/com/example/demo文件夹下的DemoApplication.java文件内容替换为如下内容：\npackage com.example.demo; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @RestController @SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @GetMapping(\u0026#34;/hello\u0026#34;) public String hello(@RequestParam(value = \u0026#34;name\u0026#34;, defaultValue = \u0026#34;World\u0026#34;) String name) { return String.format(\u0026#34;Hello %s!\u0026#34;, name); } } 这就是使用 Spring Boot 搭建一个「Hello World！」Web 服务的全部代码。\n下面解释一下用到的几个注解：\n @RestController告诉 Spring 当前类提供了一个 Web 访问端点； @GetMapping(\u0026quot;/hello\u0026quot;)告诉 Spring 使用hello()方法来响应发送至http://localhost:8080/hello的请求； @RequestParam告诉 Spring 可在请求中为name参数传值（不传的话使用默认值World）。  3 进行测试 下面，使用 Maven 构建并运行程序。\n打开命令行，进入程序根目录，然后使用如下 Maven 命令打包及运行程序：\nmvn clean package mvn spring-boot:run --quiet 程序启动完成后，使用如下 CURL 命令进行测试：\ncurl http://localhost:8080/hello Hello World! curl \u0026#39;http://localhost:8080/hello?name=Larry\u0026#39; Hello Larry! 综上，本文完成了对 Spring Boot 项目的快速搭建，可以看到 Spring Boot 项目非常的简单易用。本文涉及的完整项目代码已托管至「GitHub」，欢迎关注或 Fork。\n 参考资料\n[1] Spring Quickstart Guide | Spring - spring.io\n[2] Spring Initializr | Spring - spring.io\n[3] Spring Boot | Spring - spring.io\n[4] Download Java - OpenJDK Builds for Linux, Windows \u0026amp; macOS | BellSoft - bell-sw.com\n[5] Download Apache Maven | Maven - maven.apache.org\n ","permalink":"https://olzhy.github.io/posts/spring-boot-quick-start.html","tags":["Java","Spring","Maven"],"title":"如何快速搭建一个 Spring Boot 项目"},{"categories":["计算机"],"contents":"前两篇文章「Docker 初探」与「使用 Docker 的几个最佳实践」分别介绍了 Docker 的基本概念与使用方法，以及 Docker 在基础使用上的几个最佳实践。本文接着介绍一下 Docker 在构建安全镜像上的几个最佳实践。\n1 选择正确的基础镜像 选择正确的基础镜像是构建安全镜像的第一步。所谓「正确的基础镜像」就是选择来源可信和满足需求的最小镜像。\n来源可信是指选择可信的镜像仓库（如 Docker Hub）并从中选择可信的发布者（如从 Docker Hub 寻找基础镜像时，优先选择包含「Official Image」或「Verified Publisher」标记的镜像）。\n满足需求的最小镜像是指尽量少包含或不包含无用的依赖项，以降低从依赖项引入漏洞的风险。\n2 使用多阶段构建 多阶段构建是指使用Dockerfile描述镜像构建步骤时，可以定义多个阶段，每个阶段使用不同的基础镜像，做不同的事情。而最后可以选择性的只将某些阶段生成的内容复制到最终阶段，而其它没必要的依赖项并不会带到最终的镜像。这样，多阶段构建不仅保证了最终镜像的轻巧，还避免了无关依赖项带来漏洞的风险。\n3 关注基础镜像更新 定期关注所使用的基础镜像是否针对新发现的漏洞做了更新。若有新的补丁更新，需要对由此衍生出的镜像进行重新构建。\n4 定期做漏洞扫描 定期对镜像进行漏洞扫描。Docker Hub 支持镜像的自动扫描，开启该功能后，每当有镜像推送到仓库就会自动开始漏洞扫描，以检测是否存在已知漏洞。\n综上，本文梳理了使用 Docker 构建安全镜像的几个最佳实践。\n 参考资料\n[1] Docker security best practices | Docker Documentation - docs.docker.com\n ","permalink":"https://olzhy.github.io/posts/docker-security-best-practices.html","tags":["云原生","Docker"],"title":"使用 Docker 构建安全镜像的几个最佳实践"},{"categories":["计算机"],"contents":"上文「Docker 初探」介绍了 Docker 的基本概念与使用方法，本文接着介绍使用 Docker 的几个最佳实践。\n1 镜像瘦身最佳实践 当启动一个容器时，小的容器镜像可以更快的通过网络进行拉取，并且可以更快的加载到内存中。\n下面，列出一些为镜像瘦身的经验：\n  选择适当的基础镜像\n要使用应用程序所需的最小最适当的基础镜像。\n例如：如果应用程序需要 JDK，那么可以考虑使用官方的openjdk基础镜像；而不是使用ubuntu基础镜像，然后在此基础上去安装openjdk。\n  使用多阶段构建\n例如：对于 Maven 管理的 Java 应用程序，可在Dockerfile中编写两个阶段。第一个阶段使用maven镜像，将 Java 源码编译为jar包或war包；第二个阶段使用tomcat镜像，将第一个阶段生成的jar包或war包拷贝到对应位置。这样的话，最终的镜像只有tomcat这个阶段的部分，不会包括编译阶段的环境或拉取的依赖包，只有运行必须的包和环境。\n针对该场景，具体Dockerfile的编写请参考上文「镜像构建最佳实践：利用多阶段构建减小镜像体积 - Docker 初探」。\n  尝试减少镜像的层数\n尝试合并Dockerfile中单独的RUN命令数量来减少镜像的层数。\n如下的两个Dockerfile代码片段，第一个在镜像中创建了两层，而第二个仅创建了一层。\nRUN apt-get -y updateRUN apt-get install -y pythonRUN apt-get -y update \u0026amp;\u0026amp; apt-get install -y python  尝试制作自己的公共基础镜像\n我们自己的项目中，可能有多个镜像有大量相同的部分。可以尝试抽取共同的部分来制作我们自己的公共基础镜像，然后基于该基础镜像来编写各个镜像自定义的部分。这样的话，Docker 只需要加载一次公共层，然后它们就会被缓存下来，而由公共镜像派生的镜像也会加载的更快。\n  2 应用程序数据持久化最佳实践   避免将数据存储在容器的可写层中\n避免使用存储驱动程序将应用程序数据存储在容器的可写层中，这样会增加容器的大小，并且从 I/O 的角度看，效率也低于卷（Volume）或绑定挂载（Bind Mounts）。\n  避免在生产环境使用绑定挂载（Bind Mounts）\n绑定挂载（Bind Mounts）非常适合在开发期间使用，其可以将源代码目录或二进制文件挂载到容器，给我们带来了诸多方便。但对于生产环境，不要使用绑定挂载（Bind Mounts），而代之以卷（Volume）。\n  建议将敏感数据与非敏感数据分开存储\n对于生产环境，建议将敏感数据与非敏感数据分开存储。如：若使用的是 Kubernetes 平台，可使用Secret来存储敏感数据，使用ConfigMap来存储诸如配置文件等非敏感数据；若使用的是 Swarm 平台，使用Secret来存储敏感数据，使用Config来存储非敏感数据。\n  3 镜像稳定性与安全性保障最佳实践 修改应用程序代码并在源码控制系统创建拉取请求（Pull Request）后，建议使用 DevOps CI/CD 流水线自动去构建镜像、测试镜像，并对镜像打标签。\n更进一步，为了保障镜像的稳定性与安全性，部署到生产环境之前，需要开发、测试和安全团队对镜像进行签名。\n4 开发环境与生产环境差异化处理最佳实践   挂载方式的差异化使用\n本地开发环境使用绑定挂载（Bind Mounts）可能更方便（诸如访问源码文件夹）；而生产环境应使用卷（Volume）来代替绑定挂载。\n  Docker 的差异化安装\n本地开发环境安装 Docker 桌面会更方便一些；而生产环境应安装 Docker 引擎并结合用户空间映射，以便将 Docker 进程与主机进程更好的隔离。\n  时间漂移的差异化处理\n本地开发环境不用担心时间漂移的问题；而生产环境应在 Docker 主机和每个容器进程中安装 NTP（Network Time Protocol，网络时间协议）客户端，并将它们的状态同步至同一个 NTP 服务器。使用容器编排平台（Kubernetes 或 Swarm）的话，还需要确保 Node 的时钟与容器的时钟同步到相同的时间源。\n  综上，本文总结了使用 Docker 的几个最佳实践，包括：镜像瘦身最佳实践、应用程序数据持久化最佳实践、镜像稳定性与安全性保障最佳实践，以及开发环境与生产环境差异化处理最佳实践。希望我们在日常工作中使用 Docker 时，能遵循这些最佳实践。\n 参考资料\n[1] Docker development best practices | Docker Documentation - docs.docker.com\n ","permalink":"https://olzhy.github.io/posts/docker-development-best-practices.html","tags":["云原生","Docker"],"title":"使用 Docker 的几个最佳实践"},{"categories":["计算机"],"contents":"上文「一文了解什么是容器」介绍了容器的基本概念，本文接着介绍当前最流行的容器平台 Docker，并对其进行初步使用。\n1 Docker 概览 Docker 是一个用于开发、发布和运行应用程序的开放容器平台。Docker 能够将应用程序与基础架构分离，以便快速交付软件。使用 Docker，我们可以像管理应用程序一样管理基础架构。通过利用 Docker 快速发布、测试与部署代码的方法，我们能够显著提升编写代码与在生产环境运行代码的效率。\n1.1 Docker 平台的能力 Docker 提供在被称为容器的松散隔离环境中打包和运行应用程序的能力。容器是轻量的，其包含运行应用程序所需的一切，所以无须依赖主机当前安装的内容。Docker 的隔离性和安全性允许在同一主机同时运行多个容器。我们还可以在工作中共享容器，且能确保与我们共享容器的每个人获取的容器都能以相同方式工作。\nDocker 提供工具和平台来管理容器的生命周期，包括:\n 使用容器来开发应用程序及其支持组件； 容器称为分发和测试应用程序的单元； 准备就绪后，将应用程序作为容器部署到生成环境而不论生成环境是本地数据中心还是云环境还是混合云。  1.2 Docker 可用来做什么？   应用程序的持续快速交付\nDocker 为开发人员提供了标准的应用程序运行环境，从而简化了软件开发周期。并且，容器非常适合持续集成和持续交付工作流程，为应用程序的持续快速交付提供了保障。\n  响应式部署和扩展\nDocker 的可移植性和轻量性使得工作负载的动态管理（按照业务需要近乎实时的扩展或销毁应用程序）变得容易。\n  同样的硬件上运行更多的工作负载\nDocker 轻量而快速，较虚拟机更经济高效，允许在同样的硬件资源上运行更多的工作负载。\n  1.3 Docker 架构 Docker 使用的是 C/S（客户端-服务器）架构。\n（Docker 架构 - 引用自 Docker Documentation） Docker 客户端和 Docker 守护程序（负责构建、运行和分发 Docker 容器）使用 UNIX 套接字或网络接口之上的 REST API 进行通信。Docker 客户端与 Docker 守护程序可位于同一系统，也可以位于不同的系统上（Docker 客户端可连接远程的 Docker 守护程序）。Docker Compose 也算 Docker 客户端的一种，其允许处理多个容器组成的应用程序。\n  Docker 守护程序\nDocker 守护程序（dockerd）负责监听 Docker API 请求并管理 Docker 对象（镜像、容器、网络和卷等）。Docker 守护程序还可以与其它守护程序进行通信来管理 Docker 服务。\n  Docker 客户端\nDocker 客户端（docker）是与 Docker 交互的主要方式。当使用诸如docker run之类的命令时，Docker 客户端会使用 Docker API 调用守护程序dockerd，守护程序dockerd会处理这些命令。Docker 客户端可与多个守护程序进行通信。\n  Docker 桌面\nDocker 桌面是一个「全家桶」安装包，包含了 Docker 客户端、Docker 守护程序、Docker Compose、Kubernetes 和凭证助手等功能。\n  Docker 镜像仓库\nDocker 镜像仓库用于存储 Docker 镜像。Docker Hub 是一个所有人都可以使用的镜像仓库，也是 Docker 默认的镜像存储仓库。\n  Docker 对象\n我们使用 Docker 时，主要是使用镜像、容器、网络、卷或插件等 Docker 对象，下面会简单介绍下镜像和容器这两个对象。\n  镜像\nDocker 镜像是一个包含命令的创建 Docker 容器的只读模板。通常，一个镜像依赖另一个镜像并有一些额外的定制。\n创建自己的 Docker 镜像时，使用Dockerfile来定义构建与运行镜像的所需步骤。Dockerfile中的每条命令都会在镜像中创建一个层，当修改Dockerfile并重新构建镜像时，只有变化的层会被重新构建，这也是容器镜像比其它虚拟技术更轻量快速的原因。\n  容器\n容器是镜像的运行实例。我们可以使用 Docker 客户端或调用 Docker API 创建、启动、停止、移动或删除容器，可以为容器连接网络，给容器添加存储，甚至可以根据容器当前状态创建一个新镜像。\n默认情况下，容器与其它容器及主机是严格隔离的。当然，容器的网络、存储等与其它容器及主机的隔离程度是可以控制的。\n补充说明：Docker 是使用 Go 语言编写的，且利用了 Linux 内核提供的特性。Docker 使用命名空间技术来提供容器的空间隔离。\n    2 Docker 安装 最直接快速安装 Docker 的方法就是安装 Docker 桌面。本文使用的操作系统为 MacOS，直接从「Docker Desktop for Mac」下载最新的版本，双击运行后「Accept」即可。\n3 Docker 初步使用 3.1 容器化应用程序 下面使用一个Node.js编写的「待办列表」示例应用程序来演示 Docker 的使用。\n开始前，先将代码克隆下来：\ngit clone https://github.com/docker/getting-started.git 然后，可以看到getting-started/app文件夹下有两个子文件夹src和spec，以及一个package.json文件。\ngetting-started ├─ app │ ├─ src/ | ├─ spec/ │ └─ package.json └─ ... 下面，在getting-started/app文件夹下新建一个Dockerfile文件，并为其添加如下内容：\n# syntax=docker/dockerfile:1FROMnode:18-alpineWORKDIR/appCOPY . .RUN yarn install --productionCMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;]EXPOSE3000然后，在getting-started/app文件夹下执行docker build命令来构建镜像：\n# -t 表示给镜像起一个名字 # . 表示在当前文件夹寻找 Dockerfile docker build -t getting-started . 镜像构建完成后，使用docker run命令来启动容器：\n# -d 表示以后台方式运行 # -p 表示使用主机的 3000 端口映射容器的 3000 端口 # getting-started 即是要运行的镜像名 docker run -dp 3000:3000 getting-started 这样，浏览器访问http://localhost:3000即可以看到应用程序了：\n此外，还可以使用docker ps命令来查看容器状态，使用docker stop命令来停止容器，以及对停止的容器使用docker rm来进行移除。\n3.2 镜像推送与分享 下面，尝试将镜像推送到「Docker Hub」。\n开始前，首先需要注册一个 Docker Hub 账号，我的账号为 olzhy。\n接着，使用docker login命令登录到 Docker Hub：\ndocker login 然后，使用docker tag命令将getting-started镜像重命名：\ndocker tag getting-started olzhy/getting-started 最后，使用docker push命令将镜像推送至 Docker Hub：\ndocker push olzhy/getting-started 这样，任何人即可以在安装了 Docker 的机器上使用我们刚刚推送的镜像了：\ndocker run -dp 3000:3000 olzhy/getting-started 3.3 数据库持久化 目前的这个「待办列表」示例应用程序重启后，数据会丢失。这是因为未对数据库进行持久化，下面看一下如何持久化数据库。\n卷（Volume）提供了将容器的特定文件系统路径映射到主机的功能。\n「待办列表」示例应用程序使用的是 SQLite 数据库，其数据存储在文件/etc/todos/todo.db中。\n下面，使用docker volume create命令创建一个卷：\ndocker volume create todo-db 然后，指定挂载的卷，并启动容器：\ndocker run -dp 3000:3000 --mount type=volume,src=todo-db,target=/etc/todos getting-started 启动完成后，增加一些数据。这时，停止并移除上述容器后，再次使用如上命令启动新的容器时，仍可以看到之前添加的数据。\n最后，使用docker volume inspect命令看一下数据到底存到了哪里：\ndocker volume inspect todo-db [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2023-05-21T02:27:07Z\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/todo-db/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;todo-db\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] 挂载点（Mountpoint）显示了数据在主机的具体位置。\n除了使用卷外，还可以使用绑定挂载（Bind Mounts）来将主机的任一文件或文件夹挂载到容器。\n使用方式与卷类似，下面是使用 Bind Mounts 挂载方式运行容器的命令：\ndocker run -dp 3000:3000 --mount type=bind,src=/tmp/todos,target=/etc/todos getting-started # 亦可以直接简化为 -v 方式 docker run -dp 3000:3000 -v /tmp/todos:/etc/todos getting-started 3.4 多容器应用 下面，新建一个 MySQL 数据库容器，然后尝试用「待办列表」容器连接这个数据库。\n两个容器需要使用网络进行通信，首先需要创建网络：\ndocker network create todo-app 接着，运行 MySQL 容器：\n# 可以看到，挂载的时候，未创建卷 todo-mysql-data，这个时候 Docker 会自动帮我们创建 docker run -d \\  --network todo-app --network-alias mysql \\  -v todo-mysql-data:/var/lib/mysql \\  -e MYSQL_ROOT_PASSWORD=secret \\  -e MYSQL_DATABASE=todos \\  mysql:8.0 使用如下命令进入容器，尝试连接数据库并执行数据库命令：\ndocker exec -it \u0026lt;mysql-container-id\u0026gt; mysql -u root -p mysql\u0026gt; SHOW DATABASES; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | todos | +--------------------+ 5 rows in set (0.00 sec) 可以看到，数据库todos已被创建。\n下面，进入getting-started/app文件夹，使用如下命令来启动「待办列表」容器：\ndocker run -dp 3000:3000 \\  -w /app -v \u0026#34;$(pwd):/app\u0026#34; \\  --network todo-app \\  -e MYSQL_HOST=mysql \\  -e MYSQL_USER=root \\  -e MYSQL_PASSWORD=secret \\  -e MYSQL_DB=todos \\  node:18-alpine \\  sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; 访问应用程序，并增加一些条目。\n这时，查看数据库时，发现表里边已经写入了数据：\ndocker exec -it \u0026lt;mysql-container-id\u0026gt; mysql -p todos mysql\u0026gt; select * from todo_items; +--------------------------------------+--------------------+-----------+ | id | name | completed | +--------------------------------------+--------------------+-----------+ | c906ff08-60e6-44e6-8f49-ed56a0853e85 | Do amazing things! | 0 | | 2912a79e-8486-4bc3-a4c5-460793a575ab | Be awesome! | 0 | +--------------------------------------+--------------------+-----------+ 3.5 使用 Docker Compose 上面，启动多个容器时，需要考虑新建网络、启动容器，暴露端口和指定环境变量等一系列步骤。而如果使用 Docker Compose 的话，就会变得很简单。\nDocker Compose 是一个定义多容器应用程序的工具。\n下面，在getting-started/app文件夹下创建一个名为docker-compose.yml的文件。\n然后，将如下内容填充到该文件中：\nservices: app: image: node:18-alpine command: sh -c \u0026#34;yarn install \u0026amp;\u0026amp; yarn run dev\u0026#34; ports: - 3000:3000 working_dir: /app volumes: - ./:/app environment: MYSQL_HOST: mysql MYSQL_USER: root MYSQL_PASSWORD: secret MYSQL_DB: todos mysql: image: mysql:8.0 volumes: - todo-mysql-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: secret MYSQL_DATABASE: todos volumes: todo-mysql-data: 可以看到，该 Compose 文件配置了应用程序和 MySQL 两个服务，配置参数非常的简单明了。\n接着，使用如下命令启动容器：\ndocker compose up -d 使用如下命令查看日志：\ndocker compose logs -f 测试完成后，可使用如下命令移除容器：\n# 若要将 Volume 一并移除，需要加 --volumes 标记 docker compose down 3.6 镜像构建最佳实践 利用镜像分层缓存加快构建速度\n基于Dockerfile进行镜像构建时，一旦某一层发生变化，后面的步骤都需要重新构建。\n看一下前面构建「待办列表」应用程序的Dockerfile文件：\n# syntax=docker/dockerfile:1FROMnode:18-alpineWORKDIR/appCOPY . .RUN yarn install --productionCMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;]其存在几个问题：\n COPY 时，未指定应当忽略的文件夹，node_modules会被拷贝进去； 任何文件有修改时，都需要重新进行yarn install。  下面，在当前文件夹下新建一个.dockerignore文件，并添加如下内容：\nnode_modules 表示 COPY 时，忽略node_modules文件夹。\n接着，对Dockerfile文件进行一下改造，改造后的内容如下：\n# syntax=docker/dockerfile:1FROMnode:18-alpineWORKDIR/appCOPY package.json yarn.lock ./RUN yarn install --productionCOPY . .CMD [\u0026#34;node\u0026#34;, \u0026#34;src/index.js\u0026#34;]改造的思路是：yarn install主要依赖package.json文件，所以将这两步放到一块，这样只要不改package.json这个文件，就不用重新进行yarn install。\n经过改造后，较之前会大大节省镜像的构建时间。\n利用多阶段构建减小镜像体积\n多阶段构建可以将构建时依赖项与运行时依赖项分开，并且可以通过仅提供运行所需的内容来减小镜像的体积。\n下面用两个具体的例子来说明如何进行多阶段构建。\n一个是 Maven/Tomcat 应用程序的例子：当构建一个使用 Maven 管理的 Java 应用程序时，JDK 和 Maven 是必需的；而在运行时，JDK 和 Maven 却不是必需的。这时就可以使用多阶段构建来帮忙了。\n针对该应用程序，使用多阶段构建的Dockerfile文件内容可以是如下这样：\n# syntax=docker/dockerfile:1FROMmaven AS buildWORKDIR/appCOPY . .RUN mvn packageFROMtomcatCOPY --from=build /app/target/file.war /usr/local/tomcat/webapps可以看到，第一个阶段（build）基于Maven环境将 Java 源码编译为一个war包；第二个阶段准备了一个 Tomcat 环境，然后将第一个阶段生成的war包拷贝到对应位置。最终的镜像只有 Tomcat 这个阶段的部分，省去了很多没必要的依赖。\n另一个是 React 应用程序的例子：当构建 React 应用程序时，需要依赖Node.js环境来编译 JSX 源码文件和 SASS 样式文件；而在运行时，可以不依赖Node.js环境，直接使用 Nginx 容器 Serve 这些静态文件即可。\n针对该应用程序，使用多阶段构建的Dockerfile文件内容可以是如下这样：\n# syntax=docker/dockerfile:1FROMnode:18 AS buildWORKDIR/appCOPY package* yarn.lock ./RUN yarn installCOPY public ./publicCOPY src ./srcRUN yarn run buildFROMnginx:alpineCOPY --from=build /app/build /usr/share/nginx/html可以看到，第一个阶段（build）基于Node.js环境将 JSX 源码文件和 SASS 样式文件编译为 HTML、JS 和 CSS 静态文件；第二个阶段仅需要一个 Nginx 环境，然后将第一个阶段生成的静态文件拷贝到对应目录。\n综上，本文完成了对 Docker 的初探。阅读完本文，我们对 Docker 是什么、Docker 能做什么、Docker 的架构是什么样的以及 Docker 怎么使用都有了一个基本的了解。\n 参考资料\n[1] Get started | Docker Documentation - docs.docker.com\n ","permalink":"https://olzhy.github.io/posts/docker-getting-started.html","tags":["云原生","Docker"],"title":"Docker 初探"},{"categories":["计算机"],"contents":"正如运输行业使用集装箱作为标准单元来包装货物以快速装卸与运输一样，软件行业也在越来越多的使用容器作为标准单元来打包应用程序以简化应用程序的迁移。\n（现实生活中的集装箱 - 引用自 Ridge Cloud） 所以容器到底是什么呢？容器就是一个将软件代码和其所有依赖项打包在一起的标准单元。使用容器后，运行在一个计算环境的应用程序，可以快速可靠的运行在另一个计算环境上。\n流行的容器提供商有 Docker、RKT、和 CRI-O。\n1 为什么要使用容器？ 早以前，我们部署应用程序时，需要直接在主机上安装基础库和各项依赖。这样，当应用程序从一个环境迁移到另一个环境时，通常会有无法正常运行的情况。此类问题通常是由不同环境中基础库要求或依赖项不同所引起。\n容器通过提供轻量且不可变的基础设施来打包和部署应用程序以解决这个问题。使用容器时，应用程序以及运行该应用程序所需的一切（包括：代码、配置、依赖包、运行时、系统工具和系统库）均被打包到一起而称为容器镜像。而容器镜像一旦被运行在容器引擎上就变成了运行的容器。\n这样，使用容器后使得应用程序在不同环境间的迁移变得快速且可靠。所以，说「容器革命性的改变了应用程序的开发流程」一点都不为过。\n2 容器与虚拟机有何不同？ 虚拟化是一种将 RAM、CPU、磁盘或网络等系统单一资源虚拟化为多个资源的过程。容器（Containers）与虚拟机（Virtual Machines）都属于虚拟化技术且都具有类似的资源隔离和分配上的优势，但两者最主要的不同点是：容器仅将整个机器虚拟化到了操作系统层，而虚拟机则将其虚拟化到了硬件层。\n（容器与虚拟机在机器上的分层对比 - 引用自 Atlassian）   容器的特点\n容器是在应用层将代码和依赖项打包在一起的抽象。容器引擎（Container Engine）允许多个容器以共享操作系统内核的方式在同一台机器上运行，它们是用户空间中的独立进程。容器较虚拟机占用更少的空间（容器镜像一般占用约几十 MB 的空间）且可处理更多的应用程序。\n容器的优点是体积轻、启动快，只要基础镜像一致，就可以在任何地方运行；缺点是不可在不同操作系统间移植（如：Linux 容器不能运行在 Windows 上）。\n  虚拟机的特点\n虚拟机是将一台服务器变成多台服务器的物理硬件的抽象。虚拟机管理程序（Hypervisor）允许多个虚拟机以共享硬件资源的方式在同一台机器上运行。每个虚拟机都包含了操作系统、应用程序、必要的二进制文件和系统库的完整副本（占用约几十 GB 的空间）。\n虚拟机的优点是允许在同一台机器上运行不同的操作系统；缺点是占用空间大、启动慢。\n  3 容器有哪些使用场景？ 因容器的可靠性、便捷性与可移植性等诸多优点，其已在多个场景出现大量的应用。\n  云原生应用（Cloud-native Applications）\n云原生应用程序依赖容器来实现跨不同环境（包括：公有云、私有云和混合云）的通用操作。低开销和高密度的特点使得许多容器可以运行在同一虚拟机，并使容器成为交付云原生应用程序的理想选择。\n  零修改上云（Lift and Shift Cloud Migration）\n既想上云又不想改造现有应用程序？怎么做呢？最好的办法就是使用容器。\n  批处理（Batch Processing）\n批处理是指在无人干预的情况下使用可用资源完成的活动（如：报告生成、图像尺寸调整和文件格式转换）。因容器可以为批处理提供「用后即焚」的运行环境，所以较静态配置的虚拟机节约了更多成本。\n  机器学习（Machine Learning）\n容器可以使机器学习应用程序相互独立并自由扩展。\n  4 容器编排是什么？ 容器编排是对容器化应用进行自动化管理（包括：调度、扩展和网络通信等）的平台。\n这些自动化管理任务包括：\n 资源分配、配置管理与自动化调度； 可用性监控与自动扩展； 负载均衡与流量转发。  流行的容器编排提供商有 Kubernetes、Docker Swarm 和 Apache Mesos。\n综上，本文对容器是什么进行了基本的介绍。\n 参考资料\n[1] What is a Container? | Docker - www.docker.com\n[2] What is a container? | Microsoft Azure - azure.microsoft.com\n[3] Containers explained: What they are and why you should care? | RedHat - www.redhat.com\n[4] What are Containers and How Do They Work? | Ridge Cloud - www.ridge.co\n[5] What is a Container and How Does it Work? | DevopsCube - devopscube.com\n[6] Containers vs Virtual Machines | Atlassian - www.atlassian.com\n[7] What is container orchestration? | RedHat - www.redhat.com\n ","permalink":"https://olzhy.github.io/posts/what-is-a-container.html","tags":["云原生"],"title":"一文了解什么是容器"},{"categories":["计算机"],"contents":"OWASP（Open Worldwide Application Security Project，开放全球应用程序安全项目）是一个致力于提高软件安全性的非营利性组织，其提供 Web 应用程序安全领域的标准、工具和指导手册，被业界大量的企业作为权威来参考。\nOWASP Top 10 是 OWASP 组织定期更新的一份风险报告，其由世界各地的安全专家整理而成，重点关注 Web 应用程序安全领域的 10 个最关键的风险或漏洞。这些风险或漏洞是所有企业都应当重视和规避的。\n写作本文时，OWASP Top 10 的最新版本是 2021，本文将对其列出的 10 大风险作一一介绍。\n1 失效的访问控制 失效的访问控制为收集的数据集中名列第一的风险。需要注意的 CWE（Common Weakness Enumerations，通用缺陷列表）有：将敏感信息暴露给未经授权的参与者（CWE-200）、将敏感信息插入到发送数据（CWE-201）和跨站请求伪造（CWE-352）。\n有效的访问控制是保证用户只能执行所拥有权限内的操作，不能执行所拥有权限外的任何操作。失效的访问控制通常会造成未经授权的信息泄露。\n常见的漏洞包括：\n 违反最小权限原则或默认拒绝原则，即访问权限应只授予特定的角色，但实际上任何人都可以访问； 通过修改 URL 或 HTML 页面来绕过访问控制检查； 知道了对方的 ID，即可以查看或编辑他人的账户； API 没有对 POST、PUT 和 DELETE 方法进行有效的访问控制； 特权提升，即在未登录的情况下假扮特定用户，或在以普通用户身份登录时假扮管理员； 元数据操作，诸如篡改或操纵 JWT（JSON Web Token）访问控制令牌、Cookie 以及隐藏字段来进行特权提升； CORS（Cross-origin Resource Sharing，跨域资源共享）错误配置允许来自不可信来源的 API 访问； 以未经身份验证的用户身份浏览需要身份验证的页面或以标准用户身份浏览特权用户页面。  预防措施：\n须在服务端进行有效的访问控制，从而保证攻击者无法绕过检查或篡改数据。\n 除公共资源外，一律默认拒绝访问； 实现统一的访问控制机制并在整个应用程序中进行重用，减少跨域资源共享 (CORS) 的使用； 访问控制模型应细粒度控制用户与记录的权限，而不应让用户对任何记录都可以进行增删改查； 独特的应用程序业务限制需求应由域模型来进行强化； 禁用 Web 服务器目录查看并确保 Web 根目录不包含文件元数据（如 .git）和备份文件； 在日志中记录失效的访问控制，并在超过一定重复次数时向管理员告警； 对 API 调用进行速率限制，以减少自动化攻击工具的危害； 登出时有状态会话标识应当在服务器端失效；对于无状态的 JWT，登出时应遵循 OAuth 标准来撤销访问权限。  此外，开发人员与 QA（Quality Assurance，质量保证）人员应在单元测试和集成测试中对访问控制进行功能测试。\n2 加密机制失效 加密机制失效为收集的数据集中名列第二的风险，以前被称为「敏感数据泄漏」，但这种描述更像是问题的表现而不是根本原因，根因是与密码学相关的加密机制失效。需要注意的 CWE 有：使用硬编码的密码（CWE-259）、失效或有风险的加密算法（CWE-327）和信息熵不充分（CWE-331）。\n首先需要确认：对于传输数据和存储数据都有哪些保护需求。如密码、信用卡号、健康记录、个人信息和商业秘密一般需要额外的保护。\n对于这些数据，需要确认：\n 传输过程中是否使用了明文？除了需要严格避免其在外部网络明文传输之外，在内网依然需要验证其在各负载均衡器之间、Web 服务器之间以及后端系统之间是否使用了明文传输？ 代码中是否使用了弱的加密算法或传输协议？ 收到的服务器证书和信任链是否经过有效验证？ 是否仍在使用 MD5 或 SHA1 等已弃用的哈希函数，或者在需要加密哈希函数时是否使用了非加密哈希函数？ 是否仍在使用已弃用的加密填充方法，例如 PKCS v1.5？  预防措施：\n 对被应用程序处理的、存储的或传输的数据进行分类，并根据法律法规要求或业务需求确定哪些数据是敏感数据； 非必要情况下不存储敏感数据或者适时对敏感数据进行清除； 确保加密存储所有的敏感数据； 确保使用了最新的、最强的算法、协议和密钥，并对密钥进行妥善的管理； 使用安全协议（如 TLS，Transport Layer Security，传输层安全）来传输数据； 禁止对包含敏感数据的响应进行缓存； 根据数据的分类进行所需的安全控制； 不要使用 FTP（File Transfer Protocol，文件传输协议） 和 SMTP（Simple Mail Transfer Protocol，简单邮件传输协议）等旧协议来传输敏感数据； 使用具有工作因子的强自适应和加盐哈希函数（如 Argon2、scrypt、bcrypt 或 PBKDF2）来存储密码； 始终使用经过身份验证的加密，而不仅仅是加密； 密钥应以加密方式随机生成，并作为字节数组存储在内存中；如果使用密码，则必须通过适当的密码基密钥派生函数将其转换为密钥； 确保在适当的地方使用加密随机性，并且它没有以可预测的方式或低熵播种； 避免使用弃用的加密函数和填充方案，例如 MD5、SHA1、PKCS v1.5； 独立验证配置和设置的有效性。  3 注入 注入为收集的数据集中名列第三的风险。需要注意的 CWE 有：跨站点脚本（CWE-79）、SQL 注入（CWE-89）和文件名或路径的外部控制（CWE-73）。\n应用程序在如下情况易受到攻击：\n 应用程序未对用户提供的数据进行校验、过滤和清洗； 动态查询或无上下文感知转义的非参数化调用直接在解释器中使用； 在 ORM（Object Relational Mapping，对象关系映射）搜索参数中使用恶意数据来提取额外的敏感记录； 直接使用或连接恶意数据，SQL（Structured Query Language，结构化查询语言）或命令包含动态查询、命令或存储过程中的结构和恶意数据。  常见的注入有：SQL 注入、NoSQL（Not Only SQL，非结构化查询语言）注入、OS（Operating System，操作系统）命令注入、ORM 注入、LDAP（Lightweight Directory Access Protocol， 轻型目录访问协议）注入、EL（Expression Language，表达式语言）注入和 OGNL（Object Graph Navigation Language，对象图导航语言）注入。\nCode Review（源代码审查）是检测应用程序是否易受注入威胁的最佳方法。建议将请求参数、请求头、Cookie、JSON 请求体、SOAP 数据体和 XML 数据体进行自动化测试。此外，还建议在 CI/CD（Continuous Integration and Continuous Delivery，持续集成和持续交付）流水线中加入 SAST（Static Application Security Testing，静态应用安全测试）、DAST（Dynamic Application Security Testing，动态应用安全测试）和 IAST（Interactive Application Security Testing，交互式应用安全测试）等多种安全测试来提前发现可能的注入风险。\n预防措施：\n 首选方案是使用安全的 API（避免完全使用解释器、提供参数化接口、规范使用 ORM 工具）； 对于任何残留的动态查询，对解释器使用特定的转义语法来转义特殊字符； 在查询中使用LIMIT或其它 SQL 控件，以防止在 SQL 注入的情况下大量泄露记录。  4 不安全的设计 这是 2021 版的一个新类别，侧重于设计和架构相关的风险，呼吁更多的使用威胁建模、安全设计模式和参考架构。需要注意的 CWE 有：生成包含敏感信息的错误消息（CWE-209）、凭证存储保护不足（CWE-256）、违反信任边界（CWE-501）和凭证保护不足（CWE-522）。\n预防措施：\n 与应用程序安全专家一起建立安全的开发生命周期以评估和设计与安全和隐私相关的控制措施； 使用安全设计模式库或使用安全组件； 在身份验证、访问控制和关键流程或业务逻辑中使用威胁建模； 将安全控制集成到用户故事中； 从前端到后端在应用程序的每一层集成合理性检查； 编写单元测试和集成测试以验证所有关键流程是否都能抵抗威胁模型； 根据暴露需求和保护需求，在系统层和网络层上分离层级； 在所有层上进行租户隔离； 限制用户和服务的资源消耗。  5 安全配置错误 因安全配置错误引起的风险较之前呈上升的趋势。需要注意的 CWE 有：配置（CWE-16）、XML 外部实体引用限制不当（CWE-611）。\n易受到攻击的配置：\n 应用程序栈各部分缺少适当的安全加固，或者云服务的权限配置不当； 启用或安装了不必要的特性（如：开放了不必要的端口、服务、页面、账户或权限）； 仍在使用默认账号和密码； 暴露给用户堆栈信息或其它大量的错误信息； 对于升级的系统，最新的安全功能被禁用或未安全配置； 应用程序服务器、应用程序框架（例如 Struts、Spring、ASP.NET）、库或者数据库等的安全设置配置的值不正确。  如果没有一致的、可重复的应用程序安全配置过程，系统就会面临更高的风险。\n预防措施：\n 可重复的加固过程可以快速轻松地部署一个安全的环境，如使用自动化流水线来部署开发、QA 和生产环境； 应部署一个没有任何不必要特性、组件、文档和示例的最小平台； 作为补丁管理流程的一部分，审查和更新适用于所有安全说明、更新和补丁的配置的任务； 使用分段应用程序架构在组件或租户之间进行分段、容器化以提供有效且安全的分离； 向客户端发送安全指令，例如安全头； 使用自动化流水线来验证所有环境中的配置和设置的有效性。  6 自带缺陷和过时的组件 自带缺陷和过时的组件是一个已知问题。需要注意的 CWE 有：使用未维护的第三方组件（CWE-1104）。\n若存在如下情形，则易受到攻击：\n 不知道所使用的所有组件的版本（客户端和服务器端），包括直接使用的组件以及嵌套的依赖项； 软件易受攻击、不受支持或已过时，这包括操作系统、应用程序服务器、数据库管理系统、应用程序、API 和所有组件、运行时环境和库； 未定期扫描漏洞或未订阅所使用组件的安全公告； 未基于风险的方式及时修复或升级底层平台、框架和依赖项； 软件开发人员不测试更新、升级或修补库的兼容性。  预防措施：\n 删除未使用的依赖项、不必要的功能、组件、文件和文档； 使用 OWASP 依赖性检查、retire.js 等工具持续清点客户端和服务器端组件及其依赖项的版本，并检查是否存在漏洞； 仅通过安全链接从官方来获取组件； 监控已不再维护的组件。  7 身份识别和验证失效 之前被称为「无效的身份验证（Broken Authentication）」。需要注意的 CWE 有：含主机不匹配的证书验证不当（CWE-297）、身份验证不当（CWE-287）和会话固定（CWE-384）。\n确认用户身份、身份验证和会话管理对于防治与身份验证相关的攻击至关重要。\n若存在如下情形，则易受到攻击：\n 允许像是攻击者已经拥有有效用户名和密码列表的试错自动化攻击； 允许蛮力自动攻击； 允许默认密码、弱密码或众所周知的密码，如「123456」； 使用薄弱的忘记密码恢复过程，例如设置「大众皆知的问题和答案」； 使用纯文本或弱哈希密码数据存储； 完全没有或使用了不是很有效的多重身份验证； 在 URL 中暴露会话 ID； 成功登录后重用之前旧的会话标识符； 登出后未将会话 ID 失效。  预防措施：\n 在可能的情况下，实施多因素身份验证以防止自动凭据填充、暴力破解和被盗凭据重用攻击； 不要带着默认凭据进行部署，特别是对于管理员用户； 实施弱密码检查，如测试用户设置的密码是否在 10000 个常用密码清单中； 增大密码长度、增强复杂性并制定定期更换策略； 对所有校验结果（如注册时和恢复时）使用相同的消息以防止账户枚举攻击； 对多次失败登录进行记录并在检测到暴力破解时提醒管理员； 服务器端在用户登录后生成一个新的具有高熵的随机会话 ID，且防止将其暴露到 URL，并在注销时将其失效。  8 软件和数据完整性故障 这是 2021 版的一个新类别，侧重于在不验证完整性的情况下进行软件更新的风险。需要注意的 CWE 有：包含不受信任控制范围的功能（CWE-829）、下载未经完整性检查的代码（CWE-494）和不可信数据的反序列化（CWE-502）。\n若存在如下情形，则易受到攻击：\n 应用程序依赖来自不受信任的来源提供的插件、库或模块； 不安全的 CI/CD 流水线带来未经授权的访问或引入了恶意代码； 应用程序包含自动更新功能，且在没有充分完整性验证的情况下进行下载更新； 对象或数据被编码或序列化为攻击者可以看到和修改的结构，从而易受到不安全反序列化的影响。  预防措施：\n 使用数字签名或类似机制来验证软件或数据来自预期来源； 确保库和依赖项（例如 npm 或 Maven）使用了受信任的存储库； 使用 OWASP Dependency Check 或 OWASP CycloneDX 等安全工具来验证组件不包含已知漏洞； 对代码和配置更改进行审查，以最大限度地减少恶意代码被引入的可能性； 确保 CI/CD 流水线具有恰当的访问控制，以确保代码的完整性； 确保未签名或未加密的序列化数据不会发送到不受信任的客户端。  9 安全日志和监控故障 此类别旨在帮助检测与记录可能的违规行为，没有日志记录和监控，就无法检测到违规行为。需要注意的 CWE 有：日志伪造漏洞（CWE-117）、遗漏安全相关信息（CWE-223）和将敏感信息插入日志文件（CWE-532）。\n若存在如下情形，则易受到攻击：\n 审计性事件未被记录，如登录、登录失败和大额交易； 警告或错误信息不清晰； 未对 API 日志中的可疑活动进行监控； 日志仅存储在本地； 未设置适当的报警阈值； 无法实时对攻击进行告警。  预防措施：\n 确保所有登录、访问控制和服务器端输入验证失败被合理的记录，从而识别可疑的行为； 确保输出的日志清晰明了以能被监控系统轻松捕捉； 确保日志数据正确编码，以防止对日志记录或监控系统进行注入或攻击； 确保高额交易带有审计跟踪，以防止篡改或删除； DevSecOps 团队应该建立有效的监控和警报，以便检测到可疑活动时能快速做出响应； 制定事件响应和恢复计划。  10 服务端请求伪造 Web 应用程序在获取远程资源时没有验证用户所提供的 URL，就会出现服务端请求伪造（Server-Side Request Forgery，SSRF）风险。它允许攻击者即使受到防火墙、VPN 或 ACL (Access Control List，网络访问控制列表) 的保护下，强制应用程序也能发送一个精心构造的请求到一个意想不到的目的地。\n预防措施：\n  网络层\n 为远程资源访问功能设置单独的网段，以减少 SSRF 的影响； 应用「默认拒绝」防火墙规则，以阻止除必要的内部网络通信外的所有通信流量。    应用层\n 清洗和验证所有客户端输入； 使用白名单列表控制放行 URL 的模式、端口和目标地址； 不要向客户端发送原始响应； 禁用 HTTP 重定向； 注意 URL 一致性以避免 DNS 重新绑定和「检查时间/使用时间」(TOCTOU) 竞争条件等攻击； 不要使用黑名单或正则表达式来缓解 SSRF，攻击者拥有绕过拒绝列表的技能。    其它方面\n 不要在前端系统上部署与安全相关的服务（如 OpenID），控制这些系统上的本地流量（如 localhost）； 对于专用前端用户，可在独立系统上使用网络加密技术（如 VPN）来满足高安全保护需求。    综上，本文首先介绍了什么是 OWASP，然后根据 2021 版的 OWASP Top 10 详细介绍了当前 Web 应用程序面临的十个排名最高的风险点。\n 参考资料\n[1] OWASP | Wikipedia - en.wikipedia.org\n[2] OWASP Top Ten | OWASP Foundation - owasp.org\n[3] What is OWASP? What is the OWASP Top 10? | Cloudflare - www.cloudflare.com\n[4] OWASP Top 10 2021 全新出炉 | 郑州市网络安全协会 - www.zzwa.org.cn\n[5] OWASP—Top10（2021 知识总结）| CSDN 博客 - blog.csdn.net\n ","permalink":"https://olzhy.github.io/posts/owasp-top-ten.html","tags":["网络安全","架构设计"],"title":"什么是 OWASP Top 10？"},{"categories":["计算机"],"contents":"前两篇文章「Selenium WebDriver 基础使用」和「Selenium WebDriver 高级特性使用」分别介绍了 Selenium WebDriver 的基础功能和高级功能的使用。这两篇文章更多的是从底层实现细节的角度去练习 Selenium WebDriver API 的使用。\n本文将探讨「构建一个 Selenium 自动化测试项目的最佳实践是什么样的？」，该部分更多的是从上层设计与架构的角度自顶向下来思考一个大型测试项目的构建。包括：编码前有什么准备工作？有没有一个基本的指导思想。如何编排测试代码？如何根据情况使用适当的定位器？下面会一一讨论。\n本文涉及的所有示例程序均使用 Python 语言描述。下面列出本文所使用的 Python 版本、Selenium 版本和 Chrome 浏览器版本信息。\n Python 版本：3.11.3 Selenium 版本：4.9.1 Chrome 浏览器版本：113  1 编码前的准备工作与基本指导思想 测试一个网站就是针对该网站测试场景的一次项目开发，所以项目开发中的理念与思想可以借鉴过来。接到测试需求后，不要一开始就陷入按钮、字段、下拉框等页面元素怎么操作的技术细节当中，而要站在最终用户的角度去分析这个测试需求的交互逻辑和依赖关系，从而将其拆解为一个个相对独立的测试用例。而对于每一个测试用例，并不是每一步都必须使用 Selenium 去自动化实现，而是要根据实际情况将关键的部分自动化，其它非关键的部分可以通过植入数据或调用 API 来实现。\n拆解完成后，应该如何编排测试代码？下面将探讨这个问题。\n2 如何编排测试代码？ 如何编排测试代码？即采用何种策略组织与编排测试代码，从而让代码更简洁且更好维护。\n我们依然使用实际的例子来说明将要探讨的问题。\n下面是一个「Selenium Web 表单示例页面」的自动化测试动图：\n可以看到，该动图展示的自动化测试代码对表单页面进行了文本输入、密码输入、下拉框选项选择和日期输入，并点击了提交按钮，最后跳转至已提交页面。\n对应动图原始的 Python 测试代码（original_form_test.py）如下：\nfrom unittest import TestCase from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.select import Select from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC class TestForm(TestCase): def setUp(self) -\u0026gt; None: self.driver = webdriver.Chrome() self.addCleanup(self.driver.quit) def test_web_form(self) -\u0026gt; None: # 打开表单页面 self.driver.get(\u0026#39;https://www.selenium.dev/selenium/web/web-form.html\u0026#39;) self.assertEqual(self.driver.title, \u0026#39;Web form\u0026#39;) # Text 输入 text_input_elem = self.driver.find_element(By.ID, \u0026#39;my-text-id\u0026#39;) text_input_elem.send_keys(\u0026#39;Selenium\u0026#39;) # Password 输入 password_elem = self.driver.find_element(By.NAME, \u0026#39;my-password\u0026#39;) password_elem.send_keys(\u0026#39;Selenium\u0026#39;) # Dropdown 选择 dropdown_elem = Select(self.driver.find_element(By.NAME, \u0026#39;my-select\u0026#39;)) dropdown_elem.select_by_value(\u0026#39;2\u0026#39;) # 日期输入 date_input_elem = self.driver.find_element(By.XPATH, \u0026#39;//input[@name=\u0026#34;my-date\u0026#34;]\u0026#39;) date_input_elem.send_keys(\u0026#39;05/10/2023\u0026#39;) # 点击 Submit 按钮 submit_button_elem = self.driver.find_element(By.XPATH, \u0026#39;//button[@type=\u0026#34;submit\u0026#34;]\u0026#39;) submit_button_elem.click() # 等待进入已提交页面 WebDriverWait(self.driver, 10).until(EC.title_is(\u0026#39;Web form - target page\u0026#39;)) # 断言 message = self.driver.find_element(By.ID, \u0026#39;message\u0026#39;).text self.assertEqual(message, \u0026#39;Received!\u0026#39;) 可以看到，这是一种常见的最直接的测试代码编写方式。\n然而这种写法存在几个问题：\n  测试代码（assertEqual）和定位与操作元素的代码（find_element ... send_keys ... click）耦合在一起。这样，如果元素定位标识发生变化或者元素操作方式发生变化，这块测试代码都需要修改。\n  如果要编写针对该页面本身或者依赖该页面的其它测试代码，定位与操作元素的代码都需要重写一遍。\n  要解决如上问题，须从代码结构与代码编排上着手，即引入一种设计模式 —— 页面对象模型。\n页面对象模型（Page Object Model）借鉴了面向对象编程的思想，是一种在自动化测试中被广泛使用的设计模式，用于减少重复代码并增加代码的可维护性。页面对象是一个面向对象的类，其将同一页面的 Web 元素存储在同一个对象中；当需要与该对象的 UI 进行交互时，不直接访问该页面的 Web 元素，而通过调用该对象提供的方法来实现。这样做的好处是如果某个页面的 UI 发生了改变，测试代码无须更改，只需要更改对应页面对象内的代码即可。\n总结一下，使用页面对象模型的好处包括：\n 测试代码与特定于页面的代码分离（增加了简洁性）； 页面元素和功能被封装在页面对象的属性和方法中，而不是让其分散在整个测试代码中（减少了重复代码并增加了可维护性）。  下面就使用页面对象模型设计一下测试项目的目录结构：\n$ tree . ├─ pages │ ├─ form.py │ └─ form_target.py └─ optimized_form_test.py 可以看到，针对各个页面的页面对象被放在pages目录下，编写测试用例时调用其对应的方法即可。\n下面看一下优化后的代码。\nForm页面对象代码（form.py）：\nfrom selenium.webdriver.common.by import By from selenium.webdriver.support.select import Select from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from pages.form_target import FormTarget from typing import Self class Form: def __init__(self, driver) -\u0026gt; None: self.driver = driver def open(self) -\u0026gt; Self: self.driver.get(\u0026#39;https://www.selenium.dev/selenium/web/web-form.html\u0026#39;) return self def get_title(self) -\u0026gt; str: return self.driver.title def input_text(self, text: str) -\u0026gt; Self: elem = self.driver.find_element(By.ID, \u0026#39;my-text-id\u0026#39;) elem.send_keys(text) return self def input_password(self, password: str) -\u0026gt; Self: elem = self.driver.find_element(By.NAME, \u0026#39;my-password\u0026#39;) elem.send_keys(password) return self def select_from_dropdown(self, value: str) -\u0026gt; Self: elem = Select(self.driver.find_element(By.NAME, \u0026#39;my-select\u0026#39;)) elem.select_by_value(value) return self def input_date(self, date: str) -\u0026gt; Self: elem = self.driver.find_element(By.XPATH, \u0026#39;//input[@name=\u0026#34;my-date\u0026#34;]\u0026#39;) elem.send_keys(date) return self def submit(self) -\u0026gt; FormTarget: elem = self.driver.find_element(By.XPATH, \u0026#39;//button[@type=\u0026#34;submit\u0026#34;]\u0026#39;) elem.click() # 等待进入已提交页面 WebDriverWait(self.driver, 10).until(EC.title_is(\u0026#39;Web form - target page\u0026#39;)) # 返回 FormTarget 对象 return FormTarget(self.driver) FormTarget页面对象代码（form_target.py）：\nfrom selenium.webdriver.common.by import By class FormTarget: def __init__(self, driver) -\u0026gt; None: self.driver = driver def get_message_text(self) -\u0026gt; str: return self.driver.find_element(By.ID, \u0026#39;message\u0026#39;).text 使用如上两个页面对象后的测试代码（optimized_form_test.py）：\nfrom unittest import TestCase from selenium import webdriver from pages.form import Form class TestForm(TestCase): def setUp(self) -\u0026gt; None: self.driver = webdriver.Chrome() self.addCleanup(self.driver.quit) def test_web_form(self) -\u0026gt; None: # 打开表单页面 form_page = Form(self.driver) form_page.open() self.assertEqual(form_page.get_title(), \u0026#39;Web form\u0026#39;) # 输入 form_target_page = form_page.input_text(\u0026#39;Selenium\u0026#39;) \\ .input_password(\u0026#39;Selenium\u0026#39;) \\ .select_from_dropdown(\u0026#39;2\u0026#39;) \\ .input_date(\u0026#39;05/10/2023\u0026#39;) \\ .submit() # 断言 message = form_target_page.get_message_text() self.assertEqual(message, \u0026#39;Received!\u0026#39;) 可以看到，经过优化后的代码清晰了许多。\n下面总结一下使用页面对象模型时的几个注意事项：\n 断言是测试逻辑的一部分，应放在测试代码中，因此，页面对象中不应有断言或验证相关的代码； 页面对象只应将页面提供的服务通过公共方法暴露出来，其它内部细节不要暴露出来。  接下来关注一下实现细节，定位器的使用是编写 Selenium 测试代码时大量涉及的工作。下面看一下定位器使用相关的最佳实践。\n3 如何根据情况使用适当的定位器？ 前文「Selenium WebDriver 基础使用」中介绍了 Selenium 有 8 种基本的元素定位方法。而什么时候使用什么样的定位器呢？下面给出其最佳实践。\nid定位器为首选定位方法，准确快速；若元素没有id，则使用css 选择器；此两种不可用，再选择xpath 定位器（相对前两种性能较差）；一般在页面上会有多个相同 tag 的元素，所以tag 定位器一般用于选择一组元素。\n综上，本文介绍了构建一个大型测试项目时分析需求的基本指导思想、编排测试代码的实践策略以及使用定位器的推荐顺序。本文涉及的所有代码均已上传至本人 GitHub，欢迎关注。期待阅读完本文，我们对 Selenium 自动化测试从需求分析、编码实现到实现细节上都有了一个可以参考的规范。\n 参考资料\n[1] Test Practices | Selenium - www.selenium.dev\n ","permalink":"https://olzhy.github.io/posts/selenium-best-practices.html","tags":["Selenium","自动化测试","架构设计","Python"],"title":"Selenium 自动化测试最佳实践"},{"categories":["随笔"],"contents":"鐵西公園\n\n庫佈齊沙漠\n\n\n\n\n烏蘭木倫河\n\n\n父母做的燉牛肉和燉羊肉\n\n\n\n舅家吃的燉羊肉和豬肉燴酸菜\n\n飯店吃的莜麵和刀削麵\n\n\n","permalink":"https://olzhy.github.io/posts/ordos.html","tags":["随笔"],"title":"五一 · 鄂爾多斯之行"},{"categories":["计算机"],"contents":"上文「Selenium WebDriver 基础使用」介绍了 Selenium WebDriver 基础功能的使用；本文将接着介绍 Selenium WebDriver 高级特性的使用，涉及页面加载策略、等待策略、元素定位与操作、浏览器操作。\n本文涉及的所有示例程序均使用 Python 语言描述。此外，下面还列出了本文所使用的浏览器和 Selenium 版本信息。\n 浏览器：Chrome Selenium 版本：4.9.0  1 页面加载策略 Selenium WebDriver 的浏览器选项有三种页面加载策略可供选择，它们是：normal、eager和none。\n了解它们代表什么之前，先介绍一下加载及渲染一个 Web 页面大概有哪些阶段。\n按事件分的话，一个网页的生命周期主要有DOMContentLoaded、load、beforeunload和unload这几个阶段。\n  DOMContentLoaded\nHTML 文档已加载完成，DOM 树已构建完成，但依赖的脚本、图片、样式表、iFrame 等外部资源可能还没有加载完成。\n  load\n不仅 HTML 文档已加载完成，依赖的脚本、图片、样式表、iFrame 等外部资源均已加载完成。\n  beforeunload\n用户离开前的前置事件。\n  unload\n用户已经离开。\n  按document.readyState分的话，只有loading、interactive和complete这三个阶段。\n  loading\nHTML 文档仍在加载。\n  interactive\nHTML 文档已加载并解析完成，但依赖的脚本、图片、样式表、iFrame 等外部资源可能还没有加载完成。\n  complete\nHTML 文档以及依赖的脚本、图片、样式表、iFrame 等外部资源均已加载完成。\n  下图将这两种方式组合到一起来看一下一个网页的生命周期：\n可以看到，事件里的DOMContentLoaded对应document.readyState里的interactive；事件里的load对应document.readyState里的complete。\n而 Selenium WebDriver 支持的三种加载策略与事件和document.readyState的对应关系如下表所示：\n   Selenium 页面加载策略 对应的事件 对应的document.readyState     normal（默认值） load complete   eager DOMContentLoaded interactive   none 无 Any（任何状态都可以）    可以看到，当访问一个 URL 时，Selenium WebDriver 的默认策略是等待整个页面全部加载完成（除了使用JavaScript在load事件后再动态添加内容）。在编写自动化测试用例时，如果测试逻辑不依赖外部资源的加载，即可以将页面加载策略从默认选项normal改为eager或none来加速测试过程。\n更改 Selenium WebDriver 页面加载策略的示例 Python 代码（page_load_strategy.py）如下：\nfrom selenium import webdriver from selenium.webdriver.chrome.options import Options options = Options() options.page_load_strategy = \u0026#39;eager\u0026#39; # \u0026#39;none\u0026#39;, \u0026#39;normal\u0026#39; driver = webdriver.Chrome(options=options) driver.get(\u0026#39;http://www.baidu.com\u0026#39;) driver.quit() 2 等待策略 通俗点讲，WebDriver 是一个告诉浏览器做什么的库。因 Web 页面具有一定的异步特性，且 WebDriver 不会实时跟踪 DOM 的状态；所以，有些情况下，定位元素时，可能会出现「no such element」错误。\n下面看一段代码（no_such_element.py）：\nfrom selenium import webdriver from selenium.webdriver import Keys from selenium.webdriver.common.by import By driver = webdriver.Chrome() driver.get(\u0026#39;https://www.baidu.com/\u0026#39;) text_input = driver.find_element(By.ID, \u0026#39;kw\u0026#39;) text_input.send_keys(\u0026#39;Selenium\u0026#39; + Keys.RETURN) # 会抛出 NoSuchElementException first_result_title = driver.find_element(By.XPATH, \u0026#39;//div[@id=\u0026#34;content_left\u0026#34;]/div[1]/h3\u0026#39;).text print(first_result_title) driver.quit() 这段代码打开了百度首页，然后键入关键字Selenium后回车进行搜索，接着即找第一个结果的标题进行打印。运行该代码时，会抛出NoSuchElementException，原因是定位元素的时候，搜索结果页面还没有完全打开，因此未找到对应的元素。\n遇到这样的问题怎么办呢？可以通过 Selenium WebDriver 提供的显式等待或隐式等待功能来解决。\n2.1 显式等待 显式等待，即程序暂停执行直至传递的条件满足。显式等待非常适合被用来做 WebDriver 与 DOM 的状态同步。\n上面抛出「no such element」错误的代码（no_such_element.py）可使用显式等待的方式改造为（explicit_wait.py）：\nfrom selenium import webdriver from selenium.webdriver import Keys from selenium.webdriver.common.by import By from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC driver = webdriver.Chrome() driver.get(\u0026#39;https://www.baidu.com/\u0026#39;) text_input = driver.find_element(By.ID, \u0026#39;kw\u0026#39;) text_input.send_keys(\u0026#39;Selenium\u0026#39; + Keys.RETURN) # 等待搜索结果展示 WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, \u0026#39;content_left\u0026#39;))) # 不会抛出异常 first_result_title = driver.find_element(By.XPATH, \u0026#39;//div[@id=\u0026#34;content_left\u0026#34;]/div[1]/h3\u0026#39;).text print(first_result_title) driver.quit() 可以看到，我们新建了一个WebDriverWait对象（指定了超时时间），并使用expected_conditions.presence_of_element_located()方法为其设置了跳出条件。除此方法外，expected_conditions包下常用的方法还有expected_conditions.url_contains()与expected_conditions.title_is()等。\n2.2 隐式等待 隐式等待是告诉 WebDriver 在查找元素时，若不存在，即轮询 DOM 一段时间。其一般在新建 WebDriver 时设置，对整个会话有效。\n上面抛出「no such element」错误的代码（no_such_element.py）可使用隐式等待的方式改造为（implicit_wait.py）：\nfrom selenium import webdriver from selenium.webdriver import Keys from selenium.webdriver.common.by import By driver = webdriver.Chrome() # 设置隐式等待时间 driver.implicitly_wait(10) driver.get(\u0026#39;https://www.baidu.com/\u0026#39;) text_input = driver.find_element(By.ID, \u0026#39;kw\u0026#39;) text_input.send_keys(\u0026#39;Selenium\u0026#39; + Keys.RETURN) # 不会抛出异常 first_result_title = driver.find_element(By.XPATH, \u0026#39;//div[@id=\u0026#34;content_left\u0026#34;]/div[1]/h3\u0026#39;).text print(first_result_title) driver.quit() 真实的测试场景，一般只建议使用显式等待。\n3 元素定位与操作 定位与操作 DOM 中的元素是使用 Selenium 编写自动化测试用例的主要工作。\n3.1 元素定位 Selenium WebDriver 提供 8 种基本的元素定位方法。\n   定位方法 描述     id 查找 id 属性与搜索值匹配的元素   name 查找 name 属性与搜索值匹配的元素   class name 查找 class 名包含搜索值的元素   css selector 查找与 CSS 选择器匹配的元素   link text 查找其可见文本与搜索值匹配的锚元素   partial link text 查找其可见文本包含搜索值的锚元素。如有多个，则仅选择第一个元素。   tag name 查找 tag 名与搜索值匹配的元素   xpath 查找与 XPath 表达式匹配的元素    如下为百度搜索框 input 标签的 HTML 代码：\n\u0026lt;input id=\u0026#34;kw\u0026#34; name=\u0026#34;wd\u0026#34; class=\u0026#34;s_ipt\u0026#34; maxlength=\u0026#34;255\u0026#34; autocomplete=\u0026#34;off\u0026#34; /\u0026gt; 可使用如下几种方式来定位到该 input 元素：\ndriver.find_element(By.ID, \u0026#39;kw\u0026#39;) driver.find_element(By.CLASS_NAME, \u0026#39;s_ipt\u0026#39;) driver.find_element(By.NAME, \u0026#39;wd\u0026#39;) driver.find_element(By.XPATH, \u0026#39;//input[@name=\u0026#34;wd\u0026#34;]\u0026#39;) 此外，Selenium 在版本 4 引入了相对定位器，即可使用空间相对位置来定位一个元素，其可在传统定位器无法描述时使用。\n如下为 Selenium 官网提供的一个「Web 表单示例页面」：\n可以看到，在该页面左侧部分Text input输入框下有一个Password输入框。\n这两个输入框的 HTML 代码如下：\n\u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;my-text\u0026#34; id=\u0026#34;my-text-id\u0026#34; /\u0026gt; ... \u0026lt;input type=\u0026#34;password\u0026#34; name=\u0026#34;my-password\u0026#34; autocomplete=\u0026#34;off\u0026#34; /\u0026gt; 若Password输入框采用传统方法不好定位，则可使用相对定位器来定位：\npassword_locator = locate_with(By.TAG_NAME, \u0026#39;input\u0026#39;).below({By.ID: \u0026#39;my-text-id\u0026#39;}) driver.find_element(password_locator) 3.2 元素操作 Selenium 提供 4 个基本的元素操作命令。它们是：\n Click Send Keys Clear Select  下面使用一个例子来演示如何使用这几个命令。\n如下为百度关键字输入框和「百度一下」搜索按钮的 HTML 代码：\n\u0026lt;input id=\u0026#34;kw\u0026#34; name=\u0026#34;wd\u0026#34; class=\u0026#34;s_ipt\u0026#34; maxlength=\u0026#34;255\u0026#34; autocomplete=\u0026#34;off\u0026#34; /\u0026gt; ... \u0026lt;input type=\u0026#34;submit\u0026#34; id=\u0026#34;su\u0026#34; value=\u0026#34;百度一下\u0026#34; class=\u0026#34;bg s_btn\u0026#34; /\u0026gt; 可使用如下命令进行关键字清除、键入关键字和点击搜索按钮操作：\ninput_text = driver.find_element(By.ID, \u0026#39;kw\u0026#39;) input_text.clear() input_text.send_keys(\u0026#39;Selenium\u0026#39;) driver.find_element(By.ID, \u0026#39;su\u0026#39;).click() 关于 Select 命令的使用，同样使用 Selenium 官网的「Web 表单示例页面」作示例。\n该页面上的Dropdown (select)是一个单选框，其 HTML 代码如下：\n\u0026lt;select class=\u0026#34;form-select\u0026#34; name=\u0026#34;my-select\u0026#34;\u0026gt; \u0026lt;option selected=\u0026#34;\u0026#34;\u0026gt;Open this select menu\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;1\u0026#34;\u0026gt;One\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;2\u0026#34;\u0026gt;Two\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;3\u0026#34;\u0026gt;Three\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; 使用Select对象选择下拉选项的 Python 代码如下：\ndropdown = Select(driver.find_element(By.NAME, \u0026#39;my-select\u0026#39;)) dropdown.select_by_value(\u0026#39;2\u0026#39;) 4 浏览器操作 4.1 导航操作 进行浏览器导航操作的 Python 代码如下：\n# 打开网址 driver.get(\u0026#39;https://selenium.dev\u0026#39;) # 点击向后按钮 driver.back() # 点击向前按钮 driver.forward() # 点击刷新按钮 driver.refresh() 4.2 原生弹窗操作 可使用 Selenium WebDriver 来与三种原生的消息弹窗（Alert、Confirm 和 Prompt）交互。\n下面，先看一下用于演示这三种弹窗的 HTML 代码（alerts-test.html）：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Alerts, Prompts and Confirmations test\u0026lt;/title\u0026gt; \u0026lt;script\u0026gt; function exampleAlert() { alert(\u0026#34;This is an example alert\u0026#34;); } function exampleConfirm() { let confirmed = confirm(\u0026#34;Do you want to confirm?\u0026#34;); document.getElementById(\u0026#34;confirmed\u0026#34;).innerText = confirmed; } function examplePrompt() { let favoriteSport = prompt( \u0026#34;What is your favorite sport?\u0026#34;, \u0026#34;Basketball\u0026#34; ); document.getElementById(\u0026#34;favorite-sport\u0026#34;).innerText = favoriteSport; } \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;table border=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a onclick=\u0026#34;exampleAlert()\u0026#34;\u0026gt;Click to see an example alert\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt; \u0026lt;a onclick=\u0026#34;exampleConfirm()\u0026#34;\u0026gt;Click to see an example confirm\u0026lt;/a\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;p id=\u0026#34;confirmed\u0026#34;\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;\u0026lt;a onclick=\u0026#34;examplePrompt()\u0026#34;\u0026gt;Click to see an example prompt\u0026lt;/a\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;\u0026lt;p id=\u0026#34;favorite-sport\u0026#34;\u0026gt;\u0026lt;/p\u0026gt;\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 接着，看一下测试如上 HTML 页面三种弹窗的 Python 代码（alerts_test.py）：\nfrom unittest import TestCase from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC class TestAlerts(TestCase): def setUp(self) -\u0026gt; None: self.driver = webdriver.Chrome() self.addCleanup(self.driver.quit) def test_alert(self) -\u0026gt; None: # 打开 Alerts 示例页面 self.driver.get(\u0026#39;file:///Users/larry/Desktop/alerts-test.html\u0026#39;) # 点击超链接 \u0026#34;Click to see an example alert\u0026#34; self.driver.find_element(By.LINK_TEXT, \u0026#39;Click to see an example alert\u0026#39;).click() # 等待窗口弹出，获取 Alert 信息，点击 OK alert = WebDriverWait(self.driver, 10).until(EC.alert_is_present()) alert_message = alert.text alert.accept() # 断言 self.assertEqual(alert_message, \u0026#39;This is an example alert\u0026#39;) def test_confirm(self) -\u0026gt; None: # 打开 Alerts 示例页面 self.driver.get(\u0026#39;file:///Users/larry/Desktop/alerts-test.html\u0026#39;) # 点击超链接 \u0026#34;Click to see an example confirm\u0026#34; self.driver.find_element(By.LINK_TEXT, \u0026#39;Click to see an example confirm\u0026#39;).click() # 等待窗口弹出，点击 OK alert = WebDriverWait(self.driver, 10).until(EC.alert_is_present()) alert.accept() # 获取 `#confirmed` 文本 confirmed = self.driver.find_element(By.ID, \u0026#39;confirmed\u0026#39;).text # 断言 self.assertEqual(confirmed, \u0026#39;true\u0026#39;) def test_prompt(self) -\u0026gt; None: # 打开 Alerts 示例页面 self.driver.get(\u0026#39;file:///Users/larry/Desktop/alerts-test.html\u0026#39;) # 点击超链接 \u0026#34;Click to see an example prompt\u0026#34; self.driver.find_element(By.LINK_TEXT, \u0026#39;Click to see an example prompt\u0026#39;).click() # 等待窗口弹出，输入信息，点击 OK alert = WebDriverWait(self.driver, 10).until(EC.alert_is_present()) alert.send_keys(\u0026#39;Football\u0026#39;) alert.accept() # 获取 `#favorite-sport` 文本 favorite_sport = self.driver.find_element(By.ID, \u0026#39;favorite-sport\u0026#39;).text # 断言 self.assertEqual(favorite_sport, \u0026#39;Football\u0026#39;) 4.3 Cookie 操作 可使用 Selenium WebDriver 来操作 Cookie。\n查询、添加和删除 Cookie 的示例 Python 代码如下：\nfrom selenium import webdriver driver = webdriver.Chrome() # 打开 URL driver.get(\u0026#39;https://www.baidu.com\u0026#39;) # 将 Cookie 添加到当前浏览器 driver.add_cookie({\u0026#39;name\u0026#39;: \u0026#39;foo\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;bar\u0026#39;}) # 获取所有的 Cookie print(driver.get_cookies()) # 获取名为 foo 的 Cookie 信息 print(driver.get_cookie(\u0026#39;foo\u0026#39;)) # 删除名为 foo 的 Cookie 信息 driver.delete_cookie(\u0026#39;foo\u0026#39;) # 删除所有的 Cookie driver.delete_all_cookies() driver.quit() 4.4 窗口与选项卡操作 可使用 Selenium WebDriver 来打开、关闭和切换窗口或选项卡。\n操作窗口或选项卡的示例 Python 代码如下：\n# 获取所有的窗口或选项卡句柄 driver.window_handles # 获取当前窗口或选项卡的句柄 driver.current_window_handle # 切换窗口或选项卡 driver.switch_to.window(handle) # 新建窗口 driver.switch_to.new_window(\u0026#39;window\u0026#39;) # 新建选项卡 driver.switch_to.new_window(\u0026#39;tab\u0026#39;) # 关闭当前窗口或选项卡 driver.close() 综上，本文使用 Python 示例代码介绍了如何使用 Selenium WebDriver 对页面加载策略、等待策略、元素定位与操作、浏览器操作这些高级特性进行使用。\n 参考资料\n[1] WebDriver | Selenium - www.selenium.dev\n[2] Page: DOMContentLoaded, load, beforeunload, unload | The Modern JavaScript Tutorial - javascript.info\n[3] 重新認識 JavaScript 番外篇之網頁的生命週期 | iT 邦幫忙 - ithelp.ithome.com.tw\n[4] Document: readyState property | MDN - developer.mozilla.org\n ","permalink":"https://olzhy.github.io/posts/selenium-webdriver-advanced-features.html","tags":["自动化测试","Selenium","Python"],"title":"Selenium WebDriver 高级特性使用"},{"categories":["计算机"],"contents":"「Selenium」是一个支持 Web 浏览器自动化的开源项目，可使用其来模拟用户与浏览器的一系列交互行为。\n本文分两个部分：首先会介绍一下 Selenium 的组成部分；接着会使用一个实际的例子介绍 WebDriver 如何使用。\n整个过程中涉及的代码示例，均使用 Python 语言描述。此外，下面还列出了本文所使用的操作系统、浏览器和 Selenium 版本信息。\n 操作系统：MacOS 浏览器：Chrome Selenium 版本：4.9.0  1 Selenium 组成部分 开始使用 Selenium 前，需要了解一下一个自动化测试过程涉及的几个主要组成部分。它们是 WebDriver（Selenium 提供的针对各个语言的浏览器操作库）、Driver（浏览器驱动）和 Browser（浏览器）。\n这三个部分的交互过程如下图所示：\n可以看到，WebDriver 通过 Driver 来与 Browser 进行双向通信。即 WebDriver 通过 Driver 传递指令给 Browser；然后 WebDriver 再由 Driver 接收 Browser 的响应信息。\n需要说明的是：该图展示的情形中， WebDriver 与 Browser（及 Driver） 位于同一主机。但使用 Selenium Grid 后，WebDriver 可与 Browser（及 Driver）位于不同的主机。关于 Selenium Grid 是什么，以及如何搭建及使用，请参考我之前所写的一篇文章「Selenium Grid 搭建及使用」。\n2 WebDriver 基础使用 了解了 WebDriver 是做什么的以及其如何与浏览器进行交互后，接着开始对 WebDriver 进行基础使用。\n2.1 安装 Driver 由上面「Selenium 组成部分」知道，WebDriver 必须通过 Driver 来与 Browser 进行交互。所以，使用 WebDriver 操作浏览器前，需要先安装对应浏览器的 Driver。\nSelenium 支持的所有主流浏览器中，除了 Internet Explorer 以外，其它浏览器的 Driver 都是由浏览器厂商自己提供的。因本文使用 Chrome 浏览器作演示，所以下面仅介绍 ChromeDriver 的下载及安装过程。\n进入「ChromeDriver 官方下载页面」，下载与您机器上 Chrome 版本对应的 ChromeDriver。\n下载及安装命令如下：\ncurl -O https://chromedriver.storage.googleapis.com/112.0.5615.49/chromedriver_mac64.zip unzip chromedriver_mac64.zip sudo mkdir /usr/local/chromedriver sudo mv chromedriver /usr/local/chromedriver/ 编辑/etc/profile，将chromedriver所属文件夹添加到PATH。\nsudo vi /etc/profile # chromedriver export PATH=$PATH:/usr/local/chromedriver/ 这样，尝试执行下chromedriver命令，即可看到 ChromeDriver 启动成功的信息，说明 Driver 已安装成功。\nsource /etc/profile chromedriver ... ChromeDriver was started successfully. 2.2 WebDriver 基础使用 下面，开始对 WebDriver 进行基础使用。主要学习如何使用 WebDriver 创建浏览器对象、打开页面、定位元素、输入内容和点击按钮等操作。\n用到的页面为 Selenium 官网提供的一个「Web Form 示例页面」。\n该页面包含文本输入框、下拉框、文件上传框、日期选择框等。页面截图如下：\n接下来，针对该页面里的表单，编写一个 Python 测试用例（selenium_form_test.py）来进行输入和提交。\n代码如下：\nfrom unittest import TestCase from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.support.select import Select class TestSeleniumForm(TestCase): def setUp(self) -\u0026gt; None: # 无痕模式的 Chrome options = webdriver.ChromeOptions() options.add_argument(\u0026#39;--incognito\u0026#39;) self.browser = webdriver.Chrome(options=options) self.addCleanup(self.browser.quit) def test_web_form(self) -\u0026gt; None: # 打开表单页面 self.browser.get(\u0026#39;https://www.selenium.dev/selenium/web/web-form.html\u0026#39;) self.assertEqual(self.browser.title, \u0026#39;Web form\u0026#39;) # Text 输入 text_input = self.browser.find_element(By.ID, \u0026#39;my-text-id\u0026#39;) text_input.send_keys(\u0026#39;Selenium\u0026#39;) # Password 输入 password = self.browser.find_element(By.NAME, \u0026#39;my-password\u0026#39;) password.send_keys(\u0026#39;Selenium\u0026#39;) # Dropdown 选择 Two dropdown = Select(self.browser.find_element(By.NAME, \u0026#39;my-select\u0026#39;)) dropdown.select_by_value(\u0026#39;2\u0026#39;) # 选择文件 file_input = self.browser.find_element(By.CSS_SELECTOR, \u0026#39;input[name=\u0026#34;my-file\u0026#34;]\u0026#39;) file_input.send_keys(\u0026#39;/tmp/file.txt\u0026#39;) # 日期选择 date_input = self.browser.find_element(By.XPATH, \u0026#39;//input[@name=\u0026#34;my-date\u0026#34;]\u0026#39;) date_input.send_keys(\u0026#39;04/21/2023\u0026#39;) # 点击 Submit 按钮 submit_button = self.browser.find_element(By.XPATH, \u0026#39;//button[@type=\u0026#34;submit\u0026#34;]\u0026#39;) submit_button.click() # 等待进入已提交页面 WebDriverWait(self.browser, 10).until(EC.title_is(\u0026#39;Web form - target page\u0026#39;)) # 断言 message = self.browser.find_element(By.ID, \u0026#39;message\u0026#39;).text self.assertEqual(message, \u0026#39;Received!\u0026#39;) 如上 Python 代码即用到了 Selenium 包，运行代码前需要先安装一下selenium模块。\n安装命令如下：\n# 国内为了下载速度，选择了清华的 PyPI 源 python3 -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple selenium 安装完成后，即可运行该测试文件（selenium_form_test.py）了。\n运行命令及结果如下：\npython3 -m unittest selenium_form_test.TestSeleniumForm . ---------------------------------------------------------------------- Ran 1 test in 21.740s OK 运行效果如下图所示：\n可以看到，我们使用 WebDriver 实现了对页面表单的自动化输入与提交。\n下面总结一下，我们所使用 WebDriver 的几个关键方法。\n  browser 实例的创建\n可以指定参数来创建一个特定浏览器实例。\nself.browser = webdriver.Chrome(options=options)   页面打开\n可以使用get方法来打开一个 URL。\nself.browser.get(\u0026#39;https://www.selenium.dev/selenium/web/web-form.html\u0026#39;)   元素定位\n可以使用 ID、NAME、CSS 选择器、XPATH 等多种方式定位页面元素。\nself.browser.find_element(By.ID, \u0026#39;my-text-id\u0026#39;) self.browser.find_element(By.NAME, \u0026#39;my-password\u0026#39;) self.browser.find_element(By.CSS_SELECTOR, \u0026#39;input[name=\u0026#34;my-file\u0026#34;]\u0026#39;) self.browser.find_element(By.XPATH, \u0026#39;//input[@name=\u0026#34;my-date\u0026#34;]\u0026#39;)   对元素进行操作\n可以对元素进行输入或点击操作。\ntext_input.send_keys(\u0026#39;Selenium\u0026#39;) submit_button.click()   等待页面元素出现\n可以使用WebDriverWait来等待页面的某个元素出现。\nWebDriverWait(self.browser, 10).until(EC.title_is(\u0026#39;Web form - target page\u0026#39;))   浏览器对象的销毁\n最后需要调用quit方法来关闭浏览器窗口，释放资源。\nself.browser.quit()   综上，本文首先介绍了 Selenium 测试的组成部分；Driver 的安装；最后，通过 Python 代码编写了一个自动提交表单的示例程序，学习了 WebDriver 的基础使用。本文涉及的代码已托管至我的 GitHub，欢迎有需要的同学关注或 Fork！\n 参考资料\n[1] WebDriver | Selenium - www.selenium.dev\n[2] ChromeDriver | WebDriver for Chrome - chromedriver.chromium.org\n ","permalink":"https://olzhy.github.io/posts/selenium-webdriver.html","tags":["自动化测试","Selenium","Python"],"title":"Selenium WebDriver 基础使用"},{"categories":["随笔"],"contents":"春天來了，北方的春天來的比較晚一些，但春意一樣盎然。走在新生的花草樹木之間，能感受到一種萬物復蘇的活力。以前母親對我說「人隨春意泰」，真實不虛。\n\n\n\n\n\n\n","permalink":"https://olzhy.github.io/posts/spring-is-here.html","tags":["随笔"],"title":"春天來了"},{"categories":["计算机"],"contents":"Minikube 用于在本地搭建 Kubernetes 环境，为我们学习与实践 Kubernetes 提供了方便。\n开始安装 Minikube 前，需要确保所使用的机器满足如下要求：\n 至少 2 个 CPU 至少 2GB 可用内存 至少 20GB 可用硬盘存储 已连接互联网 至少安装了下列容器或虚拟机管理软件中的一种，如：Docker、QEMU、Podman、VirtualBox 或 VMware Workstation。  本文所使用的操作系统为 MacOS，安装的容器管理软件为 Podman。下面即开始详述 MacOS 上 Podman 的安装、Minikube 的安装以及 Minikube 的初步使用。\n1 Podman 安装 类似于大名鼎鼎的 Docker，Podman 也是一个容器引擎，可使用其来构建容器镜像、运行和管理容器。关于 Podman 的介绍与使用方法，请参阅本人之前的一篇文章「容器引擎 Podman 初探」。\nMacOS 上可使用如下brew install命令安装 Podman。\nbrew install podman 安装完成后，使用如下命令启动 Podman。这样即可以使用了。\npodman machine init podman machine start 2 Minikube 安装与使用 2.1 安装 使用如下brew install命令安装 Minikube。\nbrew install minikube 安装完成后，使用如下命令指定驱动为 Podman，并启动 Minikube。\nminikube start --driver=podman 启动完成后，即可以使用kubectl命令与 Minikube 集群进行交互了。\n2.2 使用 下面使用kubectl将一个 Nginx 应用部署到 Minikube。\n应用如下 Deployment 配置部署一个 Nginx 应用：\nkubectl apply -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 heredoc\u0026gt; EOF 使用如下kubectl expose命令暴露流量到外部：\nkubectl expose deployment nginx --type=NodePort --port=80 运行如下minikube service命令，会打开一个浏览器窗口，可以看到 Nginx 首页成功显示：\nminikube service nginx 2.3 管理 查询插件列表：\nminikube addons list 启用某个插件：\nminikube addons enable ingress 停止 Minikube 集群：\nminikube stop 综上，探索了 MacOS 上 Minikube 的安装与初步使用。\n 参考资料\n[1] Hello Minikube | Kubernetes - kubernetes.io\n[2] minikube start | minikube - minikube.sigs.k8s.io\n[3] Getting Started with Podman | Podman - podman.io\n ","permalink":"https://olzhy.github.io/posts/minikube-getting-started.html","tags":["Kubernetes","云原生"],"title":"MacOS 上 Minikube 的安装与使用"},{"categories":["计算机"],"contents":"本文探讨如何在 Python Flask 框架中使用 GitHub Auth 做授权登录。即一个 Flask 应用，如何集成第三方的 GitHub 账号系统来做登录。\n1 申请 GitHub Auth 应用 开始前，需要先「申请一个 GitHub Auth 应用」。申请完成后，记录好生成的 CLIENT_ID 和 CLIENT_SECRET。\nGitHub Auth 应用申请页面如下图所示：\n2 使用 Authlib 包集成 GitHub Auth GitHub Auth 是标准的 OAuth2 认证实现，如下 Python Flask 程序使用了 Authlib 包。\n该程序有四个页面，分别为：\n  首页（index）\n根据 Session 信息，判断用户是否已登录，若已登录，则显示欢迎信息；未登录，则显示登录链接。\n  登录页面（login）\n页面会跳转至 GitHub 认证页。\n  GitHub 回调页面（callback）\nGitHub 登录并授权后，接收 Code 并获取 Token，然后根据 Token 请求用户信息并写入 Session，最后跳转至首页。\n  登出页面（logout）\n清除 Session 信息。\n  from flask import Flask, url_for, redirect, session from authlib.integrations.flask_client import OAuth import os app = Flask(__name__) app.secret_key = \u0026#39;812848ea396c6aa794e6b6c9\u0026#39; github = OAuth(app).register( name=\u0026#39;github\u0026#39;, client_id=os.getenv(\u0026#39;CLIENT_ID\u0026#39;), client_secret=os.getenv(\u0026#39;CLIENT_SECRET\u0026#39;), access_token_url=\u0026#39;https://github.com/login/oauth/access_token\u0026#39;, access_token_params=None, authorize_url=\u0026#39;https://github.com/login/oauth/authorize\u0026#39;, authorize_params=None, api_base_url=\u0026#39;https://api.github.com/\u0026#39;, client_kwargs={\u0026#39;scope\u0026#39;: \u0026#39;user:email\u0026#39;}, ) @app.route(\u0026#39;/\u0026#39;) def index(): user = session.get(\u0026#39;user_email\u0026#39;) if user is None: login_url = url_for(\u0026#39;login\u0026#39;, _external=True) return f\u0026#39;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;{login_url}\u0026#34;\u0026gt;Login\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#39; logout_url = url_for(\u0026#39;logout\u0026#39;, _external=True) return f\u0026#39;\u0026lt;p\u0026gt;Welcome {user}! | \u0026lt;a href=\u0026#34;{logout_url}\u0026#34;\u0026gt;Logout\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#39; @app.route(\u0026#39;/login\u0026#39;) def login(): callback_uri = url_for(\u0026#39;callback\u0026#39;, _external=True) return github.authorize_redirect(callback_uri) @app.route(\u0026#39;/callback\u0026#39;) def callback(): token = github.authorize_access_token() resp = github.get(\u0026#39;user\u0026#39;, token=token) profile = resp.json() session[\u0026#39;user_email\u0026#39;] = profile[\u0026#39;email\u0026#39;] return redirect(\u0026#39;/\u0026#39;) @app.route(\u0026#39;/logout\u0026#39;) def logout(): session.pop(\u0026#39;user_email\u0026#39;, None) return redirect(\u0026#39;/\u0026#39;) if \u0026#39;__main__\u0026#39; == __name__: app.run(debug=True) 3 程序运行和验证 使用如下命令安装依赖包、指定 CLIENT_ID 和 CLIENT_SECRET 变量值并运行程序。\npython3 -m pip install -r requirements.txt export CLIENT_ID=XXX export CLIENT_SECRET=XXX python3 app.py 程序启动完成后，浏览器访问http://localhost:5000。\n登录过程如下图所示：\n综上，本文探索了如何在 Python Flask 框架中集成 GitHub Auth 来做登录。示例程序代码已托管至本人 GitHub，欢迎关注和 Fork。\n 参考资料\n[1] Flask Quickstart | Flask - flask.palletsprojects.com\n[2] New OAuth Application | GitHub - github.com\n[3] Authenticating to the REST API with an OAuth App | GitHub Docs - docs.github.com\n[4] GitHub OAuth 第三方登录示例教程 | 阮一峰的网络日志 - www.ruanyifeng.com\n[5] Authentication with Flask and GitHub | Dev Community - dev.to\n[6] Authentication with Cookies and Sessions in Flask | Rithm School - www.rithmschool.com\n ","permalink":"https://olzhy.github.io/posts/how-to-use-github-auth-in-flask.html","tags":["Python"],"title":"如何在 Flask 框架中使用 GitHub Auth 做授权登录？"},{"categories":["计算机"],"contents":"本文探索如何使用 Maven 来搭建 Spring Boot 父子项目，方便我们在搭建 Spring Boot 微服务时作参考。\n使用 Maven 搭建 Spring Boot 微服务项目时，最直接的做法是在每个项目的pom.xml中直接引用 Spring Boot Starter 父项目org.springframework.boot:spring-boot-starter-parent，并配置各项依赖。\n但有多个微服务项目时，若要升级它们各自pom.xml中的依赖会产生大量的重复工作。因此，若将这些公共依赖抽取到一个父项目中，而这些子项目都引用这个父项目，那么依赖可在父项目中统一管理；这样，子项目若需升级，只需升级一下引用的父项目版本，升级过程将会变得非常简单。\n本文接下来即会创建一个示例 Spring Boot 父项目，并尝试在子项目中引用并使用它。\n写作本文时，用到的 Java、Maven 和 Spring Boot 的版本分别为：\n Java: 1.8 Maven: 3.9.0 Spring Boot: 2.7.9  1 创建父项目 首先，开始搭建父项目starter-parent，父项目可包括多个子模块，本文的例子仅含common-utils一个子模块，该子模块用于编写项目用到的工具类。\n下面使用mvn archetype:generate来生成项目的脚手架。\n首先，使用如下命令生成starter-parent空项目，生成后删除不用的src文件夹，并修改pom.xml中的\u0026lt;packaging\u0026gt;为pom。\nmvn archetype:generate \\  -DgroupId=com.leileiluoluo \\  -DartifactId=starter-parent \\  -Dversion=1.0-SNAPSHOT \\  -DinteractiveMode=false 然后，进入starter-parent文件夹下，生成子模块common-utils，命令如下：\ncd starter-parent/ mvn archetype:generate \\  -DgroupId=com.leileiluoluo \\  -DartifactId=common-utils \\  -Dversion=1.0-SNAPSHOT \\  -DinteractiveMode=false 这样，项目脚手架就出来了，目录结构如下：\nstarter-parent |-- common-utils | |-- src/main/java | `-- pom.xml `-- pom.xml 修改下两个pom.xml文件，使配置更简单紧凑；并在common-utils下加一个工具类DataUtil.java。\n修改后，项目目录结构如下：\nstarter-parent |-- common-utils | |-- src/main/java | | `-- com/leileiluoluo/common/util/DataUtil.java | `-- pom.xml `-- pom.xml 下面看一下该项目下几个文件的源码。\n1.1 pom.xml 根 POM，注意\u0026lt;parent\u0026gt;引用了spring-boot-starter-parent；\u0026lt;packaging\u0026gt;类型为pom；\u0026lt;dependencyManagement\u0026gt;定义了所有的公共依赖；\u0026lt;pluginManagement\u0026gt;定义了所有的公共插件。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.9\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;groupId\u0026gt;com.leileiluoluo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;spring.boot.version\u0026gt;2.7.9\u0026lt;/spring.boot.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;common-utils\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- modules --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.leileiluoluo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;common-utils\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- spring boot starters --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring.boot.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- test --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${spring.boot.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.9.2\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; \u0026lt;build\u0026gt; \u0026lt;pluginManagement\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/pluginManagement\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 1.2 common-utils/pom.xml common-utils子模块 POM，注意\u0026lt;groupId\u0026gt;与\u0026lt;parent\u0026gt;中的一致，所以可以省略；\u0026lt;dependency\u0026gt;中的\u0026lt;version\u0026gt;已在父 POM 中声明，可以省略。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;project xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34; xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.leileiluoluo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;artifactId\u0026gt;common-utils\u0026lt;/artifactId\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter-api\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 1.3 common-utils 下的 DataUtil.java DataUtil.java为common-utils子模块下的 Java 类，供后面的项目使用。\npackage com.leileiluoluo.common.util; import java.time.ZonedDateTime; import java.time.format.DateTimeFormatter; public class DateUtil { public static String getCurrentTimeStr() { DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;); return ZonedDateTime.now().format(formatter); } } 接下来尝试创建一个项目来引用这个父项目。尝试之前，使用 Maven 将该项目打包并安装到本地。\n命令如下：\nmvn clean install ... [INFO] Reactor Summary for starter-parent 1.0-SNAPSHOT: [INFO] [INFO] starter-parent ..................................... SUCCESS [ 0.170 s] [INFO] common-utils ....................................... SUCCESS [ 1.330 s] [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 1.658 s [INFO] Finished at: 2023-03-14T10:34:43+08:00 [INFO] ------------------------------------------------------------------------ 2 使用父项目 下面创建子项目app-child来使用前面的父项目starter-parent。\n创建后的子项目目录结构如下：\napp-child |-- src/main/java | `-- com/leileiluoluo/app/DemoApplication.java `-- pom.xml 下面接着看一下该项目下的几个文件，然后尝试启动项目并做测试。\n2.1 pom.xml 该项目 POM，注意\u0026lt;parent\u0026gt;引用了前面搭建好的starter-parent；\u0026lt;packaging\u0026gt;采用默认类型jar；\u0026lt;dependencies\u0026gt;下引用的公共依赖均无需指定\u0026lt;version\u0026gt;；plugins下引用的公共插件也无需指定\u0026lt;version\u0026gt;。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; standalone=\u0026#34;no\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;com.leileiluoluo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;artifactId\u0026gt;app-child\u0026lt;/artifactId\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.leileiluoluo\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;common-utils\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 2.2 DemoApplication.java 仅写了一个 API hello，在其方法内调用了父项目模块common-utils中的DateUtil.getCurrentTimeStr()方法。\npackage com.leileiluoluo.app; import com.leileiluoluo.common.util.DateUtil; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @SpringBootApplication @RestController public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } @GetMapping(\u0026#34;/hello\u0026#34;) public String hello(@RequestParam(value = \u0026#34;name\u0026#34;, defaultValue = \u0026#34;World\u0026#34;) String name) { return String.format(\u0026#34;Hello %s! %s\u0026#34;, name, DateUtil.getCurrentTimeStr()); } } 2.3 启动与测试 下面使用 Maven 将该项目打包，然后运行打包好的jar包，命令如下：\ncd app-child/ mvn clean package java -jar target/app-child-1.0-SNAPSHOT.jar 启动完成后，使用curl请求一下，会得到如下内容。\ncurl \u0026#39;http://localhost:8080/hello?name=Larry\u0026#39; Hello Larry! 2023-03-13 16:31:23 综上，本文探索了如何使用 Maven 来搭建 Spring Boot 父子项目，方便我们在搭建 Spring Boot 微服务时作参考。本文中涉及的父项目starter-parent与子项目app-child的完整代码已托管至 GitHub，欢迎关注或 Fork。\n 参考资料\n[1] Introduction to the POM | Maven - maven.apache.org\n[2] Introduction to the Dependency Mechanism | Maven - maven.apache.org\n[3] Guide to Working with Multiple Modules | Maven - maven.apache.org\n[4] Spring Boot | Spring - spring.io\n[5] Maven by Example: A Multi-Module Project | TheNEXUS - books.sonatype.com\n[6] Multi-Module Project with Maven | Baeldung - www.baeldung.com\n[7] Spring Boot Multi-Module Maven Project | HowToDoInJava - howtodoinjava.com\n ","permalink":"https://olzhy.github.io/posts/spring-boot-parent-child-projects-with-maven.html","tags":["Maven","Java","Spring"],"title":"如何使用 Maven 搭建 Spring Boot 父子项目？"},{"categories":["计算机"],"contents":"尽管大型机（Mainframe）已服役约半个多世纪，但其在安全性、可靠性、一致性和性能上久经考验，依然承担着目前世界上多数银行、保险、通信公司以及政府系统中极其关键的部分。\n目前，数字化转型正进行的如火如荼，似乎有种大型机要完全被取代的趋势。但因大型机系统非常的复杂，而改造又存在着极大的风险，所以在当下，短期内完全彻底的替代大型机等遗留系统是不现实的。\n接下来几年的趋势是一种混合的解决方案：即大型机系统中能改造的部分会逐步进行现代化改造，而极难改造的部分也会借助现代化技术来为其提升效率。这些现代化技术中很重要的一个就是 DevOps，所以本文要探讨的即是如何为现代大型机做 DevOps？\n1 文化上 大型机开发人员在其它团队看来有种“与世隔绝”的感觉。要让大型机团队更好的拥抱 DevOps，首先得需要从文化上做出改变。主要有如下几个方面：\n  跨团队互相穿插人员\n将来自一个团队的成员穿插到其它团队中，这样即可以了解其它团队的日常活动，从而了解看问题的不同视角和解决问题的不同思路。长此以往，即可以对团队的文化有所改变。\n  划分好职责\n清晰的划分好职责，是 DevOps 落地的前提。\n  引入“混世魔猴”\nChaos monkey（混世魔猴）为 Netflix 首创，通过时不时有意的破坏系统来促使系统更具韧性。\n  “对事不对人”的事后复盘\n通过“对事不对人”的事后复盘，以从问题中吸取教训。\n  拥抱“基础设施即代码”\n通过拥抱 Infrastructure as code（基础设施即代码）来提升 IT 基础设施的可靠性与敏捷性。\n  拥抱 DevSecOps\n安全为企业之本。注重效率的同时，绝不能丢掉安全。通过拥抱 DevSecOps 来为交付安全的软件保驾护航。\n  2 管理上 管理上可以引入如下关键绩效指标（key performance indicators，KPIs）来测试 DevOps 推进的怎么样。\n  平均恢复时间\n平均恢复时间（Mean time to recovery，MTTR），故障发生到恢复所用的时间。\n  平均检测时间\n平均检测时间（Mean time to detection，MTTD），识别问题所用的时间。其涉及复杂的监控系统。\n  部署效率\n代码发布到生产所用的时间。为敏捷的重要指标。\n  变更失败率\n变更失败率（Change failure rate），为变更失败百分比。\n  3 工具上   配置管理\n尝试使用 Chef 、 Puppet 和 Ansible 等配置管理工具管理基础设施。\n  CI/CD\n尝试使用 Jenkins 做 CI/CD。如在 Jenkins 上安装 Topaz IDE 插件来实现与 COBOL 批处理文件、CICS 和 IMS 程序的交互。\n  综上，本文从文化、管理和工具上对如何为现代大型机做 DevOps 做了初步探索。\n 参考资料\n[1] DevOps - Modern Mainframe Development | O\u0026rsquo;Reilly - learning.oreilly.com\n ","permalink":"https://olzhy.github.io/posts/devops-for-modern-mainframe.html","tags":["DevOps"],"title":"如何对现代大型机（Mainframe）做 DevOps？"},{"categories":["计算机"],"contents":"云迁移是将数字系统的一部分或全部迁移到云上的过程。主要有三种迁移方向：On-Premise 到云、云到云以及云到 On-Premise。\n在进行迁移时，主要有 5 种方法或策略可以选择。如下为 Gartner 于 2011 年定义的“5 Rs”迁移方法：\n  Rehost（Lift and shift）\n将应用程序原样搬到云上，不涉及架构和明显的代码改动。即应用程序在本地数据中心怎么运行的，迁移到云上也按同样的方式运行。目前，诸如 AWS 和 Azure 等云提供商已使得该种迁移方式变得越来越方便快捷。一个例子是，云提供商提供的文件服务允许将文件共享挂载到虚拟机上，这样应用程序在云上也可以像访问本机文件一样无需任何改动。\n  Refactor\n重构或改造应用程序来更好的适应云环境，会涉及大量的代码改动，需要进行充足的测试。\n  Replatform\n一个介于 Rehost 与 Refactor 之间的方案，不涉及特别大的架构或代码调整，但会针对云环境的特性对应用程序作适当的改造。\n  Rebuild\n从头开始重写应用程序。\n  Replace\n废弃旧的应用程序，而使用新的技术（诸如云原生等）或 SAAS 产品（如使用 SaaS 产品替换老的 HR 系统）来完全替代它。\n  下面分析这 5 种云迁移方法的适用场景及优缺点。\n   迁移方案 \\ 优缺点 优点 缺点     Rehost 应用程序与基础设施架构基本不变，省去了大量的开发与测试时间 没有充分利用云的特性（诸如：自动扩展、高可用和灾难恢复等）   Refactor 充分利用了云的特性 对新技能（诸如：云产品特性、DevOps 和自动化等）要求较高，且可能与特定云提供商的产品发生耦合   Replatform 改造成本适中，试错成本低 需充分控制改动范围，否则会变成大的重构   Rebuild 可以采用最新的技术，最适合的云产品 非常耗时耗力   Replace 省去了改造现有系统的时间 上下游系统依赖较多的话，比较难解决    综上，本文对业界常用的云迁移方法进行了介绍，并对它们的适用场景及优缺点进行了梳理，便于即将进行云迁移的朋友来参考。\n 参考资料\n[1] Cloud Migration Approaches and Their Pros and Cons | BlueXP - bluexp.netapp.com\n[2] Pros and Cons of 6R\u0026rsquo;s in Cloud Migration | Heptabit - www.heptabit.at\n ","permalink":"https://olzhy.github.io/posts/cloud-migration-approaches.html","tags":["架构设计"],"title":"5 种常用的云迁移方法及其优缺点"},{"categories":["计算机"],"contents":"什么是 DevOps？如下是一段来自 Atlassian 对 DevOps 的定义。\n \u0026ldquo;DevOps is a set of practices, tools, and a cultural philosophy that automate and integrate the processes between software development and IT teams. It emphasizes team empowerment, cross-team communication and collaboration, and technology automation.\u0026rdquo; \u0026mdash; Atlassian\n 翻译一下，意思是：“DevOps 是实践、工具和文化理念的集合，可将软件开发和 IT 团队之间的流程进行自动化与集成。它强调团队赋能，跨团队沟通协作以及技术自动化”。\n我们知道 DevOps 一词是 Development 和 Operations 的组合。DevOps 大概起源于 2007 年，主要是为了解决传统开发与运维间沟通协作不畅的问题。传统的开发与运维分别属于不同的部门，两者有着不同的思维模式，彼此缺乏深入的沟通与了解。开发人员想让代码尽快的发布，运维人员想让系统尽可能的稳定，这种模式无法快速的交付需求，所以 DevOps 这种新的工作模式，文化变革应运而生。\n（图片引用自 Kovair Blog） 1 DevOps 生命周期 下图展示了 DevOps 生命周期的各个阶段，可以看到 DevOps 的生命周期大概由 8 个阶段组成，左侧是开发部分，右边是运维部分，是一个无限循环。\n（图片引用自 Atlassian） 下面简单看一下，每个阶段都是做什么的：\n  发现（Discover）\n着手前的准备工作，团队开展头脑风暴等活动探索及明确要做的事情及其优先级。\n  计划（Plan）\n计划阶段，采用敏捷等方法将需求拆分为更易于快速交付的工作。\n  构建（Build）\n对代码进行编译构建。\n  测试（Test）\n正式部署到生产环境前，采用自动化测试来确保变更的准确性。\n  部署（Deploy）\n负责以自动化的方式将功能部署到生产环境。\n  运维（Operate）\n负责 IT 基础设施的运行和维护工作。\n  观测（Observe）\n通过监控及告警来快速发现并解决各种功能或性能问题。\n  持续反馈（Continuous feedback）\n回顾当前版本并搜集客户反馈以发现不足，以期在下次发布做出改进。\n  2 DevOps 五原则 要充分发挥 DevOps 的潜力，团队需要遵循如下五个原则：\n  协作（Collaboration）\n协作是 DevOps 的前提，开发与运维团队合并为一个共享与协作的职能团队来提供更快更好的交付。\n  自动化（Automation）\nDevOps 的一个必要实践是将软件生命周期中尽可能多的阶段进行自动化。这样即可以在更短的时间内响应客户的反馈。\n  持续改进（Continuous Improvement）\n持续改进已成为敏捷开发与精益制造的重要部分。这是一种专注于实验、最大化地降低成本与提升效率的实践。其与持续交付也有关联性，DevOps 持续推送更新，从而提升软件迭代的效率。\n  以客户为中心的行动（Customer-centric action）\nDevOps 通过使用与最终用户简短的反馈环来开发以用户需求为中心的产品或服务。这种以客户为中心的实践或行动包括通过实时监控与快速部署来快速搜集与响应用户的反馈。\n  以终为始（Create with the end in mind）\nDevOps 团队应对产品的创建到实现有一个整体的了解，从而更深入的了解产品与理解需求，从而交付真正解决客户问题的产品或服务。\n  3 DevOps 流水线 DevOps 流水线是一组自动化的流程和工具，允许开发和运维人员协作来将代码构建并部署到生产环境。\n下图演示了一个 DevOps 流水线大概长什么样，可以看到其包含代码提交后的构建、测试及部署等阶段。但业界没有一个标准的 DevOps 流水线，每个组织会根据自己的场景及技术栈来构建适合自己的流水线。\n（图片引用自 Atlassian） 4 DevSecOps DevSecOps 是一个将安全性集成到持续集成、持续交付和持续部署流水线中的实践。\n下图演示了如何在 DevOps 生命周期中注入安全。\n（图片引用自 Atlassian）   计划阶段（Plan）\n可以通过威胁建模等方法来识别软件可能存在的安全漏洞。\n  构建阶段（Build）\n可以通过使用静态应用安全测试（Static Application Security Testing，SAST）软件（诸如 SonarQube）来扫描代码可能存在的漏洞。\n  测试阶段（Test）\n可以通过动态应用安全测试（Dynamic Application Security Testing，DAST）软件来测试是否存在诸如 OWASP Top 10 等列出的安全漏洞。\n  部署阶段（Deploy）\n部署阶段需要检查基础设施及运行环境的配置是否安全，包括网络防火墙、密钥数据等。一些流行的配置管理工具包括 Ansible、Puppet、HashiCorp Terraform 和 Chef 等。\n  运维阶段（Operate）\n通过使用 Web 应用防火墙（Web Application Firewall，WAF）等提供额外安全保障。\n  综上，本文首先介绍了 DevOps 的概念、生命周期及实践的原则，然后对 DevOps 流水线与 DevSecOps 作了解释。以期阅读完本文的朋友对什么是 DevOps 能有一个基本的了解。\n 参考资料\n[1] What is DevOps? | Atlassian - www.atlassian.com\n[2] DevOps Lifecycle : Different Phases in DevOps | BrowserStack - www.browserstack.com\n[3] How DevOps Tools Work Together? | TechieRoop - techieroop.com\n[4] The Battle – Dev vs Ops | Kovair Blog - www.kovair.com\n ","permalink":"https://olzhy.github.io/posts/what-is-devops.html","tags":["DevOps"],"title":"一文了解什么是 DevOps"},{"categories":["计算机"],"contents":"1 聚合流水线概念 MongoDB 聚合流水线用于处理文档，其由一个或多个阶段（Stage）组成。每个阶段会对文档执行一类操作，一个阶段输出的文档会传递到下一个阶段，使用聚合流水线可以对文档进行过滤、分组和聚合计算（如计算平均值、最大值、最小值或总值）。除了进行聚合查询以外，自 MongoDB 4.2 版本起，还可以使用聚合流水线来进行文档更新。\n注意：使用db.collection.aggregate()方法运行聚合流水线时，除非该流水线包含$merge或$out阶段，否则不会更改集合中的文档。\n2 准备数据 对比 SQL 来学习 MongoDB 的聚合操作会比较容易理解。该部分会准备一下数据，以方便后面的对比学习。\n考虑有一张用于存放手机信息的表 phones，其有字段 id（主键）、name（名称）、type（类型）、price（价格）、quantiy（数量）和published_at（发布时间）这几个字段。\nphones 表的建表语句如下：\nCREATE TABLE phones ( id serial, -- 主键 \tname varchar(100), -- 名称 \ttype varchar(10), -- 类型（standard 或 plus） \tprice int, -- 价格 \tquantity int, -- 数量 \tpublished_at timestamp, -- 发布时间 \tPRIMARY KEY (id) ); 给 phones 表插入 10 条数据，命令如下：\nINSERT INTO phones (name, type, price, quantity, published_at) VALUES (\u0026#39;Apple\u0026#39;, \u0026#39;plus\u0026#39;, 7000, 10, \u0026#39;2023-01-16 16:08:00\u0026#39;), (\u0026#39;Apple\u0026#39;, \u0026#39;standard\u0026#39;, 6000, 10, \u0026#39;2023-01-16 16:08:00\u0026#39;), (\u0026#39;XIAOMI\u0026#39;, \u0026#39;plus\u0026#39;, 3000, 30, \u0026#39;2023-02-16 16:08:00\u0026#39;), (\u0026#39;XIAOMI\u0026#39;, \u0026#39;standard\u0026#39;, 2000, 30, \u0026#39;2023-02-16 16:08:00\u0026#39;), (\u0026#39;OPPO\u0026#39;, \u0026#39;plus\u0026#39;, 2000, 20, \u0026#39;2023-03-16 16:08:00\u0026#39;), (\u0026#39;OPPO\u0026#39;, \u0026#39;standard\u0026#39;, 1000, 20, \u0026#39;2023-03-16 16:08:00\u0026#39;), (\u0026#39;HUAWEI\u0026#39;, \u0026#39;plus\u0026#39;, 5000, 40, \u0026#39;2023-04-16 16:08:00\u0026#39;), (\u0026#39;HUAWEI\u0026#39;, \u0026#39;standard\u0026#39;, 4000, 40, \u0026#39;2023-04-16 16:08:00\u0026#39;), (\u0026#39;VIVO\u0026#39;, \u0026#39;plus\u0026#39;, 3000, 50, \u0026#39;2023-05-16 16:08:00\u0026#39;), (\u0026#39;VIVO\u0026#39;, \u0026#39;standard\u0026#39;, 2000, 50, \u0026#39;2023-05-16 16:08:00\u0026#39;); 使用 MongoShell 在 MongoDB 插入与如上命令相同的 10 条数据，命令如下：\ndb.phones.insertMany( [ { _id: 1, name: \u0026#34;Apple\u0026#34;, type: \u0026#34;plus\u0026#34;, price: 7000, quantity: 10, published_at: ISODate( \u0026#34;2023-01-16T16:08:00Z\u0026#34; ) }, { _id: 2, name: \u0026#34;Apple\u0026#34;, type: \u0026#34;standard\u0026#34;, price: 6000, quantity: 10, published_at: ISODate( \u0026#34;2023-01-16T16:08:00Z\u0026#34; ) }, { _id: 3, name: \u0026#34;XIAOMI\u0026#34;, type: \u0026#34;plus\u0026#34;, price: 3000, quantity: 30, published_at: ISODate( \u0026#34;2023-02-16T16:08:00Z\u0026#34; ) }, { _id: 4, name: \u0026#34;XIAOMI\u0026#34;, type: \u0026#34;standard\u0026#34;, price: 2000, quantity: 30, published_at: ISODate( \u0026#34;2023-02-16T16:08:00Z\u0026#34; ) }, { _id: 5, name: \u0026#34;OPPO\u0026#34;, type: \u0026#34;plus\u0026#34;, price: 2000, quantity: 20, published_at: ISODate( \u0026#34;2023-03-16T16:08:00Z\u0026#34; ) }, { _id: 6, name: \u0026#34;OPPO\u0026#34;, type: \u0026#34;standard\u0026#34;, price: 1000, quantity: 20, published_at: ISODate( \u0026#34;2023-03-16T16:08:00Z\u0026#34; ) }, { _id: 7, name: \u0026#34;HUAWEI\u0026#34;, type: \u0026#34;plus\u0026#34;, price: 5000, quantity: 40, published_at: ISODate( \u0026#34;2023-04-16T16:08:00Z\u0026#34; ) }, { _id: 8, name: \u0026#34;HUAWEI\u0026#34;, type: \u0026#34;standard\u0026#34;, price: 4000, quantity: 40, published_at: ISODate( \u0026#34;2023-04-16T16:08:00Z\u0026#34; ) }, { _id: 9, name: \u0026#34;VIVO\u0026#34;, type: \u0026#34;plus\u0026#34;, price: 3000, quantity: 50, published_at: ISODate( \u0026#34;2023-05-16T16:08:00Z\u0026#34; ) }, { _id: 10, name: \u0026#34;VIVO\u0026#34;, type: \u0026#34;standard\u0026#34;, price: 2000, quantity: 50, published_at: ISODate( \u0026#34;2023-05-16T16:08:00Z\u0026#34; ) } ] ) 数据准备完成，下面会使用对比 SQL 语句的方式来学习 MongoDB 的聚合流水线知识。\n3 对比 SQL 来学习使用聚合流水线 本部分以出问题的形式来设定一个查询场景，然后分别以 SQL 及 聚合流水线两种方式来实现。\n3.1 按字段过滤，然后进行分组和排序 问题描述：找出类型为standard的手机，然后按名称分组并计算其对应的总数量，返回结果包含名称和总数量两列，按总数量降序排序。\n针对上述问题，SQL 中首先会使用WHERE来进行筛选，然后使用GROUP BY来分组，使用聚集函数sum来进行累加，最后使用ORDER BY来进行排序。\nSQL 查询语句及运行结果如下：\nSELECT name, sum(quantity) AS total_quantity FROM phones WHERE type=\u0026#39;standard\u0026#39; GROUP BY name ORDER BY total_quantity DESC; name | total_quantity --------+---------------- VIVO | 50 HUAWEI | 40 XIAOMI | 30 OPPO | 20 Apple | 10 该问题若使用 MongoDB 的聚合流水线来实现，需要有 3 个阶段：\n  第一个阶段$match\n过滤类型为standard的文档，并将结果传递到下一个阶段。\n  第二个阶段$group\n针对输入文档，按名称进行分组，然后对每个名称计算新字段totalQuantity的值，该字段值为数量的累加。完成后，将结果传递到下一个阶段。\n  第三个阶段$sort\n针对输入文档，按totalQuantity进行降序排序，完成后返回结果。\n  MongoShell aggregate 聚合查询命令及运行结果如下：\ndb.phones.aggregate( [ { $match: { type: \u0026#34;standard\u0026#34; } }, { $group: { _id: \u0026#34;$name\u0026#34;, totalQuantity: { $sum: \u0026#34;$quantity\u0026#34; } } }, { $sort: { totalQuantity: -1 } } ] ) [ { _id: \u0026#39;VIVO\u0026#39;, totalQuantity: 50 }, { _id: \u0026#39;HUAWEI\u0026#39;, totalQuantity: 40 }, { _id: \u0026#39;XIAOMI\u0026#39;, totalQuantity: 30 }, { _id: \u0026#39;OPPO\u0026#39;, totalQuantity: 20 }, { _id: \u0026#39;Apple\u0026#39;, totalQuantity: 10 } ] 可以看到，MongoDB 聚合流水线的查询结果与上面的 SQL 语句查询结果是一致的。\n3.2 对时间字段限定范围，然后进行分组和排序 问题描述：找出发布时间published_at在 2023 年 2 月到 2023 年 4 月这三个月所发布的手机，然后按发布年月计算当月发布的手机总数量，返回结果包含发布年月和总数量两列，按总数量降序排序。\n针对上述问题，SQL 中首先会将时间戳转换为年月格式，然后使用WHERE来限定日期范围，然后使用GROUP BY来分组，使用聚集函数sum来进行累加，最后使用ORDER BY来进行排序。\nSQL 查询语句及运行结果如下：\nSELECT to_char(published_at, \u0026#39;YYYY-MM\u0026#39;) AS year_month, sum(quantity) AS total_quantity FROM phones WHERE published_at BETWEEN \u0026#39;2023-02-01 00:00:00\u0026#39; AND \u0026#39;2023-05-01 00:00:00\u0026#39; GROUP BY year_month ORDER BY year_month DESC; year_month | total_quantity ------------+---------------- 2023-04 | 80 2023-03 | 40 2023-02 | 60 该问题若使用 MongoDB 的聚合流水线来实现，亦需要有 3 个阶段：\n  第一个阶段$match\n过滤发布日期published_at在2023-02-01 00:00:00与2023-05-01 00:00:00之间的文档，并将结果传递到下一个阶段。\n  第二个阶段$group\n针对输入文档，将发布日期转换为%Y-%m格式后按其进行分组，然后计算该年月发布的手机总数量totalQuantity。完成后，将结果传递到下一个阶段。\n  第三个阶段$sort\n针对输入文档，按年月字段进行降序排序，完成后返回结果。\n  MongoShell aggregate 聚合查询命令及运行结果如下：\ndb.phones.aggregate( [ { $match: { published_at: { $gte: new ISODate(\u0026#34;2023-02-01 00:00:00\u0026#34;), $lt: new ISODate(\u0026#34;2023-05-01 00:00:00\u0026#34;), }, }, }, { $group: { _id: { $dateToString: { format: \u0026#34;%Y-%m\u0026#34;, date: \u0026#34;$published_at\u0026#34;, }, }, totalQuantity: { $sum: \u0026#34;$quantity\u0026#34;, }, }, }, { $sort: { _id: -1, }, }, ] ) [ { _id: \u0026#39;2023-04\u0026#39;, totalQuantity: 80 }, { _id: \u0026#39;2023-03\u0026#39;, totalQuantity: 40 }, { _id: \u0026#39;2023-02\u0026#39;, totalQuantity: 60 } ] 可以看到，聚合流水线的查询结果与 SQL 语句查询结果也是一致的。\n综上，本文对比 SQL 来学习了最基本的 MongoDB 聚合操作，对于聚合操作更复杂一点的特性，待后面有时间来学习整理。\n 参考资料\n[1] MongoDB Aggregation Operations - www.mongodb.com\n[2] Practical MongoDB Aggregations Book - www.practical-mongodb-aggregations.com\n[3] Format SQL Statements Online - sqlformat.org\n ","permalink":"https://olzhy.github.io/posts/mongodb-aggregation.html","tags":["MongoDB"],"title":"对比 SQL 来学习 MongoDB 的聚合操作"},{"categories":["计算机"],"contents":"本文介绍在 Python 中如何使用 PyMongo 来封装一个简单易用的 MongoDB 工具类。\n进行编码之前，应考虑一下这个工具类应具有的几个基本的功能：\n  新增 insert\n支持插入单条记录，并返回插入后所生成的 ID。\n  查询条数 count\n查询满足条件的数据条数。\n  查询单条记录 get\n查询满足条件的一条记录。\n  分页查询一组记录 list_with_pagination\n指定查询条件、页码（page_no）、单页记录数(page_size)和排序规则进行查询，返回满足条件的一组排好序的记录。\n  更新 update\n指定查询条件和待更新字段健值对来进行更新。\n  删除 delete\n指定条件对数据进行删除。\n  工具类应具备的功能确定以后，就可以着手进行编码了。依赖包为pymongo，只在其上做了简单的封装。\n1 封装后的工具类 封装后的工具类文件名为connection.py，源码如下：\nfrom typing import Dict, List, Tuple import os import pymongo from bson import ObjectId class Connection: def __init__(self, collect_name: str): conn_str = os.getenv(\u0026#39;MONGO_URL\u0026#39;) db_name = os.getenv(\u0026#39;MONGO_DB\u0026#39;) if conn_str is None or db_name is None: raise EnvironmentError(\u0026#34;MONGO_URL or MONGO_DB is not set\u0026#34;) self.collection = pymongo.MongoClient(conn_str)[db_name][collect_name] def insert(self, item: Dict) -\u0026gt; ObjectId: return self.collection.insert_one(item).inserted_id def count(self, condition: Dict) -\u0026gt; int: return self.collection.count_documents(condition) def get(self, condition: Dict) -\u0026gt; Dict: return self.collection.find_one(condition) def list_with_pagination(self, condition: Dict, page_no: int = 1, page_size: int = 10, sort_tuples: List[Tuple] = list()) -\u0026gt; List[Dict]: items_skipped = (page_no - 1) * page_size cursor = self.collection.find(condition).skip(items_skipped) if len(sort_tuples) \u0026gt; 0: cursor = cursor.sort(sort_tuples).limit(page_size) else: cursor = cursor.limit(page_size) items = [] for item in cursor: items.append(item) return items def update(self, condition: Dict, update_dict: Dict) -\u0026gt; None: self.collection.update_many(condition, {\u0026#39;$set\u0026#39;: update_dict}) def delete(self, condition: Dict) -\u0026gt; None: self.collection.delete_many(condition) 创建该工具类需要隐式提供两个环境变量：MONGO_URL与MONGO_DB。MONGO_URL为 MongoDB 连接地址，格式为mongodb://{username}:{password}@{host}:{port}；MONGO_DB为数据库名。\n该工具类Connection中，除list_with_pagination方法外，其它方法的实现都比较简单。\n下面仅对list_with_pagination的实现细节作一下说明：\nself.collection.find(condition)会返回一个Cursor实例，可对其进行遍历。可以看到我们使用skip、sort和limit来分别进行跳过记录、排序和限制返回条目，这样即很好的实现了带排序的分页查询。\n2 对工具类进行测试 封装完成后，我们对connection.py编写测试类来进行单元测试。\n测试文件connection_test.py的源码如下：\nimport datetime from unittest import TestCase import pymongo import mongomock from bson import ObjectId from connection import Connection class TestConnection(TestCase): def setUp(self) -\u0026gt; None: self.connection = Connection(\u0026#39;users\u0026#39;) # mock self.connection.collection = mongomock.MongoClient().db.collection # insert initial data id = ObjectId(\u0026#39;63eca79d252cd5ac908a7f06\u0026#39;) user = self.connection.get({\u0026#39;_id\u0026#39;: id}) if user is None: user = { \u0026#39;_id\u0026#39;: id, \u0026#39;name\u0026#39;: \u0026#39;Larry\u0026#39;, \u0026#39;age\u0026#39;: 19, \u0026#39;createdAt\u0026#39;: \u0026#39;2023-02-16 14:43:45\u0026#39;, \u0026#39;updatedAt\u0026#39;: \u0026#39;2023-02-16 14:43:45\u0026#39; } self.connection.insert(user) def test_insert(self): now = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;) user = { \u0026#39;_id\u0026#39;: ObjectId(\u0026#39;63eca79d252cd5ac908a7f07\u0026#39;), \u0026#39;name\u0026#39;: \u0026#39;Larry\u0026#39;, \u0026#39;age\u0026#39;: 19, \u0026#39;createdAt\u0026#39;: now, \u0026#39;updatedAt\u0026#39;: now } id = self.connection.insert(user) self.assertEqual(user[\u0026#39;_id\u0026#39;], id) def test_count(self): condition = {\u0026#39;name\u0026#39;: \u0026#39;Larry\u0026#39;} user_count = self.connection.count(condition) self.assertTrue(user_count \u0026gt; 0) def test_get(self): condition = {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;63eca79d252cd5ac908a7f06\u0026#39;)} user = self.connection.get(condition) self.assertIsNotNone(user) def test_list_with_pagination(self): condition = {\u0026#39;name\u0026#39;: \u0026#39;Larry\u0026#39;} page_no = 1 page_size = 20 sort_tuples = [(\u0026#39;createdAt\u0026#39;, pymongo.DESCENDING)] users = self.connection.list_with_pagination(condition, page_no, page_size, sort_tuples) self.assertTrue(len(users) \u0026gt; 0) def test_update(self): condition = {\u0026#39;_id\u0026#39;: ObjectId(\u0026#39;63eca79d252cd5ac908a7f06\u0026#39;)} now = datetime.datetime.now().strftime(\u0026#39;%Y-%m-%d%H:%M:%S\u0026#39;) update_dict = { \u0026#39;name\u0026#39;: \u0026#39;Larry Update\u0026#39;, \u0026#39;updatedAt\u0026#39;: now } # update self.connection.update(condition, update_dict) # get user = self.connection.get(condition) self.assertEqual(update_dict[\u0026#39;name\u0026#39;], user[\u0026#39;name\u0026#39;]) def test_delete(self): condition = {\u0026#39;name\u0026#39;: \u0026#39;Larry\u0026#39;} # delete self.connection.delete(condition) count = self.connection.count(condition) self.assertEqual(0, count) 下面，对几个比较关键的方法作一下说明：\n  setUp方法\n使用mongomock做了一个 Mock 的collection，不会真的对 MongoDB 数据库进行连接和测试。\n  test_insert 方法\n调用self.connection.insert(user)后，返回的 ID 非str类型，而是bson.ObjectId类型。插入数据时，若不想让 MongoDB 自动生成一个随机 ID，而要自己指定 ID 的话，也需要指定为bson.ObjectId类型。\n  test_get 方法\n可以看到，如想根据 ID 查询一条记录，同样查询条件中的_id需要为bson.ObjectId类型。\n  test_list_with_pagination 方法\n调用self.connection.list_with_pagination(condition, page_no, page_size, sort_tuples)进行分页查询时，排序条件sort_tuples是一个数组，所以可以依次按多个字段进行排序。如先按创建时间降序再按姓名升序，则sort_tuples可以设置为[('createdAt', pymongo.DESCENDING), ('name', pymongo.ASCENDING)]。\n  下面跑一下connection_test.py，可以看到所有 6 个测试用例全部通过。\nexport MONGO_URL=xxx export MONGO_DB=test python3 -m unittest connection_test.py ---------------------------------------------------------------------- Ran 6 tests in 0.008s OK 综上，完成了对 PyMongo 的封装，实现了一个简单易用的 Python MongoDB 工具类并对其进行了测试。本文涉及的代码已托管至 GitHub，欢迎关注或 Fork。\n 参考资料\n[1] PyMongo 4.3.3 documentation - pymongo.readthedocs.io\n ","permalink":"https://olzhy.github.io/posts/design-mongodb-util-using-pymongo.html","tags":["Python","MongoDB"],"title":"使用 PyMongo 封装一个易用的 MongoDB 工具类"},{"categories":["计算机"],"contents":"在上文「Power Platform 是什么？」中，我们知道 Power Automate 是一个流程自动化工具，可以使用其来将重复性的工作进行自动化处理。\n本文会详细介绍一下 Power Automate 的概念和功能，并且尝试使用其去构建一些常用的自动化流程。\n1 Power Automate 基础概念 Power Automate 可以做哪些事情呢？\n 处理日常重复性工作（如系统间同步数据）。 工作流。 通过 Connector 或 API 与外部系统集成。  Power Automate 流程主要有两部分组成：Trigger（只有一个） 和 Action（一个或多个）。\n  Trigger 是流程的触发点，如收件箱收到一封新邮件或 SharePoint 列表新增了一个条目。\n  Action 是 Trigger 被调用后，你真正想做的事情。如收到新邮件后将附件存储到「OneDrive for Business」或 SharePoint 新增一个条目后通知相关人员做某些操作。\n  Trigger 有如下几种类型：\n  When something changes\n当数据更改时执行的触发器。如 SharePoint 中创建了新的条目、Dynamics 中的线索被更新等。\n  On a schedule\n定时执行的触发器。如每天 8 点检查是否有新的订单生成，有的话发送给相关人员处理。\n  On a button press\n当 Power Apps 或第三方应用中某个 Button 被点击时执行的触发器。该种类型给了用户按需控制流程执行的能力。\n  Action 有哪些类型呢？\n  Loop\n对某个 Action 一直循环执行直到退出条件满足后才进入流程的下一步。\n  Switch\n根据输入条件判断是否执行当前的单个 Action。\n  Do Until\n执行一组 Action，直到指定条件为 true。\n  Apply to each\n对输入数组的每一条都执行一组 Action。\n  Expressions\nPower Automate 流程中描述实际运行逻辑的表达式。如 Trigger 为「在 SharePoint 某文件夹下创建了一个新文件」，那么在 Action 中获取该文件内容的表达式为@{triggerOutputs()?['body']。\n  2 构建一个自动化流程 下面就尝试使用 Power Automate 去构建一些常用的自动化流程。\n2.1 基于模板创建流程 登录「Power Automate」 后，点击左侧菜单栏的「Templates」，可以看到有大量的模板可供使用。\n下面使用模板「Save Office 365 email attachments to OneDrive for Business」来创建一个自动将邮件附件保存到「OneDrive for Business」的流程。\n创建过程非常简单，只要按照提示点击按钮、授权即可。\n创建完成后，编辑流程，可以看到该流程有 Trigger 和 Action 两部分组成。\nTrigger 条件为：「当收到了新邮件」。Action 的逻辑为：对邮件中的每个附件，在「OneDrive for Business」的「/Email attachments from Power Automate」文件夹下将其创建出来。\n可以看到，基于模板创建流程的操作非常简单。\n2.2 构建定时任务流程 下面尝试使用 Power Automate 来构建一个每天定时将所指定城市的天气发送到邮箱的任务。\n登录 Power Automate 后，在左侧菜单栏点击「Create」按钮并选择「Scheduled cloud flow」。\n在弹出的对话框里可以设定任务名称、执行时间和频率，然后点击「Create」。\n点击「Create」后即跳转到流程设计页面。在 Trigger「Recurrence」下新增「Get current weather」和「Send an email (V2)」两个 Action 并填写相关的字段。这样即实现了需要的功能。\n2.3 构建按钮触发流程 按钮触发流程是使用手动触发的方式来实现的。\n登录 Power Automate 后，在左侧菜单栏点击「Create」按钮并选择「Instant cloud flow」来新建手动触发流程。\n可以添加对应的 Action 来实现想做的事情。如调用 HTTP 接口来实现与外部系统集成。\n2.4 构建审批流程 还可以使用 Power Automate 来构建审批流程。其主要用到的一个 Action 步骤是「Start and wait for an approval」。\n可以在需要审批操作的流程中加入该步骤，然后判断审批结果并执行后续的操作。\n综上，我们完成了对 Power Automate 的初探。\n 参考资料\n[1] Introduction to Power Automate - microsoft.com\n[2] How to build an automated solution - microsoft.com\n ","permalink":"https://olzhy.github.io/posts/power-automate.html","tags":["Power Platform"],"title":"Power Automate 初探"},{"categories":["计算机"],"contents":"在上文「Power Platform 是什么？」中，我们知道 Power Apps 是一个低代码开发工具，可以使用其来快速构建一个定制化应用程序（支持 Web 应用和移动应用）。\n本文会详细介绍一下 Power Apps 里边的基础概念，并且尝试使用其去构建一个画布应用。\n1 Power Apps 基础概念 我们可以使用 Power Apps 来编写一些简单的电子表单来替代 Excel 表格或纸质的表单，也可以使用其来构建诸如订单管理系统等复杂的业务应用。\nPower Apps 一些常用的数据源包括：Dataverse、SharePoint、Dynamics 365、Azure SQL（或 SQL Server） 和 Office 365。\n可以使用 Power Apps 创建三种类型的应用，分别为：画布应用（Canvas App）、模型驱动应用（Model-driven App）和门户应用（Portal App）。\n下面分别看一下各种类型的应用所适用的用户和场景：\n  画布应用（Canvas App）\n可以选择一个适应平板或手机屏幕的空白画布，然后添加数据源，并可以拖拽各种控件进来，以及像写 Excel 公式一样来为控件添加各种功能。\n如下为一个机场所构建的移动画布应用的示例：\n  模型驱动应用（Model-driven App）\n模型驱动应用基于 Dataverse 中的数据所构建。我们无需编写公式，只需在 Dataverse 数据层定义表单、关系、视图和业务规则即可对业务结果进行完全控制。\n如下为一个用于捐款跟踪的模型驱动应用的示例：\n  门户应用（Portal App）\n门户应用同样基于 Dataverse 中的数据所构建。同样是可以通过以拖拽控件的方式来构建一个具有交互功能的网站。还可以为网站添加登录认证。\n如下为一个门户应用的示例：\n  此外，Power Apps 还可以充分借助 Azure 机器学习服务和认知服务的能力来为应用程序添加 AI（人工智能）能力，包括图像识别、文本分类、结果预测等。\n了解了这些概念，下面尝试动手练习去构建一个画布应用。\n2 构建一个画布应用 构建 Power Apps 应用无需下载客户端，只需使用浏览器打开 Power Apps Studio（make.powerapps.com），即可在其中完成所有工作。\n开始构建画布应用前，需要选择应用的样式，有两个选项：移动（Mobile）和平板（Tablet）。一经选定即无法更改。\n下面简单介绍一下 Power Apps 的几个重要组件。\n  Gallery 控件\n负责控制表格的显示（控制显示哪些列以及它们的格式）。\n  Form 控件\n负责单条记录的处理（包括新建、显示、编辑与保存）。\n  输入控件\n包括文本输入框、按钮、下拉框、滑动条、日志输入控件等，每种输入都可以设置默认值、样式、触发动作等。\n  智能控件\n包括硬件支撑控件（如相机、GPS 等）和服务支撑控件（如名片扫描器、对象检测器等）。\n  函数\n函数是将控件和数据源绑定到一起的粘合剂。使用多个函数组成的公式即可实现各种行为。\n  接下来，我们基于 Excel 来生成一个画布应用。\n我们本地的原始 Excel 文件（Students.xlsx）里有如下几条记录。开始工作前先将该文件上传至「OneDrive for Business」。\n基于 Excel 生成画布应用的步骤如下：\n 1 打开 Power Apps Studio - https://make.powerapps.com 2 点击「Start from Excel」-\u0026gt;「New Connection」-\u0026gt;「OneDrive for Business」，然后选择 Students.xlsx，并点击「Connect」。  可以看到，Power Apps 根据我们提供的数据自动生成了最初的应用。\n点击「Preview the app」，可以进行模拟使用，发现有列表、详情、编辑和新增这几个页面，可以进行增删改查等操作。\n这些最基本的功能对于一个商业应用来说是远远不够的。要完善这个应用，就需要借助公式的能力。使用公式可以实现页面跳转、数据过滤、排序，字段校验等功能。我们可以借助 Power Apps 提供的海量函数来组成一个公式去解决复杂的业务问题。\n综上，我们完成了对 Power Apps 的初探。\n 参考资料\n[1] Introduction to Power Apps - microsoft.com\n[2] How to build a canvas app - microsoft.com\n ","permalink":"https://olzhy.github.io/posts/power-apps.html","tags":["Power Platform"],"title":"Power Apps 初探"},{"categories":["计算机"],"contents":"在上文「Power Platform 是什么？」中，我们对 Microsoft Dataverse 是什么作过一个简单的介绍。\n本文会稍微深入的了解一下 Dataverse。\nDataverse 是一个云上的低代码数据服务，主要为 Power Apps、Power Automate 和 Power BI 所使用，负责安全的存储与管理数据。\nDataverse 有什么功能呢？请看下图：\n可以看到，Dataverse 提供安全控制、逻辑处理、数据处理、数据存储及与外部系统集成等诸多功能。下面分别看一下：\n  安全控制\nDataverse 使用 Azure AD（Azure Active Directory，Azure 身份认证与访问管理服务）作身份验证。可以提供细致到行或列级别的权限控制，并提供丰富的审计功能。\n  逻辑处理\nDataverse 允许在数据级别进行业务逻辑处理。如进行重复检查、业务规则应用和工作流处理等。\n  数据处理\nDataverse 允许对数据进行塑形。\n  数据存储\nDataverse 将数据存储在 Azure 云上，让使用者完全无需关心数据的存储和扩展。\n  系统集成\nDataverse 支持多种数据导出方式，并可通过各种 API 与外部系统进行集成。\n  可以看到，Dataverse 是一个非常易于使用的云数据服务，其使用表（由行和列来组成）来存储数据。一个 Dataverse 数据库为 Dataverse 的一个单独的实例，我们可以根据业务需要创建一个或多个 Dataverse 数据库实例来存储数据（每个实例最多可存储 4T 的数据）。\n创建 Dataverse 数据库实例后，可以自定义列来存储数据。数据存好后，可以在 Power Apps、Power Automate 和 Power BI 中直接使用，也可以使用 Connector 或 API 来访问以供业务应用使用。因其提供诸如基于角色或基于业务规则的多种权限控制策略，这样不论数据以何种方式访问，都可以得到安全的控制。\nDataverse 数据库中的表遵循通用数据模型标准（Common Data Model，由 Microsoft 和伙伴公司发起，提供模式、表与关系的设计标准），这样即可与任何遵循该标准的应用进行无缝集成。\n有了如上对 Dataverse 的简单介绍后，下面会从 Dataverse 中的表、表与表的关系、业务规则设置及 Dataverse 环境与管理中心几个方面分别作介绍。\n1 Dataverse 中的表 表由行和列组成，为表示一组数据的逻辑结构。Dataverse 包含三种类型的表，分别为：标准表、托管表和自定义表。\n 标准表（Standard Table） - 为 Dataverse 环境中自带的表，也称为“开箱即用”表，如帐户、业务部门和联系人等都为 Dataverse 中的标准表。大多数标准表都支持自定义。 托管表（Managed Table） - 无法自定义的表。其作为托管解决方案的一部分被导入到 Dataverse 环境中。 自定义表（Custom Table）- 由非托管解决方案导入的非托管表或在 Dataverse 环境中新创建的表。  列用于存储表中行的离散信息，其有具体的类型（如用于存储日期的 Date 类型列和用于存储数字的 Number 类型列）。\n2 Dataverse 中表与表的关系 为了高效且便于扩展，通常需要将数据拆分到不同的表（Dataverse 中也叫 Container）中，如一个销售管理系统通常会包含 Customer、Product、Order 和 OrderItem 等表。拆分为多个表后，表与表之间通常会具有关系，常见的有一对多和多对多，这两种关系都是 Dataverse 所支持的。\n一对多关系也被称为“父子关系”，如在上述例子中，Order 为父表，OrderItem 为子表。一个 Order 可以有 0 个或多个 OrderItem，但一个 OrderItem 仅属于一个 Order，而且不可能存在没有 Order 的 OrderItem。\n而且，一个表中只允许唯一值的列（如 Order 表的 ID 列）被称为主键。若一个表的主键被另一个表的某一列所引用（如 Order 表的 ID 列被 OrderItem 的 OrderID 列所引用），则其被称为另一个表的外键。\n3 Dataverse 中的业务规则 我们可以在 Dataverse 中定义业务规则，这样即可以在数据层（而非应用层）应用业务逻辑。这有助于提高数据的准确性、简化应用程序开发和简化呈现给最终用户的表单。\n业务规则可被画布应用（Canvas App）和模型驱动应用（Model-driven App）用于填充或清空一个表某些列的值，也可被用于校验数据或展示错误信息。\n此外模型驱动应用还可以使用业务规则来显示或隐藏某些列，启用或禁用某些列，以及使用商业智能来创建推荐信息。\n下面举一个使用业务规则的例子：\n若信用限额字段Credit Limit的值超过 100 万美元，则将信用限额 VP 审批人字段Credit Limit VP Approver设置为必填字段。\n这样，在数据层应用业务规则，可以使得数据不论以何种方式访问（如在 Power Apps 或 Power Automate 中直接访问或通过 API 访问），都可以得到一致的控制。\n4 Dataverse 环境与管理中心 环境用于在 Power Platform 中存储、管理和共享企业的数据、应用程序和流程。我们可以在每个环境下新建对应的 Dataverse 数据库，从而在对应环境下进行访问控制、安全设置及存储配置。\n每个环境是在 Azure AD（Azure Active Directory）租户下创建的，因此只有对应租户下的人才有访问对应环境的权限。同时，环境也与地理位置（如 United States）绑定，所以在同一环境下创建的资源同属一个地理位置。\n我们可以以用途来区分环境（如开发、测试和生产）或以地理位置来区分环境（如 Asia、Europe 和 America）。\nPower Platform 的多数管理任务都可以在 Power Platform 管理中心进行，页面如下图所示：\n管理中心包括如下几个管理大项：\n 环境（Environments）- 列出所有的环境。可以从中看到所有的 Dataverse 实例。 数据策略（Data policies）- 用于设置数据策略。可以设置什么数据可以流入或流出 Dataverse 表。 数据集成（Data integration）- 管理及监控与外部系统的连接。如建立或监控 Dataverse 与 Salesforce 或 SQL Server 的连接。  综上，完成了对 Dataverse 基础知识的介绍。\n 参考资料\n[1] Introduction to Dataverse - microsoft.com\n[2] Microsoft Dataverse documentation - microsoft.com\n[3] Microsoft Dataverse - microsoft.com\n ","permalink":"https://olzhy.github.io/posts/microsoft-dataverse.html","tags":["Power Platform"],"title":"Microsoft Dataverse 基础"},{"categories":["计算机"],"contents":"Microsoft Power Platform 由 Power Apps、Power Automate、Power BI 和 Power Virtual Agents 四个产品组成，可以使用其来快速构建应用程序、自动化流程、分析数据和构建聊天机器人。\n本文首先会对 Power Platform 的四个产品的功能及应用场景作一个基础的介绍；接着会介绍一下 Power Platform 的商业价值。以期阅读完本文后，我们会对 Power Platform 有一个整体及初步的了解。\n1 Power Platform 的四大产品 下面是 Power Platform 的概览图。\n接下来，会对其几个主要的组成部分作一下简单介绍。\n  Power Apps\nPower Apps 提供一个低代码开发环境，其包含一组应用、服务和连接器套件，可以使用其来快速构建定制化的应用程序（支持 Web 应用和移动应用）。\n  Power Automate\nPower Automate 允许用户在各个服务和应用程序之间创建自动化工作流，可以使用其来将重复性的业务流程进行自动化。\n  Power BI\nPower BI（Business Intelligence，商业智能）是一个商业分析服务，可分析数据以提供见解，这些见解可通过数据可视化（报告或仪表盘）来进行分享，以实现快速、明智的决策。\n  Power Virtual Agents\nPower Virtual Agents 允许人们使用引导式、无代码的方式创建一个功能强大的聊天机器人。还可以借助 Power Automate 及数百个 Connectors 与现有系统进行无缝集成，如使用聊天机器人调用 Power Automate 流程（流程也可以调用后台系统）来执行一些操作。\n  除了上述的四个主要部分，下面列出的几个功能也为 Power Platform 注入了强大的潜能。\n  AI Builder\nAI Builder 允许开发人员无需编码即可为工作流和 Power Apps 添加 AI 能力。\n  Microsoft Dataverse\nMicrosoft Dataverse 是一个可扩展的数据服务，可存储和管理来自于多个数据源的数据。\n  Connectors\nConnectors 使您能够连接云上的应用、数据和设备。可将 Connectors 视为信息和命令传输的桥梁。Power Platform 有 600 多个连接器，流行的连接器有 Salesforce、Office 365 和 Twitter 等。\n  2 Power Platform 的商业价值 当今时代，很多公司缺乏足够的资源来交付快速变化的业务需求。他们面对的挑战主要有如下几个方面：\n  劳动力发生了变化\n千禧一代已经称为当今企业劳动力的主要组成部分。他们在定制化体验和社交媒体的相互作用及影响下长大。要充分运用他们的创造力，企业应该提供一些简单、协作、定制化的工具来适应他们的工作方式。\n  定制化应用程序开发成本高\n定制化应用程序的开发与维护成本高且费时费力。\n  业务需求多变且交付时间变短\n多年前，企业可以花数月来发布一个应用，然后用数周来做一些版本更新。如今，业务需求快速变化，企业需要具备快速交付这些需求的能力。\n  Power Platform 可以帮助企业更好的应对如上挑战。为什么呢？因 Power Platform 提供一系列低代码开发工具与企业级应用开发工具，借用这些工具可以使平民开发人员（Citizen Developers，低代码业内术语，如使用 Power 产品开发应用程序的开发人员）和专业开发人员合力进行更加快速高效的交付。\n具体如何做呢？比如要构建一个 Power App 项目来实现某个行业的某个应用场景的需求。最初可以让最了解需求最了解使用场景的一线用户或平民开发使用 Power Apps 来做出应用程序的原型；然后再由专业开发人员丰富、打磨和扩展这个 App 的功能。这种协作的开发方式被称为融合开发，它可以更加有效的利用资源。\n这种使用融合开发模式的开发周期如下图所示：\n说明：终端用户提出反馈后，先由平民开发做出原型，再由专业开发进行打磨，完成后再交给终端用户进行验证。如此循环往复来进行开发。\n借助了 Power Platform 低代码工具提供的能力，可以大大节省人力、财力，并缩短交付时间；借助预置的 600 多个 Connectors 可以轻松与内部或外部系统进行集成；这样即可以大大扩展企业的交付能力。这就是 Power Platform 的商业价值。\n此外，尽管我们生活在一个数据驱动的世界里，但企业一般比较难将各种数据利用起来。通过使用 Power Apps 开发的定制化应用、Power Automate 实现的自动化流程以及 Power BI 生成的分析报告，可以帮助企业更好的利用数据并做出决策。\n综上，本文对 Power Platform 的组成部分做了初步介绍并分析了其可能带来的商业价值。\n 参考资料\n[1] Introduction to Microsoft Power Platform - microsoft.com\n[2] Microsoft Power Platform - microsoft.com\n[3] What is the Power Platform? - encorebusiness.com\n ","permalink":"https://olzhy.github.io/posts/what-is-power-platform.html","tags":["Power Platform"],"title":"Power Platform 是什么？"},{"categories":["计算机"],"contents":"「喜马拉雅」是本人非常喜欢的一款音频软件。里面有很多优质的音频节目，丰富了我的日常生活。\n本文的诞生来自于我的个人需求：本人是喜马拉雅 APP 的重度用户，几乎每天都会使用其来听一些经典，它的确是给了我「每一天的精神食粮」。有时，有一些音频想下载下来反复听，但受限于手机的存储容量，不能随心将想听的音频进行下载。因此，便萌生了写点代码将音频下载到 U 盘，电脑或 MP3 设备的想法。\n需要特别说明的是：本程序不涉及登录，不涉及付费内容盗取，下载的是喜马拉雅网站免登录状态下完全公开的内容。\n接下来便依序介绍从 API 调研到 Golang 代码实现，以及如何使用及如何进行音频格式转换的整个过程。\n1 API 调研 专辑 ID 如何获取？ 要获取一个专辑（Album）下所有的音频，必须要知道专辑的 ID。\n如下图所示，访问喜马拉雅的某个专辑时，从浏览器地址栏可以看到该专辑的 ID（本例中，248003 就是「张庆祥讲孟子」专辑的 ID）。\n获取音频的 API 写作本文时，喜马拉雅网站：\n  根据专辑 ID 分页获取音频列表的 API 为：https://www.ximalaya.com/revision/album/v1/getTracksList?albumId=:albumId\u0026amp;pageNum=:pageNum\n  根据音频 ID 获取音频下载链接的 API 为：https://www.ximalaya.com/revision/play/v1/audio?id=:trackId\u0026amp;ptype=1\n  这样，有了专辑 ID，就可以根据上面第一个 API 查询到其下的所有音频详情（包括音频 ID、音频标题等）；有了音频 ID，就可以根据第二个 API 查询到音频地址，然后进行下载了。\n2 Golang 实现 实现音频下载的 Golang 代码已托管至本人 GitHub。\n因实现逻辑简单，整个实现的代码也不到 200 行，下面只看一下 main 函数的逻辑。\nfunc main() { // parameter validation  if len(os.Args) \u0026lt; 2 { fmt.Println(\u0026#34;please provide an album id\u0026#34;) return } albumId, err := strconv.Atoi(os.Args[1]) if err != nil { fmt.Println(\u0026#34;album id should be an integer\u0026#34;) return } fmt.Printf(\u0026#34;album id: %d\\n\u0026#34;, albumId) // get all track list  tracks, err := getAllTrackList(albumId) if err != nil { fmt.Printf(\u0026#34;error in get all track list, err: %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;all track list got, total: %d\\n\u0026#34;, len(tracks)) // get audio addresses  for _, track := range tracks { audioAddr, err := getAudioAddress(track.TrackId) if err != nil { fmt.Printf(\u0026#34;error in get audio address, err: %v\\n\u0026#34;, err) break } // download  filePath, err := download(audioAddr, track.Title, track.AlbumTitle) if err != nil { fmt.Printf(\u0026#34;error in audo download, err: %v\\n\u0026#34;, err) continue } fmt.Printf(\u0026#34;downloaded! file: %s\\n\u0026#34;, filePath) } } 可以看到，在main函数中：\n 首先作了参数校验（必须提供一个专辑 ID）； 调用 getAllTrackList 函数获取指定专辑下的所有 tracks（每个 track 中有音频 ID、标题等信息）； 遍历 tracks 数组，对每个 track，调用 getAudioAddress 函数获取音频地址，然后进行下载。  3 如何使用？ 使用起来非常简单，拥有 Go 运行环境的话，指定专辑 ID 直接运行main.go即可。示例如下：\ngo run main.go 248003 album id: 248003 all track list got, total: 140 downloaded! file: 张庆祥讲孟子/1.治国的根本.m4a downloaded! file: 张庆祥讲孟子/2.君王如何才能安心享乐？.m4a downloaded! file: 张庆祥讲孟子/3.五十步笑百步.m4a downloaded! file: 张庆祥讲孟子/4.仁者无敌.m4a ... 程序会在当前运行命令的文件夹下新建一个以该专辑命名的文件夹，并将该专辑下所有的音频逐条下载至该专辑文件夹下。下载结果如下图所示：\n4 m4a 如何转 mp3？ 下载完成后还有一个问题，就是喜马拉雅的音频格式为.m4a，该格式在苹果设备上播放没有问题，但其它设备的默认音频播放器不支持播放该种格式。\n怎么办呢？我们可以借助工具将其转换为更加通用的mp3格式。\n最简单直观的方式是搜索在线 m4a 到 mp3 转换工具（如 freeconvert.com/m4a-to-mp3），在网页上传文件并等待转换完成后进行下载即可。\n但作为程序员，本人更喜欢使用命令的方式进行转换。下面会介绍一个小工具 - FFmpeg，使用其即可进行 m4a 到 mp3 的音频转换。\n首先到「FFmpeg 下载页」找到与您当前操作系统匹配的可执行文件下载链接（本人使用的是 Mac，找到的下载链接为 evermeet.cx/ffmpeg），然后使用如下命令对压缩包下载、解压并将解压后的可执行文件ffmpeg移动至/usr/local/bin：\ncurl -O https://evermeet.cx/ffmpeg/ffmpeg-109029-g1800a0da09.zip unzip ffmpeg-109029-g1800a0da09.zip sudo mv ffmpeg /usr/local/bin 安装好了ffmpeg，即可使用如下命令对m4a文件进行转换了：\ncd 张庆祥讲孟子/ mkdir after for f in *.m4a; do ffmpeg -i \u0026#34;$f\u0026#34; -codec:v copy -codec:a libmp3lame -q:a 2 after/\u0026#34;${f%.m4a}.mp3\u0026#34;; done 如上命令首先进入待转换的m4a音频文件所在文件夹，然后创建转换后的文件存储文件夹after，最后对所有的m4a文件使用ffmpeg将其转换为mp3格式并放至文件夹after。转换后的结果如下图所示：\n有了这些更通用的 mp3 格式，即可在离线情况下在几乎任何可以播放音频的设备上随心去听这些音频了。\n综上，本文介绍了如何使用 Golang 实现喜马拉雅音频下载及 m4a 到 mp3 的格式转换方法。总结一下，主要作自己使用，也希望对感兴趣的同学有所帮助。\n 参考资料\n[1] 喜马拉雅 - ximalaya.com\n[2] FFmpeg - ffmpeg.org\n[3] FFMPEG: Convert m4a to mp3 without significant loss - superuser.com\n ","permalink":"https://olzhy.github.io/posts/ximalaya-audio-downloader.html","tags":["Golang"],"title":"使用 Golang 实现喜马拉雅音频下载"},{"categories":["随笔"],"contents":"周杰倫的磁帶 前段時間公司獎勵了一些「豆」，翻看可兌換的獎品，被一台錄音機吸引了。可以播放磁帶，也可以用來收聽廣播。\n錄音機收到後，從哪裡買磁帶卻成了頭疼的問題。從地圖上搜索「影像店」，將電話打過去，老闆說早已不賣磁帶了。\n網上搜索磁帶，發現沒幾年的時間，這種東西竟然成了古董。想買都很難買著。\n最後去二手平台淘到了幾盤周杰倫的帶，想著找找中學時代那種聽歌的感覺。播放鍵按下後，似乎這經歷幾多歲月的帶子裡的聲音也像被風吹雨打一樣，早已不是記憶裡的那般清脆動人了。\n\n\n\n\n\n\n\n\n\n80 後記憶裡的玩具 太太給孩子從網上買了一些我們當年小時候玩的玩具。今天到貨，打開袋子一看，一下子勾起了我們倆人各自童年裡關於這些玩具的很多快樂記憶。\n從現在的眼光來看這些玩具，顯得略有點粗糙。但在當時，卻讓我們這些 80 後愛不釋手，玩的不亦樂乎！\n\n\n\n\n","permalink":"https://olzhy.github.io/posts/huiyisha.html","tags":["随笔"],"title":"回憶殺"},{"categories":["计算机"],"contents":"本文依据文末 Azure 参考资料进行翻译及整理，作学习及知识总结之用。\n关系型数据是大多数业务应用程序的核心，也是构建许多企业数据解决方案的基础。 Azure 提供用于管理关系型数据库的服务，使您能够构建新应用程序或将现有应用程序迁移到云。\n1 基本的关系型数据概念 在计算系统的早期，每个应用程序都以自己独特的结构存储数据。当开发人员想要构建应用程序来使用这些数据时，他们必须对特定的数据结构有很多了解才能找到他们需要的数据。这些数据结构效率低下，难以维护，并且难以优化以获得良好的应用程序性能。关系数据库模型旨在解决多个任意数据结构的问题。关系模型提供了一种表示和查询可供任何应用程序使用的数据的标准方法。关系数据库模型的主要优势之一是它使用表，这是一种直观、高效且灵活的存储和访问结构化信息的方式。\n各种类型和规模的组织都使用简单而强大的关系模型来满足各种信息管理需求。关系型数据库用于跟踪库存、处理电子商务交易、管理大量关键任务客户信息等。关系型数据库可用于存储任何包含相关数据元素的信息，这些数据元素必须以基于规则的一致结构进行组织。\n在本部分，您将了解关系型数据库的关键特征，并探索关系数据结构。\n1.1 了解关系型数据 在关系型数据库中，您将来自现实世界的实体集合建模为表。实体可以是您要为其记录信息的任何事物；通常是重要的对象和事件。例如，在零售系统示例中，您可以为客户、产品、订单和订单中的行项目创建表。一个表包含多行，每一行代表一个实体的一个实例。\n关系表是结构化数据的一种格式，表中的每一行都有相同的列；尽管在某些情况下，并非所有列都需要有值（如上图中的 MiddleName）。\n每列存储特定数据类型的数据。如，客户表中的电子邮件列可能被定义为存储字符类型的数据（长度可能是固定的或可变的），产品表中的价格列可能被定义为存储十进制数字数据，Order 表中的 Quantity 列可能被限制为整数数值，而 Order 表中的 OrderDate 列用来存储日期或时间值。定义表时可以使用的数据类型取决于您使用的数据库系统，尽管大多数数据库系统都支持 ANSI（American National Standards Institute，美国国家标准协会）定义的标准数据类型。\n1.2 了解规范化 规范化是数据库专业人员用于模式设计过程的一个术语，其将数据重复降至最低并加强了数据完整性。\n虽然有许多复杂的规则定义了将数据重构为不同级别的规范化过程，但一个出于实际目的的简单定义为：\n 将每个实体分割到自己的表中； 将每个离散属性分成自己的列； 使用主键唯一标识每个实体实例（行）； 使用外键列链接相关实体。  要了解规范化的核心原则，假设下表代表公司用来跟踪其销售的电子表格。\n请注意，如上表中客户和产品详细信息对于所售出的每件商品都是重复的；并且客户姓名和邮政地址，以及产品名称和价格组合在同一个电子表格单元格中。\n现在让我们看看规范化如何改变数据的存储方式。\n数据中表示的每个实体（客户、产品、销售订单和行条目）都存储在自己的表中，这些实体的每个离散属性都在自己的列中。\n将实体的每个实例记录为实体特定表中的一行可以消除数据重复。例如，要更改客户的地址，您只需修改单行中的值。\n将属性分解为单独的列可确保将每个值限制为适当的数据类型。例如，产品价格必须是十进制值，而行条目数量必须是整数。此外，单个列的创建为查询数据提供了有用的粒度级别。例如，您可以轻松地将客户筛选为居住在特定城市的人。\n每个实体的实例由一个 ID（或称为主键） 唯一标识，当一个实体引用另一个实体时（例如，订单有关联的客户），相关实体的主键被存储为外键。您可以通过引用 Customer 表中的相应记录来查找 Order 表中每条记录的客户地址（仅存储一次）。通常，关系数据库管理系统 (RDBMS) 可以强制执行参照完整性，以确保输入到外键字段中的值在相关表中具有存在的对应主键 —— 例如，阻止不存在的客户订单。\n在某些情况下，可以将键（主键或外键）定义为基于多个列的唯一组合的组合键。例如，上例中的 LineItem 表使用 OrderNo 和 ItemNo 的唯一组合来识别单个订单中的行项目。\n1.3 探索 SQL SQL 代表结构化查询语言（Structured Query Language），用于与关系数据库进行通信。SQL 语句用于执行诸如更新或检索数据等任务。一些使用 SQL 的常见关系数据库管理系统包括 Microsoft SQL Server、MySQL、PostgreSQL、MariaDB 和 Oracle。\nSQL 最初于 1986 年由美国国家标准协会 (ANSI，American National Standards Institute) 标准化，并于 1987 年由国际标准化组织 (ISO，International Organization for Standardization) 标准化。从那时起，随着关系数据库供应商向其系统中添加新功能，该标准已多次扩展.此外，大多数数据库供应商都包括他们自己的专有扩展，这些扩展不属于标准的一部分，这导致了各种 SQL 方言。\n您可以使用 SELECT、INSERT、UPDATE、DELETE、CREATE 和 DROP 等 SQL 语句来完成几乎所有需要对数据库执行的操作。尽管这些 SQL 语句是 SQL 标准的一部分，但许多数据库管理系统也有自己的附加专有扩展来处理该数据库管理系统的细节。这些扩展提供 SQL 标准未涵盖的功能，并包括安全管理和可编程性等领域。\n例如，基于 SQL Server 数据库引擎的 Microsoft SQL Server 和 Azure 数据库服务使用了 Transact-SQL。该实现包括用于编写存储过程、触发器（可以存储在数据库中的应用程序代码）以及管理用户帐户的专有扩展。 PostgreSQL 和 MySQL 也有自己的这些特性的版本。\n一些流行的 SQL 方言包括：\n Transact-SQL (T-SQL)，此版本的 SQL 由 Microsoft SQL Server 和 Azure SQL 服务所使用。 pgSQL，这是在 PostgreSQL 中实现了扩展的 SQL 方言。 PL/SQL，这是甲骨文使用的方言。 PL/SQL 代表 Procedural Language/SQL。  计划专门使用单个数据库系统的用户应该了解他们首选的 SQL 方言和平台的复杂性。\n除非另有说明，本模块中的 SQL 代码示例均基于 Transact-SQL 方言。其他方言的语法大体相似，但在某些细节上可能有所不同。\nSQL 语句类型 SQL 语句主要分三种：\n 数据定义语言 (Data Definition Language，DDL) 数据控制语言 (Data Control Language，DCL) 数据操作语言 (Data Manipulation Language，DML)  DDL 语句\n您可以使用 DDL 语句来创建、修改和删除数据库中的表和其他对象（表、存储过程、视图等）。\n最常见的 DDL 语句是：\n   语句 描述     CREATE 在数据库中创建一个新对象，例如表或视图。   ALTER 修改对象的结构。例如，更改表以添加新列。   DROP 从数据库中删除一个对象。   RENAME 重命名现有对象。    DROP 语句非常强大。删除表时，该表中的所有行都将丢失。除非您有备份，否则您将无法检索此数据。\n下面的示例创建了一个新的数据库表。括号之间内指定了每列的详细信息，包括名称、数据类型、值是否非空（NOT NULL）以及列中的数据是否用于唯一（PRIMARY KEY ）。每个表都应该有一个主键，尽管 SQL 不强制执行此规则。\n标记为 NOT NULL 的列称为强制列。如果省略 NOT NULL 子句，则可以创建列中不包含值的行。行中的空列被称为含有 NULL 值。\nCREATE TABLE Product ( ID INT PRIMARY KEY, Name VARCHAR(20) NOT NULL, Price DECIMAL NULL ); 表中列可用的数据类型因不同的数据库管理系统而异。但是，大多数数据库管理系统都支持诸如 INT 数值类型（整数）、DECIMAL（十进制数）和字符串（如 VARCHAR）类型。\nDCL 语句\n数据库管理员通常使用 DCL 语句来授予、拒绝或撤销特定用户或组的权限来管理对数据库中对象的访问。\n三个主要的 DCL 语句是：\n   语句 描述     GRANT 授予执行特定操作的权限。   DENY 拒绝执行特定操作的权限。   REVOKE 删除之前授予的权限。    例如，以下 GRANT 语句允许名为 user1 的用户读取、插入和修改 Product 表中的数据。\nGRANT SELECT, INSERT, UPDATE ON Product TO user1; DML 语句\n您使用 DML 语句来操作表中的行。这些语句使您能够检索（查询）数据、插入新行或修改现有行。如果不再需要某些行，也可以删除它们。\n四个主要的 DML 语句是：\n   语句 描述     SELECT 从表中读取行。   INSERT 向表中插入新行。   UPDATE 修改现有行中的数据。   DELETE 删除现有行。    INSERT 语句的基本形式是一次插入一行。默认情况下，SELECT、UPDATE 和 DELETE 语句应用于表中的每一行。通常在这些语句中应用 WHERE 子句来指定条件，只有符合这些条件的行才会被选择、更新或删除。\nSQL 没有二次确认提示。因此在使用不带 WHERE 子句的 DELETE 或 UPDATE 时要小心，因为您可能会丢失或修改大量数据。\n以下代码是一个 SQL 语句示例，该语句从 Customer 表中选择 City 列值为“Seattle”的所有列（用 * 表示）：\nSELECT * FROM Customer WHERE City = \u0026#39;Seattle\u0026#39;; 仅从表中检索特定的列，请在 SELECT 子句中列出它们，SQL 如下：\nSELECT FirstName, LastName, Address, City FROM Customer WHERE City = \u0026#39;Seattle\u0026#39;; 如果查询返回许多行，它们不一定以任何特定顺序出现。如果要对数据进行排序，可以添加 ORDER BY 子句。数据将按指定列排序：\nSELECT FirstName, LastName, Address, City FROM Customer WHERE City = \u0026#39;Seattle\u0026#39; ORDER BY LastName; 您还可以在 SELECT 语句中使用 JOIN 子句从多个表中检索数据。JOIN 指示一个表中的行如何与另一个表中的行连接以确定要返回的数据。典型的连接条件匹配一个表中的外键及其在另一个表中的关联主键。\n以下查询显示了连接 Customer 和 Order 表的示例。当指定在 SELECT 子句中检索哪些列以及在 JOIN 子句中匹配哪些列时，查询使用表别名来缩写表名。\nSELECT o.OrderNo, o.OrderDate, c.Address, c.City FROM Order AS o JOIN Customer AS c ON o.Customer = c.ID 下一个示例显示如何使用 SQL 修改现有行。对于 ID 列中值为 1 的行，它会更改 Customer 表中 Address 列的值。所有其它行保持不变：\nUPDATE Customer SET Address = \u0026#39;123 High St.\u0026#39; WHERE ID = 1; 如果省略 WHERE 子句，UPDATE 语句将修改表中的所有行。\n使用 DELETE 语句删除行。指定表名，以及标识要删除的行的 WHERE 子句：\nDELETE FROM Product WHERE ID = 162; 如果省略 WHERE 子句，则 DELETE 语句将删除表中的所有行。\nINSERT 语句的形式略有不同。在 INTO 子句中指定表和列，以及要存储在这些列中的值列表。标准 SQL 仅支持一次插入一行，如下例所示。某些方言允许您指定多个 VALUES 子句以一次添加多行：\nINSERT INTO Product(ID, Name, Price) VALUES (99, \u0026#39;Drill\u0026#39;, 4.99); 1.4 描述数据库对象 除了表之外，关系数据库还可以包含其他有助于优化数据组织、封装编程操作和提高访问速度的结构。在本单元中，您将更详细地了解其中三种结构：视图、存储过程和索引。\n什么是视图？ 视图是基于 SELECT 查询结果的虚拟表。您可以将视图视为一个或多个基础表中指定行的窗口。例如，您可以在 Order 和 Customer 表上创建一个视图，该视图检索订单和客户数据以提供单个对象，以便轻松确定订单的交货地址：\nCREATE VIEW Deliveries AS SELECT o.OrderNo, o.OrderDate, c.FirstName, c.LastName, c.Address, c.City FROM Order AS o JOIN Customer AS c ON o.Customer = c.ID; 您可以像查询表一样查询视图和过滤数据。以下查询查找居住在 Seattle 的客户的订单详细信息：\nSELECT OrderNo, OrderDate, LastName, Address FROM Deliveries WHERE City = \u0026#39;Seattle\u0026#39;; 什么是存储过程？ 存储过程定义了可以在命令上运行的 SQL 语句。存储过程用于将程序逻辑封装在数据库中，以用于应用程序在处理数据时需要执行的操作。\n您可以使用参数定义存储过程，为可能需要根据特定键或条件应用于数据的常见操作创建灵活的解决方案。例如，可以定义以下存储过程来根据指定的产品 ID 更改产品的名称。\nCREATE PROCEDURE RenameProduct @ProductID INT, @NewName VARCHAR(20) AS UPDATE Product SET Name = @NewName WHERE ID = @ProductID; 当需要重命名产品时，您可以传递产品的 ID 和要分配的新名称来执行如下存储过程：\nEXEC RenameProduct 201, \u0026#39;Spanner\u0026#39;; 什么是索引？ 索引可帮助您搜索表中的数据。将表格上的索引想象成书后的索引。书籍索引包含一组排序的参考文献，以及每个参考文献出现的页面。当你想在书中找到对某个项目的引用时，你可以通过索引来查找它。您可以使用索引中的页码直接转到书中正确的页面。如果没有索引，您可能必须通读整本书才能找到您要查找的参考资料。\n在数据库中创建索引时，您从表中指定一列，并且索引包含该数据排序后的副本，这些副本带有指向表中相应行的指针。当用户运行在 WHERE 子句中指定该列的查询时，数据库管理系统可以使用该索引来更快地获取数据，而不是必须逐行扫描整个表。\n例如，您可以使用以下代码在 Product 表的 Name 列上创建索引：\nCREATE INDEX idx_ProductName ON Product(Name); 该索引创建了一个基于树的结构，数据库系统的查询优化器可以使用该结构根据指定的名称在 Product 表中快速查找行。\n对于包含少量行的表，使用索引可能并不比简单地读取整个表并查找请求的行更有效（在这种情况下，查询优化器将忽略索引）。但是，当一个表有很多行时，索引可以显著提高查询的性能。\n您可以在一个表上创建许多索引。因此，如果您还想根据价格查找产品，则在 Product 表的 Price 列上创建另一个索引可能会很有用。但是，索引不是免费的。索引会消耗存储空间，每次在表中插入、更新或删除数据时，都必须维护该表的索引。这种额外的工作会减慢插入、更新和删除操作。您必须在拥有加快查询速度的索引与执行其他操作的成本之间取得平衡。\n2 Azure 中的关系型数据库服务 Azure 支持多种数据库服务，使您能够在云上运行流行的关系型数据库管理系统，例如 SQL Server、PostgreSQL 和 MySQL。\n大多数 Azure 数据库服务都是完全托管的，从而节省了您原本用于管理数据库的宝贵时间。具有内置高可用性的企业级性能意味着您可以快速扩展并实现全球分布，而无需担心代价高昂的停机时间。开发人员可以利用行业领先的创新，例如具有自动监控和威胁检测功能的内置安全性、自动调整以提高性能。除了所有这些功能之外，您还可以保证可用性。\n在本模块中，你将探索 Azure 中的关系型数据库服务的可用选项。\n2.1 Azure SQL 服务和功能 Azure SQL 是 Azure 中一系列基于 Microsoft SQL Server 的数据库服务的统称。特定的 Azure SQL 服务包括：\n  Azure 虚拟机上的 SQL Server - 在 Azure 虚拟机上运行的 SQL Server。使用 VM 使此选项成为基础架构即服务 (IaaS) 解决方案，可虚拟化用于 Azure 中的计算、存储和网络的硬件基础架构；使其成为将现有 On-Premise SQL Server 安装“直接迁移”到云的绝佳选择。\n  Azure SQL 托管实例 - 一种平台即服务 (PaaS) 选项，可提供与本地 SQL Server 实例近 100% 的兼容性，同时抽象底层硬件和操作系统。该服务包括自动化软件更新管理、备份和其他维护任务，减少了支持数据库服务器实例的管理负担。\n  Azure SQL Database - 一种完全托管、高度可扩展的 PaaS 数据库服务，专为云而设计。此服务包括 On-Premise SQL Server 的核心数据库级功能，当您需要在云上创建新应用程序时，它是一个不错的选择。\n  Azure SQL Edge - 针对需要处理流时间序列数据的物联网 (IoT) 场景进行了优化的 SQL 引擎。\n  Azure SQL 服务比较     Azure 虚拟机上的 SQL Server Azure SQL 托管实例 Azure SQL Database     云服务类型 IaaS PaaS PaaS   SQL Server 兼容性 与 On-Premise 物理和虚拟化安装完全兼容。应用程序和数据库可以轻松地“直接迁移”而无需更改。 与 SQL Server 几乎 100% 兼容。可以使用 Azure 数据库迁移服务迁移大多数本地数据库，只需作最少的代码更改。 支持 SQL Server 的大多数核心数据库级功能。On-Premise 应用程序所依赖的某些功能可能不可用。   架构 SQL Server 实例安装在虚拟机中。每个实例可以支持多个数据库。 每个托管实例可以支持多个数据库。此外，实例池可用于在较小的实例之间有效地共享资源。 您可以在专用的托管（逻辑）服务器中配置单个数据库；或者您可以使用弹性池在多个数据库之间共享资源并利用按需可扩展性。   可用性 99.99% 99.99% 99.995%   管理 您必须管理服务器的所有方面，包括操作系统和 SQL Server 更新、配置、备份和其他维护任务。 全自动更新、备份和恢复。 全自动更新、备份和恢复。   使用场景 当您需要迁移或扩展 On-Premise SQL Server 解决方案并保留对服务器和数据库配置的完全控制时，请使用此选项。 此选项适用于大多数云迁移方案，尤其是当您需要对现有应用程序进行最小更改时。 此选项适用于新的云解决方案，或迁移具有最少实例级依赖性的应用程序。    Azure 虚拟机上的 SQL Server 虚拟机上的 SQL Server 使您能够在云上使用完整版本的 SQL Server，而无需管理任何本地硬件。这是 IaaS 方法的一个示例。\n在 Azure 虚拟机上运行的 SQL Server 有效地复制了在真实 On-Premise 硬件上运行的数据库。从 On-Premise 运行的系统迁移到 Azure 虚拟机与将数据库从一个 On-Premise 服务器移动到另一个服务器没有什么不同。\n此方法适用于需要访问 PaaS 级别可能不支持的操作系统功能的迁移和应用程序。 SQL 虚拟机已为需要快速迁移到云且更改最少的现有应用程序做好了直接迁移的准备。您还可以使用 Azure VM 上的 SQL Server 将现有的本地应用程序扩展到混合部署中的云。\n混合部署是一个系统，其中部分操作在本地运行，部分在云中运行。尽管数据库元素可能托管在云中，但您的数据库可能是在本地运行的更大系统的一部分。\n您可以在虚拟机中使用 SQL Server 来开发和测试传统的 SQL Server 应用程序。使用虚拟机，您对 DBMS 和操作系统拥有完全的管理权限。当组织已经拥有可用于维护虚拟机的 IT 资源时，这是一个完美的选择。\n这些功能使您能够：\n 当您不想购买 On-Premise 非生产 SQL Server 硬件时，创建快速开发和测试方案。 为需要快速迁移到云的现有应用程序做好直接迁移准备，只需极少更改或无需更改。 通过为虚拟机分配更多内存、CPU 算力和磁盘空间来扩展运行 SQL Server 的平台。您可以快速调整 Azure 虚拟机的规格，而无需重新安装在其上运行的软件。  商业效益\n在虚拟机上运行 SQL Server 允许您通过结合 On-Premise 部署和云托管部署来满足独特且多样化的业务需求，同时在这些环境中使用相同的服务器产品、开发工具和专业知识集。\n企业将其 DBMS 转换为完全托管的服务并不总是那么容易。为了迁移到需要对数据库和使用它的应用程序进行更改的托管服务，可能必须满足特定要求。出于这个原因，使用虚拟机可以提供解决方案，但使用它们并不能消除像在 On-Premise 一样仔细管理 DBMS 的需要。\nAzure SQL 数据库托管实例 Azure SQL 托管实例有效地在云中运行完全可控的 SQL Server 实例。您可以在同一个实例上安装多个数据库。您可以完全控制此实例，就像控制 On-Premise 服务器一样。 SQL 托管实例可自动执行备份、软件修补、数据库监控和其它常规任务，但您可以完全控制数据库的安全性和资源分配。\n托管实例依赖于其它 Azure 服务，例如用于备份的 Azure 存储、用于遥测的 Azure 事件中心、用于身份验证的 Azure Active Directory、用于透明数据加密 (TDE) 的 Azure Key Vault 以及一些提供安全性和可支持性功能的 Azure 平台服务。托管实例与这些服务建立连接。\n所有通信都使用证书加密和签名。为了检查通信方的可信度，托管实例通过证书吊销列表不断验证这些证书。如果证书被吊销，托管实例将关闭连接以保护数据。\n使用场景\n如果您想将 On-Premise SQL Server 实例及其所有数据库直接迁移到云端，而不会产生在虚拟机上运行 SQL Server 的管理开销，请考虑使用 Azure SQL 托管实例。\nAzure SQL 托管实例提供 Azure SQL Database 中不可用的功能。如果您的系统使用链接服务器、Service Broker（可用于跨服务器分发工作的消息处理系统）或数据库邮件（使您的数据库能够向用户发送电子邮件）等功能，那么您应该使用托管实例。要检查与现有本地系统的兼容性，您可以安装数据迁移助手 (Data Migration Assistant，DMA)。此工具分析您在 SQL Server 上的数据库并报告可能阻止迁移到托管实例的任何问题。\n商业效益\nAzure SQL 托管实例使系统管理员能够在管理任务上花费更少的时间，因为该服务可以为您执行这些任务或大大简化这些任务。自动化任务包括操作系统和数据库管理系统软件安装和修补、动态实例规格调整和配置、备份、数据库复制（包括系统数据库）、高可用性配置以及运行状况和性能监控数据流的配置。\nAzure SQL 托管实例与本地运行的 SQL Server 企业版几乎 100% 兼容。\nAzure SQL 托管实例支持 SQL Server 数据库引擎登录和与 Azure Active Directory (AD) 集成的登录。 SQL Server 数据库引擎登录包括用户名和密码。每次连接到服务器时都必须输入凭据。 Azure AD 登录使用与当前计算机登录关联的凭据，您无需在每次连接到服务器时都提供这些凭据。\nAzure SQL Database Azure SQL Database 是 Microsoft 的 PaaS 产品。您在云上创建托管数据库服务器，然后在此服务器上部署您的数据库。\nSQL 数据库服务器是一个逻辑结构，它充当多个单一或共用数据库、登录、防火墙规则、审核规则、威胁检测策略和故障转移组的中央管理点。\nAzure SQL Database 可用作单一数据库或弹性池。\n单一数据库\n此选项使您能够快速设置和运行单个 SQL Server 数据库。您在云上创建并运行数据库服务器，并通过该服务器访问您的数据库。 Microsoft 管理服务器，因此您所要做的就是配置数据库、创建表并用您的数据填充它们。如果您需要更多的存储空间、内存或处理能力，您可以扩展数据库。默认情况下，资源是预先分配的，您需要按小时为您请求的资源付费。您还可以指定无服务器配置。在此配置中，Microsoft 创建自己的服务器，该服务器可能由属于其他 Azure 订阅者的数据库共享。 Microsoft 确保您的数据库的隐私。您的数据库会自动扩展，并根据需要分配或取消分配资源。\n弹性池\n此选项与 Single Database 类似，不同之处在于默认情况下，多个数据库可以通过多租户共享相同的资源，例如内存、数据存储空间和处理能力。这些资源称为池。您创建池，并且只有您的数据库可以使用该池。如果您的数据库的资源需求随时间变化，此模型很有用，并且可以帮助您降低成本。例如，您的工资单数据库在每个月末处理工资单时可能需要大量的 CPU 算力，但在其它时候数据库可能会变得不那么活跃。您可能有另一个用于运行报告的数据库。随着管理报告的生成，该数据库可能会在月中的几天内处于活动状态，但在其他时间负载较轻。弹性池允许您使用池中可用的资源，然后在处理完成后释放资源。\n使用场景\nAzure SQL Database 为你提供了低成本和最少管理的最佳选择。它与 On-Premise SQL Server 安装不完全兼容。它通常用于新的云项目中，其中应用程序设计可以适应对应用程序的任何所需更改。\nAzure SQL Database 通常用于：\n 需要使用最新稳定 SQL Server 功能的现代云应用程序。 需要高可用性的应用程序。 具有可变负载的系统需要数据库服务器快速向上和向下扩展。  商业效益\nAzure SQL Database 会自动更新和修补 SQL Server 软件，以确保您始终运行最新且最安全的服务版本。\nAzure SQL Database 的可扩展性功能确保您可以增加可用于存储和处理数据的资源，而无需执行成本高昂的手动升级。\n该服务提供高可用性保证，以确保您的数据库至少有 99.995% 的时间可用。 Azure SQL Database 支持按时间点还原，使你能够将数据库恢复到过去任何时候的状态。数据库可以复制到不同的区域，以提供更多的弹性和灾难恢复。\n高级威胁防护提供高级安全功能，例如漏洞评估，以帮助检测和修复数据库的潜在安全问题。威胁防护还检测异常活动，这些活动表明访问或利用您的数据库的异常和潜在有害尝试。它持续监控您的数据库中的可疑活动，并针对潜在漏洞、SQL 注入攻击和异常数据库访问模式提供即时安全警报。威胁检测警报提供可疑活动的详细信息，并就如何调查和缓解威胁提出建议。\n审核跟踪数据库事件并将它们写入 Azure 存储帐户中的审核日志。审计可以帮助您保持合规性，了解数据库活动，并深入了解可能表明业务问题或可疑安全违规的差异和异常。\nAzure SQL Database 通过提供加密保护存储在数据库中的数据（静态）和通过保护网络传输（动态）的数据来使您的数据更安全。\n2.2 针对开源数据库的 Auzre 服务 除了 Azure SQL 服务之外，Azure 数据服务还可用于其它流行的关系型数据库系统，包括 MySQL、MariaDB 和 PostgreSQL。这些服务的主要原因是使在 On-Premise 应用程序中使用它们的组织能够快速迁移到 Azure，而无需对其应用程序进行重大更改。\nMySQL、MariaDB 和 PostgreSQL 是什么？ MySQL、MariaDB 和 PostgreSQL 是为不同专业量身定制的关系型数据库管理系统。\nMySQL 最初是一个简单易用的开源数据库管理系统。它是适用于 Linux、Apache、MySQL 和 PHP (LAMP) 技术栈应用程序的领先开源关系型数据库。它有多个版本；社区版、标准版和企业版。社区版是免费提供的，并且作为在 Linux 下运行的 Web 应用程序的数据库管理系统在历史上一直很受欢迎。也有 Windows 版本。标准版提供更高的性能，并使用不同的技术来存储数据。企业版提供了一套全面的工具和功能，包括增强的安全性、可用性和可扩展性。标准版和企业版是商业组织最常使用的版本，尽管这些版本的软件不是免费的。\nMariaDB 是一种较新的数据库管理系统，由 MySQL 的原始开发人员创建。此后，数据库引擎已被重写和优化以提高性能。 MariaDB 提供与 Oracle 数据库的兼容性。 MariaDB 的一个显着特点是其对时态数据的内置支持。一个表可以包含多个版本的数据，使应用程序能够查询过去某个时间点出现的数据。\nPostgreSQL 是一个混合关系对象数据库。您可以将数据存储在关系表中，但 PostgreSQL 数据库还使您能够存储自定义数据类型，以及它们自己的非关系属性。数据库管理系统可扩展；您可以将代码模块添加到数据库中，这些模块可以通过查询运行。另一个关键特性是能够存储和操作几何数据，例如线、圆和多边形。\nPostgreSQL 有自己的查询语言，称为 pgsql。该语言是标准关系查询语言 SQL 的变体，具有使您能够编写在数据库中运行的存储过程的功能。\nAzure Database for MySQL Azure Database for MySQL 是 Azure 云上基于 MySQL 社区版的 PaaS 实现。\nAzure Database for MySQL 服务包括无需额外费用的高可用性和所需的可伸缩性。您只需为使用的内容付费。提供自动备份和时间点恢复。\n服务器提供连接安全性以强制执行防火墙规则，并且可以选择要求 SSL 连接。许多服务器参数使您能够配置服务器设置，例如锁定模式、最大连接数和超时。\nAzure Database for MySQL 提供了一个全局数据库系统，可以扩展到大型数据库，而无需管理硬件、网络组件、虚拟服务器、软件补丁和其他底层组件。\nAzure Database for MySQL 不支持某些操作。这些功能主要与安全和管理有关。 Azure 管理数据库服务器本身的这些方面。\nAzure Database for MySQL 的优势\nAzure Database for MySQL 具有以下功能：\n 内置高可用性功能。 可预测的性能。 轻松扩展，快速响应需求。 保护静态和动态数据。 过去 35 天的自动备份和时间点恢复。 企业级安全性和法规遵从性。  该系统使用即用即付定价，因此您只需为使用的产品付费。\nAzure Database for MySQL 服务器提供监视功能以添加警报以及查看指标和日志。\nAzure Database for MariaDB Azure Database for MariaDB 是适用于在 Azure 中运行的基于 MariaDB 社区版的数据库管理系统的实现。\n该数据库由 Azure 完全管理和控制。配置服务并传输数据后，系统几乎不需要额外的管理。\nAzure Database for MariaDB 的优势\nAzure Database for MariaDB 提供：\n 内置高可用性，无需额外费用。 可预测的性能，使用包容性的即用即付定价。 在几秒钟内根据需要进行伸缩。 对静态和动态敏感数据进行安全保护。 自动备份和时间点恢复长达 35 天。 企业级安全性和合规性。  Azure Database for PostgreSQL 如果你更喜欢 PostgreSQL，可以选择 Azure Database for PostgreSQL 在 Azure 云上运行 PostgreSQL 的 PaaS 实现。该服务提供与 MySQL 服务相同的可用性、性能、可扩展性、安全性和管理优势。\nOn-Premise PostgreSQL 数据库的某些功能在 Azure Database for PostgreSQL 中不可用。这些功能主要与用户可以添加到数据库以执行特定任务的扩展有关，例如用各种编程语言（除了可用的 pgsql）编写存储过程，以及直接与操作系统交互。支持一组最常用的扩展核心，并且可用扩展列表正在持续审查中。\nAzure Database for PostgreSQL Flexible Server\nPostgreSQL 的灵活服务器部署选项是完全托管的数据库服务。它提供高级别的控制和服务器配置定制，并提供成本优化控制。\nAzure Database for PostgreSQL 的优势\nAzure Database for PostgreSQL 是一个高可用性服务。它包含内置的故障检测和故障转移机制。\nPostgreSQL 的用户熟悉 pgAdmin 工具，您可以使用它来管理和监控 PostgreSQL 数据库。你可以继续使用此工具连接到 Azure Database for PostgreSQL。但是，某些以服务器为中心的功能（例如执行服务器备份和还原）不可用，因为服务器由 Azure 管理和维护。\nAzure Database for PostgreSQL 记录有关针对服务器上的数据库运行的查询的信息，并将它们保存在名为 azure_sys 的数据库中。您查询 query_store.qs_view 视图以查看此信息，并使用它来监控用户正在运行的查询。如果您需要微调应用程序执行的查询，这些信息将证明是非常宝贵的。\n 参考资料\n[1] Exam DP-900: Microsoft Azure Data Fundamentals - learn.microsoft.com\n ","permalink":"https://olzhy.github.io/posts/dp-900-part2-relational-data.html","tags":["Azure"],"title":"DP-900：Azure 数据基础第二部分之 Azure 中的关系型数据"},{"categories":["计算机"],"contents":"本文依据文末 Azure 参考资料进行翻译及整理，作学习及知识总结之用。\n介绍常见的数据格式、工作负载，以及角色和服务。\n1 核心数据概念 1.1 常用数据格式 数据是事实的集合，如用于记录信息的数字、描述和观察结果。组织这些数据的数据结构通常表示对组织很重要的实体（如：客户、产品、销售订单等）。每个实体通常具有一个或多个属性（如：客户可能有姓名、地址和电话号码等）。\n数据可被分类为：结构化、半结构化和非结构化。\n结构化数据 结构化数据是遵循固定模式的数据，因此所有数据都具有相同的字段或属性。结构化数据实体的模式通常是表格化的：即数据由一个或多个表表示，表由行和列（行表示数据实体的实例，列表示数据实体的属性）来组成。\n例如，下图展示了 Customer 和 Product 数据实体的表格化表示。\n结构化数据通常存储在数据库中，其中多个表可以通过使用关系模型中的键值来相互引用。\n半结构化数据 半结构化数据是具有某种结构的信息，但它允许实体实例之间存在一些变化。例如，虽然大多数客户可能只有一个电子邮件，但有些可能有多个，有些可能一个都没有。\n半结构化数据的一种常见格式是 JavaScript Object Notation（JSON）。下面的示例显示了一对表示客户信息的 JSON 文档。每个客户文档都包含地址和联系信息，但具体字段因客户而异。\n// Customer 1 { \u0026#34;firstName\u0026#34;: \u0026#34;Joe\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Jones\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;streetAddress\u0026#34;: \u0026#34;1 Main St.\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;New York\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;NY\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;10099\u0026#34; }, \u0026#34;contact\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555 123-1234\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;joe@litware.com\u0026#34; } ] } // Customer 2 { \u0026#34;firstName\u0026#34;: \u0026#34;Samir\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Nadoy\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;streetAddress\u0026#34;: \u0026#34;123 Elm Pl.\u0026#34;, \u0026#34;unit\u0026#34;: \u0026#34;500\u0026#34;, \u0026#34;city\u0026#34;: \u0026#34;Seattle\u0026#34;, \u0026#34;state\u0026#34;: \u0026#34;WA\u0026#34;, \u0026#34;postalCode\u0026#34;: \u0026#34;98999\u0026#34; }, \u0026#34;contact\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;samir@northwind.com\u0026#34; } ] } JSON 只是可以表示半结构化数据的众多方式之一。这里的重点不是提供对 JSON 语法的详细检查，而是说明半结构化数据表示的灵活性。\n非结构化数据 并非所有数据都是结构化或半结构化的。例如，文档、图像、音频和视频数据以及二进制文件可能没有特定的结构。这种数据被称为非结构化数据。\n数据存储 因数据有结构化、半结构化和非结构化这三种类型，所以常用的数据存储可归为两大类：数据库和文件存储。\n1.2 文件存储 用于存储数据的特定文件格式取决于许多因素，包括：\n 存储的数据类型（结构化、半结构化或非结构化）； 需要读取、写入和处理数据的应用程序和服务； 需要人类可读的数据文件，或为有效存储和处理而优化的数据文件。  下面讨论一些常见的文件格式。\n分隔的文本文件 数据通常以带有特定字段分隔符和行终止符的纯文本格式存储。最常见的分隔数据格式是字段由逗号分隔，行由回车/换行符终止。第一行可以是值也可以是字段名称。\n其他常见的格式包括制表符分隔 (TSV) 和空格分隔（其中制表符或空格用于分隔字段），以及为每个字段分配固定数量字符的固定宽度数据。\n对于需要以人类可读格式访问的各种应用程序和服务的结构化数据，分隔文本是一个不错的选择。\n以下示例以逗号分隔格式显示一组客户数据：\nFirstName,LastName,Email Joe,Jones,joe@litware.com Samir,Nadoy,samir@northwind.com JSON JSON 是一种普遍存在的格式，其中使用分层文档模式来定义具有多个属性的数据实体（对象）。JSON 中的每个属性可以是一个对象（或对象的集合）；使 JSON 成为一种适用于结构化和半结构化数据的灵活格式。\n如下示例显示了一个包含客户集合的 JSON 文档。每个客户都有三个属性（名字、姓氏和联系人），而联系人属性可包含多个联系方式。\n{ \u0026#34;customers\u0026#34;: [ { \u0026#34;firstName\u0026#34;: \u0026#34;Joe\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Jones\u0026#34;, \u0026#34;contact\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;number\u0026#34;: \u0026#34;555 123-1234\u0026#34; }, { \u0026#34;type\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;joe@litware.com\u0026#34; } ] }, { \u0026#34;firstName\u0026#34;: \u0026#34;Samir\u0026#34;, \u0026#34;lastName\u0026#34;: \u0026#34;Nadoy\u0026#34;, \u0026#34;contact\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;address\u0026#34;: \u0026#34;samir@northwind.com\u0026#34; } ] } ] } XML XML 是一种人类可读的数据格式，在 1990 年代和 2000 年代流行。目前，它在很大程度上已被不那么冗长的 JSON 格式所取代，但仍有一些系统在使用 XML 来表示数据。\nXML 使用括在尖括号 (\u0026lt;../\u0026gt;) 中的标签来定义元素和属性，示例如下：\n\u0026lt;Customers\u0026gt; \u0026lt;Customer name=\u0026#34;Joe\u0026#34; lastName=\u0026#34;Jones\u0026#34;\u0026gt; \u0026lt;ContactDetails\u0026gt; \u0026lt;Contact type=\u0026#34;home\u0026#34; number=\u0026#34;555 123-1234\u0026#34;/\u0026gt; \u0026lt;Contact type=\u0026#34;email\u0026#34; address=\u0026#34;joe@litware.com\u0026#34;/\u0026gt; \u0026lt;/ContactDetails\u0026gt; \u0026lt;/Customer\u0026gt; \u0026lt;Customer name=\u0026#34;Samir\u0026#34; lastName=\u0026#34;Nadoy\u0026#34;\u0026gt; \u0026lt;ContactDetails\u0026gt; \u0026lt;Contact type=\u0026#34;email\u0026#34; address=\u0026#34;samir@northwind.com\u0026#34;/\u0026gt; \u0026lt;/ContactDetails\u0026gt; \u0026lt;/Customer\u0026gt; \u0026lt;/Customers\u0026gt; BLOB 最终，所有文件都存储为二进制数据（1 和 0），但在上面讨论的人类可读格式中，二进制数据的字节被映射到可打印字符（通常通过字符编码方案，如 ASCII 或 Unicode）。然而，特别是对于非结构化数据的一些文件格式，将数据存储为必须由应用程序解释和呈现的原始二进制文件。以二进制形式存储的常见数据类型包括图像、视频、音频和特定于应用程序的文档。\n数据专业人员通常将该类数据文件称为 BLOB（Binary Large Objects，二进制大对象）。\n优化的文件格式 虽然结构化和半结构化数据的人类可读格式可能很有用，但它们通常没有针对存储空间或处理进行优化。随着时间的推移，已经开发出一些支持压缩、索引以及高效存储和处理的专用文件格式。\n一些常见的优化文件格式，包括 Avro、ORC 和 Parquet：\n  Avro 是一种基于行的格式。其由 Apache 创建。每条记录都包含一个消息头，该消息头描述了记录中数据的结构。此消息头存储为 JSON，数据存储为二进制。应用程序使用消息头中的信息来解析二进制数据并提取其中包含的字段。Avro 是一种很好的格式，可用于压缩数据并最大限度地减少存储和网络带宽需求。\n  ORC（Optimized Row Columnar format，优化行列格式）将数据组织成列而不是行。其由 HortonWorks 开发，用于优化 Apache Hive 中的读写操作（Hive 是一个数据仓库系统，支持对大型数据集进行快速汇总和查询）。ORC 文件包含一组数据条，每个数据条保存一列或多列数据。数据条包含数据条中行的索引、每行的数据以及用于保存每列的统计信息（计数、总和、最大值、最小值等）的页脚。\n  Parquet 是另一种列数据格式，其由 Cloudera 和 Twitter 创建。Parquet 文件包含行组，每列的数据一起存储在同一行组中，每个行组包含一个或多个数据块。Parquet 文件包含描述在每个块中找到的行集的元数据。应用程序可以使用此元数据快速定位给定行集的正确块，并检索这些行的指定列中的数据。Parquet 擅长高效地存储和处理嵌套数据类型，支持非常有效的压缩和编码方案。\n  1.3 数据库 数据库用于定义可以存储和查询数据的中央系统。简单来说，存储文件的文件系统是一种数据库；但是当我们在专业数据上下文中使用该术语时，我们通常指的是用于管理数据记录而非文件的专用系统。\n关系型数据库 关系型数据库通常用于存储和查询结构化数据。数据存储在代表实体的表中，例如客户、产品或销售订单。实体的每个实例都分配有一个唯一标识它的主键，这些键用于引用其它表中的实体实例。例如，可以在销售订单记录中引用客户的主键来指示哪个客户下的订单。使用键来引用数据实体可以使关系型数据库变得规范化，这可以避免重复值。例如，单个客户的详细信息仅存储一次，而不是针对客户下的每个销售订单都存储一次。这些表是使用基于 ANSI 标准的 SQL 语言 (Structured Query Language，结构化查询语言) 来管理和查询的，因此在多个数据库系统中是类似的。\n非关系型数据库 非关系数据库是不对数据应用关系模式的数据管理系统。非关系型数据库通常被称为 NoSQL 数据库（尽管有些支持 SQL 语言的变体）。\n常用的非关系型数据库有四种常见类型：\n 键值数据库，其中每条记录都由唯一键和关联值组成，可以是任何格式。   文档数据库，这是一种特定形式的键值数据库，其中值是 JSON 文本（系统已对其进行了优化以解析和查询）。   列族数据库，它存储包含行和列的表格数据，但您可以将列划分为称为列族的组。每个列族包含一组逻辑上相关的列。   图数据库，将实体存储为具有连接的节点，以定义它们之间的关系。  1.4 事务数据处理 事务数据处理系统是大多数人认为的业务计算的主要功能。事务系统记录了组织想要跟踪的封装特定事件的事务。事务可以是金融交易，例如银行系统中账户之间的资金流动，也可以是零售系统的一部分，跟踪客户对商品和服务的支付。可将事务视为一个小的、离散的工作单元。\n交易系统通常是大容量的，有时在一天内处理数百万笔交易。正在处理的数据必须可以非常快速地访问。事务系统执行的工作通常称为 OLTP (Online Transactional Processing，联机事务处理)。\nOLTP 解决方案依赖于数据库系统，其中数据存储针对读取和写入操作进行了优化，以支持创建、检索、更新和删除数据记录（通常称为 CRUD 操作）的事务工作负载。这些操作使用了事务，以确保存储在数据库中数据的完整性。为此，OLTP 系统确保了支持所谓 ACID 语义的事务：\n  原子性 - 每个事务都被视为一个单元，完全成功或完全失败。例如，有一个事务为从一个账户扣款和到一个账户的等额打款，这两项操作必须同时执行。如果其中一个操作不能完成，那么另一个也必须失败。\n  一致性 - 事务只能将数据库中的数据从一种有效状态转移到另一种有效状态。借用上面的扣款和入账示例，交易的完成状态必须反映资金从一个账户转移到另一个账户。\n  隔离性 - 并发事务不能相互干扰，并且必须有一致的数据库状态。例如，虽然将资金从一个账户转移到另一个账户的事务正在进行中，但检查这些账户余额的另一个事务必须返回一致的结果。余额检查事务不能为付款账户返回一个转账前的余额，而为收款账户返回一个到账后的余额。\n  持久性 - 当一个事务被提交时，它将保持提交状态。账户转账事务完成后，修改后的账户余额会被持久化，这样即使数据库系统被关闭，再次开启时也会反映提交的事务。\n  OLTP 系统通常用于支持处理业务数据的实时应用程序，通常称为业务线 (LOB) 应用程序。\n1.5 分析数据处理 分析数据处理通常会使用存储大量历史数据或业务指标的只读系统。分析可以基于给定时间点的一个数据快照或一组快照来进行。\n分析处理系统的具体细节可能因解决方案而异，但企业级分析系统的通用架构如下图所示：\n 数据文件可以存储在中央数据湖中以供分析。 提取、转换和加载 (ETL) 过程将数据从文件和 OLTP 数据库复制到针对读取活动进行了优化的数据仓库中。通常，数据仓库模式会基于包含您要分析的数值（例如，销售额）的事实表，以及表示您要衡量它们的实体（例如，客户或产品）的相关维度表。 数据仓库中的数据可以聚合并加载到联机分析处理 (OLAP) 模型或多维数据集中。事实表的聚合数值（度量）是为了维度表中维度的交集计算。例如，销售收入可能按日期、客户和产品来累加。 数据湖、数据仓库和分析模型中的数据可被查询以生成报告、可视化图表和仪表板。  数据湖在大规模数据分析处理场景中很常见，需要收集和分析大量基于文件的数据。\n数据仓库是一种将数据存储在关系模式中的既定方式，该模式针对读取操作进行了优化，主要是支持报告和数据可视化的查询。数据仓库模式可能需要对 OLTP 数据源中的数据进行一些逆规范化（引入一些重复以使查询执行得更快）。\nOLAP 模型是一种聚合类型的数据存储，其针对分析工作负载进行了优化。\n数据聚合在不同层次的维度上，使您能够查看多个层次级别的聚合；例如，按区域、城市或单个地址查找总销售额。由于 OLAP 数据是预先聚合的，因此可以快速运行返回其包含的摘要的查询。\n不同类型的用户可能在整个架构的不同阶段执行数据分析工作。例如：\n 数据科学家可能会直接使用数据湖中的数据文件来探索和建模数据。 数据分析师可能直接在数据仓库中查询表以生成复杂的报告和可视化。 业务用户可能会以报告或仪表板的形式在分析模型中使用预先聚合的数据。  2 数据角色和服务 数据专业人员在管理、使用和控制数据时通常扮演不同的角色。在本模块中，将介绍数据专业人员的各种角色、与这些角色相关的任务和职责，以及用于执行这些任务的 Azure 服务。\n2.1 数据世界中的角色 管理、控制和使用数据涉及各种各样的角色。有些角色是面向业务的，有些更多涉及工程，有些专注于研究，有些是结合数据管理不同方面的混合角色。您的组织可能以不同的方式定义角色，或者给它们不同的名称，但本单元中描述的角色涵盖了最常见的任务和职责划分。\n大多数组织中处理数据的三个关键角色是：\n 数据库管理员，负责管理数据库、为用户分配权限、存储数据的备份并在发生故障时恢复数据。 数据工程师，负责管理整个组织的数据集成基础设施和流程，应用数据清理协程，识别数据治理规则，并实现在系统之间传输和转换数据的流水线。 数据分析师，负责探索和分析数据以创建可视化图表，使组织能够做出明智的决策。  工作角色定义了不同的任务和职责。但在某些组织中，同一个人可能会担任多个角色。\nDBA（数据库管理员） 数据库管理员负责 On-Premise 或托管在云上的数据库系统的设计、实施和维护。他们负责数据库的整体可用性和性能优化。他们与利益相关者合作，以实施备份和恢复计划的规则、工具和流程，以便在自然灾害或人为错误后恢复。\n数据库管理员还负责管理数据库中数据的安全性，为数据授权，授予或撤销用户的访问权限。\n数据工程师 数据工程师与利益相关者合作设计和实施与数据相关的工作负载，包括数据提取管道、清理和转换活动以及用于分析工作负载的数据存储。他们使用广泛的数据平台技术，包括关系型和非关系型数据库、文件存储和数据流。\n他们还负责确保维护云上以及自 On-Premise 到云上的各种存储的数据的隐私。\n数据分析师 数据分析师使企业能够最大化其数据资产的价值。他们负责探索数据以找到趋势和关联性，设计和构建分析模型，并通过报告和可视化图表实现高级分析功能。\n数据分析师根据业务需求将原始数据处理成相关的洞察力。\n此处描述的角色代表了大多数大中型组织中与数据相关的关键角色。还有其它与数据相关的角色这里没有提到，比如数据科学家和数据架构师；还有其他与数据打交道的技术专业人员，包括应用程序开发人员和软件工程师。\n2.2 数据服务 下面介绍 Azure 提供的一些最常用的数据云服务。\nAzure SQL Azure SQL 是基于 Microsoft SQL Server 数据库引擎的一系列关系型数据库解决方案的统称。特定的 Azure SQL 服务包括：\n Azure SQL Database - 完全托管的平台即服务 (PaaS) 数据库。 Azure SQL Managed Instance - 具有自动维护功能的 SQL Server 托管实例，比 Azure SQL Database 拥有更灵活的配置，但所有者需要承担更多的管理责任。 Azure SQL VM - 安装了 SQL Server 的虚拟机，可实现最大的可配置性并承担全部管理责任。  数据库管理员通常预配和管理 Azure SQL 数据库系统，以支持需要存储事务数据的业务线 (LOB，Line of Business) 应用程序。\n数据工程师可以使用 Azure SQL 数据库系统作为管道的数据源，这些管道执行提取、转换和加载 (ETL) 操作以将事务数据提取到分析系统中。\n数据分析师可以直接查询 Azure SQL 数据库以创建报告，但在大型组织中，这些数据通常与分析数据存储中的其它数据相结合，以支持企业分析。\n用于开源关系数据库的 Azure Database Azure 提供诸多流行开源关系型数据库的托管服务，包括：\n Azure Database for MySQL - 一个易于使用的开源数据库管理系统，通常用于 Linux、Apache、MySQL 和 PHP (LAMP) 技术栈应用程序。 Azure Database for MariaDB - 一种较新的数据库管理系统，由 MySQL 的原始开发人员创建。该数据库引擎已被重写并针对性能作了优化。 MariaDB 提供与 Oracle 数据库的兼容性。 Azure Database for PostgreSQL - 一种混合关系对象数据库。您可以将数据存储在关系表中，但 PostgreSQL 数据库还允许存储自定义数据类型，以及它们自己的非关系属性。  Azure Cosmos DB Azure Cosmos DB 是一个全球规模的非关系型 (NoSQL) 数据库，它支持多个应用程序接口 (API)，使你能够以 JSON 文档、键值对、列族和图形的形式存储和管理数据。\nAzure Storage Azure 存储是一项核心 Azure 服务，可让您将数据存储在：\n Blob containers - 可扩展、经济高效的二进制文件存储。 File shares - 网络文件共享，例如您通常在公司网络中找到的文件。 Tables - 需要快速读写数据的应用程序的键值存储。  数据工程师使用 Azure Storage 来托管数据湖 - 具有分层命名空间的 blob 存储，使文件能够在分布式文件系统的文件夹中组织。\nAzure Data Factory Azure Data Factory 是一个可让你定义和调度数据流水线以传输和转换数据的 Azure 服务。您可以将流水线与其他 Azure 服务集成，使您能够从云数据存储中提取数据，以及使用云上的计算服务处理数据，并将结果保存在另一个数据存储中。\n数据工程师使用 Azure Data Factory 构建 ETL（提取、转换和加载）解决方案，这些解决方案使用来自整个组织的事务系统的数据来填充分析数据存储。\nAzure Synapse Analytics Azure Synapse Analytics 是一个全面、统一的数据分析解决方案，它为多种分析功能提供单一服务接口，包括：\n 流水线 - 基于与 Azure Data Factory 相同的技术。 SQL - 一种高度可扩展的 SQL 数据库引擎，针对数据仓库工作负载进行了优化。 Apache Spark - 一个开源分布式数据处理系统，支持多种编程语言和 API，包括 Java、Scala、Python 和 SQL。 Azure Synapse Data Explorer - 一种高性能数据分析解决方案，针对使用 Kusto 查询语言 (KQL) 实时查询日志和遥测数据进行了优化。  数据工程师可以使用 Azure Synapse Analytics 创建统一的数据分析解决方案，通过单一服务将数据提取管道、数据仓库存储和数据湖存储结合起来。\n数据分析师可以通过交互式 Notebook 使用 SQL 和 Spark 池来探索和分析数据，并利用与 Azure 机器学习和 Microsoft Power BI 等服务的集成来创建数据模型并从数据中提取洞察力。\nAzure Databricks Azure Databricks 是流行的 Databricks 平台的 Azure 集成版本，它将 Apache Spark 数据处理平台与 SQL 数据库语义和集成管理界面相结合，以实现大规模数据分析。\n数据工程师可以使用现有的 Databricks 和 Spark 技能在 Azure Databricks 中创建分析数据存储。\n数据分析师可以使用 Azure Databricks 中的原生 Notebook 支持来在 Web 界面中查询和查看数据视图。\nAzure HDInsight Azure HDInsight 为流行的 Apache 开源大数据处理技术提供 Azure 托管的集群，包括：\n Apache Spark - 一个开源分布式数据处理系统，支持多种编程语言和 API，包括 Java、Scala、Python 和 SQL。 Apache Hadoop - 一个分布式系统，它使用 MapReduce 作业跨多个集群节点有效地处理大量数据。 MapReduce 作业可以用 Java 编写，也可以通过 Apache Hive（一种在 Hadoop 上运行的基于 SQL 的 API）等接口进行抽象。 Apache HBase - 一个用于大规模 NoSQL 数据存储和查询的开源系统。 Apache Kafka - 用于数据流处理的消息组件。 Apache Storm - 一个通过 spout 和 bolts 拓扑进行实时数据处理的开源系统。  数据工程师可以使用 Azure HDInsight 来支持依赖多种开源技术的大数据分析工作负载。\nAzure Stream Analytics Azure 流分析是一个实时流处理引擎，它从输入中捕获数据流，从输入流中应用查询来提取和操作数据，并将结果写入输出以进行分析或进一步处理。\n数据工程师可以将 Azure Stream Analytics 整合到数据分析体系结构中，这些体系结构捕获流数据以将其引入分析数据存储或进行实时可视化。\nAzure Data Explorer Azure Data Explorer 是一个独立的服务，它提供与 Azure Synapse Analytics 中的 Azure Synapse Data Explorer 运行时相同的高性能日志和遥测数据查询能力。\n数据分析师可以使用 Azure Data Explorer 查询和分析包含时间戳属性的数据，例如通常在日志文件和物联网 (IoT) 遥测数据中找到的数据。\nMicrosoft Purview Microsoft Purview 为企业范围的数据治理和可发现性提供了解决方案。您可以使用 Microsoft Purview 创建数据地图并跨多个数据源和系统跟踪数据沿袭，使您能够找到可靠的数据进行分析和报告。\n数据工程师可以使用 Microsoft Purview 在整个企业中实施数据治理，并确保用于支持分析工作负载的数据的完整性。\nMicrosoft Power BI Microsoft Power BI 是一个用于分析数据建模和报告展示的平台，数据分析师可以使用它来创建和共享交互式数据可视化报告。Power BI 报表可使用 Power BI Desktop 应用程序创建，然后通过基于 Web 的报表发布到 Power BI 服务以及 Power BI 移动应用中。\n 参考资料\n[1] Exam DP-900: Microsoft Azure Data Fundamentals - learn.microsoft.com\n ","permalink":"https://olzhy.github.io/posts/dp-900-part1-concepts.html","tags":["Azure"],"title":"DP-900：Azure 数据基础第一部分之核心数据概念"},{"categories":["随笔"],"contents":"今天週末，去小區後山的地裡挖紅薯。\n找紅薯的時候就像是在捉迷藏，得慢慢用鐵鍬探尋它們出現的地方。如果心急，就容易一鐵鍬鏟下去把一個大紅薯鏟斷了，那樣心裡就有種愧疚感。看到一個紅薯，就會有一批一塊出現，這時就像砸中了彩蛋，很開心。\n挖完了，將那一個個紅薯身上的泥土刮掉然後整整齊齊排列好，很有收獲感。\n帶回家時，竟連那像大拇指那麼大小的都捨不得扔掉。\n\n\n\n","permalink":"https://olzhy.github.io/posts/wa-hong-shu.html","tags":["随笔"],"title":"挖紅薯"},{"categories":["计算机"],"contents":"Selenium 测试的主要组成部分有：测试代码、WebDriver、Grid（Selenium Server，非必需）、浏览器驱动（Driver）和浏览器。\n当我们编写完 Selenium 测试用例在本地调试时，WebDriver 通过浏览器驱动直接与浏览器进行交互。这时，WebDriver、浏览器驱动和浏览器位于同一主机。这种最基本的交互方式如下图所示。\n（图片引自 selenium.dev） 本地调试完成，使用自动化流水线触发执行测试用例时，一般不会使用上述这种 WebDriver 与浏览器（驱动）直接交互的方式，而会选择远程交互的方式。\n远程交互方式是指 WebDriver 通过 Grid（Selenium Server）来与浏览器（驱动）远程交互。这时，Grid 可以不与浏览器及其驱动位于同一主机，测试代码及 WebDriver 也可以不与 Grid 或浏览器位于同一主机。这种远程交互的方式如下图所示。\n（图片引自 selenium.dev） 可以看到，使用 Grid 以后，测试用例只需知道 Grid 的地址即可，无需安装浏览器及驱动，使得测试用例的执行变得非常简单。\n本文主要关注 Grid 的搭建及使用。接下来，主要有如下几个部分。\n 引入一段测试代码，使用本地直接交互的方式来执行。 使用原始jar文件的方式搭建 Grid 环境，并执行测试代码。 使用 Docker 镜像的方式搭建 Grid 环境。 使用 Kubernetes 描述文件的方式搭建 Grid 环境。  1 测试代码 如下是一段使用 Python 编写的 Selenium 测试代码。是针对 Github 搜索功能的一个简单测试场景。\n有下面几个步骤：\n 打开 GitHub 首页； 在搜索框键入关键字Selenium，并回车； 点击第一个搜索结果，并等待仓库首页打开； 断言仓库首页标题包含关键字Selenium。  import unittest from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.wait import WebDriverWait from selenium.webdriver.support import expected_conditions as EC class GithubTestCase(unittest.TestCase): def setUp(self): self.browser = webdriver.Chrome() self.addCleanup(self.browser.quit) def test_search(self): # 打开 GitHub 首页 self.browser.get(\u0026#39;https://github.com/\u0026#39;) # 在搜索框键入关键字 Selenium，并回车 search_box_elem = self.browser.find_element(By.XPATH, \u0026#39;//input[@name=\u0026#34;q\u0026#34;]\u0026#39;) search_box_elem.send_keys(\u0026#39;Selenium\u0026#39; + Keys.RETURN) # 点击第一个搜索结果 first_result_elem = self.browser.find_element(By.XPATH, \u0026#39;//ul[@class=\u0026#34;repo-list\u0026#34;]/li//div[@class=\u0026#34;d-flex\u0026#34;]//a\u0026#39;) first_result_elem.click() # 等待 Code Tab 页出现，即仓库首页打开 WebDriverWait(self.browser, 10).until(EC.presence_of_element_located((By.ID, \u0026#39;code-tab\u0026#39;))) # 断言仓库首页标题包含 Selenium self.assertIn(\u0026#39;Selenium\u0026#39;, self.browser.title) if \u0026#39;__main__\u0026#39; == __name__: unittest.main(verbosity=2) 如上测试代码使用的是 WebDriver 与浏览器（驱动）直接交互的方式。\n运行该代码前需要在本地安装有 Chrome 浏览器及 ChromeDriver（从 chromium.org 下载 Chrome 对应的驱动并解压至指定目录，并将安装目录添加至系统环境变量），测试代码会驱动本地浏览器执行完指定步骤后打印成功的信息。\n2 使用 jar 文件的方式搭建 Grid Grid jar 文件依赖的 Java 版本为 11 或以上。\n欲使用 Grid，Standalone 模式是最简单快速的一种。\n可以从 github.com/SeleniumHQ/selenium 发布页面下载最新的selenium-server-\u0026lt;version\u0026gt;.jar文件，然后使用如下命令启动：\njava -jar selenium-server-\u0026lt;version\u0026gt;.jar standalone Grid 启动完成后，打开网址http://localhost:4444可以看到可使用的所有浏览器类型以及会话的状态。\n接着，对测试代码稍作修改（获取browser的方式替换为如下写法）即可成功运行。\nself.browser = webdriver.Remote( command_executor=\u0026#39;http://localhost:4444\u0026#39;, options=webdriver.ChromeOptions() ) 而要想更好的使用 Grid，需要了解其里边的几个角色。\n Hub：负责将从 WebDriver 接收的浏览器操作指令分发至对应的 Node，并将从 Node 接收的结果返回给 WebDriver。 Node：负责接收来自 Hub 的指令，并调用浏览器驱动来完成页面操作。  Hub 与 Node 可位于不同的主机，通过 HTTP 协议来通信。\n使用 Hub 与 Node 分工的方式来启动 Grid 的命令如下：\n# 启动 Hub java -jar selenium-server-\u0026lt;version\u0026gt;.jar hub # 启动 Node 1 java -jar selenium-server-\u0026lt;version\u0026gt;.jar node --port 5555 # 启动 Node 2 java -jar selenium-server-\u0026lt;version\u0026gt;.jar node --port 6666 启动完成后，从网址http://localhost:4444可以看到有两个可以使用的 Node。\n测试代码使用 Grid 的方式不会因此发生变化，仍指向http://localhost:4444即可。\n3 使用 Docker 镜像的方式搭建 Grid 使用 Docker 快速启动一个 Standalone 模式 Grid 的命令如下：\n# 启动一个 Chrome Standalone Grid docker run -d -p 4444:4444 -p 7900:7900 --shm-size=\u0026#34;2g\u0026#34; selenium/standalone-chrome:4.4.0 打开http://localhost:4444同样可以看到控制台页面。\n新版 Grid 另一个非常便捷的功能是，直接在浏览器打开http://localhost:7900（密码为secret）即可看到运行测试的桌面。\n测试代码同样只需将 RemoteWebDriver 地址指向http://localhost:4444即可运行。\n使用 Hub 与 Node 分工的方式启动 Grid 的 Docker 命令如下：\n# 创建网络 docker network create grid # 启动 Hub docker run -d -p 4442-4444:4442-4444 --net grid --name selenium-hub selenium/hub:4.4.0 # 启动一个 Chrome Node docker run -d -p 7900:7900 --net grid -e SE_EVENT_BUS_HOST=selenium-hub \\  --shm-size=\u0026#34;2g\u0026#34; \\  -e SE_EVENT_BUS_PUBLISH_PORT=4442 \\  -e SE_EVENT_BUS_SUBSCRIBE_PORT=4443 \\  selenium/node-chrome:4.4.0 还可以按需添加别的浏览器 Node，如下命令添加了 Edge 和 Firefox Node。\n# 添加一个 Edge Node docker run -d -p 7901:7900 --net grid -e SE_EVENT_BUS_HOST=selenium-hub \\  --shm-size=\u0026#34;2g\u0026#34; \\  -e SE_EVENT_BUS_PUBLISH_PORT=4442 \\  -e SE_EVENT_BUS_SUBSCRIBE_PORT=4443 \\  selenium/node-edge:4.4.0 # 添加一个 Firefox Node docker run -d -p 7902:7900 --net grid -e SE_EVENT_BUS_HOST=selenium-hub \\  --shm-size=\u0026#34;2g\u0026#34; \\  -e SE_EVENT_BUS_PUBLISH_PORT=4442 \\  -e SE_EVENT_BUS_SUBSCRIBE_PORT=4443 \\  selenium/node-firefox:4.4.0 这就是 Grid 的威力所在，其提供一个浏览器池，测试项目只需指向 Grid 地址即可使用所需的浏览器，非常的方便。\n想查看浏览器运行桌面，直接访问http://localhost:7900（Chrome）、http://localhost:7901（Edge）或http://localhost:7902（Firefox）即可。\n另外一种传统的查看浏览器运行桌面的方式是 VNC。VNC 是一种远程桌面显示技术，有服务端和客户端两部分组成。Grid 的各 Node 镜像开放了5900端口来提供 VNC 服务。我们想使用这种方式查看 Node 内部的桌面，需要下载一个 VNC Client。\n本文使用的 VNC Client 为 VNC Viewer。从 VNC Viewer 下载页 下载并安装好 VNC Viewer 后。\n使用如下命令再启动一个 Chrome Node，并开放5900端口：\ndocker run -d -p 5900:5900 --net grid -e SE_EVENT_BUS_HOST=selenium-hub \\  --shm-size=\u0026#34;2g\u0026#34; \\  -e SE_EVENT_BUS_PUBLISH_PORT=4442 \\  -e SE_EVENT_BUS_SUBSCRIBE_PORT=4443 \\  selenium/node-chrome:4.4.0 打开 VNC Viewer，键入localhost:5900后回车，输入密码（secret）后即可看到 Chrome Node 的桌面。\n4 使用 Kubernetes 描述文件的方式搭建 Grid 拥有 Kubernetes 环境的话，在 Kubernetes 搭建好 Grid，会变得非常实用。这样，从 Kubernetes Ingress 暴露 Grid 的 URL 出来，可以供团队内的任何测试项目使用。自动化流水线的测试阶段也变得简单，无需准备测试用例运行环境，直接指向 Grid 的 URL 即可。\n本文基于 Docker Desktop 自带的 Kubernetes 环境，基于官方提供的 Kubernetes 描述文件 稍作改动来搭建一个 Selenium Hub/Node 环境（修改后的 K8s Yaml 文件已整理至我的 个人 GitHub 仓库），然后开放 Grid Hub 的 URL 出来供测试项目使用，再开放一个 Chrome Node 的桌面 URL 出来供测试人员查看。\n应用 Selenium Hub/Node Deployment及Service 描述文件。\nkubectl apply -f selenium-hub-deployment.yaml kubectl apply -f selenium-node-chrome-deployment.yaml kubectl apply -f selenium-hub-service.yaml kubectl apply -f selenium-node-chrome-service.yaml 暴露 Selenium Hub：\nkubectl expose deployment selenium-hub --name=selenium-hub-external --labels=\u0026#34;app=selenium-hub,external=true\u0026#34; --type=LoadBalancer 暴露 Selenium Chrome Node：\nkubectl expose deployment selenium-node-chrome --name=selenium-node-chrome-external --labels=\u0026#34;app=selenium-hub,external=true\u0026#34; --type=LoadBalancer 这样，在本地访问http://localhost:4444和http://localhost:7900就可以分别看到 Selenium 控制台和 Chrome Node 桌面。\n同样可以使用 VNC 的方式访问位于 Kubernetes 内浏览器 Node 的桌面。\n使用如下命令将 Chrome Pod 的 5900 端口转发到本地：\nPODNAME=`kubectl get pods --selector=\u0026#34;app=selenium-node-chrome\u0026#34; --output=template --template=\u0026#34;{{with index .items 0}}{{.metadata.name}}{{end}}\u0026#34;` kubectl port-forward $PODNAME 5900:5900 这样，使用 VNC Viewer 访问localhost:5900即可看到 Chrome Node 所在的桌面。\n综上，我们引入一段 Selenium 测试代码，分别使用jar文件方式、Docker 镜像方式和 Kubernetes 描述文件方式搭建了 Selenium Grid，并介绍了使用方法；还说明了在各种环境下浏览器运行桌面的查看方式。为使用 Selenium 的朋友在本地调试或实际测试场景中的环境准备上提供了参考经验。\n 参考资料\n[1] Selenium Grid Documentation - selenium.dev\n[2] SeleniumHQ/docker-selenium - github.com\n[3] Selenium with Python - readthedocs.io\n ","permalink":"https://olzhy.github.io/posts/selenium-grid.html","tags":["自动化测试","Python","Selenium"],"title":"Selenium Grid 搭建及使用"},{"categories":["计算机"],"contents":"Apache JMeter 是一个使用纯 Java 编写的、由 Apache 软件基金会开源的、用于度量软件性能的负载测试工具。\n其最初是为了测试 Web 应用而设计，但后来支持测试的应用类型已非常丰富。\nJMeter 目前支持测试的协议或应用类型具体如下：\n Web - HTTP、HTTPS（实现语言可以是 Java、NodeJS、PHP 及 ASP.NET 等） SOAP 或 REST Web 服务 FTP 数据库（使用 JDBC） LDAP 面向消息的中间件（使用 JMS） 邮件功能 - SMTP(S)、POP3(S) 或 IMAP(S) 原生命令或 Shell 脚本 TCP Java 对象  此外，JMeter 还具有如下特性：\n GUI 模式功能齐全，支持从浏览器或原生应用录制测试计划，支持调试。 命令行模式支持在任何可以运行 Java 的操作系统上进行负载测试。 HTML 报告功能丰富，易于使用。 可以非常方便的从 HTML、JSON、XML 等流行的文本响应格式中提取数据。 以多线程方式来模拟并发访问。 支持通过插件来扩展数据可视化能力。 支持脚本化采样器（可使用诸如 Groovy 与 BeanShell 等 JSR223 兼容的语言来编写采样脚本）。  需要注意的是 JMeter 不是浏览器，其在协议级别工作。不会像浏览器一样解析 JavaScript，也不会渲染页面。\n本文接下来分三部分对 JMeter 进行初探：首先会梳理 JMeter 中常用的一些概念或组件；接着会介绍 JMeter 的下载与安装；最后会对其进行简单的使用，即构建一个 Web 测试计划。\n1 概念梳理 JMeter 中常用的一些概念或组件梳理如下。\n  测试计划（Test Plan）\n测试计划是 JMeter 整个测试任务树的根结点。\n  线程组（Thread Group）\n线程组用于模拟执行测试的一组用户，是测试计划的起始点。控制器（Controller）及采样器（Sampler）必须置于线程组下；诸如监听器（Listener）等虽可以直接置于测试计划下，但会应用到所有的线程组。线程组用于设置线程数、加速期及循环次数。\n多个线程用于模拟对服务器应用的并发访问，线程之间相互独立，各自完整的执行测试计划。\n加速期（Ramp-up）用于告诉 JMeter 需要多长时间来“加速到”设定的全部线程数量。如设置线程数为 30，加速期为 120 秒，每个线程将会在上一个线程启动 4（120/30）秒后启动。需要根据实际需求来设定加速期。加速期设置的太短会导致在测试开始时工作负载太大；而设置的太长又会使负载太小，而没有达到预期的并发量（即第一个线程已完成工作，而最后一个线程还没启动运行）。\n  逻辑控制器（Logic Controller）\nJMeter 有采样器（Sampler）和逻辑控制器（Logic Controller）两类控制器。\n逻辑控制器用于控制采样器的处理顺序。\n常见的逻辑控制器有 Simple Controller、Loop Controller 和 If Controller 等。Simple Controller 没什么别的功能，只用于将其它逻辑控制器或采样器进行组合；Loop Controller 用于对其它逻辑控制器或采样器执行多次（如将一个 HTTP Request 采样器添加到循环次数为 2 的 Loop Controller，并将 Thread Group 循环次数配置为 3，那么 JMeter 将总共执行 2 * 3 = 6 次 HTTP Request）；If Controller 用于控制在其下的测试元素是否执行。\n  采样器（Sampler）\n采样器用于对被测试应用发起请求。每个采样器都会生成一组采样结果（有成功/失败、耗时、数据大小等属性），这些采样结果可在监听器（Listener）中查看。\n常用的采样器有 HTTP Request、FTP Request 和 JDBC Request 等。HTTP Request 用于向 Web 服务器发送 HTTP/HTTPS 请求；FTP Request 用于向 FTP 服务器发送“检索文件”或“上传文件”请求；JDBC Request 用于向数据库发送 JDBC 请求（SQL 查询）。\n  监听器（Listener）\n监听器用于监听、保存和读取测试结果，一般在测试流程的最后执行。默认情况下测试结果保存在一个后缀为.jtl的 XML 文件里。\n常用的监听器有 Graph Results 和 View Results in Table 等。Graph Results 会将测试结果以图形的方式显示（很直观，但很占用内存和 CPU，所以仅在 GUI 端调试测试用例时使用）；View Results in Table 会将测试结果按表格方式显示，也仅在调试用例时使用。\n  配置元素（Configuration Element）\n配置元素可用来设置变量和默认值以供后面的采样器使用。\n常用的配置元素有 CSV Data Set Config、HTTP Header Manager 等。CSV Data Set Config 用于从文件中读取行，并将它们拆分为变量；HTTP Header Manager 用于设置 HTTP 请求头参数，以供多个 HTTP Request 采样器复用。\n  断言（Assertion）\n断言用于对采样器结果执行额外的检查，一般会添加到对应的采样器下。\n常用的断言有 Response Assertion 等。Response Assertion 用于断言响应体是否包含或匹配某个指定的字符串。\n  计时器（Timer）\n计时器用于控制采样的时间间隔。\n常用的计时器有 Constant Timer 等。Constant Timer 会将每个线程在请求之前暂停相同的时间。\n  预处理器（Pre Processor）\n预处理器用于在采样器执行前做一些前置工作。\n常用的预处理器有 JSR223 PreProcessor 等。JSR223 PreProcessor 允许在采样前使用 JSR223 脚本代码做一些前置工作。\n  后处理器（Post Processor）\n后处理器用于在采样器执行后做一些后置工作。\n常用的后处理器有 Regular Expression Extractor、XPath Extractor 等。Regular Expression Extractor 允许用户使用正则表达式从响应中提取信息；XPath Extractor 允许用户使用 XPath 查询语言从响应（XML 或 HTML）中提取信息。\n  JMeter 中常用的一些概念或组件即梳理完了。下面接着介绍下 JMeter 的下载与安装。最后会对 JMeter 进行一些初步的使用。\n2 下载安装 安装 JMeter 前请确保已安装 JDK，且保证版本为 1.8 及以上。\n从官网（http://jmeter.apache.org/download_jmeter.cgi）下载最新版的 JMeter 压缩包，解压至指定文件夹，并设置系统环境变量。键入jmeter --version命令能正常输出版本号，即说明安装成功。\njmeter --version 3 构建一个 Web 测试计划 该部分以测试jmeter.apache.org做示例，演示如何创建一个用于测试 Web 站点的最基本的测试计划。我们会创建 5 个用户，对 JMeter 网站的 2 个页面发送请求，并且会让用户运行 2 次这样的测试。所以，请求总数为：(5 个用户) x (2 个请求) x (重复 2 次) = 20 个 HTTP 请求。要构建测试计划，将用到如下元素：线程组（Thread Group）、HTTP Request、HTTP Request Defaults 和 Graph Results.\n具体步骤如下：\n  添加用户\n每个 JMeter 测试计划的第一步都是添加一个线程组（Thread Group）。线程组会告诉 JMeter 用于模拟的用户数量及用户发送请求的频率和数目。\n选择 Test Plan，然后单击鼠标右键，选择 Add -\u0026gt; Threads -\u0026gt; Thread Group，即可看到 Thread Group 元素的控制面板。\n接下来，修改默认属性值：将 Number of Threads 修改为 5；Ramp-up period 使用默认值 1（其为启动每个用户的延迟时间，假设有 5 个用户，Ramp-up period 为 5，则意味着每隔 1 秒启动一个用户）；将 Loop Count 设置为 2（该属性告诉 JMeter 重复执行测试的次数）。\n修改后的 Thread Group 如下图所示。\n  添加默认 HTTP 请求属性\n上一步定义好了用户，现在开始定义它们要执行的任务。\n这一步指定 HTTP 请求的默认设置，从而为下一步的 HTTP Request 元素所使用。\n选择 Thread Group，然后单击鼠标右键，选择 Add -\u0026gt; Config Element -\u0026gt; HTTP Request Defaults，即可看到 HTTP Request Defaults 的控制面板。\n接下来，填写属性值：将 Server Name or IP 填写为 jmeter.apache.org（对于我们正在构建的测试计划，所有 HTTP 请求都将发送到该服务器）；其它字段无需填写，采用默认值即可。\nHTTP Request Defaults 元素不会告诉 JMeter 发送 HTTP 请求，它只是定义了 HTTP Request 元素使用的默认值。\n配置后的 HTTP Request Defaults 如下图所示。\n  添加 Cookie 支持\n几乎绝大多数的网站都需要 Cookie 支持。\n欲添加 Cookie 支持，需要在测试计划的每个 Thread Group 下添加一个 HTTP Cookie Manager。这将确保每个线程都有自己的 Cookie，但在所有 HTTP Request 对象之间共享。\n添加方式为：选择 Thread Group，然后单击鼠标右键，选择 Add -\u0026gt; Config Element -\u0026gt; HTTP Cookie Manager，即可看到 HTTP Cookie Manager 的控制面板。\n添加 HTTP Cookie Manager 后的 Test Plan 如下图所示。\n  添加 HTTP 请求\n在我们的 Test Plan 中，会对两个页面发送 HTTP 请求。第一个是 JMeter Home 页（http://jmeter.apache.org/），第二个是 Changes 页（http://jmeter.apache.org/changes.html）。\nJMeter 会按照 HTTP Request 在树中出现的顺序依次发送请求。\nHTTP Request 添加方式为：选择 Thread Group，然后单击鼠标右键，选择 Add -\u0026gt; Sampler -\u0026gt; HTTP Request，即可看到 HTTP Request 的控制面板。\n先添加第一个：将 Name 字段填写为 “Home Page”；将 Path 字段填写为 “/”。Server Name or IP 字段无需填写，因为已在 HTTP Request Defaults 元素中设置。\n第一个 HTTP Request 添加完后，如下图所示。\n再添加第二个：将 Name 字段填写为 “Changes”；将 Path 字段填写为 “/changes.html”。\n第二个 HTTP Request 添加完后，如下图所示。\n  添加监听器以查看测试结果\n最后一个要添加的元素是一个 Listener，名为 View Results Tree，用于查看每个请求的发送次序及响应结果。\n添加方式为：选择 Thread Group，然后单击鼠标右键，选择 Add -\u0026gt; Listener -\u0026gt; View Results Tree 即可。\n添加 View Results Tree 后的 Test Plan 如下图所示。\n  所有步骤已配置完成，最终的jmx文件（build-web-test-plan.jmx）已托管至 本人 GitHub。\n点击 Start 即可在 GUI 端调试运行。运行结果如下图所示：\n可以在 View Results Tree 看到，一共发送了 20 次请求，点击某个请求可查看具体的采样结果和响应信息。\n真正做负载测试时，需要使用命令行模式进行。\n基于jmx文件执行测试，生成结果数据文件以及生成报告文件夹的命令如下：\njmeter -n -t build-web-test-plan.jmx -l build-web-test-plan.jtl -e -o build-web-test-plan.report 执行完成后，打开报告文件夹（build-web-test-plan.report）的index.html文件，即可看到测试报告。效果如下图所示。\n综上，完成了对 JMeter 的初探。\n 参考资料\n[1] Apache JMeter - apache.org\n[2] JMeter Wiki - cwiki.apache.org\n[3] Apache JMeter - GitHub\n ","permalink":"https://olzhy.github.io/posts/apache-jmeter-introduction.html","tags":["自动化测试","JMeter"],"title":"Apache JMeter 初探"},{"categories":["计算机"],"contents":"本文整理自『MDN Web 开发入门』。\n本文由如下几个小节组成，通过依次对各个小节的学习，我们将从零开始开发一个网页，并将其上线。\n  基础软件安装\n安装基础 Web 开发所需的软件。\n  网页设计\n编写代码前，应对网页展示的内容、字体的类型及颜色有一个初步的规划与设计。\n  文件编排\n为组成一个网站的诸多文件（如：代码文件、样式表文件与媒体文件等）进行合理的编排。\n  HTML 基础\nHTML（Hypertext Markup Language，超文本标记语言）是用于构建 Web 内容的标准语言。使用其来控制诸如：一组文本上方应有图片，文本下方应有一个表格。\n  CSS 基础\nCSS（Cascading Stylesheets，层叠样式表）是用于定义网站样式的代码。使用其来定义诸如：网页的背景图片、文本的颜色和模块的位置。\n  JavaScript 基础\nJavaScript 是用于为网站添加交互功能的编程语言。使用其来控制诸如：输入表单或点击按钮后应发生的事情。\n  网站发布\nWeb 网站编码完成后，可将其发布到服务器上。这样，人们就可以通过网址对其进行访问了。\n  Web 工作机制初探\n当访问一个网站时，其背后的机制会比较复杂。本文会概述访问一个网站时所发生的事情。\n  1 基础软件安装 本小节会介绍进行基础 Web 开发所需的软件或工具。\n专业人士都用啥工具？\n  一台电脑\n一个专业的 Web 开发人员应有一台运行 Windows、macOS 或 Linux 的台式机或笔记本电脑。\n  一个文本编辑器\n一个用来编写代码的文本编辑器。诸如 Visual Studio Code、Notepad++、Sublime Text、Atom、GNU Emacs 与 VIM 等文本编辑器；或诸如 Dreamweaver 与 WebStorm 等混合编辑器。\n  一个 Web 浏览器\n一个用来测试代码的 Web 浏览器。最常用的有 Chrome、Firefox、Opera、Safari、Internet Explorer 与 Microsoft Edge。\n  一个图形编辑器\n一个用来为网站制作图像或图形的编辑器。诸如 GIMP、Figma、Paint.NET、Photoshop、Sketch 与 XD。\n  一个版本控制系统\n一个用来进行代码管理及团队协作的工具。Git 及基于其衍生的 GitHub 托管服务是目前最流行的版本控制系统。\n  一个自动化工具\n一个用来执行重复性任务（如：压缩代码或运行测试）的自动化工具。常见的有 Webpack、Grunt 与 Gulp。\n  此外，还可能会涉及一些常用的库或框架等，基于其可以加快功能的实现。\n现在需要用啥工具？\n针对本文的练习，只需安装好一个文本编辑器和一个浏览器即可。\n2 网页设计 编写代码前必须要考虑清楚的问题 —— 做一个什么样的网站？展示哪些信息？采用何种字体和色调？本小节会给出一个大概的着手办法。\n  规划基础内容\n想一想，做一个关于什么的网站？美食？健身？旅游？ 选好主题后，写上标题和几段话，并考虑想在页面上显示的一幅图片。 接下来，构思网站应该长什么样？采用什么背景颜色？何种字体？\n小提示：大型项目一般需要一套详尽的设计指导手册，包括颜色、字体、页面模块间的间距等都有严格的要求。业界一般叫作设计系统。诸如 Firefox 的 Photon Design System、IBM 的 Carbon Design System 和 Google 的 Material Design。\n  勾勒网站的轮廓\n接下来，拿起纸和笔试着勾勒出网站的雏形（这个习惯很重要）。\n小提示：即使对于现实场景的复杂网站设计，设计团队也通常会先在纸上构思网站的轮廓。\n  选择字体和色调\n现在，将之前规划的内容（标题、段落和图片等）组织到一起。\n借助『取色器』选择喜欢的颜色（颜色采用 6 位十六进制代码表示，诸如#660066），并记录好。\n借助『谷歌图片』找一张喜欢的图片（诸如firefox-logo.png），并保存好。\n借助『Google Fonts』寻找喜欢的字体，并记下来。\n  3 文件编排 一个网站由诸多文件组成，有文本内容、代码、样式表和媒体文件等。我们需要将这些文件按照一个合理的结构组织在一起。本小节会探讨文件的组织及注意的问题。\n  创建网站文件夹\n在磁盘的某个位置，创建一个名为诸如workspace的文件夹，用于存放所有的项目代码；在其下创建一个诸如test-site的文件夹，用于本次的练习。\n文件及文件夹命名小提示：因浏览器、服务器与编程语言对空格的处理不一致，且因一些服务器是大小写敏感的，所以进行文件或文件夹命名时，不要使用空格及大小写混合方式。建议直接使用全小写字母加短横线的方式进行文件及文件夹命名。如test-site/my-file.html即是一个规范的命名方式。\n  编排网站目录结构\n接下来，考虑如何编排网站的目录结构。\n一个 Web 项目中常见的文件有：index.html文件、图像文件、CSS样式文件与JavaScript脚本文件。\n因此，一个初步的网站目录结构如下所示：\ntest-site |--- images # 存放所有的图片文件 | \\--- firefox-logo.png |--- styles # 存放所有的 CSS 样式文件 |--- scripts # 存放所有的 JavaScript 脚本文件 \\--- index.html # 网站主页   指定文件引用路径\n要使文件可以正确的找到对方，需要指定好文件的路径。下面，在index.html文件引用前面找好的图片firefox-logo.png。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;My test site\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;images/firefox-logo.png\u0026#34; alt=\u0026#34;My test image\u0026#34; /\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 将index.html保存好后，使用浏览器打开，效果如下图所示：\n  4 HTML 基础 HTML（HyperText Markup Language，超文本标记语言）用于构建网页的内容，如使用一组文字段落、一组列表、一组图片或几张表格构成一个网页。本小节会对 HTML 和其功用有一个基本的介绍。\n到底啥是 HTML？\nHTML 是定义网页内容结构的一种标记语言。HTML 由一系列元素组成，这些元素用于封装内容的不同部分，以使其按照某种方式显现。闭合标签可以做诸如控制文本或图片超链接到某个地方及控制字体的大小等各种事情。\n比如，想让一句话独立显示，可以使用\u0026lt;p\u0026gt;\u0026lt;/p\u0026gt;段落标签将其包起来：\n\u0026lt;p\u0026gt;My cat is very grumpy\u0026lt;/p\u0026gt;   HTML 元素剖析\n下面剖析一下刚刚用到的段落元素。\n可以看到，这个段落元素由开始标签、结束标签和内容这几个部分组成。\n开始标签（这里的\u0026lt;p\u0026gt;）表示元素从哪里开始（这里表示段落开始的地方）；结束标签（这里的\u0026lt;/p\u0026gt;）表示元素到哪里结束（这里表示段落结束的地方）；元素内容即表示元素的内容（这里的一句话）。\n小提示：忘加结束标签是初学者常犯的错误之一，会导致各种奇怪的结果，所以要多加注意。\n此外，元素还可以有属性。示例如下：\n属性包含您不希望出现在实际内容中的额外元素信息。这里的class是属性名，editor-note是属性值，使用class属性可以控制元素的样式。\n  元素嵌套\n用一组元素包含另一组元素，叫作元素嵌套。\n如下示例，在段落元素内使用\u0026lt;strong\u0026gt;\u0026lt;/strong\u0026gt;元素将单词very特别强调。\n\u0026lt;p\u0026gt;My cat is \u0026lt;strong\u0026gt;very\u0026lt;/strong\u0026gt; grumpy.\u0026lt;/p\u0026gt; 小提示：使用元素嵌套时，要注意元素开闭的顺序，确保使用正确。\n  空元素\n有些元素没有内容，称为空元素。前面在index.html用到的\u0026lt;img\u0026gt;元素即是一个空元素。\n\u0026lt;img src=\u0026#34;images/firefox-icon.png\u0026#34; alt=\u0026#34;My test image\u0026#34; /\u0026gt; 可以看到，该元素有两个属性src与alt，没有结束标签\u0026lt;/img\u0026gt;，也没有内容。这是因为，图像元素不需要有文本内容，其目的是将图片嵌入 HTML 页面的某一处。\n  HTML 文档剖析\n下面剖析一下前面已见过的index.html文件的内容，看看多个独立的元素如何组合到一起形成一个完整的 HTML 页面。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;My test page\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;images/firefox-icon.png\u0026#34; alt=\u0026#34;My test image\u0026#34; /\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ① \u0026lt;!DOCTYPE html\u0026gt;\n标准写法，没什么可说的，表示文档类型。\n② \u0026lt;html\u0026gt;\u0026lt;/html\u0026gt;\n\u0026lt;html\u0026gt;元素，包含页面的所有内容，称为根元素。\n③ \u0026lt;head\u0026gt;\u0026lt;/head\u0026gt;\n\u0026lt;head\u0026gt;元素，包含页面需要使用的，但不向页面访问者直接显示的所有内容。包含诸如：关键字、页面描述、CSS 文件路径和字符集声明等。\n④ \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt;\n此元素说明该网页使用的字符集为UTF-8（该字符集可以兼容世界上绝大多数国家的文字）。\n⑤ \u0026lt;title\u0026gt;\u0026lt;/title\u0026gt;\n\u0026lt;title\u0026gt;元素，用于设置页面的标题（页面加载完成后，浏览器选项卡中的标题）。\n⑥ \u0026lt;body\u0026gt;\u0026lt;/body\u0026gt;\n\u0026lt;body\u0026gt;元素，其包含需要向页面访问者显示的所有内容，包含文本、图像与视频等。\n  图像\n再次将注意力转回到\u0026lt;img\u0026gt;元素。\n\u0026lt;img src=\u0026#34;images/firefox-icon.png\u0026#34; alt=\u0026#34;My test image\u0026#34; /\u0026gt; 正如前面所讲，该元素将一个图片嵌入到页面的某个位置。其通过src（source）属性指定了图片的路径；通过alt（alternative）属性为因某种原因（比如图片路径不对）看不到图片的用户显示一段描述。\n文本标记\n该部分将介绍用于标记文本的几个基本元素。\n  标题\n标题元素用于将一段内容指定为标题或子标题。HTML 文档有 6 个标题级别\u0026lt;h1\u0026gt; - \u0026lt;h6\u0026gt;，通常最多用到 3-4 个。\n\u0026lt;!-- 4 heading levels: --\u0026gt; \u0026lt;h1\u0026gt;My main title\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;My top level heading\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;My subheading\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;My sub-subheading\u0026lt;/h4\u0026gt; 小提示：HTML 中使用\u0026lt;!-- ... --\u0026gt;作注释，浏览器显示页面时会将其忽略。\n  段落\n如前面所讲，\u0026lt;p\u0026gt;元素用于标记文本段落，即常规文本内容。\n\u0026lt;p\u0026gt;This is a single paragraph\u0026lt;/p\u0026gt;   列表\nHTML 为列表内容分配了专门的元素。常见的有有序列表和无序列表两种类型。\n无序列表（对应\u0026lt;ul\u0026gt;元素）用于对顺序无关紧要的场景（如：购物列表）；有序列表（对应\u0026lt;ol\u0026gt;元素）用于对顺序重要的场景（如：食谱步骤）。\n列表中的每个条目都被包在一个\u0026lt;li\u0026gt;元素中。\n示例如下：\n\u0026lt;p\u0026gt;At Mozilla, we\u0026#39;re a global community of\u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;technologists\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;thinkers\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;builders\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt;working together ...\u0026lt;/p\u0026gt;   链接\n链接很重要，其是 Web 成为 Web 的关键所在。\n要添加一个链接，使用\u0026lt;a\u0026gt;（anchor 的缩写，意思是“锚”）元素即可。\n示例如下：\n\u0026lt;a href=\u0026#34;https://www.mozilla.org/en-US/about/manifesto/\u0026#34;\u0026gt;Mozilla Manifesto\u0026lt;/a\u0026gt; 这里的\u0026lt;a\u0026gt;元素使用href属性指定要链接到的一个网址。\n小提示：href为 Hypertext Reference 的缩写，即超文本引用。\n  本小节的内容跟着一步步走下来，最终看到的页面如下图所示（比对下index.html的源码）。\n5 CSS 基础 CSS（Cascading Style Sheets，层叠样式表）是为网页内容添加样式的代码。诸如：如何使文本显示为红色？如何使内容显示在网页布局的某个位置？如何给网页指定背景图片或背景颜色？都是使用 CSS 来实现的。本小节将会对 CSS 的基础使用作一个介绍。\n到底啥是 CSS？\nCSS 与 HTML 一样，都不是编程语言。CSS 是一种样式表语言，是用来选择性设置 HTML 元素样式的工具。\n如下示例使用 CSS 将文本段落设置为红色：\np { color: red; } 现在做几点改动，看一下效果：\n① 使用文本编辑器，将如上 3 行 CSS 代码拷入styles目录下的style.css文件中；\n② 编辑index.html，将如下代码粘贴在\u0026lt;head\u0026gt;与\u0026lt;/head\u0026gt;标签之间；\n\u0026lt;link href=\u0026#34;styles/style.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; ③ 保存index.html，并再次使用浏览器打开，即可看到如下效果。\n  CSS 规则集剖析\n下面剖析一下上面的 3 行 CSS 代码，以了解其运作机制。\n整个结构称为一个规则集。主要有如下几个部分组成：\n① 选择器\n出现在规则集头部，说明要为哪些 HTML 元素指定样式（本例中，是为\u0026lt;p\u0026gt;元素指定样式）。\n② 声明\n单个规则（如这里的color: red），指定设置元素的哪些属性。\n③ 属性\n待设置的属性（本例中，color是\u0026lt;p\u0026gt;元素的属性）。\n④ 属性值\n待设置的属性值（如这里将颜色设置为red，还可以选择别的颜色）。\n此外，需要注意：除了选择器，规则集需要使用花括号括起来；每个声明中，须使用冒号将属性与其值隔开；声明之间使用分号分隔。\n  多个元素选择\n可将一个规则集应用到多个 HTML 元素上（只需将多个选择器按逗号分开即可）。\np, li, h1 { color: red; }   选择器的类型\n上面示例中使用的是元素选择器，此外还有其它类型的选择器，下面列出常见的选择器类型：\n   选择器名 选择的目标 示例     元素选择器（有时也称为标签或类型选择器） 指定类型的所有 HTML 元素 p选择\u0026lt;p\u0026gt;   ID 选择器 页面上具有指定 ID 的元素（同一页面上 id 应唯一） #my-id选择\u0026lt;p id=\u0026quot;my-id\u0026quot;\u0026gt;或\u0026lt;a id=\u0026quot;my-id\u0026quot;\u0026gt;等   类选择器 页面上具有指定类的元素（同一页面上可以有相同类名的多个实例） .my-class选择\u0026lt;p class=\u0026quot;my-class\u0026quot;\u0026gt;、\u0026lt;a class=\u0026quot;my-class\u0026quot;\u0026gt;等   属性选择器 页面上具有指定属性的元素 img[src]选择\u0026lt;img src=\u0026quot;myimage.png\u0026quot;\u0026gt;而非\u0026lt;img\u0026gt;   伪类选择器 页面上在指定状态的指定元素 a:hover选择仅当鼠标指针悬停在链接上的\u0026lt;a\u0026gt;    此外，还有其它的选择器可供探索。详情请参阅『MDN 选择器指南』。\n  字体与文本\n现在有了 CSS 的一些基础知识，我们可以接着在style.css文件试着加入更多的规则来丰富示例页面的外观。\n① 修改index.html文件\n找到前面『网页设计』小节选好的字体，使用\u0026lt;link\u0026gt;元素将其放在标签\u0026lt;head\u0026gt;与\u0026lt;/head\u0026gt;之间。\n\u0026lt;link href=\u0026#34;https://fonts.googleapis.com/css?family=Open+Sans\u0026#34; rel=\u0026#34;stylesheet\u0026#34; /\u0026gt; 如上代码将Open Sans字体加载到了页面。\n② 清除style.css文件现有规则集\n③ 将如下规则集添加到style.css文件\nhtml { font-size: 10px; /* px means \u0026#34;pixels\u0026#34;: the base font size is now 10 pixels high */ font-family: \u0026#34;Open Sans\u0026#34;, sans-serif; /* this should be the rest of the output you got from Google fonts */ } 如上代码指定了页面的字体大小与类型。因\u0026lt;html\u0026gt;是整个页面的父元素，所以该页面所有的元素都会继承该font-size与font-family设置。\n小提示：CSS 中使用/* ... */作注释，浏览器在渲染页面时同样会忽略它们。\n④ 更改元素样式\nh1 { font-size: 60px; text-align: center; } p, li { font-size: 16px; line-height: 2; letter-spacing: 1px; } 如上代码指定了\u0026lt;h1\u0026gt;、\u0026lt;li\u0026gt;与\u0026lt;p\u0026gt;元素的字体大小，且让\u0026lt;h1\u0026gt;元素居中，并指定了\u0026lt;li\u0026gt;与\u0026lt;p\u0026gt;的行高与字母间距以使内容更具可读性。\n样式调整后的页面效果如下图所示：\n关于“框”的一切\n编写 CSS 时，大家可能会注意到：很多都是关于“框”的。包括设置尺寸、颜色和位置。页面上的众多 HTML 元素可被认为是建在“框”上面的“框”。\nCSS 布局主要是基于“框”模型实现的。占用页面空间的每个“框”都具有以下属性：\n① padding（内边距）\n边框与元素内容之间的空间。\n② border（边框）\n内边距外的边框（实线）。\n③ margin（外边距）\n元素边框外的空白区域。\n此外，本小节还会用到：width（元素宽度）、background-color（元素内容和padding的背景色）、color（元素内容的颜色）、text-shadow（元素内的文本的投影）与display（元素的显示模式）。\n接下来，继续在style.css文件添加更多的规则集，看看页面外观有什么变化。\n  改变页面背景色\nhtml { background-color: #00539f; } 如上规则集为整个页面设置了背景颜色。\n  给页面 body 指定样式\nbody { width: 600px; margin: 0 auto; background-color: #ff9500; padding: 0 20px 20px 20px; border: 5px solid black; } ① width: 600px;\n强制设定页面 body 的宽度为 600 像素。\n② margin: 0 auto;\n设定margin或padding时，值可以为 1 ～ 4 个。 1 个值时，表示应用到所有四个方向； 2 个值时，分别表示垂直和水平； 3 个值时，分别表示上、水平、下； 4 个值时，分别表示上、右、下、左。\n本例中，margin的设置仅有两个值，表示将页面 body 垂直方向的外边距设置为 0，水平方向的外边距设置为自动平分。\n③ background-color: #FF9500;\n设置页面 body 的背景颜色。\n④ padding: 0 20px 20px 20px;\n设置页面 body 上、右、下、左的内边距为 0、20 像素、20 像素、20 像素。\n⑤ border: 5px solid black;\n设置页面 body 边框的宽度、样式和颜色。这里，给页面 body 设置了一个 5 像素宽的实心黑色边框。\n  给页面标题指定位置与样式\nh1 { margin: 0; padding: 20px 0; color: #00539f; text-shadow: 3px 3px 1px black; } 您可能注意到 body 顶部有一块很大的空白，这是因为浏览器对\u0026lt;h1\u0026gt;元素添加了默认的样式。 如上 CSS 代码首先使用margin: 0;将外边距设置为了 0，覆盖了浏览器的默认样式。 接下来，使用padding: 20px 0;将上、下内边距设置为了 20 个像素。 然后，使用color: #00539F;将标题颜色设置为了跟 html 背景色一样的颜色。 最后，使用text-shadow: 3px 3px 1px black;为标题设置阴影。text-shadow的四个值分别表示：水平偏移量、垂直偏移量、阴影模糊半径、阴影颜色。\n  图片居中对齐\nimg { display: block; margin: 0 auto; } 接下来，同样使用margin: 0 auto;结合display: block;将图片水平居中。\n\u0026lt;body\u0026gt;是一个块元素，这意味着它会占用页面上的空间。应用于块元素的边距同样会被页面上的其它元素遵循。相比之下，图像是内联元素，要让自动边距设置在此图像上起作用，必须结合使用display: block;将其指定为块级行为。\n小提示：如上设置假定图片的宽度小于 body 的宽度（600 像素），若图片宽度大于 600 像素，则会溢出到 body 的外边。这时，可以使用\u0026lt;img\u0026gt;元素的width属性来调整图片的显示宽度。\n小提示：若现在对display: block;或块元素和内联元素之间的区别不太理解，也不必太担心。随着对 CSS 学习的深入，就会明白了。\n  本小节的内容跟着一步步走下来，最终看到的页面如下图所示（比对下style.css的源码）。\n6 JavaScript 基础 JavaScript 是一种为网站增加交互性的编程语言。如控制按钮点击或表单输入后的行为以及作样式的动态控制等事情。本小节会对 JavaScript 的基础作一介绍。\n到底啥是 JavaScript？\nJavaScript 由 Brendan Eich（Mozilla 联合创始人）发明，其语法简单且应用广泛，可以用来做很多事情。\nHello world 样例\n下面，使用 JavaScript 将页面标题动态修改为“Hello world”。步骤如下：\n① 在网站根目录的scripts文件夹下新建main.js，并将如下代码拷入文件中。\nconst myHeading = document.querySelector(\u0026#34;h1\u0026#34;); myHeading.textContent = \u0026#34;Hello world!\u0026#34;; ② 编辑index.html文件，将如下代码加入\u0026lt;/body\u0026gt;标签前。\n\u0026lt;script src=\u0026#34;scripts/main.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 这与使用\u0026lt;link\u0026gt;元素引入 CSS 一样，其将 JavaScript 引入页面，因此可以对 HTML （包括页面上的 CSS）产生影响。\n这样，使用浏览器重新加载index.html页面，即会看到如下效果：\n小提示：将\u0026lt;script\u0026gt;元素放置在 HTML 文件底部的原因是由于浏览器按照文件中代码的先后顺序来读取代码，这样做是为了保证 HTML 加载完成后再加载 JavaScript。若将 JavaScript 放在待控制 HTML 的前面，就不会达到该效果。详情请参阅『JavaScript 加载策略』。\n可以看到，标题被更改为了“Hello world!”。下面，解释一下上面这段 JavaScript 的作用：\n使用querySelector()函数获取对标题的引用，然后将其存储在名为myHeading的变量中；接着，代码将myHeading变量的textContent属性（表示标题的内容）值设置为Hello world!。\n小提示：querySelector()方法及textContent属性均属 DOM（Document Object Model，文档对象模型）API 的一部分，其具有操作文档的能力。\nJavaScript 快速入门\n为了更好的理解 JavaScript 的工作原理，下面简单过一下其核心特性。\n  变量\n变量为存储值的容器。可使用let关键字声明一个变量：\nlet myVariable; 关于变量命名规则，请参阅『JavaScript 变量』。\nJavaScript 是大小写敏感的。所以myVariable与myvariable不同。\n变量声明后，可以为其赋值：\nmyVariable = \u0026#34;Bob\u0026#34;; 也可以同时进行声明和赋值：\nlet myVariable = \u0026#34;Bob\u0026#34;; 变量可以持有的值的数据类型如下：\n   变量类型 说明 示例     String 字符串 let myVariable = 'Bob';   Number 数值 let myVariable = 10;   Boolean 布尔值 let myVariable = true;   Array 数组 let myVariable = [1, 'Bob', 'Steve', 10];   Object 对象，JavaScript 中的一切都是对象，都可以存储在变量中 let myVariable = document.querySelector('h1');      注释\nJavaScript 中分别使用/* ... */与//作多行与单行注释，浏览器会忽略标记为注释的文本。\n  运算符\n运算符是基于两个值（或变量）来做计算的数学符号。下表列出 JavaScript 中常用的运算符：\n   运算符 说明 示例     +（加） 将两数相加或将两个字符串合并 6 + 9; 'Hello ' + 'world!';   -（减）、*（乘）、/（除） 基础数学运算 9 - 3; 8 * 2; 9 / 3;   =（赋值） 将值赋给一个变量 let myVariable = 'Bob';   ===（相等） 测试两个值是否相等 let myVariable = 3; myVariable === 4   !（非）、!==（不相等） 返回与之前逻辑相反的值 let myVariable = 3; !(myVariable === 3); // 为 false let myVariable = 3; myVariable !== 3; // 为 false    JavaScript 中完整的运算符列表请参阅『JavaScript 表达式与运算符』。\n小提示：做运算时，混合数据类型会导致一些奇怪的结果。如：'35' + '25'会被认为是字符串连接，而不会将两数相加。所以在引用变量时，要多加注意其类型。\n  条件语句\n条件语句用来测试表达式是否为true。常见的条件语句是if ... else。示例如下：\nlet iceCream = \u0026#34;chocolate\u0026#34;; if (iceCream === \u0026#34;chocolate\u0026#34;) { alert(\u0026#34;Yay, I love chocolate ice cream!\u0026#34;); } else { alert(\u0026#34;Awwww, but chocolate is my favorite...\u0026#34;); } 如上示例中，若表达式iceCream === \u0026quot;chocolate\u0026quot;为true，则运行第一个代码块；否则，运行else代码块。\n  函数\n函数是一种重用代码的方式。\n函数通常带有参数，参数使用括号括起来。若有多个参数，则用逗号分隔。如下示例定义了一个将两数相乘的函数：\nfunction multiply(num1, num2) { let result = num1 * num2; return result; } 调用方式如下：\nmultiply(4, 7); multiply(20, 20); multiply(0.5, 3); 小提示：return语句告诉解释器从函数中返回结果变量，以便结果可以被使用。\n  事件处理\n在网站上实现交互需要事件处理器的帮助。事件处理器会监听指定的行为，当行为发生时即会触发响应。典型的例子是浏览器的点击行为处理器。\n下面，将如下代码在浏览器 Console 中执行。然后点击当前页面，看看效果。\ndocument.querySelector(\u0026#34;html\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, function () { alert(\u0026#34;Ouch! Stop poking me!\u0026#34;); }); 如上代码先选择\u0026lt;html\u0026gt;元素，然后调用其addEventListener()函数，传入一个要监听的事件名称（点击事件）和事件发生时要运行的函数。\n这里传递给addEventListener()的函数称为匿名函数（因其没有名称）。还有另一种编写匿名函数的方法，称之为箭头函数。箭头函数使用() =\u0026gt;代替function ()。\ndocument.querySelector(\u0026#34;html\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { alert(\u0026#34;Ouch! Stop poking me!\u0026#34;); });   完善示例网页\n对 JavaScript 基础进行速览后，接着使用其向示例网页添加一些新功能。\n开始之前，先删除main.js文件现有内容。\n  添加图像切换器\n现在使用 JavaScript 和 DOM API 来实现一个图片交替显示的功能。\n① 选择一张与firefox-logo.png大小相近的图片，将其命名为firefox2.png，同样放在网站根目录的images文件夹下。\n② 将如下 JavaScript 代码拷贝至文件main.js中。\nlet myImage = document.querySelector(\u0026#34;img\u0026#34;); myImage.onclick = function () { let mySrc = myImage.getAttribute(\u0026#34;src\u0026#34;); if (mySrc === \u0026#34;images/firefox-logo.png\u0026#34;) { myImage.setAttribute(\u0026#34;src\u0026#34;, \u0026#34;images/firefox2.png\u0026#34;); } else { myImage.setAttribute(\u0026#34;src\u0026#34;, \u0026#34;images/firefox-logo.png\u0026#34;); } }; 使用浏览器重新加载index.html页面，点击图片，将看到当前图片会被切换为另一张不同的图片。\n这是因为，我们使用myImage变量存储了对\u0026lt;img\u0026gt;元素的引用。然后，为onclick事件指定了一个匿名函数。该函数拿到图片的src属性值，判断其若与当前图片地址一样就改变其为另一张图片地址。\n  添加欢迎信息\n接下来，使用 JavaScript 将页面标题更改为一个与登录用户相关的个性化欢迎语。\n① 修改index.html文件，在\u0026lt;script\u0026gt;元素前增加如下代码。\n\u0026lt;button\u0026gt;Change user\u0026lt;/button\u0026gt; ② 修改main.js文件，在底部加入如下代码。\nlet myButton = document.querySelector(\u0026#34;button\u0026#34;); let myHeading = document.querySelector(\u0026#34;h1\u0026#34;); function setUserName() { let myName = prompt(\u0026#34;Please enter your name.\u0026#34;); if (!myName) { setUserName(); } else { localStorage.setItem(\u0026#34;name\u0026#34;, myName); myHeading.textContent = \u0026#34;Mozilla is cool, \u0026#34; + myName; } } if (!localStorage.getItem(\u0026#34;name\u0026#34;)) { setUserName(); } else { let storedName = localStorage.getItem(\u0026#34;name\u0026#34;); myHeading.textContent = \u0026#34;Mozilla is cool, \u0026#34; + storedName; } myButton.onclick = function () { setUserName(); }; 分析一下上面的代码：\n前两句，声明了两个变量myButton与myHeading，分别对button和h1元素作引用；\n接下来，定义了一个函数setUserName()，该函数用于设置个性化问候语（函数中的prompt()用于获取用户输入，若用户输入为空即重新弹窗；否则使用localStorage浏览器 API 将用户输入的字符串存下来，并将myHeading的textContent设置为该字符串）；\n接下来，是一个if ... else块，是一段初始化代码（首先判断localStorage里是否有设置的信息，没有的话即调用setUserName()函数进行设置，否则直接获取并更新textContent）；\n最后，将事件处理器绑定到button上。\n  本小节的内容跟着一步步走下来，最终看到的页面如下图所示（比对下main.js的源码）。\n7 网站发布 网站代码编写完成后，目的是让人们可以访问。本小节将介绍如何以最简单的方式将样例代码发布到线上。\n为了更好的控制网站内容和用户体验，多数人一般会选择购置域名和虚拟主机来实现。\n用于创建个人博客或日常练习的话，用 『GitHub Pages』就够了。非常简单，只需要创建一个仓库，上传代码并稍作配置即可。具体可以参阅相关文档。\n8 Web 工作机制初探 当我们使用浏览器访问一个网站时，其背后做了什么？本小节会概述 Web 的工作机制。虽然这块内容短期内对我们在编码上没多大帮助，但了解一下 Web 的工作机制，长远来说还是很有用的。\n 客户端与服务器  连接到 Web 的计算机被称为客户端（Clients）和服务器（Servers）。\n客户端为用户的网络连接设备（台式机、笔记本、手机或其它可上网设备）和这些设备上的 Web 访问软件（如 Chrome 或 Firefox 浏览器）。 服务器为存储网页或应用的计算机。当客户使用设备访问一个网页时，该网页会被下载到客户端机器，然后再被 Web 浏览器显示。\n 其它方面  此外，从客户端到服务器还需要经过如下几个部分，先做一下名词解释：\n① 用户本地网络连接\n允许用户在网络上发送和接收数据。\n② DNS\n即域名系统（Domain Name System），域名与 IP 的映射表。浏览器知道域名对应的服务器 IP 才能将消息正确的发送到目的地。\n③ TCP/IP\n即传输协议和网络协议（Transmission Control Protocol and Internet Protocol），定义了数据如何在网络上传输。\n④ HTTP\n即超文本传输协议（Hypertext Transfer Protocol），客户端与服务器相互交流的语言。\n⑤ 网页文件\n一个网站由多种文件组成，主要有代码文件（HTML、CSS 与 JavaScript）和资产文件（图片、视频与文档）。\n 整体交互顺序概述  当用户在浏览器输入一个网址时，发生的事情依序为：\n① 浏览器询问 DNS 服务器，找到网址对应服务器的 IP 地址。\n② 浏览器向服务器发送一条 HTTP 请求消息，要求它向客户端发送一份网站副本。此消息以及客户端和服务器之间发送的所有其它数据均通过用户本地网络连接的 TCP/IP 协议发送。\n③ 若服务器同意客户端的请求，则会向客户端发送“200 OK”消息，然后将网站的文件以多个数据块的方式连续发送到浏览器。\n④ 浏览器接收到全部数据块后，将其组装成一个完整的网页来显示给用户。\n 网页文件解析顺序概述  当浏览器向服务器发送 HTML 文件请求时，这些 HTML 文件通常会以\u0026lt;link\u0026gt;方式引用外部 CSS 或以\u0026lt;script\u0026gt;方式引用外部 JavaScript 。当浏览器加载页面时，了解这些文件的解析顺序很重要：\n① 浏览器解析 HTML 文件，找到所有引用 CSS 和 JavaScript 的\u0026lt;link\u0026gt;元素和\u0026lt;script\u0026gt;元素。\n② 根据上一步搜集的 CSS 和 JavaScript 地址，请求对应服务器，获取这些 CSS 文件和 JavaScript 文件。\n③ 浏览器将 HTML 解析好后，在内存中生成一个 DOM（Document Object Model，文档对象模型）树；将 CSS 解析好后，在内存中生成一个 CSSOM（CSS Object Model，CSS 对象模型）树；将 JavaScript 解析好后，开始编译并执行。\n④ 随着浏览器对 DOM 树的构建、CSSOM 树中样式的应用与 JavaScript 的执行，页面就一步步在屏幕上画出来了。\n综上，我们对 Web 网站的开发工具、开发流程、文件结构、组成部分与运行机制有了一个整体和初步的了解。\n 参考资料\n[1] Getting started with the web - Learn web development | MDN\n ","permalink":"https://olzhy.github.io/posts/getting-started-with-the-web.html","tags":["前端开发","HTML","CSS","JavaScript"],"title":"Web 开发入门"},{"categories":["计算机"],"contents":"1 何为感知机？ 感知机是一个单层人工神经网络，是一个用于二分类的算法，其也是线性分类器的一种。\n其可被抽象为下图所示模型：即一个神经元接收到来自 n 个其它神经元的输入信号；对这些输入信号，通过带权值的连接进行计算（各个连接线的权值与对应输入值相乘，然后进行累加），然后判断计算出来的累加值是否超过阈值（Threshold）；若等于或超过阈值，则输出 y 为 1，表示该神经元激活，否则输出 y 为 -1 表示该神经元抑制。\n所以，感知机模型可被描述为：\n针对输入$\\boldsymbol x = \\{x_{1}, x_{2}, ... , x_{n}\\}$，有 $z = w_{1} x_{1} + w_{2} x_{2} + ... + w_{n} x_{n}$，当$z$超过阈值$\\theta$时，将其输出为 1，否则输出为 -1。即由符号函数$f(z)$来决定输出的类别。\n$f(z)=\\left\\{\\begin{matrix} 1, z\\geq \\theta \u0026amp;\\\\-1, z\u0026lt; \\theta \\end{matrix}\\right.$\n下面，试着将$\\theta$并入线性方程，让$z$看起来更紧凑一点： $z = w_{1} x_{1} + w_{2} x_{2} + ... + w_{n} x_{n} - \\theta \\\\ z = w_{1} x_{1} + w_{2} x_{2} + ... + w_{n} x_{n} + (-\\theta) \\cdot 1$\n这样，将$-\\theta$当作$w_{0}$，1 当作$x_{0}$，上式即可变为：\n$z = w_{0} x_{0} + w_{1} x_{1} + ... + w_{n} x_{n} = \\boldsymbol{w}^T \\boldsymbol{x}$\n几何上为$\\boldsymbol{w}$与$\\boldsymbol{x}$两个向量的内积。\n符号函数$f(z)$变为： $f(z)=\\left\\{\\begin{matrix} 1, z\\geq 0 \u0026amp;\\\\-1, z\u0026lt; 0 \\end{matrix}\\right.$\n2 感知机学习算法 原始的感知机学习策略比较简单，即对线性可分数据集（N 表示样本数）：\n$T = \\{(\\boldsymbol x^{(1)}, y^{(1)}), (\\boldsymbol x^{(2)}, y^{(2)}), ..., (\\boldsymbol x^{(N)}, y^{(N)})\\}$\n i) 将 n+1 维的权重向量$\\boldsymbol{w}$初始化为一组随机非 0 值（不可全初始化为 0，否则，调整只会影响跨度不会改变方向）； ii) 对每个 n+1 维的输入样例$\\boldsymbol{x}^{(i)}$，根据$f(z)$计算其输出值$\\hat{y}^{(i)}$，若其与真实值$y^{(i)}$不一致，则更新$\\boldsymbol{w}$权重向量，即将$\\boldsymbol{w}$中的每一个维度的值更新为$w_{j}=w_{j}+\\Delta w_{j}$，增量$\\Delta w_{j}=\\eta (y^{(i)} - \\hat{y}^{(i)})x_{j}^{(i)}$，其中$0 \u0026lt;\\eta \\leq 1$为学习率，也称为步长； iii) 循环直至所有样例均正确分类或达到最大循环次数退出循环。  上述步骤 ii)中，增量$\\Delta w_{j}=\\eta (y^{(i)} - \\hat{y}^{(i)})x_{j}^{(i)}$设置的非常巧妙。对输入样例$\\boldsymbol{x}^{(i)}$，若计算值$\\hat{y}^{(i)}$为 -1，真实值$y^{(i)}$为 1，增量$\\Delta w_{j}$表示向正类靠近一点；反之，则向负类靠近一点；若计算值与真实值一致，则增量为 0。\n3 感知机学习算法的 Python 实现 下面使用 Python 对如上感知机学习算法进行实现，Perceptron 类包含如下几个方法。\n __init__ 构造方法，新建 Perceptron 对象时可指定学习率 eta 及最大迭代次数 max_iter； fit 训练方法，首先，根据样本输入 x 的维度（若为 n），将权重向量 _w 初始化为一个 n+1 维的随机非 0 数值；然后，根据样本输入 x（N 个 n 维向量）及样本输出 y（N 个由 +1 或 -1 组成的数值）对权重向量 _w 进行调整，若在最大迭代次数 max_iter 内找到合适的 _w 可将样本数据集进行正确划分，则退出；否则，若已达到最大迭代次数仍未找到合适的 _w，则训练失败并退出； _predict 计算权重向量 _w 与某个样本输入 xi 向量点积的私有方法，返回值为 float 类型； predict 调用_predict算得两向量的点积后判断其是否非负，若非负返回 +1；否则返回 -1，返回值为 int 类型。该方法可供fit方法在训练时使用，亦可在训练成功后，使用其对新的输入进行预测。  $ cat perceptron.py from typing import List import random class Perceptron: \u0026#34;\u0026#34;\u0026#34; Perceptron binary classifier \u0026#34;\u0026#34;\u0026#34; def __init__(self, eta: float = 1.0, max_iter: int = 1000): \u0026#34;\u0026#34;\u0026#34; Perceptron constructor :param eta: 0 \u0026lt; eta \u0026lt;= 1.0 :param max_iter: max iteration \u0026#34;\u0026#34;\u0026#34; self.eta = eta self.max_iter = max_iter random.seed(1) def fit(self, x: List[List[float]], y: List[int]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; Fit is used for data training after the execution, _w (weight vector) will be produced :param x: input, vector list :param y: output, class label list :return: None \u0026#34;\u0026#34;\u0026#34; if len(x) \u0026lt;= 0: return # initialize weight vector to random integers self._w = [random.randint(-10, 10) for _ in range(len(x[0]) + 1)] times = 0 while times \u0026lt; self.max_iter: times += 1 errors = 0 for xi, yi in zip(x, y): y_predict = self.predict(xi) if yi - y_predict != 0: errors += 1 for i in range(len(xi)): # update vector _w self._w[i + 1] += self.eta * (yi - y_predict) * xi[i] self._w[0] += self.eta * (yi - y_predict) print(\u0026#39;times: {}, xi: {}, yi: {}, y_predict: {}, _w: {}\u0026#39;.format(times, xi, yi, y_predict, self._w)) if 0 == errors: break def _predict(self, xi: List[float]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34; Calculate the predictive value for a single sample input xi :param x: a single sample input xi :return: dot product of vector _w and xi \u0026#34;\u0026#34;\u0026#34; return sum([self._w[i + 1] * xi[i] for i in range(len(xi))]) + self._w[0] def predict(self, xi: List[float]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; Predict xi belongs to class +1 or -1 :param xi: a single sample input xi :return: class +1 or -1 \u0026#34;\u0026#34;\u0026#34; return 1 if self._predict(xi) \u0026gt;= 0 else -1 下面使用该 Perceptron 模型对仅有两个维度的样本输入 X={(3, 3), (4, 3), (1, 1)}，输出 Y={1, 1, -1}进行划分：\n$ cat test.py #!/usr/bin/env python3 import perceptron if \u0026#39;__main__\u0026#39; == __name__: p = perceptron.Perceptron(eta=1.0, max_iter=100) x = [[3, 3], [4, 3], [1, 1]] y = [1, 1, -1] p.fit(x, y) 根据如下打印结果可以看到，该算法经过 3 轮，6 次判断，3 次权重调整后得到一个可将样本数据正确划分的权重向量 [-8.0, 10.0, -6.0]。\ntimes: 1, xi: [3, 3], yi: 1, y_predict: -1, _w: [-4.0, 14.0, -2.0] times: 1, xi: [4, 3], yi: 1, y_predict: 1, _w: [-4.0, 14.0, -2.0] times: 1, xi: [1, 1], yi: -1, y_predict: 1, _w: [-6.0, 12.0, -4.0] times: 2, xi: [3, 3], yi: 1, y_predict: 1, _w: [-6.0, 12.0, -4.0] times: 2, xi: [4, 3], yi: 1, y_predict: 1, _w: [-6.0, 12.0, -4.0] times: 2, xi: [1, 1], yi: -1, y_predict: 1, _w: [-8.0, 10.0, -6.0] times: 3, xi: [3, 3], yi: 1, y_predict: 1, _w: [-8.0, 10.0, -6.0] times: 3, xi: [4, 3], yi: 1, y_predict: 1, _w: [-8.0, 10.0, -6.0] times: 3, xi: [1, 1], yi: -1, y_predict: -1, _w: [-8.0, 10.0, -6.0] 因该样例的样本点是二维的，所以可将其表示为平面空间的点，权重向量为一条直线。寻找权重向量的过程如下图所示。\n4 对 Iris 数据集进行训练及预测 下面使用 3 中的代码对iris(鸢尾花)数据集进行训练及预测。该数据集共包含三类鸢尾花品种，因 Perceptron 模型是一个仅支持二分类的分类器。所以仅选取该数据集中的两类数据（Iris-setosa 与 Iris-versicolor）进行训练及预测（该两类数据均有 50 条，分别取两者的前 40 条作为训练样本，两者的后 10 条作为预测样本）。\n测试代码，测试数据，与 Perceptron 模型代码（perceptron.py）的目录结构如下：\n$ tree . perceptron.py test.py iris.data predict.data iris.data的内容如下（每一行的数值表示：萼长 - Sepal length、萼宽 - Sepal width、瓣长 - Petal length、瓣宽 - Petal width、花的种类）：\n5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa ... 7.0,3.2,4.7,1.4,Iris-versicolor 6.4,3.2,4.5,1.5,Iris-versicolor ... 调用 Perceptron 的代码如下：\n$ cat test.py #!/usr/bin/env python3 import perceptron import csv if \u0026#39;__main__\u0026#39; == __name__: # perceptron instance p = perceptron.Perceptron(eta=1.0, max_iter=100) # training x = [] y = [] with open(\u0026#39;iris.data\u0026#39;) as f: for sample in csv.reader(f): x.append([float(i) for i in sample[:4]]) y.append(1 if sample[4] == \u0026#39;Iris-setosa\u0026#39; else -1) p.fit(x, y) # predict with open(\u0026#39;predict.data\u0026#39;) as f: for sample in csv.reader(f): xi, yi = [float(i) for i in sample[:4]], sample[4] print(\u0026#39;predict label: {}, real: {}\u0026#39;.format(p.predict(xi), yi)) 观察输出可以发现，该 Perceptron 算法经过 7 轮迭代找到合适的权重向量，然后采用训练的模型对 20 条测试数据进行预测，结果准确无误。\n... predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: 1, real: Iris-setosa predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor predict label: -1, real: Iris-versicolor 综上，对感知机算法的原理作了学习与了解，并使用 Python 对其进行了实现及初步应用。本文代码已托管至GitHub，欢迎 Fork。\n 参考资料\n[1] Sebastian \u0026amp; Vahid. Python Machine Learning (Second Edition).\n[2] 李航.(2012). 统计学习方法(第 2 版). 清华大学出版社, 北京.\n ","permalink":"https://olzhy.github.io/posts/perceptron-python-implementation.html","tags":["机器学习","Python","算法"],"title":"感知机算法及 Python 实现"},{"categories":["随笔"],"contents":"週六和太太帶孩子去植物園賞花。\n迎春花開，櫻花綻放。\n各種顏色的鬱金香開的亭亭玉立，婀娜多姿，香氣撲鼻。\n還有漫山遍野的小野花也在爭春。\n春天，萬物復蘇，一個個開始綻放的生命讓人心情開朗，生機勃勃。\n\n\n\n\n\n\n2022 年 4 月 16 日於大連\n","permalink":"https://olzhy.github.io/posts/botanical-garden.html","tags":["随笔"],"title":"遊覽植物園"},{"categories":["计算机"],"contents":"Azure 流水线（Azure Pipelines）是 Azure DevOps 的一部分。Azure 流水线结合了持续集成（CI）和持续交付（CD）来构建和测试代码，并可将其发布到任何目标环境。Azure 流水线有经典（可视化）和 YAML 两种配置使用方式。作为开发，本文仅关注 YAML 这种配置方式。\nAzure 流水线支持的场景或环境\n  支持的版本控制系统\n为应用配置 CI/CD 前，须将源码提交到一个版本控制系统上。Azure DevOps 支持 GitHub、 Azure Repos 和 Bitbucket 等版本控制平台。\n  支持的开发语言或应用\nAzure 流水线支持在 Linux、macOS 及 Windows 三种代理节点上构建、测试及部署绝大多数语言（诸如：Python、Java、JavaScript、PHP、Ruby、C#、C++ 和 Go 等）开发的应用。\nAzure 流水线提供一组现成的 Task（任务）来构建或测试这些常用语言编写的应用。此外，还支持在流水线直接运行命令行、PowerShell 或 Shell 脚本。\n  支持的目标部署环境\nAzure 流水线支持绝大多数的目标部署环境，包括：虚拟机环境、容器环境、On-Premise 环境、和云平台环境。也可以借助 Azure 流水线将一个移动 App 发布到应用商店。\n  支持的测试方式\nAzure 流水线支持部署在云上或 On-Premise 环境上应用的自动化测试。支持用户选择自己喜欢的测试技术和测试框架，且提供丰富的测试报告。\n  支持的包格式及仓库\nAzure 流水线支持市面上常用的打包工具（如 NuGet、npm 及 Maven 等），且支持将构建好的包发布到 Azure 流水线内置的包管理仓库或其它外部包管理仓库（如 Nexus、Artifactory 等）。\n  Azure 流水线的编写方式\n须在工程根目录新建azure-pipelines.yml YAML 文件来定义 Azure 流水线。\nAzure 流水线定义文件与项目代码同属一个仓库，一同进行版本控制，并遵循相同的分支策略（如 feature-xxx =\u0026gt; develop =\u0026gt; master 分支策略）。通过检查 PR（拉取请求，Pull Request）来验证更改。\n所以，使用 Azure 流水线的基本步骤为：\n 为 Git 代码仓库配置 Azure 流水线； 在工程根目录新建azure-pipelines.yml文件定义流水线步骤（编译、打包、发布及部署等）； 当更新了代码并提交 PR 到指定的分支时，会自动触发 PR 阶段对应流水线中定义的步骤，完成后，检查运行结果；然后，决定是否合并 PR 到指定的分支，从而决定是否触发该分支对应的流水线步骤。  下面就一步一步探索下 Azure 流水线的具体使用方法。\n1 开始使用 Azure 流水线 1.1 注册 Azure 流水线 打开Azure 流水线介绍页，然后点击Start free或Start free with Github，其会引导使用微软账号或 Github 账号来注册 Azure 流水线；完成后，需要创建一个 Azure DevOps 组织，创建好以后即可以用 URL 的方式进行访问了（如本文 Azure DevOps 组织地址为：https://dev.azure.com/olzhy）。组织建好后，其会引导创建一个项目，项目建好后，即可在其下看到有流水线。\n1.2 创建第一条流水线 下面，使用一个 Java 编写的示例应用创建我们的第一条流水线。\n  Fork 示例仓库\n将如下仓库 Fork 到自己的 Github 账号\nhttps://github.com/MicrosoftDocs/pipelines-java 我的 Github 地址为：https://github.com/olzhy；Fork 完成后的仓库地址为：https://github.com/olzhy/pipelines-java。\n该工程是一个普通的 Java 工程，仅有一个样例代码文件（Demo.java）和一个单元测试文件（MyTest.java）。\n目录结构如下：\npipelines-java |--- pom.xml |--- src | |--- main # 代码目录 | | \\--- com.microsoft.demo.Demo.java | \\--- test/java # 单元测试目录 | \\--- MyTest.java \\--- README.md   创建流水线\n打开上一步创建好的项目（https://dev.azure.com/olzhy/test），点击 Pipelines 后新建一条流水线；选择从 Github 获取源码，选择推荐的 Maven 流水线模板，保存并运行。会发现，YAML 流水线文件azure-pipelines.yml已被自动创建并提交至仓库。\nazure-pipelines.yml文件的内容为：\n# Maven # Build your Java project and run tests with Apache Maven. # Add steps that analyze code, save build artifacts, deploy, and more: # https://docs.microsoft.com/azure/devops/pipelines/languages/java trigger: - master pool: vmImage: ubuntu-latest steps: - task: Maven@3 inputs: mavenPomFile: \u0026#34;pom.xml\u0026#34; mavenOptions: \u0026#34;-Xmx3072m\u0026#34; javaHomeOption: \u0026#34;JDKVersion\u0026#34; jdkVersionOption: \u0026#34;1.8\u0026#34; jdkArchitectureOption: \u0026#34;x64\u0026#34; publishJUnitResults: true testResultsFiles: \u0026#34;**/surefire-reports/TEST-*.xml\u0026#34; goals: \u0026#34;package\u0026#34; 初步看，该流水线内容为使用maven package将代码打包并执行单元测试的一个过程。流水线的编写方式及这里配置参数的意思后面会详细分析。\n流水线的运行结果如下图所示：\n可以看到，日志最后打印说将测试结果发布到了一个地址，打开后发现，单元测试结果被自动发布到了 Azure DevOps 的另一个服务模块 Test Plan 下。\n  添加 Github 状态标识\n将如下内容添加到工程根目录README.md文件最上面，并提交至仓库，即可在 Github 仓库上（https://github.com/olzhy/pipelines-java）看到流水线状态了。\n[![Build Status](https://dev.azure.com/olzhy/test/_apis/build/status/olzhy.pipelines-java?branchName=master)](https://dev.azure.com/olzhy/test/_build/latest?definitionId=3\u0026amp;branchName=master)   1.3 自定义流水线内容 自定义流水线内容前，先分析一下当前azure-pipelines.yml的内容。\n# Maven # Build your Java project and run tests with Apache Maven. # Add steps that analyze code, save build artifacts, deploy, and more: # https://docs.microsoft.com/azure/devops/pipelines/languages/java trigger: - master pool: vmImage: ubuntu-latest steps: - task: Maven@3 inputs: mavenPomFile: \u0026#34;pom.xml\u0026#34; mavenOptions: \u0026#34;-Xmx3072m\u0026#34; javaHomeOption: \u0026#34;JDKVersion\u0026#34; jdkVersionOption: \u0026#34;1.8\u0026#34; jdkArchitectureOption: \u0026#34;x64\u0026#34; publishJUnitResults: true testResultsFiles: \u0026#34;**/surefire-reports/TEST-*.xml\u0026#34; goals: \u0026#34;package\u0026#34;  trigger部分说明：当 Git 仓库的master分支有新的提交时，该流水线即会被触发； pool部分说明：该流水线将会在一台 Linux 节点上运行，节点采用的镜像为ubuntu-latest； steps部分说明：该流水线只有一步，即运行 Maven 任务。  初步了解了这些参数的意思后，就可以根据自己的需求对流水线做一些修改了。\n如想更改构建平台，即可将pool下面的vmImage指定为windows-latest或macos-latest。\n如想增加一步测试覆盖率报告，即可在steps下加一个任务PublishCodeCoverageResults@1：\nsteps: - task: Maven@3 ... - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: \u0026#34;JaCoCo\u0026#34; summaryFileLocation: \u0026#34;$(System.DefaultWorkingDirectory)/**/site/jacoco/jacoco.xml\u0026#34; reportDirectory: \u0026#34;$(System.DefaultWorkingDirectory)/**/site/jacoco\u0026#34; failIfCoverageEmpty: true 如想在多个平台并行运行作业，可将pool部分替换为如下配置：\nstrategy: matrix: linux: imageName: \u0026#34;ubuntu-latest\u0026#34; mac: imageName: \u0026#34;macOS-latest\u0026#34; windows: imageName: \u0026#34;windows-latest\u0026#34; maxParallel: 3 pool: vmImage: $(imageName) 如想更改触发分支，可更改trigger部分。如下示例说明仅master分支或releases/*分支有提交时才会触发构建。\ntrigger: - master - releases/* 将trigger替换为pr后，说明针对列出的分支有新的 PR 时即会触发构建。\npr: - master - releases/* 如想设置流水线启停策略或更改流水线文件（azure-pipelines.yml）路径，需要到流水线详情页上去改，这些关于流水线设置类的功能未在 YAML 文件中管理。\n此外，还可以为流水线添加错误处理器。\n如下示例流水线有两个Job，第一个Job模拟普通流水线工作。若其发生错误，会触发第二个Job，该Job会直接使用命令的方式调用工作项的 REST API 在项目中创建一个 Bug。\n# When manually running the pipeline, you can select whether it # succeeds or fails. parameters: - name: succeed displayName: Succeed or fail type: boolean default: false trigger: - master pool: vmImage: ubuntu-latest jobs: - job: Work steps: - script: echo Hello, world! displayName: \u0026#34;Run a one-line script\u0026#34; # This malformed command causes the job to fail # Only run this command if the succeed variable is set to false - script: git clone malformed input condition: eq(${{ parameters.succeed }}, false) # This job creates a work item, and only runs if the previous job failed - job: ErrorHandler dependsOn: Work condition: failed() steps: - bash: |az boards work-item create \\ --title \u0026#34;Build $(build.buildNumber) failed\u0026#34; \\ --type bug \\ --org $(System.TeamFoundationCollectionUri) \\ --project $(System.TeamProject) env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) displayName: \u0026#34;Create work item on failure\u0026#34; 本节，我们对 Azure 流水线的使用有了一个最基本的了解，下面详细看一下 Azure 流水线的组成部分及基础概念。\n2 Azure 流水线基础概念 一条 Azure 流水线由多个 Stage 组成，一个 Stage 由多个 Job 组成，一个 Job（运行在 Agent 上） 由多个 Step 组成，Step 可以是 Script 或 Task。\n下面看一下 Azure 流水线常用到的几个术语：\n  Pipeline（流水线）\nPipeline 定义了应用的整个持续集成与部署流程，其由多个 Stage 组成。可将 Pipeline 认为成是一个定义如何执行构建、测试、部署等步骤的工作流。\n  Stage（阶段）\nStage 是流水线中的逻辑边界，可使用其将关注的部分进行分离（如分成构建，QA，部署等）。一个 Stage 由一个或多个 Job 组成。一个流水线中的多个 Stage 默认按顺序执行。我们可以指定 Stage 运行的条件。\n  Job（作业）\nJob 表示一组 Step 执行的边界，其需要在 Agent 上运行，一个 Job 的所有 Step 均在同一个 Agent 上运行。一个 Stage 包含一个或多个 Job。\n  Step（步骤）\nStep 是一条流水线最小的构建块，每一个 Step 可以是 Script，也可以是 Task。\n  Task（任务）\nTask 是一条流水线中定义自动化的一个构建块。\n  Script（脚本）\nScript 是流水线中使用命令行、PowerShell 或 Bash 来运行的代码。\n  Agent（代理）\nAgent 为运行 Job 的基础环境。\n  Approvals（审批）\nApprovals 为构建或部署前的一组校验，如用于控制发布到生产环境的手动审批校验。\n  Artifact（制品）\nArtifact 为构建产生的一组包或文件，其用于后续的部署 Task。\n  Deployment（部署）\nDeployment 为流水线中的部署 Job，即部署到一个目标环境的一组步骤。\n  Deployment group（部署组）\nDeployment group 为一组安装代理的目标部署机器。\n  Environment（环境）\nEnvironment 为托管应用的一组资源（如请求域名，虚拟机，容器等）。一条流水线可能在构建和测试完成后将应用部署到多个环境。\n  Run（运行）\nRun 代表流水线的一次执行。其会收集与运行步骤相关的日志及测试结果。\n  Trigger（触发器）\nTrigger 用来告诉流水线何时运行。Trigger 可以配置为由代码提交至仓库时、调度时间到来时或当另一个构建完成时。\n  Library（库）\n库包括安全文件和变量组。安全文件是一种在多条流水线共享文件的方法。如在构建流水线将一个文件存储成 DevOps 级别，然后在部署流水线使用它。变量组是跨流水线传递值或密钥的方法。\n  下面会详细看看如何使用这些基础功能。\n2.1 Trigger Trigger 即触发器，用于定义流水线的自动执行策略。有 CI/PR Trigger、定时 Trigger 和流水线完成 Trigger 三种类型。\n  CI/PR Trigger\nCI Trigger 或 PR Trigger 会因代码仓库的类型不同而有所差别。本文仅关注 Github 仓库的 Trigger 配置。\nCI Trigger 用于指定当哪些分支或 Tag 有新的提交时触发流水线运行。\n最简单的配置方式为：\n# 仅`master`分支和`releases/*`分支有提交时触发构建 trigger: - master # 指定分支名 - releases/* # 采用通配符 稍微复杂点的配置方式为：\n# 仅`master`分支和`releases/*`分支（`releases/old*`除外）有提交时触发构建 trigger: branches: include: - master - releases/* exclude: - releases/old* 设置批量运行的配置方式：\n# 若团队成员提交频繁，可将流水线设置为 batch 运行，即待当前流水线运行完成后，再统一运行一次最新的提交 trigger: batch: true branches: include: - master 指定包含或排除的 Tag：\n# 若有`v2.*`的新 Tag（`v2.0`除外），即会触发构建 trigger: tags: include: - v2.* exclude: - v2.0 PR Trigger 用于指定当哪些目标分支有新的 PR 时（或 PR 有更新时）触发流水线运行。\n简单一点的配置如下：\n# 当如下分支有 PR 时会触发构建 pr: - master - develop - releases/* 复杂一点的配置如下：\n# 当`master`分支与`releases/*`分支（`releases/old*`除外）有 PR 时会触发构建 pr: branches: include: - master - releases/* exclude: - releases/old*   定时 Trigger\n支持配置 Cron 表达式来定时触发流水线。Cron 表达式的时区采用 UTC 时间，若使用了模板，须将调度规则配置在主文件，不可配置在其它模板文件。\n如下的例子定义了两个调度：\nschedules: - cron: \u0026#34;0 0 * * *\u0026#34; # 每天半夜构建，仅当自上次运行成功后有新的提交 displayName: Daily midnight build branches: include: - main - releases/* exclude: - releases/ancient/* - cron: \u0026#34;0 12 * * 0\u0026#34; # 每周日中午构建，不管自上次运行成功后有没有新的提交 displayName: Weekly Sunday build branches: include: - releases/* always: true Cron 表达式的语法是业界通用的。\n表达式字段意思如下：\nmm HH DD MM DW \\ \\ \\ \\ \\__ 一周中的哪一天，自周日（0）起 \\ \\ \\ \\____ 月 \\ \\ \\______ 天 \\ \\________ 时 \\__________ 分 复杂 Cron 示例（如下的几种写法等价）：\n# 每周一、三、五18点触发 0 18 * * Mon,Wed,Fri 0 18 * * 1,3,5 0 18 * * 1-5/2 # 每6小时触发一次 0 0,6,12,18 * * * 0 */6 * * * 0 0-18/6 * * *   流水线完成 Trigger\n若想在一条流水线运行完成后触发另一条流水线。可以通过配置流水线资源实现。\n下面示例了两条流水线，第一条流水线 security-lib-ci 运行完成时触发第二条流水线 app-ci：\n# 流水线 security-lib-ci steps: - bash: echo \u0026#34;The security-lib-ci pipeline runs first\u0026#34; # 当指定分支的流水线 security-lib-ci 运行完成后会触发当前流水线 app-ci 运行 resources: pipelines: - pipeline: securitylib source: security-lib-ci # 被当前流水线资源引用的流水线名 project: FabrikamProject # 仅当源流水线在别的项目时，需要指定项目名称 trigger: # 仅当源流水线的`releases/*`分支（`releases/old*`除外）完成运行时，触发当前流水线运行 branches: include: - releases/* exclude: - releases/old* steps: - bash: echo \u0026#34;app-ci runs after security-lib-ci completes\u0026#34; 若想更加精确的指定源流水线的哪个阶段完成才触发当前流水线，可以参阅文档稍作配置即可实现。\n  2.2 Task 及模板 Task 是定义管道中自动化的构建块。一个 Job 有一个或多个 Task，运行 Job 时，所有 Task 依次运行。Azure 流水线除了提供诸多内置的 Task 满足基本的构建与部署场景外，还支持创建自定义 Task。\n  Task 版本\nTask 是有版本号的，使用时需要指定主版本号。如您指定使用的 Task 的主版本为 1，那对应该主版本有新的次版本时（如 1.2），会自动使用最新的；但有另一个主版本 2 出现时，若非您显示指定，流水线使用的还是版本 1。\n可在 Task 名后加@指定版本，示例如下：\nsteps: - task: PublishTestResults@2   Task 控制选项\nTask 的控制选项可以用 Key/Value 的方式来指定。\n- task: string # 指定 Task 名和版本，如 VSBuild@1 condition: expression # 运行条件，如设置为 succeededOrFailed()，表示不管前面步骤成功失败都运行这一步；设置为 failed()，表示只有前面步骤失败了才运行这一步；always() 表示无论如何要运行这一步 continueOnError: boolean # 若设置为 true，表示即使这一步失败了，后续步骤也应运行；默认为 false enabled: boolean # 是否运行此步骤；默认为 true retryCountOnTaskFailure: number # Task失败时，最大重试次数；默认值为 0 timeoutInMinutes: number # 最长等待时间 target: string # 主机或目标容器资源的名称 Task 一般在 Agent 上运行，若指定target（可以是 host 或定义的容器资源），则会在指定的目标机器或容器上运行。\nresources: containers: - container: pycontainer image: python:3.8 steps: - task: SampleTask@1 target: host - task: AnotherTask@1 target: pycontainer   Task 环境变量\n可以使用env属性来指定 Task 需要用到的环境变量。\n如下示例分别使用 Scrpt 快捷语法和与其等价的 Task 语法来执行一条命令，命令中会用到环境变量AZURE_DEVOPS_EXT_PAT：\n# 使用 Script 快捷语法 - script: az pipelines variable-group list --output table env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) displayName: \u0026#34;List variable groups using the script step\u0026#34; # 使用 Task 语法 - task: CmdLine@2 inputs: script: az pipelines variable-group list --output table env: AZURE_DEVOPS_EXT_PAT: $(System.AccessToken) displayName: \u0026#34;List variable groups using the command line task\u0026#34;   模板可以用来定义可重用的逻辑，也可以用来控制流水线的安全性。\n  模板作重用\n可以将一些公共的 Job、Stage、Step 等抽取到模板，而在azure-pipelines.yml直接使用。\n如下示例将 Job 抽取成模板，而在azure-pipelines.yml引用：\n# 文件：templates/npm-with-params.yml parameters: - name: name # defaults for any parameters that aren\u0026#39;t specified default: \u0026#34;\u0026#34; - name: vmImage default: \u0026#34;\u0026#34; jobs: - job: ${{ parameters.name }} pool: vmImage: ${{ parameters.vmImage }} steps: - script: npm install - script: npm test # 文件：azure-pipelines.yml jobs: - template: templates/npm-with-params.yml # Template reference parameters: name: Linux vmImage: \u0026#34;ubuntu-latest\u0026#34; - template: templates/npm-with-params.yml # Template reference parameters: name: macOS vmImage: \u0026#34;macOS-latest\u0026#34; - template: templates/npm-with-params.yml # Template reference parameters: name: Windows vmImage: \u0026#34;windows-latest\u0026#34;   模板作安全控制\n为了提高安全性，可以强制流水线必须从特定的模板扩展。\n如下示例会对非法的 Task 名作报错处理：\n# 文件：start.yml parameters: - name: buildSteps # 参数名为 buildSteps type: stepList # 数据类型为 StepList default: [] # 默认值为空列表 stages: - stage: secure_buildstage pool: vmImage: windows-latest jobs: - job: secure_buildjob steps: - script: echo This happens before code displayName: \u0026#34;Base: Pre-build\u0026#34; - script: echo Building displayName: \u0026#34;Base: Build\u0026#34; - ${{ each step in parameters.buildSteps }}: - ${{ each pair in step }}: ${{ if ne(pair.value, \u0026#39;CmdLine@2\u0026#39;) }}: ${{ pair.key }}: ${{ pair.value }} ${{ if eq(pair.value, \u0026#39;CmdLine@2\u0026#39;) }}: # 若使用 Task \u0026#39;CmdLine@2\u0026#39; 会抛错 \u0026#34;${{ pair.value }}\u0026#34;: error - script: echo This happens after code displayName: \u0026#34;Base: Signing\u0026#34; # 文件：azure-pipelines.yml trigger: - main extends: template: start.yml parameters: buildSteps: - bash: echo Test # 会被正常解析 displayName: succeed - bash: echo \u0026#34;Test\u0026#34; displayName: succeed # 这一步解析会报 YAML 语法错误 `Unexpected value \u0026#39;CmdLine@2\u0026#39;` - task: CmdLine@2 inputs: script: echo \u0026#34;Script Test\u0026#34; # 这一步解析会报 YAML 语法错误 `Unexpected value \u0026#39;CmdLine@2\u0026#39;` - script: echo \u0026#34;Script Test\u0026#34;   模板抽到一个仓库\n可以将模板文件抽取到一个仓库（类似 Jenkins 的SharedLibrary），供需要的工程使用。\n如下示例azure-pipelines.yml引用另一个仓库的模板：\n# 仓库：Contoso/LinuxProduct # 文件：azure-pipelines.yml resources: # 配置模板所在的仓库信息 repositories: - repository: templates type: github name: Contoso/BuildTemplates endpoint: myServiceConnection # 配置 Azure DevOps 服务连接信息 ref: refs/tags/v1.0 # 配置引用的 Tag jobs: - template: common.yml@templates # 使用`文件@模板名`的方式引用模板   此外，关于模板参数类型，变量重用，模板表达式等更详细的配置，请参阅文档。\n2.3 Job 及 Stage Job 是顺序运行的一系列步骤。\n  Job 定义\n如下是定义一个 Job 的完整语法：\n- job: string  # Job名称 `[A-Za-z0-9_]` displayName: string  # UI 显示名称 dependsOn: string | [ string ] condition: string strategy: # 注意：`parallel` 与 `matrix` 互斥，仅可指定一个 parallel: # 并行策略 matrix: # 矩阵策略 maxParallel: number # 仅针对 `matrix` 使用，最大并行数 continueOnError: boolean  # 设置为 true，表示即使当前 Job 失败，也会继续运行后续 Job；默认为 false pool: pool # Agent 池 workspace: clean: outputs | resources | all # Job 运行前是否清除工作空间 container: containerReference # 指定运行该 Job 的容器 timeoutInMinutes: number # 自动终止前的最长运行时间 cancelTimeoutInMinutes: number # 在终止任务之前，给“即使任务被取消，也要始终运行的Job”多少时间 variables: { string: string } | [ variable | variableReference ] steps: [ script | bash | pwsh | powershell | checkout | task | templateReference ] services: { string: string | container } # 作为服务容器运行的容器资源 uses: # 此作业所需的尚未引用的任何资源（仓库或池） repositories: [ string ] # 引用 Azure Git 仓库 pools: [ string ] # 池名称，一般在使用矩阵策略时会用到 若 Job 的主要工作是部署（非构建或测试），可以使用一个叫做 Deployment 的特殊 Job。\n示例如下：\n- deployment: string # 取代 job，使用 deployment 关键字 pool: name: string demands: string | [ string ] environment: string strategy: runOnce: deploy: steps: - script: echo Hi!   Job 的类型\nJob 有几种类型，用于区分在哪里运行，分别为：Agent Job、Server Job 和 Container Job。\nAgent Job 为最常见的 Job 类型，它们在 Agent 池中的 Agent 上运行。当使用 Microsoft 托管的 Agent 时，流水线中的每个 Job 都会获得一个新的 Agent。若使用自托管 Agent 的来满足特定 Job 的需求时，根据 Agent 的数目多少，Job 可能会使用到相同的 Agent。Job 默认为 Agent 类型。\nServer Job 在服务器上运行，不需要 Agent 或任何目标机器。目前，仅少数 Task 支持在 Server Job 执行。不需要 Agent 的 Task 有：Azure 函数调用任务、REST API 调用任务、手动校验任务和查询工作项任务等。\n如下示例指定一个 Server Job：\njobs: - job: somejob pool: server   Job 的依赖与条件\n可以指定 Job 运行的依赖及条件。\n示例如下：\n# Job B 依赖 Job A，仅当 Job A 运行成功并且分支为 master 时才运行 Job B jobs: - job: A steps: - script: echo hello - job: B dependsOn: A condition: and(succeeded(), eq(variables[\u0026#39;build.sourceBranch\u0026#39;], \u0026#39;refs/heads/master\u0026#39;)) steps: - script: echo this only runs for master   Job 的工作空间\n当运行 Agent Job 时，其会在 Agent 上创建一个工作区。工作区是一个文件夹，流水线在其中下载代码，运行步骤及输出结果。可以在流水线中引用工作区目录。\n其中，Build.SourcesDirectory表示源码下载目录；Build.ArtifactStagingDirectory为下载制品的目录及生成制品在发布前的目录；Build. BinariesDirectory为输出文件的目录；Common.TestResultsDirectory为上传测试结果的目录。\n前面已提到，工作区清理选项只对自托管 Agent 适用；对于 Microsoft 托管的 Agent，Job 每次运行使用的是一个新的 Agent。\n工作区清理的配置如下：\n# 工作区清理选项，只对自托管 Agent 适用 - job: myJob workspace: clean: outputs | resources | all # Job 运行前，需要清理的内容   Job 中的制品下载\n如下示例有Build与Deploy两个 Job，分别作制品上传与制品下载。\n# 将 Build 作业中构建的制品命名为 `WebSite` 并发布到工作区，然后在 Deploy 作业中下载它 jobs: - job: Build pool: vmImage: \u0026#34;ubuntu-latest\u0026#34; steps: - script: npm test - task: PublishBuildArtifacts@1 inputs: pathtoPublish: \u0026#34;$(System.DefaultWorkingDirectory)\u0026#34; artifactName: WebSite # 仅当 Build Job 构建成功，才会下载并部署制品 - job: Deploy pool: vmImage: \u0026#34;ubuntu-latest\u0026#34; steps: - checkout: none # 跳过默认检出仓库内容 - task: DownloadBuildArtifacts@0 displayName: \u0026#34;Download Build Artifacts\u0026#34; inputs: artifactName: WebSite downloadPath: $(System.DefaultWorkingDirectory) dependsOn: Build condition: succeeded()   使用容器 Job\n默认情况下，Job 在安装了 Agent 的主机上运行。若想自己控制 Job 运行的环境，可以使用容器 Job。\n下面看一个简单的示例：\n# 从 Docker Hub 获取版本为 18.04 的 ubuntu 镜像，然后启动容器，完成后在其中使用 printenv 命令。 pool: vmImage: \u0026#34;ubuntu-18.04\u0026#34; container: ubuntu:18.04 steps: - script: printenv   使用 Stage\n可以将流水线 Job 组织成 Stage。Stage 是流水线中的逻辑边界，如：构建、测试、部署到测试环境及部署到生产环境是现实运用中常见的 Stage。\n指定一个 Stage 的完整语法如下：\nstages: - stage: string  # Stage 名称，支持`[A-Za-z0-9_]` displayName: string  # UI 展示名称 dependsOn: string | [ string ] # 指定依赖的 Stage condition: string # 指定依赖的 Stage 的条件，如 failed() 等 pool: string | pool # 若在 Stage 上指定了 pool，除非在 Job 中覆盖该选项，否则所有该 Stage 下的 Job 都会使用所指定的 pool variables: { string: string } | [ variable | variableReference ] jobs: [ job | templateReference]   Deployment Job\n建议将部署类的步骤放在一个称为 Deployment 的特殊 Job 中。使用 Deployment 的益处有：可以获得流水线的部署历史以及特定的资源和部署状态，以便进行审核；可以自定义部署策略，如应用的升级方式（目前支持runOnce、rolling和canary三种升级方式）。\n此外，Deployment Job 还有些限制：不自动克隆代码仓库（若需检出代码，需要指定checkout: self，且 Deployment Job 仅支持一次检出）。\n定义一个 Deployment Job 的完整语法如下：\njobs: - deployment: string # 名称，支持`[A-Za-z0-9_]` displayName: string # UI 显示名称 pool: # 对虚拟机资源是不需要的 name: string # 池名称 demands: string | [ string ] workspace: clean: outputs | resources | all # Job 运行前需要清理什么 dependsOn: string condition: string continueOnError: boolean # 若为 true，表示即使该Job失败，也要运行其它的 Job；默认为 false container: containerReference # 运行该 Job 的容器 services: { string: string | container } # 作为服务容器运行的容器资源 timeoutInMinutes: nonEmptyString # 自动终止前的最长等待时间 cancelTimeoutInMinutes: nonEmptyString # 在终止任务前，给“即使任务被取消，也要运行”的 Job 多长时间 variables: # 变量 environment: string # 目标环境名及记录部署历史的资源名（可选）； 格式为：\u0026lt;environment-name\u0026gt;.\u0026lt;resource-name\u0026gt; strategy: runOnce: # 部署策略，除了 runOnce 还支持 rolling 和 canary deploy: steps: [ script | bash | pwsh | powershell | checkout | task | templateReference ] 如下是一个使用 Deployment Job 将应用部署到 Kubernetes 的示例：\njobs: - deployment: DeployWeb displayName: deploy Web App pool: vmImage: \u0026#34;ubuntu-latest\u0026#34; environment: \u0026#34;smarthotel-dev.bookings\u0026#34; strategy: runOnce: deploy: steps: # 无须显示传递连接信息 - task: KubernetesManifest@0 displayName: Deploy to Kubernetes cluster inputs: action: deploy namespace: $(k8sNamespace) manifests: | $(System.ArtifactsDirectory)/manifests/* imagePullSecrets: | $(imagePullSecret) containers: | $(containerRegistry)/$(imageRepository):$(tag)   装饰器\n使用装饰器可以为每个 Job 的开头和结尾自动注入额外的步骤。如：可以使用装饰器自动对整个团队流水线的构建输出作病毒扫描。\n关于如何开发、安装及使用装饰器，请参阅文档。\n  2.4 Library、变量与安全文件   Library\nLibrary 是 Azure DevOps 存放构建和发布资产的地方。可以使用安全模型配置哪些人可以创建或使用相应的资产。\n  变量\n变量有预定义变量（如系统变量）、环境变量和用户自定义变量三种。\n预定义变量也可用作环境变量使用，如预定义变量Build.ArtifactStagingDirectory的环境变量名为BUILD_ARTIFACTSTAGINGDIRECTORY。关于预定义变量有哪些，请参阅文档。\n看一个用户自定义变量的例子：\nvariables: VMS_USER: $(vmsUser) VMS_PASS: $(vmsAdminPass) pool: vmImage: \u0026#34;ubuntu-latest\u0026#34; steps: - task: AzureFileCopy@4 inputs: SourcePath: \u0026#34;my/path\u0026#34; azureSubscription: \u0026#34;my-subscription\u0026#34; Destination: \u0026#34;AzureVMs\u0026#34; storage: \u0026#34;my-storage\u0026#34; resourceGroup: \u0026#34;my-rg\u0026#34; vmsAdminUserName: $(VMS_USER) vmsAdminPassword: $(VMS_PASS) 其它复杂点的使用方式，请参阅文档。\n  使用变量组中的密钥\n可以在变量组中创建密钥类和非密钥类的变量，然后在流水线中使用。\n下面是一个使用参数和变量（包括自定义变量和系统变量）的示例：\nparameters: - name: image displayName: \u0026#34;Pool image\u0026#34; default: ubuntu-latest values: - windows-latest - ubuntu-latest - macOS-latest - name: test displayName: Run Tests? type: boolean default: false variables: - group: \u0026#34;Contoso Variable Group\u0026#34; - name: va value: $[variables.a] - name: vcontososecret value: $[variables.contososecret] trigger: - master pool: vmImage: ubuntu-latest steps: - script: |echo \u0026#34;Hello, world!\u0026#34; echo \u0026#34;Pool image: ${{ parameters.image }}\u0026#34; echo \u0026#34;Run tests? ${{ parameters.test }}\u0026#34; displayName: \u0026#34;Show runtime parameter values\u0026#34; - script: |echo \u0026#34;a=$(va)\u0026#34; echo \u0026#34;b=$(vb)\u0026#34; echo \u0026#34;contososecret=$(vcontososecret)\u0026#34; echo echo \u0026#34;Count up to the value of the variable group\u0026#39;s nonsecret variable *a*:\u0026#34; for number in {1..$(va)} do echo \u0026#34;$number\u0026#34; done echo \u0026#34;Count up to the value of the variable group\u0026#39;s secret variable *contososecret*:\u0026#34; for number in {1..$(vcontososecret)} do echo \u0026#34;$number\u0026#34; done displayName: \u0026#34;Test variable group variables (secret and nonsecret)\u0026#34; env: SYSTEM_ACCESSTOKEN: $(System.AccessToken)   设置变量\n通常会有在一个 Stage（或 Job 与 Task）设置一个变量值，然后在后续的 Stage（或 Job 与 Task） 使用的场景。变量设置可通过task.setvariable实现。\n下面看一下示例：\n# Job A 设置变量 `myOutputVar=this is from job A`，然后在 Job B 打印 jobs: - job: A steps: - bash: | echo \u0026#34;##vso[task.setvariable variable=myOutputVar;isoutput=true]this is from job A\u0026#34; name: passOutput - job: B dependsOn: A variables: myVarFromJobA: $[ dependencies.A.outputs[\u0026#39;passOutput.myOutputVar\u0026#39;] ] steps: - bash: | echo $(myVarFromJobA)   运行时参数\n使用运行时参数可以控制传入流水线的参数值。\n如下示例将 Trigger 设置为 none，只允许用户手动触发流水线，触发时需要选择具体的参数：\nparameters: - name: image displayName: Pool Image type: string default: ubuntu-latest # 若用户未选择具体的参数，会使用该默认值 values: - windows-latest - ubuntu-latest - macOS-latest trigger: none jobs: - job: build displayName: build pool: vmImage: ${{ parameters.image }} steps: - script: echo building $(Build.BuildNumber) with ${{ parameters.image }}   使用 Azure Key Vault 密钥\nAzure Key Vault 提供 API Key、密钥和证书等敏感信息管理。\n可以使用 Azure 命令（az keyvault）或直接在页面设置密钥信息。然后在流水线使用需要的密钥值。\n示例如下：\ntrigger: - main pool: vmImage: ubuntu-latest steps: - task: AzureKeyVault@2 inputs: azureSubscription: \u0026#34;Your-Azure-Subscription\u0026#34; KeyVaultName: \u0026#34;Your-Key-Vault-Name\u0026#34; SecretsFilter: \u0026#34;*\u0026#34; RunAsPreJob: false - task: CmdLine@2 inputs: script: \u0026#34;echo $(Your-Secret-Name) \u0026gt; secret.txt\u0026#34; - task: CopyFiles@2 inputs: Contents: secret.txt targetFolder: \u0026#34;$(Build.ArtifactStagingDirectory)\u0026#34; - task: PublishBuildArtifacts@1 inputs: PathtoPublish: \u0026#34;$(Build.ArtifactStagingDirectory)\u0026#34; ArtifactName: \u0026#34;drop\u0026#34; publishLocation: \u0026#34;Container\u0026#34; 关于 Azure Key Vault 的使用及策略设置，请查阅文档。\n  2.5 审批、检查与门禁 可以在一条流水线的任意 Stage 前后插入审批和门禁来控制其工作流。\n审批与门禁的设置需要在 UI 上进行，详情请参阅文档。\n如下是一个配置等待手动校验的示例：\n# 等待手动校验 pool: vmImage: ubuntu-latest jobs: - job: waitForValidation displayName: Wait for external validation pool: server timeoutInMinutes: 4320 # Job 最长3天超时 steps: - task: ManualValidation@0 timeoutInMinutes: 1440 # Task 最长1天超时 inputs: notifyUsers: | someone@example.com instructions: \u0026#34;Please validate the build configuration and resume\u0026#34; onTimeout: \u0026#34;resume\u0026#34; 综上，我们对 Azure 流水线的基础概念及 YAML 配置方式有了一个整体的了解。后面通过一些真实场景的项目实践，相信会对其有更好的掌握。\n 参考资料\n[1] Azure Pipelines documentation\n[2] Azure Pipelines\n ","permalink":"https://olzhy.github.io/posts/azure-pipelines.html","tags":["DevOps"],"title":"Azure 流水线使用详解"},{"categories":["计算机"],"contents":"PostgreSQL 的表空间允许在文件系统中定义数据库对象存储的位置。实际上就是为表、序列和索引等数据库对象的数据文件的存储指定了一个目录。\nPostgreSQL 使用操作系统的文件系统进行存储。这与 Oracle 有点不同，后者实现了自己的“文件系统”。\nPostgreSQL 中，一个表空间可供多个数据库使用；而一个数据库可以使用多个表空间，属“多对多”的关系。而在 Oracle 中，一个表空间只可供一个数据库使用；而一个数据库可以拥有多个表空间，属“一对多”的关系。\n1 何时使用表空间？   控制磁盘布局\n因数据的不断增长，原有文件系统快满了，又因某些原因无法得到扩展。这时，即可在挂载的其它文件系统上创建新的表空间，并将现有对象移动到新的表空间上。\n  优化性能\n表空间允许管理员根据数据库对象的使用模式来优化性能。如，可以使用表空间将使用频率高的索引或表的数据存储在一个 IOPS 更高的磁盘（如一种昂贵的固态设备）上；而将使用频率低或对性能要求不高的表的数据存储在价格低或慢一些的磁盘上。\n  一句话概括，即使用表空间可以合理利用磁盘的性能和空间，从而以最优的物理存储方式来管理数据库对象。\n2 默认表空间 从前文在 CentOS 上以源码安装 PostgreSQL知道，PostgreSQL 初始化时需要指定一个数据目录（$PGDATA），命令如下：\n$ initdb -D /usr/local/pgsql/data 初始化完成后，该目录下会包含 PostgreSQL 要启动时的所有东西（配置文件、数据文件和消息队列等）。\nPostgreSQL 启动后，所有数据库对象的数据文件都是在该文件夹下存储的。\n$ pg_ctl -D /usr/local/pgsql/data -l server.log start 该文件夹下的内容如下：\n$ ls -lht /usr/local/pgsql/data total 124K drwx------ 2 postgres postgres 4.0K Mar 28 09:00 pg_stat_tmp drwx------ 4 postgres postgres 4.0K Mar 8 17:52 pg_logical drwx------ 2 postgres postgres 4.0K Mar 8 17:48 global drwx------ 2 postgres postgres 4.0K Mar 8 17:47 pg_stat -rw------- 1 postgres postgres 87 Mar 8 17:47 postmaster.pid -rw------- 1 postgres postgres 59 Mar 8 17:47 postmaster.opts drwx------ 6 postgres postgres 4.0K May 13 2021 base drwx------ 2 postgres postgres 4.0K May 13 2021 pg_subtrans drwx------ 3 postgres postgres 4.0K May 13 2021 pg_wal drwx------ 2 postgres postgres 4.0K May 13 2021 pg_xact -rw------- 1 postgres postgres 1.6K May 13 2021 pg_ident.conf -rw------- 1 postgres postgres 4.7K May 13 2021 pg_hba.conf -rw------- 1 postgres postgres 88 May 13 2021 postgresql.auto.conf -rw------- 1 postgres postgres 28K May 13 2021 postgresql.conf drwx------ 2 postgres postgres 4.0K May 13 2021 pg_dynshmem drwx------ 4 postgres postgres 4.0K May 13 2021 pg_multixact drwx------ 2 postgres postgres 4.0K May 13 2021 pg_notify drwx------ 2 postgres postgres 4.0K May 13 2021 pg_replslot drwx------ 2 postgres postgres 4.0K May 13 2021 pg_serial drwx------ 2 postgres postgres 4.0K May 13 2021 pg_snapshots drwx------ 2 postgres postgres 4.0K May 13 2021 pg_tblspc drwx------ 2 postgres postgres 4.0K May 13 2021 pg_twophase -rw------- 1 postgres postgres 3 May 13 2021 PG_VERSION drwx------ 2 postgres postgres 4.0K May 13 2021 pg_commit_ts 而简短一点说，表空间即是告诉 PostgreSQL 服务器数据库对象物理文件存储位置的一种方式。\n在psql中使用\\db+命令即可列出表空间的详情：\npostgres=# \\db+ List of tablespaces Name | Owner | Location | Access privileges | Options | Size | Description ------------+----------+----------+-------------------+---------+--------+------------- pg_default | postgres | | | | 31 MB | pg_global | postgres | | | | 559 kB | (2 rows) 这两个表空间（pg_default与pg_global）是在 PostgreSQL 初始化后自动创建的。pg_default是template0与template1数据库的默认表空间（因此，也将是其它数据库的默认表空间）；pg_global是共享系统目录表（pg_database、pg_authid、pg_tablespace、pg_shdepend等）及其索引的表空间。\n我们注意到，上面的信息没有 Location。这是因为它们总是对应 PostgreSQL 数据目录（$PGDATA）下的两个子目录：pg_default使用base子目录，pg_global使用global子目录。\n3 使用表空间 3.1 创建表空间 要创建一个新的表空间，需要提前创建一个新的空文件夹（注意不要在 PostgreSQL 数据文件夹$PGDATA下创建），且该文件夹的所有者须是postgres系统用户。示例如下：\n$ mkdir -p /data/postgres/testspace $ chown -R postgres:postgres /data/postgres/testspace 超级用户（superuser）可使用CREATE TABLESPACE命令来创建一个表空间。示例如下：\n$ psql -U postgres postgres postgres=# CREATE TABLESPACE myspace LOCATION \u0026#39;/data/postgres/testspace\u0026#39;; 这时，查阅$PGDATA/pg_tblspc目录，即可看到一个符号链接指向了新建表空间对应文件夹的位置（数字24577是表空间的 OID）：\n$ ls -lht /usr/local/pgsql/data/pg_tblspc/ lrwxrwxrwx 1 postgres postgres 24 Mar 28 15:17 24577 -\u0026gt; /data/postgres/testspace 要想让普通用户使用新建的表空间，须为普通用户赋予该表空间的CREATE权限。下面示例演示为普通用户testuser赋权限：\npostgres=# GRANT CREATE ON TABLESPACE myspace TO testuser; 随后，使用表空间myspace的所有对象都会将数据存储在该文件夹（/data/postgres/testspace）下。\n下面示例演示使用普通用户testuser连接到数据库postgres，建表并为其指定表空间myspace：\n$ psql -U testuser postgres postgres=\u0026gt; CREATE TABLE foo(id int) TABLESPACE myspace; 除了为表指定表空间外，还可以为索引或数据库指定表空间。示例如下：\npostgres=\u0026gt; CREATE INDEX foo_idx ON foo(id) TABLESPACE myspace; postgres=# CREATE DATABASE testdb TABLESPACE myspace; 3.2 更改表空间 使用对应的ALTER语句，可将现有数据库对象由一个表空间移动到另一个表空间。\n下面示例演示使用ALTER TABLE和ALTER INDEX为表和索引指定新的表空间：\npostgres=\u0026gt; ALTER TABLE foo SET TABLESPACE pg_default; postgres=\u0026gt; ALTER INDEX foo_idx SET TABLESPACE pg_default; 也可以使用如下语句将一个表空间中的所有表或索引移至另一个表空间：\npostgres=\u0026gt; ALTER TABLE ALL IN TABLESPACE myspace SET TABLESPACE pg_default; postgres=\u0026gt; ALTER INDEX ALL IN TABLESPACE myspace SET TABLESPACE pg_default; 当重新指定表空间时，受影响的表或索引会被锁定，直至数据移动完成。\n3.3 更新表空间属性 根据上述表空间使用场景，表空间常见用途是将表或索引移动到更快（IOPS 更高）的文件系统上。这时，就需要告知 PostgreSQL 查询规划器新的表空间到底有多快，这样即可使其更好的评估查询性能。\n若您测评发现新的表空间的顺序访问及随机访问速度是之前的两倍，则可使用如下语句更新表空间属性：\nALTER TABLESPACE myspace SET (seq_page_cost=0.5, random_page_cost=0.5); 有关这两个参数的详情，请参阅文档seq_page_cost 及random_page_cost。\n3.4 临时表空间 PostgreSQL 允许使用temp_tablespaces参数来指定临时表空间（可使用逗号分隔，指定多个表空间）。临时表空间用于定义临时表、临时索引以及大型 SQL（大型数据集的排序或聚合等）可能产生的临时文件的存储位置。PostgreSQL 每次创建这些临时对象时，即会从指定的表空间列表随机获取并使用。临时表空间参数未指定时，会使用默认表空间pg_default。\n下面演示如何指定临时表空间。\n首先，创建两个空文件夹，并将所有者设定为postgres：\n$ mkdir /data/postgres/tempspace1 $ mkdir /data/postgres/tempspace2 $ chown -R postgres:postgres /data/postgres/tempspace1 $ chown -R postgres:postgres /data/postgres/tempspace2 使用superuser新建两个表空间，位置对应刚刚建好的两个文件夹；并为普通用户testuser赋这两个表空间的CREATE权限：\n$ psql -U postgres postgres postgres=# CREATE TABLESPACE tempspace1 LOCATION \u0026#39;/data/postgres/tempspace1\u0026#39;; postgres=# CREATE TABLESPACE tempspace2 LOCATION \u0026#39;/data/postgres/tempspace2\u0026#39;; postgres=# GRANT CREATE ON TABLESPACE tempspace1 TO testuser; postgres=# GRANT CREATE ON TABLESPACE tempspace2 TO testuser; 使用普通用户登录，将temp_tablespaces变量设置为tempspace1, tempspace2：\n$ psql -U testuser postgres postgres=\u0026gt; SET temp_tablespaces = tempspace1, tempspace2; postgres=\u0026gt; SHOW temp_tablespaces; temp_tablespaces ------------------------ tempspace1, tempspace2 (1 row) 这样，即可使用了。需要注意的是，该种设置方式只在当前会话生效。若要永久生效，需要更改系统配置（postgresql.conf）。\n3.5 删除表空间 表空间一旦创建，只要用户有权限，即可供任意数据库使用。要想删除表空间，须先将使用该表空间的数据库对象全部移除。\n这样，即可使用DROP TABLESPACE命令来删除一个空表空间了：\n$ psql -U postgres postgres postgres=# DROP TABLESPACE myspace; 4 表空间相关的系统表 除了上面使用过的在 psql 使用\\db+命令外，PostgreSQL 还有一些与表空间相关的系统目录或系统表。\n查看已创建的表空间：\nSELECT * FROM pg_tablespace; -- 表空间名及目录位置 SELECT spcname, pg_tablespace_location(oid) FROM pg_tablespace; 查看某个表空间被哪些表或索引使用：\nSELECT c.relname FROM pg_class c, pg_tablespace t WHERE c.reltablespace = t.oid AND t.spcname=\u0026#39;myspace\u0026#39;; SELECT * FROM pg_tables WHERE tablespace=\u0026#39;myspace\u0026#39;; SELECT * FROM pg_indexes WHERE tablespace=\u0026#39;myspace\u0026#39;; 综上，完成了对 PostgreSQL 表空间使用场景及使用方式的总结。\n 参考资料\n[1] PostgreSQL: Documentation: 14: 23.6. Tablespaces\n[2] All About Tablespaces in PostgreSQL\n[3] When to use tablespaces in PostgreSQL\n[4] How can I tell what is in a Postgresql tablespace?\n[5] PostgreSQL 的表空间\n ","permalink":"https://olzhy.github.io/posts/postgres-tablespaces.html","tags":["PostgreSQL"],"title":"PostgreSQL 表空间使用详解"},{"categories":["计算机"],"contents":"PostgreSQL 外部数据包装器，即 PostgreSQL Foreign Data Wrappers（下面简称为 FDW），是现实数据库使用场景中一个非常实用的功能。PostgreSQL 的 FDW 类似于 Oracle 的 dblink，DB2 的 Federation，使用其可以将本地数据库与外部数据库建立连接，从而可以像操作本地数据一样来操作外部数据。\nFDW 有何用？\n  数据分片\n使用 FDW 将数据分布式存储在多个数据库上从而实现数据分片（如 pg_shardman 插件，即是使用 postgres_fdw 和 pg_pathman 插件来实现数据分片的）。\n  数据同步\n使用 FDW 建立本地数据库与外部数据库的连接，即可定时同步外部数据至本地。\n  数据迁移\n使用 FDW 建立本地数据库与外部数据库的连接，即可进行数据迁移。\n  ETL（Extract-Transform-Load，抽取转换加载）\n使用 FDW 将来自不同类型数据库的数据抽取到一个数据仓库中，便于统一化访问。\n  PostgreSQL FDW 发展概况\n2003 年，SQL/MED（SQL Management of External Data）被加入 SQL 标准，其为外部数据管理提供了规范。在 2011 年发行的 PostgreSQL 9.1 开始支持外部数据读，2013 发行的 PostgreSQL 9.3 开始支持外部数据写。\n目前，PostgreSQL （本文写作时，使用的版本为 PostgreSQL 14）已提供多种扩展来支持对各种类型外部数据库或文件的操作（如 postgres_fdw 支持连接外部 PostgreSQL 数据库，oracle_fdw 支持连接外部 Oracle 数据库，mysql_fdw 支持连接外部 MySQL 数据库，jdbc_fdw 支持以 JDBC 协议连接外部常用关系型数据库，file_fdw 支持连接外部特定格式的文件等）。\n（图片来自CART\u0026rsquo;s Blog）\n本文仅关注 postgres_fdw，即 PostgreSQL 数据库如何与外部 PostgreSQL 数据库进行连接以及其如何对外部数据进行管理。\n1 使用 postgres_fdw 要想使用 postgres_fdw 对远程数据库进行访问，主要有如下几个步骤：\n 安装 postgres_fdw 扩展 创建外部服务器 创建用户映射 创建外部表或导入外部模式  本文使用本地 PostgreSQL 数据库模拟远程数据库和本地数据库，开始正式的步骤前，需要提前做一点准备工作。\n  检查 PostgreSQL 版本\n$ psql --version psql (PostgreSQL) 14.2   在远程 PostgreSQL 数据库创建用户\n使用 superuser 在远程 PostgreSQL 数据库执行如下语句创建普通用户fdw_user，供后面本地数据库建立 FDW 连接时使用。\nCREATE USER fdw_user WITH ENCRYPTED PASSWORD \u0026#39;secret\u0026#39;;   在远程 PostgreSQL 数据库创建表\n在远程数据库创建用于测试的天气表weather，插入测试数据，并为用户fdw_user授权针对该表的增删改查权限。\nCREATE TABLE weather ( city varchar(80), -- city name (城市名)  temp_low int, -- low temperature (最低温度)  temp_high int, -- high temperature (最高温度)  prcp real, -- precipitation (降水量)  date date -- date (日期) ); INSERT INTO weather (city, temp_low, temp_high, prcp, date) VALUES (\u0026#39;Beijing\u0026#39;, 18, 32, 0.25, \u0026#39;2021-05-19\u0026#39;), (\u0026#39;Beijing\u0026#39;, 20, 30, 0.0, \u0026#39;2021-05-20\u0026#39;), (\u0026#39;Dalian\u0026#39;, 16, 24, 0.0, \u0026#39;2021-05-21\u0026#39;); GRANT SELECT,INSERT,UPDATE,DELETE ON TABLE weather TO fdw_user; 在本地使用用户fdw_user对远程数据库（本文特殊，使用本机数据库同时模拟本地与远程，所以远程 host 也是 localhost）进行连接，并校验所授权的权限。\n$ psql -h localhost -U fdw_user postgres postgres=\u0026gt; SELECT * FROM weather; city | temp_low | temp_high | prcp | date ---------+----------+-----------+------+------------ Beijing | 18 | 32 | 0.25 | 2021-05-19 Beijing | 20 | 30 | 0 | 2021-05-20 Dalian | 16 | 24 | 0 | 2021-05-21 (3 rows) 注意：若是真实的远程数据库，要想从本地建立连接，需要在远程数据库的 pg_hba.conf 配置文件增加记录以对访问 IP 开通防火墙。\n  在本地 PostgreSQL 数据库创建用户\n使用 superuser 在本地 PostgreSQL 数据库执行如下语句创建普通用户local_user。\nCREATE USER local_user WITH ENCRYPTED PASSWORD \u0026#39;secret\u0026#39;;   所有准备工作都做好了，现在可以使用 superuser 在本地数据库开始正式的步骤了。\n安装 postgres_fdw 扩展 使用CREATE EXTENSION语句安装postgres_fdw扩展。\nCREATE EXTENSION postgres_fdw; 为用户local_user授权postgres_fdw的使用权限。\nGRANT USAGE ON FOREIGN DATA WRAPPER postgres_fdw TO local_user; 创建外部服务器 使用CREATE SERVER语句创建外部服务器，需要指定远程数据库的主机、端口及数据库名。\nCREATE SERVER foreign_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host \u0026#39;localhost\u0026#39;, port \u0026#39;5432\u0026#39;, dbname \u0026#39;postgres\u0026#39;); 为用户local_user授权外部服务器foreign_server的使用权限。\nGRANT USAGE ON FOREIGN SERVER foreign_server TO local_user; 创建用户映射 使用CREATE USER MAPPING语句创建远程用户与本地用户的映射，需要提供远程用户的用户名及密码。\nCREATE USER MAPPING FOR local_user SERVER foreign_server OPTIONS (user \u0026#39;fdw_user\u0026#39;, password \u0026#39;secret\u0026#39;); 创建外部表或导入外部模式 使用CREATE FOREIGN TABLE语句创建远程表。需要注意各列的类型需与实际的远程表相匹配，列名也最好保持一致，否则您需要使用column_name参数为每一列单独指定远程表中的列名。\nCREATE FOREIGN TABLE foreign_weather ( city varchar(80), temp_low int, temp_high int, prcp real, date date ) SERVER foreign_server OPTIONS (schema_name \u0026#39;public\u0026#39;, table_name \u0026#39;weather\u0026#39;); 外部表多的话，这样一个一个新建会比较痛苦，多数情形下，您只要使用IMPORT FOREIGN SCHEMA语句直接将外部模式下的所有表导入本地指定的模式即可。\n注意：因未给 super_user 指定用户映射，如下语句需要使用用户local_user执行，否则会报ERROR: user mapping not found for \u0026quot;super_user\u0026quot;错误。\n-- 导入外部模式下的所有表 IMPORT FOREIGN SCHEMA public FROM SERVER foreign_server INTO public; -- 导入外部模式下的指定表 IMPORT FOREIGN SCHEMA public LIMIT TO (weather) FROM SERVER foreign_server INTO public; 为local_user授权 public 模式下所有表（包括外部表）的增删改查权限。\nGRANT SELECT,INSERT,UPDATE,DELETE ON ALL TABLES IN SCHEMA public TO local_user; 这样，使用用户local_user连接到本地数据库，即可以对外部表进行操作了。\n$ psql -U local_user postgres postgres=\u0026gt; SELECT * FROM foreign_weather; city | temp_low | temp_high | prcp | date ---------+----------+-----------+------+------------ Beijing | 18 | 32 | 0.25 | 2021-05-19 Beijing | 20 | 30 | 0 | 2021-05-20 Dalian | 16 | 24 | 0 | 2021-05-21 (3 rows) postgres=\u0026gt; UPDATE foreign_weather SET prcp=0 WHERE city=\u0026#39;Beijing\u0026#39; AND date=\u0026#39;2021-05-19\u0026#39;; UPDATE 1 至此，我们已基本掌握了postgres_fdw的使用方式。本文接下来会看一下跟 FDW 相关的系统表及函数，最后看一下 FDW 的事务管理及性能优化，以便对 FDW 有一个更深入的了解。\n2 建立 postgres_fdw 时的几个重要参数   updatable\n该选项用于设置外部表是否可被更新，即 postgres_fdw 是否允许使用INSERT、UPDATE和DELETE命令修改外部表，默认是true。其可以指定在外部表上，也可以指定在外部服务器上，指定在表上的会覆盖指定在服务器上的。\n设置或更新该参数的具体语句如下：\n-- 创建外部服务器时指定 CREATE SERVER foreign_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (..., updatable \u0026#39;false\u0026#39;, ...); -- 创建外部表时指定 CREATE FOREIGN TABLE foreign_weather ( ... ) SERVER foreign_server OPTIONS (schema_name ..., table_name ..., updatable \u0026#39;false\u0026#39;, ...); -- 更新外部服务器参数选项 ALTER SERVER foreign_server OPTIONS (updatable \u0026#39;false\u0026#39;); -- 更新外部表参数选项 ALTER FOREIGN TABLE foreign_weather OPTIONS(updatable \u0026#39;false\u0026#39;); 当然，如果远程表实际上不可更新，那么无论如何都会发生错误。使用此选项主要是允许在本地抛出错误，而无需查询远程服务器。\n  truncatable\n该选项用于设置外部表是否可被截断，即 postgres_fdw 是否允许使用TRUNCATE命令截断外部表，默认是true。该参数同样可以指定在外部表上，也可以指定在外部服务器上，指定在表上的会覆盖指定在服务器上的。\n设置或更新该参数的具体语句同上述updatable完全一样。\n当然，如果远程表实际上不可被截断，那么无论如何都会发生错误。使用此选项同样主要是可以允许在本地抛出错误，而无需查询远程服务器。\n  keep_connections\n该选项用于设置 postgres_fdw 是否将与远程服务器的连接保留在本地会话（local session），以方便重用，默认是on（若设置为off，则在每个事务结束时将放弃与外部服务器的所有连接）。其只可以指定在外部服务器上。设置或更新该参数的具体语句如下：\n-- 创建外部服务器时指定 CREATE SERVER foreign_server FOREIGN DATA WRAPPER postgres_fdw OPTIONS (..., keep_connections \u0026#39;off\u0026#39;, ...); -- 更新外部服务器参数选项 ALTER SERVER foreign_server OPTIONS (keep_connections \u0026#39;off\u0026#39;);   3 FDW 相关的系统表及函数 系统表\n跟 FDW 相关的系统表如下（对于_pg_*表，super_user 才有权限访问）：\ninformation_schema._pg_foreign_data_wrappers information_schema._pg_foreign_servers information_schema._pg_foreign_tables information_schema._pg_foreign_table_columns information_schema._pg_user_mappings information_schema.foreign_data_wrappers information_schema.foreign_data_wrapper_options information_schema.foreign_server_options information_schema.foreign_servers information_schema.foreign_tables information_schema.foreign_table_options 函数\n  postgres_fdw_get_connections()\n调用该函数会返回 postgres_fdw 从本地会话（local session）到外部服务器所建立的所有开放连接的外部服务器名及连接是否有效。\n注意：该函数获取的是当前本地会话与外部服务器的连接状态，非本地数据库与外部服务器的连接状态。所以，另开一个 Shell Tab 进行的远程表查询不会被当前本地会话记录。\n本文创建外部服务器时对keep_connections参数采用的是默认选项（on），所以会保留连接。\n可以看到如下psql连接到本地数据库，进行外部表查询后，查询postgres_fdw_get_connections()函数会返回一行记录。\n$ psql -U local_user postgres postgres=\u0026gt; SELECT * FROM foreign_weather; ... postgres=\u0026gt; SELECT * FROM postgres_fdw_get_connections(); server_name | valid ----------------+------- foreign_server | t (1 row)   postgres_fdw_disconnect(server_name text)\n根据传入的名称，断开 postgres_fdw 从本地会话（local session）到指定外部服务器的所有连接。\n使用不同的用户映射可以有多个到给定服务器的连接（使用多个用户访问外部服务器时，配置了多个用户映射，postgres_fdw 会为每个用户映射建立一个连接）。若连接正在当前本地事务中使用，则不会断开，会输出警告消息。若至少断开一个连接，则返回 true，否则返回 false。若未找到具有给定名称的外部服务器，则会报错（ERROR: server \u0026quot;...\u0026quot; does not exist）。\n接着刚刚的会话，执行SELECT postgres_fdw_disconnect('foreign_server')，返回true；再次查询postgres_fdw_get_connections()函数发现已没有连接。\npostgres=\u0026gt; SELECT postgres_fdw_disconnect(\u0026#39;foreign_server\u0026#39;); postgres_fdw_disconnect ------------------------- t (1 row) postgres=\u0026gt; SELECT * FROM postgres_fdw_get_connections(); server_name | valid -------------+------- (0 rows)   postgres_fdw_disconnect_all()\n断开 postgres_fdw 从本地会话（local session）到外部服务器的所有连接。使用方式与postgres_fdw_disconnect(server_name text)类似，这里不再赘述。\n  4 FDW 事务管理及性能优化 事务管理\n当查询远程表时，若尚未打开与当前本地事务对应的事务，postgres_fdw 会在远程服务器上新开一个事务。当本地事务提交或中止时，远程事务也被提交或中止。保存点（Savepoints）同样通过创建相应的远程保存点来管理。\n当本地事务具有可序列化（SERIALIZABLE）隔离级别时，远程事务也使用该隔离级别；否则，使用可重复读（REPEATABLE READ）隔离级别。\n若一个查询在远程服务器上执行多个表扫描，此选项可确保其对所有扫描将得到快照一致性（snapshot-consistent）结果。结果是，即使其它活动在远程服务器上进行了并发更新，单个事务中的连续查询将看到来自远程服务器的相同数据。若本地事务使用可序列化（SERIALIZABLE）或可重复读（REPEATABLE READ）隔离级别，那么这种行为是可预期的，但对于读已提交（READ COMMITTED）隔离级别的本地事务来说，这可能会令人惊讶。未来的 PostgreSQL 版本可能会修改这些规则。\n性能优化\npostgres_fdw 会比较智能的判断一个查询语句（待检测的查询语句包括SELECT、UPDATE、DELETE语句，语句中涉及运算符、函数、连接、过滤条件及聚集函数等）是否应该下移到远程服务器执行。\n最理想的情况是，所涉及的表都在远程服务器上，运算符、函数等都为内置类型，这样 postgres_fdw 将整个查询发送给远程服务器进行计算，然后取结果就好了。而多数情况是 postgres_fdw 需要将必要的数据取到本地来进行连接、过滤及聚集函数处理等操作。即 postgres_fdw 会优化发送到远程服务器的查询（优化 WHERE 子句，及不获取不需要的列）以减少来自远程服务器的数据传输。\n下面我们看两个例子：\n  纯远程表查询\n原始查询语句：\nSELECT * FROM foreign_weather; 使用EXPLAIN VERBOSE查看实际发送到远程服务器的查询（Remote SQL）为：\nSELECT city, temp_low, temp_high, prcp, date FROM public.weather $ psql -U local_user postgres postgres=\u0026gt; EXPLAIN VERBOSE SELECT * FROM foreign_weather; QUERY PLAN ---------------------------------------------------------------------------------- Foreign Scan on public.foreign_weather (cost=100.00..121.25 rows=375 width=194) Output: city, temp_low, temp_high, prcp, date Remote SQL: SELECT city, temp_low, temp_high, prcp, date FROM public.weather (3 rows)   远程表与本地表连接查询\n新建本地表cities，并插入测试数据：\nCREATE TABLE cities ( name varchar(80), -- city name (城市名)  location point -- point为PostgreSQL特有类型，该字段表示地理坐标(经度, 纬度) ); INSERT INTO cities (name, location) VALUES (\u0026#39;Beijing\u0026#39;, \u0026#39;(116.3, 39.9)\u0026#39;), (\u0026#39;Shanghai\u0026#39;, \u0026#39;(121.3, 31.1)\u0026#39;); 对于查询：\nSELECT * FROM cities c, foreign_weather w WHERE c.name = w.city; postgres_fdw 发送给远程服务器的 SQL 为：\nSELECT city, temp_low, temp_high, prcp, date FROM public.weather; $ psql -U local_user postgres postgres=\u0026gt; EXPLAIN VERBOSE SELECT * FROM cities c, foreign_weather w WHERE c.name = w.city; QUERY PLAN ------------------------------------------------------------------------------------------ Hash Join (cost=118.10..163.91 rows=675 width=388) Output: c.name, c.location, w.city, w.temp_low, w.temp_high, w.prcp, w.date Hash Cond: ((w.city)::text = (c.name)::text) -\u0026gt; Foreign Scan on public.foreign_weather w (cost=100.00..121.25 rows=375 width=194) Output: w.city, w.temp_low, w.temp_high, w.prcp, w.date Remote SQL: SELECT city, temp_low, temp_high, prcp, date FROM public.weather -\u0026gt; Hash (cost=13.60..13.60 rows=360 width=194) Output: c.name, c.location -\u0026gt; Seq Scan on public.cities c (cost=0.00..13.60 rows=360 width=194) Output: c.name, c.location (10 rows) 即 postgres_fdw 会将foreign_weather的全部数据获取到本地后与表cities进行连接计算。\n而对于查询：\nSELECT c.name, max(w.temp_high) FROM cities c, foreign_weather w WHERE c.name = w.city AND w.temp_high \u0026lt;= 30 GROUP BY c.name; postgres_fdw 发送给远程服务器的 SQL 为：\nSELECT city, temp_high FROM public.weather WHERE (temp_high \u0026lt;= 30) $ psql -U local_user postgres postgres=\u0026gt; EXPLAIN VERBOSE SELECT c.name, max(w.temp_high) FROM cities c, foreign_weather w WHERE c.name = w.city AND w.temp_high \u0026lt;= 30 group by c.name; QUERY PLAN ------------------------------------------------------------------------------------------------------ HashAggregate (cost=143.17..145.17 rows=200 width=182) Output: c.name, max(w.temp_high) Group Key: c.name -\u0026gt; Hash Join (cost=119.25..141.98 rows=238 width=182) Output: c.name, w.temp_high Hash Cond: ((c.name)::text = (w.city)::text) -\u0026gt; Seq Scan on public.cities c (cost=0.00..13.60 rows=360 width=178) Output: c.name, c.location -\u0026gt; Hash (cost=117.60..117.60 rows=132 width=182) Output: w.temp_high, w.city -\u0026gt; Foreign Scan on public.foreign_weather w (cost=100.00..117.60 rows=132 width=182) Output: w.temp_high, w.city Remote SQL: SELECT city, temp_high FROM public.weather WHERE ((temp_high \u0026lt;= 30)) (13 rows) 即 postgres_fdw 会优化发给远程服务器的WHERE条件，仅从远程表foreign_weather获取所需要的数据，然后在本地与表cities进行连接、过滤及聚集函数处理等计算。\n  综上，我们对 PostgreSQL 外部数据包装器的基础概念及 postgres_fdw 的使用方式有了一个比较详细的了解。\n 参考资料\n[1] PostgreSQL: Documentation: 14: F.35. postgres_fdw\n[2] Foreign data wrappers - PostgreSQL Wiki\n[3] CARTO\u0026rsquo;s Use of Foreign Data Wrappers\n[4] PostgreSQL fdw 详解\n[5] Postgresql fdw 原理及 postgres_fdw 使用\n[6] PostgreSQL 中的 postgres_fdw 扩展\n[7] PostgreSQL 14 中的 postgres_fdw 增强功能\n ","permalink":"https://olzhy.github.io/posts/postgres-foreign-data-wrappers.html","tags":["PostgreSQL"],"title":"PostgreSQL 外部数据包装器 postgres_fdw 使用详解"},{"categories":["计算机"],"contents":"Azure Database for PostgreSQL 是 Azure 提供的基于开源 PostgreSQL 数据库的云上数据库服务。它是一种完全托管的数据库服务，具有性能可预测、安全、高可用和动态扩展能力，可处理任务关键型工作负载。\nAzure Database for PostgreSQL 提供：\n 内置高可用性； 自动备份和指定时间点恢复（备份数据最长保留 35 天）； 对底层硬件、操作系统和数据库引擎进行自动化维护，以保持服务安全和版本最新； 可预测的性能，使用即用即付定价模型； 秒级弹性扩展； 企业级安全性和行业领先的合规性，可保护静态和动态敏感数据； 丰富的监控和自动化特质； 行业领先的支持体验。  这些功能几乎不需要管理，所有功能无需额外费用即可提供。使您能够专注于应用程序开发并加快面市时间，而不是将宝贵的时间和资源花在管理虚拟机和基础架构上。\nAzure Database for PostgreSQL 提供三种部署模式：单服务器、灵活服务器和大规模 (Citus) 集群。下面会一一介绍。\n1 单服务器 Azure Database for PostgreSQL 单服务器是多个数据库的中央管理点。它与您在 On-Premise 环境中搭建一个 PostgreSQL 服务器的构造相同。\nAzure Database for PostgreSQL 的单服务器部署：\n 在 Azure 订阅中创建； 是数据库的父资源； 为数据库提供命名空间； 是一个具有强生命周期语义的容器（删除服务器将删除包含的数据库）； 在一个区域中配置资源； 为服务器和数据库访问提供连接点； 提供适用于其数据库的管理策略范围：登录、防火墙、用户、角色、配置等； 支持多个 PostgreSQL 版本（有哪些？）； 支持 PostgreSQL 扩展（支持哪些扩展？）。  您可以在单服务器中创建一个或多个数据库以供一个或多个应用独占或共享资源。收费价格会根据定价层、vCore 和存储 (GB) 的配置计算。\n如何连接\n  鉴权\nAzure Database for PostgreSQL 单服务器支持原生 PostgreSQL 身份验证。您可使用管理员账号进行连接。\n  协议\n该服务支持 PostgreSQL 使用的基于消息的协议。\n  TCP/IP\n如上协议被 TCP/IP 和 Unix 域套接字支持。\n  防火墙\n为了保护数据，Azure 默认关闭所有访问，您在 Server 端设置防火墙规则 （IP 白名单）后才可以对指定 IP 开放访问。\n防火墙根据每个请求的原始 IP 地址来判断其是否有访问权限。您需要通过 Azure 门户或 Azure CLI 在 Server 端设置防火墙规则（允许的 IP 地址范围），同一逻辑服务器下的所有数据库都遵循这些规则。请使用订阅所有者或订阅贡献者创建防火墙规则。\n  您可以选择强制开启 TLS 来加强安全性。\n这样即可下载证书然后使用 psql 进行连接了：\n$ wget --no-check-certificate https://dl.cacerts.digicert.com/DigiCertGlobalRootCA.crt.pem $ psql --host=mydemoserver-pg.postgres.database.azure.com --port=5432 --username=myadmin --dbname=postgres --set=sslmode=require --set=sslrootcert=DigiCertGlobalRootCA.crt.pem # psql \u0026#34;host=mydemoserver-pg.postgres.database.azure.com port=5432 user=myadmin dbname=postgres sslmode=verify-full sslrootcert=DigiCertGlobalRootCA.crt.pem\u0026#34; 服务器管理\n您可以使用 Azure 门户或 Azure CLI 管理 Azure Database for PostgreSQL 单服务器。\n创建服务器时，您为管理员用户设置密码。管理员用户是您在服务器上拥有的最高权限用户。它属于 azure_pg_admin 角色。此角色没有完全的超级用户权限。\nPostgreSQL 超级用户权限分配给了 azure_superuser，属托管服务持有。您无权访问此角色。\n该服务器下有如下几个默认数据库：\n postgres - 创建服务器后即可连接的默认数据库； azure_maintenance - 此数据库用于将提供托管服务的进程与用户操作分开，您无权访问此数据库； azure_sys - 用于 Query Store 的数据库，此数据库在 Query Store 关闭时不累积数据，默认是关闭的。  服务器参数\n服务器参数决定服务器的配置。在 Azure Database for PostgreSQL 中，可以使用 Azure 门户或 Azure CLI 查看和编辑参数列表。\n作为一个托管服务，Azure Database for PostgreSQL 的可配置参数是自建 Postgres 实例可配置参数的子集（有关 Postgres 参数的详细信息，请参阅 PostgreSQL 运行时配置）。\n您的 Azure Database for PostgreSQL 服务器在创建时使用的是每个参数的默认值。用户无法配置某些需要重新启动或超级用户才有权限更改的参数。\n2 灵活服务器 Azure Database for PostgreSQL 灵活服务器是一种完全托管的数据库服务，旨在为数据库管理功能和配置设置提供更精细的控制和灵活性。该服务提供了更多的灵活性和基于用户需求的服务器配置定制。灵活服务器架构允许用户将数据库引擎与客户端服务置于同一位置以降低延迟，在单个可用区及跨多个可用区选择高可用性。灵活服务器还提供更好的成本优化控制，能够启停您的服务器和可突发计算层，非常适合不需要持续完整计算容量的工作负载。该服务目前支持 PostgreSQL 11、12 和 13 社区版本。该服务目前在绝大多数 Azure 地域均可用。\n灵活服务器非常适合如下几种情况：\n 需要更好的控制和定制的应用程序开发； 区域冗余高可用性； 托管维护窗口。  高可用性\n灵活服务器部署模型设计用于支持单个可用区及跨多个可用区的高可用性。该架构将计算和存储分开。数据库引擎在 Linux 虚拟机内的容器上运行，而数据文件存储在 Azure 存储中。存储维护数据库文件的三个本地冗余同步副本，以确保数据的持久性。\n在计划内或计划外故障转移事件期间，如果服务器出现故障，该服务将使用以下自动化程序保持服务器的高可用性：\n 预配一个新的计算 Linux VM； 带有数据文件的存储映射到新的虚拟机； PostgreSQL 数据库引擎在新的虚拟机上上线。  下图显示了 VM 和存储故障的过渡：\n如果配置了区域冗余高可用性，该服务将在同一 Azure 地域内的可用区域中预配和维护一个热备用服务器。源服务器上的数据变化同步复制到备服务器，保证数据零丢失。借助区域冗余高可用性，一旦触发了计划内或计划外的故障转移事件，备用服务器将立即上线处理请求事务。这允许服务在支持同一个 Region 内的多个可用性区域的故障中恢复，如下图所示。\n使用托管维护窗口进行自动修补\n该服务可进行底层硬件、操作系统和数据库引擎的自动修补。包括安全补丁和软件更新。对于 PostgreSQL 引擎，小版本升级也包含在计划的维护版本中。用户可以将修补计划配置为系统托管或自定义维护时间。在维护期间，将应用修补程序，并且可能需要重新启动服务器以完成更新。通过自定义维护时间，用户可以使他们的补丁周期可预测，并选择对业务影响最小的维护窗口。通常，作为持续集成和发布的一部分，该服务每月发布一次。\n自动备份\n灵活服务器会自动备份数据并将它们存储在同一 Region 内的区域冗余存储 (ZRS) 上。备份可用于将您的服务器恢复到备份保留期内的任何时间点。默认备份保留期为 7 天，最长可配置为 35 天。所有备份均使用 AES 256 加密算法进行加密。\n在几秒内扩容\n灵活服务器有三种计算层可选择：突发、通用和内存优化。 突发型最适合不需要持续完整计算容量的低成本开发和低并发工作负载。通用型和内存优化型更适合需要高并发、大规模和可预测性能的生产工作负载。您可以每月花费几美元在小型数据库上构建您的第一个应用程序，然后无缝调整规模以满足您的解决方案需求。\n启停服务器以降低 TCO\n灵活服务器允许您按需停止和启动服务器以降低您的 TCO。当服务器停止时，计算层计费将立即停止。这可以让您在开发、测试和有时限的可预测生产工作负载期间显著节省成本。除非提前重新启动，否则服务器将保持停止状态 7 天。\n企业级安全\n灵活服务器使用经过 FIPS 140-2 验证的加密模块对静态数据进行存储加密。运行查询时创建的数据（包括备份和临时文件）均已加密。该服务使用 Azure 存储加密中包含的 AES 256 密钥，并且密钥可以由系统管理（默认）。该服务使用默认加强的传输层安全性 (SSL/TLS) 对动态数据进行加密。该服务仅加强和支持 TLS 版本 1.2。\n允许使用 Azure 虚拟网络（VNet 集成）对服务器进行完全私有访问。 Azure 虚拟网络中的服务器只能通过专用 IP 地址访问和连接。通过 VNet 集成，公共访问被拒绝，并且无法使用公共端点访问服务器。\n监控及告警\n灵活服务器配备了内置的性能监控和告警功能。所有 Azure 指标采用一分钟采集一次的频率，每个指标提供 30 天的历史记录。您可以在指标上配置警报。主机服务器指标开放访问以用于监控资源利用率并允许配置慢查询日志。使用这些工具，您可以快速优化您的工作负载，并可优化您的配置以获得最佳性能。\n内置 PgBouncer\n灵活服务器带有一个内置的 PgBouncer（一个连接池）。您可以选择启用它并通过 PgBouncer 使用相同的主机名和端口 6432 将您的应用程序连接到您的数据库服务器。\n数据迁移\n该服务运行 PostgreSQL 社区版本。这允许完全的应用程序兼容性，并且只需最小的重构成本即可将在 PostgreSQL 引擎上开发的现有应用程序迁移到 Azure 灵活服务器。\n  转储和恢复\n对于离线迁移，用户可以承受一些停机时间，使用 pg_dump 和 pg_restore 等社区工具进行转储和恢复。\n  Azure 数据库迁移服务\n为了以最短的停机时间无缝迁移到灵活服务器，可以使用 Azure 数据库迁移服务。\n  3 大规模（Citus）集群 大规模（Citus）集群是一种使用分片在多台机器上水平扩展查询的部署选项。其查询引擎对传入的 SQL 在这些服务器上进行并行查询，以便对大型数据集做出更快的响应。其比上面两个部署选项规模更大性能更好。其工作负载通常接近或超过 100 GB。\n大规模集群提供如下能力：\n 使用分片在多台机器上进行水平扩展； 在多台机器上进行并行查询，以更快地响应大型数据集； 对多租户应用程序、实时操作分析和高吞吐量事务工作负载的出色支持。  以 PostgreSQL 构建的应用程序可以使用标准连接库和最少的更改即可在大规模（Citus）集群上运行分布式查询。\n节点分工\n大规模（Citus）集群托管类型允许 Azure Database for PostgreSQL 服务器（称为节点）在“无共享”体系结构中相互协调。与单个服务器相比，服务器组中的节点共同拥有更多的数据并使用更多的 CPU 内核。该架构还允许通过向服务器组添加更多节点来扩展数据库。\n每个服务器组都有一个协调节点和多个工作节点。应用程序将它们的查询发送到协调节点（应用程序无法直接连接到工作节点），协调节点将其转发给相关的工作节点并整合它们的结果。\n大规模（Citus）集群允许 DBA 定义表分片规则以在不同的工作节点上存储不同的行。分布式表是大规模（Citus）集群性能强劲的关键。未分片的表会将数据完全存在协调节点上，这样即无法利用多机器的并行优势。\n对于分布式表上的每个查询，协调器根据所需数据的分布，决定将其路由到单个工作节点上或将其并行化路由到多个节点上。协调器通过查询元数据表来决定如何做，这些表会跟踪工作节点的 DNS 名称和运行状态，以及跨节点的数据分布。\n表类型\n大规模（Citus）集群服务器组中有三种类型的表，每种表在节点上的存储方式不同，用于不同的目的。\n  分布式表\n对 SQL 语句来说分布式表就像普通表一样，但它们在工作节点之间水平分区。这意味着表的行存储在不同的节点上（在称为分片的分段表中）。大规模（Citus）集群不仅在整个集群中运行 SQL，还运行 DDL 语句。更改分布式表的模式会级联更新所有工作节点上表的分片。\n大规模（Citus）集群使用分片算法将行存储在不同的节点上，这是由分布列来决定的。DBA 或集群管理员在分发表时必须指定该列。该列的选择对于性能和功能很重要。\n  引用表\n引用表是一种特殊的分布式表，其全部内容都集中在一个分片中。分片被复制到每个工作节点上。任何工作节点的查询都可以在本地进行，无需请求别的节点的行，节省了网络开销。引用表没有分布列，因为不需要对行区分单独的分片。\n引用表通常很小，用于存储与在任何工作节点上运行的查询相关的数据。枚举值是一个示例，例如订单状态或产品类别。\n  本地表\n当您使用大规模（Citus）集群时，您连接到的协调器节点是常规 PostgreSQL 数据库。您可以在协调器上创建普通表并选择不对其进行分片。\n本地表的一个很好的适用场景是不参与连接查询的小型管理表。用于应用程序登录和身份验证的用户表就是一个很好的示例。\n  分片\n接着上面的部分，讨论分片的技术细节。\n协调器上的pg_dist_shard元数据表记录系统中每个分布式表的每个分片的信息。这些信息是分片 ID 与哈希空间中的整数范围（shardminvalue、shardmaxvalue）的匹配。\nSELECT * from pg_dist_shard; logicalrelid | shardid | shardstorage | shardminvalue | shardmaxvalue ---------------+---------+--------------+---------------+--------------- github_events | 102026 | t | 268435456 | 402653183 github_events | 102027 | t | 402653184 | 536870911 github_events | 102028 | t | 536870912 | 671088639 github_events | 102029 | t | 671088640 | 805306367 (4 rows) 如果协调器节点想确定哪个分片保存了 github_events 的行，它会对该行中的分布列的值进行哈希处理。然后检查哪个分片的范围包含该散列值。定义范围以便散列函数的图像是它们的不相交并集。\n假设分片 102027 与想要请求的行相关联。该行在其中一个工作节点的名为 github_events_102027 的表中读取或写入。哪个工作节点？这完全由元数据表决定。分片到工作节点的映射称为分片放置。\n协调器节点将查询重写为引用特定表（如 github_events_102027）的片段，并在适当的工作节点上运行这些片段。下面是一个在后台运行查询以查找持有分片 ID 102027 的节点的示例。\nSELECT shardid, node.nodename, node.nodeport FROM pg_dist_placement placement JOIN pg_dist_node node ON placement.groupid = node.groupid AND node.noderole = \u0026#39;primary\u0026#39;::noderole WHERE shardid = 102027; ┌─────────┬───────────┬──────────┐ │ shardid │ nodename │ nodeport │ ├─────────┼───────────┼──────────┤ │ 102027 │ localhost │ 5433 │ └─────────┴───────────┴──────────┘ Table Colocation\nTable Colocation 意味着将相关信息一起存储在相同的节点上。这样从一个节点一次拿到想要的数据，即可节省网络开销，查询可以更快速。将相关数据放在不同节点上可以让查询在每个节点上高效地并行运行。\n如果某行的分布列对应的值哈希运算后落在一个分片的哈希范围内，则将该行存储在该分片中。具有相同哈希范围的分片总是放在同一个节点上。具有相等分布列值的行始终位于同一节点的不同分片表上。\n考虑如下可能是 SaaS 多租户场景的 Web 分析系统用到的表：\nCREATE TABLE event ( tenant_id int, event_id bigint, page_id int, payload jsonb, primary key (tenant_id, event_id) ); CREATE TABLE page ( tenant_id int, page_id int, path text, primary key (tenant_id, page_id) ); 下面考虑一个可能由该系统 Web 页面 Dashboard 发起的查询：“返回租户 6 中所有以 \u0026lsquo;/blog\u0026rsquo; 开头的页面在过去一周的访问次数”。\n如果我们的数据存在单服务器中，我们可以使用如下 SQL 轻松的进行查询：\nSELECT page_id, count(event_id) FROM page LEFT JOIN ( SELECT * FROM event WHERE (payload-\u0026gt;\u0026gt;\u0026#39;time\u0026#39;)::timestamptz \u0026gt;= now() - interval \u0026#39;1 week\u0026#39; ) recent USING (tenant_id, page_id) -- 相当于 ON p.tenant_id = r.tenant_id AND p.page_id = r.page_id WHERE tenant_id = 6 AND path LIKE \u0026#39;/blog%\u0026#39; GROUP BY page_id; 使用大规模（Citus）集群时，需要作一定的改造，主要有两种可选的分片方式。\n  按 ID 分片\n随着租户数量的增加以及各租户数据的增长，单服务器查询开始变慢，内存和 CPU 都会成为瓶颈。\n在这种情况下，大规模（Citus）集群就派上用场了。当我们决定分片时，最重要的是确定分布列。现在不妨先试着将event_id和page_id分别作为event表和page表的分布列。\n-- naively use event_id and page_id as distribution columns SELECT create_distributed_table(\u0026#39;event\u0026#39;, \u0026#39;event_id\u0026#39;); SELECT create_distributed_table(\u0026#39;page\u0026#39;, \u0026#39;page_id\u0026#39;); 当数据分散在不同的工作节点时，我们无法像在单个 PostgreSQL 节点上那样执行连接操作。我们需要发起两个查询：\n-- (Q1) get the relevant page_ids SELECT page_id FROM page WHERE path LIKE \u0026#39;/blog%\u0026#39; AND tenant_id = 6; -- (Q2) get the counts SELECT page_id, count(*) AS count FROM event WHERE page_id IN (/*…page IDs from first query…*/) AND tenant_id = 6 AND (payload-\u0026gt;\u0026gt;\u0026#39;time\u0026#39;)::date \u0026gt;= now() - interval \u0026#39;1 week\u0026#39; GROUP BY page_id ORDER BY count DESC LIMIT 10; 运行这两个查询会查阅分散在各个节点上的分片中的数据。\n之后，需要应用程序整合这两个步骤的结果。\n在这种情况下，数据分布会产生很大的缺陷：\n 查询每个分片和运行多个查询的开销； Q1 的开销将许多行返回给客户端； Q2 变的很大； 分隔为多次查询，需要对应用程序作更改。  数据是分散的，因此可以并行化查询。只有当查询的工作量远远大于查询许多分片的开销时，它才是有益的。\n  按租户分片\n在大规模（Citus）集群中，具有相同分布列值的行保证位于同一节点上。由此，选择tenant_id作为分布列再合适不过了。\n-- co-locate tables by using a common distribution column SELECT create_distributed_table(\u0026#39;event\u0026#39;, \u0026#39;tenant_id\u0026#39;); SELECT create_distributed_table(\u0026#39;page\u0026#39;, \u0026#39;tenant_id\u0026#39;, colocate_with =\u0026gt; \u0026#39;event\u0026#39;); 这样，针对可在单 PostgreSQL 节点上运行的原始查询，无需作修改：\nSELECT page_id, count(event_id) FROM page LEFT JOIN ( SELECT * FROM event WHERE (payload-\u0026gt;\u0026gt;\u0026#39;time\u0026#39;)::timestamptz \u0026gt;= now() - interval \u0026#39;1 week\u0026#39; ) recent USING (tenant_id, page_id) WHERE tenant_id = 6 AND path LIKE \u0026#39;/blog%\u0026#39; GROUP BY page_id; 由于对 tenant_id 进行过滤和连接，大规模（Citus）集群知道整个查询可以通过使用包含该特定租户数据的同位置分片集来响应。单个 PostgreSQL 节点可以在一个步骤中响应查询。\n在某些情况下，必须修改查询和表模式以将租户 ID 包含在唯一约束和连接条件中。这种修改相对来说比较简单。\n  至此，我们完成了对 Azure 提供的三种类型的 PostgreSQL 服务的初步了解。\n 参考资料\n[1] Azure Database for PostgreSQL Documentation\n ","permalink":"https://olzhy.github.io/posts/azure-postgres.html","tags":["Azure","PostgreSQL"],"title":"Azure Database for PostgreSQL 学习总结"},{"categories":["计算机"],"contents":"1 开始使用 Azure DevOps 1.1 Azure DevOps 是什么？ Azure DevOps 提供团队工作计划、代码开发协作以及应用程序的构建和部署。 Azure DevOps 支持协作文化和一组流程以使开发人员、项目经理和贡献者聚集在一起开发软件。它允许组织以比传统软件开发方法更快的速度创建和改进产品。\n您可以根据情况使用 Azure 云上的 DevOps Services，或使用 On-Premise 上的 Azure DevOps Server 工作。\nAzure DevOps 提供了可通过 Web 浏览器或 IDE 客户端访问的集成功能。您可以根据业务需求使用以下一项或多项独立服务：\n  Azure Repos\n提供 Git 或其它源码版本控制功能。\n  Azure Pipelines\n提供构建和发布以支持持续集成与持续交付。\n  Azure Boards\n提供一套敏捷工具（使用看板或 Scrum 方法）来支持工作计划及跟进，代码缺陷及问题追踪。\n  Azure Test Plans\n提供多种工具来支持应用程序测试，包括手动测试、探索性测试和持续测试。\n  Azure Artifacts\n公共或私有制品存储库，支持软件或包（如 Maven、npm 等）的共享，可与流水线集成。\n  此外，Azure DevOps 还支持与其它流行工具（如 GitHub、Slack，Trello 等）的集成，也支持开发自定义扩展。\n2 Azure Pipelines Azure Pipelines 结合了持续集成 (CI) 和持续交付 (CD) 来测试和构建代码并将其发布到任何地方。\n持续集成 (CI) 是开发团队使用的自动化合并和测试代码的实践。\n持续交付 (CD) 是构建、测试代码并将其部署到一个或多个测试和生产环境的过程。\n持续测试 (CT) 是使用自动化的“构建-部署-测试”流程，选择一组技术和框架，以快速、可扩展和高效的方式持续测试您的代码修改。\nAzure Pipelines 支持绝大多数的开发语言与应用类型，支持多种部署环境（On-Premise，虚拟机，容器，云平台等）。\n2.1 开始使用 Azure Pipelines 可以使用 YAML 文件azure-pipelines.yml来定义流水线。\n 流水线与代码一起进行版本控制。它遵循相同的分支结构。您可以通过拉取请求（Pull Request）和分支构建策略中的代码审查（Code Review）来验证您的更改。 通过修改每个分支下的 azure-pipelines.yml 文件来修改流水线。 修改构建过程可能会导致中断或意外结果。因只更改流水线部分的逻辑，所以可以更轻松地识别问题。  请遵循以下基础步骤：\n 配置 Azure Pipelines 以使用您的 Git 仓库。 编辑 azure-pipelines.yml 文件以定义您的构建。 将您的代码推送到 Git 仓库。此操作会启动默认触发器来构建和部署，然后监控结果。  下面列出您在使用 YAML 定义流水线时可用的一组特性和任务：\n   特性 说明     Agents 指定流水线运行所需的资源   Approvals 定义完成部署阶段之前所需的一组验证   Artifacts 支持发布或使用的包类型   Caching 通过重用运行的输出或下载的依赖来减少构建时间   Conditions 指定运行作业之前要满足的条件   Container jobs 指定要在容器中运行的作业   Demands 确保在运行流水线之前满足的要求，需要自托管代理   Dependencies 指定下一个作业或阶段运行前必须满足的依赖   Deployment groups 为目标部署机器定义一个逻辑组   Deployment jobs 定义部署步骤   Environment 表示以部署为目标的资源集合   Jobs 定义一组步骤的执行顺序   Service connections 启用与在作业中执行任务所需的远程服务的连接   Service containers 使您能够管理容器化服务的生命周期   Stages 在流水线中组织作业   Tasks 定义构成流水线的构建块   Templates 定义可重用的内容、逻辑和参数   Triggers 定义触发流水线运行的事件   Variables 表示要传递到流水线的变量值   Variable groups 用于存储您想要控制并在多个流水线中使用的变量值     参考资料\n[1] Azure DevOps Documentation\n[2] Azure DevOps Services Overview\n[3] Azure Pipelines Documentation\n ","permalink":"https://olzhy.github.io/posts/azure-devops-services.html","tags":["Azure","DevOps"],"title":"Azure DevOps 服务学习总结"},{"categories":["计算机"],"contents":"本文依据文末参考资料进行翻译及整理，作学习及知识总结之用。\nAzure 是一个云计算平台，提供一组不断扩展的服务，来帮助我们构建满足业务目标的各种解决方案。Azure 的服务范围可由简单的 Web 服务到全虚拟计算机。其提供一组基于云的服务，包括诸如：远程存储、数据库托管，及中心化的账号管理等。此外还提供 AI 及 IoT（物联网）等新能力。\n本节，我们将对 Azure 及其能力进行一个入门与端到端的了解。将为后续章节的学习打下基础。\n1 何为云计算？ 何为云计算？其为在互联网上提供的计算服务，也就是所谓的“云”。这些服务包括：服务器、存储、数据库、网络、软件，分析及智能服务。云计算提供更快的创新、灵活的资源，及规模经济。\n为何云计算使用起来更便宜？\n云计算采用“即用即付”定价模式。您一般只对使用的云服务付费，从而可以帮助您：节省运维成本、更便捷的运行基础设施，及按需扩展业务。\n换句话说，云计算是从别人的数据中心租用算力和存储的一种方式，可将这些云资源看作是自有数据中心的资源来用。用完即还，仅对用量收费。\n您无须在数据中心维护 CPU 及存储，只在需要的时候租用它们，云提供商负责为您维护底层的基础设施。云可以使您快速应对棘手的业务挑战，并给您的客户带来前沿的解决方案。\n为何您应该上云？\n云可以帮助您以之前不可能的方式前进及创新。\n在我们不断变化的数字世界，两种趋势兴起：\n 团队以创纪录的速度为用户交付新功能； 用户期待通过他们的设备及软件获得越来越丰富及更身临其境的体检。  软件发布曾经以数月甚至数年为单位。今天，团队以更小的批次发布特性，常以数天或数周为单位。一些团队甚至持续发布软件更新，有时在一天有多个发布。\n想象您与设备交互的所有方式在几年前是不可能做的。许多设备能够对您进行人脸设别并对语音命令作响应。增强现实改变了您与物理世界交互的方式。家用电器已变得智能化。这些技术仅是几个例子，它们中的许多是由云驱动的。\n为了支持您的服务并更快的交付创新及新颖的用户体检。云为如下内容提供了按需访问：\n 一个接近无限的原始计算、存储，及网络组件池； 语音识别及其它认知服务帮助您的应用在人群中脱颖而出； 从您的软件及设备提供遥测数据的分析服务。  2 何为 Azure？ Azure 是一个持续扩展的云服务集合，帮助您应对当前及未来的业务挑战。Azure 让您使用喜欢的工具及框架在庞大的全球网络上自由的构建、管理，及部署应用。\nAzure 提供什么？\n在 Azure 的帮助下，您拥有构建下一个出色解决方案所需的一切。如下列出 Azure 带来的多项益处：\n 为未来做准备：来自 Microsoft 的持续创新支持您今天的开发及明天的产品愿景； 按您的条件来构建：您有更多选择。通过对开源的承诺及对所有语言和框架的支持，您可以按您想要的方式构建并部署到您想要的地方； 无缝运维混合云：On-Premise、云上，或边缘计算。我们与您在您所在的地方会面。使用专为混合云解决方案设计的工具及服务来集成及管理您的环境； 构建您的可信云：背靠专家团队，以及被企业、政府，及创业公司信任的合规性，使您在安全性上平步青云。  我可以用 Azure 做什么？\nAzure 提供 100 多个服务，使您能够完成从在虚拟机运行现有应用到探索新的软件范例（如智能机器人和混合现实）的所有事情。\n许多团队探索云是由将现有应用迁移到运行在 Azure 上的虚拟机开始的。这是一个好的开始，但云不仅仅是换个地方跑应用。\nAzure 门户是什么？\nAzure 门户是一个基于 Web 的统一控制台，是命令行工具的替代方案。通过 Azure 门户，您可以使用图形化用户界面来管理 Azure 订阅。您可以：\n 构建、管理，及监控从简单的 Web 应用到复杂的云部署的所有内容； 为组织好的资源视图创建自定义仪表盘； 配置可访问性选项以获得最佳体验。  Azure 门户旨在实现弹性及持续可用性。每个 Azure 数据中心都有。这使得其可以灵活应对单个数据中心故障，并通过靠近用户来规避网络延迟。Azure 门户持续更新且维护活动没有宕机时间。\nAzure 市场是什么？\nAzure 市场帮助用户与 Microsoft 合作伙伴、独立软件供应商，及提供方案与服务（这些方案与服务已经过优化以在 Azure 上运行）的创业公司连接起来。\nAzure 市场的客户可以从数百家领先的服务提供商中查找、试用、购买，及提供应用与服务。\n所有的解决方案和服务都经过认证以在 Azure 上运行。\n解决方案目录涵盖多个行业类别，诸如：开源容器平台、虚拟机镜像、数据库、应用构建及部署软件、开发工具、威胁检测，及区块链等。\n使用 Azure 市场，您可以快速及可靠地提供托管在您自己 Azure 环境上的端到端解决方案。在撰写本文时，已有 8000 多个条目。\nAzure 市场专门为 IT 专业人员和云开发人员而设计。Microsoft 合作伙伴还将其用作所有联合上市活动的启动点。\n3 Azure 服务预览 Azure 可以帮助您应对严峻的业务挑战。您带来了您的需求、创造力，和最喜欢的软件开发工具。Azure 带来了一个庞大的全球基础设施，使您可以在其上构建您的应用。\n让我们快速预览一下 Azure 提供的高级服务。\nAzure 服务\n下图为 Azure 服务及特性一览。\n下面会介绍最常用的几个类别。\n  计算\n计算服务通常是公司迁移到 Azure 平台的主要原因之一。Azure 提供一系列用于托管应用及服务的选项。如下是 Azure 中计算服务的几个示例。\n   服务名称 服务功能     Azure 虚拟机 Azure 托管的 Windows 或 Linux 虚拟机   Azure Kubernetes 服务 容器编排服务   Azure 函数服务 事件驱动的无服务计算服务   \u0026hellip; \u0026hellip;      网络\n连接计算资源和提供对应用的访问是 Azure 网络的关键功能。Azure 的网络功能包括一系列选项，用于连接外部世界与 Azure 全球数据中心的服务。如下是 Azure 中网络服务的几个示例。\n   服务名称 服务功能     Azure 虚拟网络 连接虚拟机到进入的虚拟私有网络连接   Azure 负载均衡器 平衡应用程序或服务端点的入站或出战连接   Azure 应用网关 优化应用服务器场交付同时提升应用安全性   Azure DNS 提供超快 DNS 响应及超高域名可用性   \u0026hellip; \u0026hellip;      存储\nAzure 提供四种主要的存储服务类型。\n   服务名称 服务功能     Azure Blob 存储 超大对象（诸如视频文件或位图等）存储服务   Azure 文件存储 可以像文件服务器一样访问和管理的文件共享服务   Azure 队列存储 用作队列或用于在应用间可靠传递消息的数据存储服务   Azure 表存储 表存储是一种在云上存储非关系型结构化数据（NoSQL）的服务    这几个服务有如下几个共性：\n 高可用 具有冗余性和复制特性； 安全性 通过自动加密和基于角色的访问控制（RBAC）确保安全； 可扩展 几乎接近无限存储； 管理好 维护及处理任何关键问题； 访问性 可通过 HTTP 或 HTTPS 从世界任何地方访问。    移动端\n借助 Azure，开发人员可以快速轻松地为 iOS、Android，及 Windows 应用创建移动后端服务。过去耗时长且增加项目风险的特性（如引入登录，然后连接到 On-Premise 上的 SAP、Oracle、SQL Server， 及 SharePoint 资源），现在可以轻松包含在内。\n该服务的其它特性包括：\n 离线数据同步； 与 On-Premise 数据的连接； 广播通知推送； 按业务需要自动扩展。    数据库\nAzure 提供多种数据库服务来存储各种数据类型及数据卷。并通过全球连接，用户可以即时使用这些数据。\n   服务名称 服务功能     Azure Cosmos DB 支持 NoSQL 选项的全球分布式数据库   Azure SQL Database 具有自动扩展、集成智能，和强大的安全性的完全托管的关系型数据库   Azure Database for MySQL 具有高可用性和安全性的完全托管和可扩展的 MySQL 数据库   Azure Database for PostgreSQL 具有高可用性和安全性的完全托管和可扩展的 PostgreSQL 数据库   SQL Server on Azure Virtual Machines 在云上托管企业 SQL Server 应用的服务   Azure Cache for Redis 用于缓存经常使用的静态数据以减少数据和应用延迟的完全托管的 Redis 服务   \u0026hellip; \u0026hellip;      Web\n在当今的商业世界中，拥有出色的 Web 体验至关重要。 Azure 包括一流的支持来构建和托管 Web 应用和基于 HTTP 的 Web 服务。以下 Azure 服务专注于 Web 托管。\n   服务名称 服务功能     Azure App 服务 快速创建功能强大的基于 Web 的云应用程序   \u0026hellip; \u0026hellip;      物联网\n当今，人们能够访问比以往更多的信息。个人数字助理引领智能手机，现在有智能手表、智能温度计，甚至智能冰箱。个人电脑曾经是常态。现在，互联网允许任何可以连网的设备来访问有价值的信息。设备获取及转发信息以用于数据分析的能力被称为物联网。\n许多服务可以协助和推动 Azure 上物联网的端到端解决方案。\n   服务名称 服务功能     IoT Central 完全托管的全球物联网软件即服务 (SaaS) 解决方案，可轻松连接、监控和管理大规模物联网资产   \u0026hellip; \u0026hellip;      大数据\n数据有各种格式和大小。当我们谈论大数据时，通常指的是大宗数据。来自天气系统、通信系统、基因组研究、成像平台和许多其它的场景会产生数百 GB 的数据。如此庞大的数据量使分析和决策变得困难。如此巨量的数据，使得传统的处理和分析方式不再适用。\n开源集群技术已被用来处理这些大数据。 Azure 提供广泛的技术和服务，以对大数据进行处理和分析。\n   服务名称 服务功能     Azure Synapse Analytics 使用基于云的企业数据仓库大规模运行分析，该数据仓库利用大规模并行处理在 PB 级数据中快速运行复杂查询   \u0026hellip; \u0026hellip;      人工智能\n在云计算背景下，人工智能基于一组广泛的服务，其核心是机器学习。机器学习是一种数据科学技术，它允许计算机使用现有数据来预测未来的行为、结果和趋势。使用机器学习，计算机无需显式编程即可学习。\n机器学习的预测可以使应用和设备更智能。例如，当您在线购物时，机器学习可基于您之前购买的商品来推荐您可能喜欢的其它商品。还有，当您的信用卡被刷卡时，机器学习会将交易与数据库进行校对，并帮助检测欺诈行为。再有，当您的扫地机器人对房间进行打扫时，机器学习会帮助它决定工作是否完成。\n以下是 Azure 中一些最常见的 AI 和机器学习服务类型。\n   服务名称 服务功能     Azure Machine Learning Service 可用于开发、训练、测试、部署、管理和跟踪机器学习模型的云环境，可自动生成模型并自动调整   \u0026hellip; \u0026hellip;    还有一组相关产品是认知服务。您可以在应用中使用这些预置 API（图像识别、语音识别、自然语言处理等） 来解决复杂的问题。\n  DevOps\nDevOps 通过自动化软件交付将人员、流程和技术结合在一起，为您的用户提供持续的价值。借助 Azure DevOps，您可以创建构建和发布流水线，为您的应用提供持续集成、持续交付和持续部署。您可以集成代码库和自动化测试、进行应用程序监控，及使用构建包。您还可以集成一系列第三方工具和服务（如 Jenkins 和 Chef）。所有这些功能都已与 Azure 紧密集成，以便为您的应用提供一致、可重复的部署，从而提供流水线的构建和发布流程。\n   服务名称 服务功能     Azure DevOps 使用开发协作工具，如高性能流水线、免费的私有 Git 仓库、可配置的看板，及自动化和基于云的负载测试   \u0026hellip; \u0026hellip;      4 开始使用 Azure 账号 要创建和使用 Azure 服务，需要一个 Azure 订阅。当您完成学习模块时，大多数情况下会为您创建一个临时订阅，该订阅在沙盒环境中运行。当您需要处理自己的应用和业务需求时，须创建一个 Azure 账号，创建完成后，会自动为您创建一个订阅。后面，您还可以自由创建其它订阅。例如，您的公司可能为你们的业务使用同一个 Azure 账号，但为开发、市场和销售部门创建各自的订阅。 订阅创建后，您就可以使用其创建 Azure 资源了。\n如果您刚开始使用 Azure，可以在 Azure 网站上注册一个免费账号来进行探索。准备就绪后，可以选择升级您的免费帐户。您可以创建一个新的订阅来为收费的服务付费。\n创建 Azure 账号\n您可以通过在 Azure 网站上直接注册或通过 Microsoft 代表从 Microsoft 购买 Azure 访问权限。您还可以通过 Microsoft 合作伙伴购买 Azure 访问权限。云解决方案提供商合作伙伴为 Azure 提供一系列完整的托管云解决方案。\nAzure 免费账号\nAzure 免费账号包括：\n 12 个月免费访问流行的 Azure 产品； 前 30 天可使用的赠送金； 访问超过 25 种始终免费的产品。  Azure 免费账号是新用户入门和探索的绝佳方式。注册时，需要手机号码、信用卡号，和一个 Microsoft 或 GitHub 账号。信用卡信息仅用于身份验证。在升级到付费订阅之前，您不会为任何服务付费。\nAzure 学生免费账号\nAzure 免费学生账号优惠包括：\n 在 12 个月内免费访问某些 Azure 服务； 在前 12 个月内可使用的赠送金； 免费访问某些软件开发工具。  什么是沙箱学习环境？\n许多练习环境都使用了一种称为沙箱的技术，该技术会为您的 Azure 账号创建一个临时订阅。该临时订阅允许您在学习某个模块期间创建 Azure 资源。在您完成该模块的学习后，这些临时资源会自动清理。\n当您完成某个模块的学习时，欢迎使用您的个人订阅来完成模块中的练习。沙盒是首选的方式，因为使用其创建和测试 Azure 资源是免费的。\n5 不同类型的云模型 什么是公有云、私有云及混合云？\n云计算有三种部署模式：公有云、私有云和混合云。各个部署模式都有自己的适应场景，您在上云时应该考虑这些方面。\n  公有云\n服务通过公共互联网提供，任何想要购买的人都可以使用。云资源（如服务器和存储）由第三方云服务提供商拥有和运营，并通过互联网交付。\n  私有云\n私有云由一个企业或组织的用户专门使用的计算资源组成。私有云的物理位置可能在您组织的 On-Premise（自建）数据中心，或由第三方服务提供商托管。\n  混合云\n混合云是一种结合了公共云和私有云的计算环境，且允许数据和应用在两种云之间共享。\n  三种云模型比较\n  公有云\n 扩展无需资本支出； 应用可被快速提供或取消提供； 用户仅对用量付费。    私有云\n 必须购买硬件用于启动和维护； 用户可以完全控制资源和安全性； 用户自己负责硬件维护和更新。    混合云\n 提供最大的灵活性； 用户决定在哪里运行他们的应用程序； 用户自己控制安全性、合规性或法律要求。    6 云的好处和注意事项 云计算有哪些优势？\n与物理环境相比，云计算有如下几个优势：\n  高可用性\n根据您选择的服务水平协议 (SLA)，基于云的应用可以提供持续的用户体验，而不会出现明显的停机时间，即使出现问题也是如此。\n  可扩展性\n云上的应用可以垂直和水平扩展：通过向虚拟机增加 RAM 或 CPU 来增加计算容量以进行垂直扩展；通过添加资源实例来扩充计算容量（如增加虚拟机数量）以进行横向扩展。\n  弹性\n您可将云应用配置为自动扩容，这样即可始终拥有所需的资源。\n  敏捷性\n随着应用需求的变化，快速部署和配置云上的资源。\n  地理分布\n您可以将应用和数据部署到全球各区域数据中心，从而确保您的客户始终在其所在地区获得最佳性能。\n  灾难恢复\n通过利用云上的备份服务、数据复制和地理分布，您可以放心地部署应用，因为您知道在发生灾难时您的数据是安全的。\n  资本支出与运营支出\n您应该考虑两种不同类型的费用模型：\n  资本支出 (CapEx)\n是在物理基础设施上的前期支出，然后随着时间的推移扣除该前期费用。资本支出的前期成本的价值会随着时间的推移而降低。\n  运营支出 (OpEx)\n现在在服务或产品上花钱，那现在就为它们付费。您可以在花费的同一年扣除这笔费用。没有前期费用，因为您在使用时才开始花钱买服务或产品。\n  换句话说，On-Premise 用户在买了基础设施起，购买的设备即作为资产进入了资产负债表。由于进行了资本投资，会计师将此交易归类为资本支出。随着时间的推移，资产会折旧或坏损。\n另一方面，云服务因其消费模式而被归类为运营支出。 云使用者没有资产会折旧，其云服务提供商 (Azure) 负责管理物理设备的购买和使用寿命相关的成本。因此，运营支出对净利润、应税收入和资产负债表上的相关费用有直接影响。\n总而言之，资本支出需要大量的前期财务成本，以及持续的维护和支持支出。相比之下，OpEx 是基于消费的模型，因此上云后只需考虑计算资源的使用成本。\n云计算是基于消费的模型\n云服务提供商以基于消费的模型运行，这样用户只需为他们使用的资源付费。\n基于消费的模型有几个优点：\n 没有前期费用。 无需购买和管理用户可能无法充分利用的昂贵基础设施。 在需要时支付额外资源的能力。 停止为不再需要的资源付费的能力。  7 不同类型的云服务模型 有哪些云服务模型\n如果您已经接触过云计算一段时间，您可能已经看到了不同云服务模型 （PaaS、IaaS 和 SaaS） 的缩略词。这些模型定义了云提供商和云租户负责的不同级别的共享责任。\n  基础设施即服务 IaaS (Infrastructure-as-a-Service)\n这种云服务模式最接近于管理物理服务器；云提供商将使硬件保持最新，但操作系统维护和网络配置取决于作为云租户的您。例如，Azure 虚拟机是在 Microsoft 数据中心运行的完全可操作的虚拟计算设备。这种云服务模型的一个优势是可以快速部署和更新。设置新的虚拟机比购买、安装和配置物理服务器要快得多。\n  平台即服务 PaaS (Platform-as-a-Service)\n此云服务模型是一个管理的托管环境。云提供商管理虚拟机和网络资源，云租户将其应用部署到托管环境中。例如，Azure App Services 提供了一个管理托管环境，开发人员可以在其中上传他们的 Web 应用程序，而不用管物理硬件和软件要求。\n  软件即服务 SaaS (Software-as-a-Service)\n在这种云服务模型中，云提供商管理应用环境的各个方面，例如虚拟机、网络资源、数据存储和应用程序。云租户只需将其数据提供给云提供商管理的应用程序即可。例如，Microsoft Office 365 提供了在云上运行的完整版 Microsoft Office。您需要做的就是创建内容，Office 365 会处理其他所有事情。\n  下图演示了可能在每个云服务模型中运行的服务。\n各类云服务模型的优缺点\n   服务名称 优点 缺点     IaaS 无资本支出，敏捷，完全托管，灵活    PaaS 除了 Iaas 有的优点外，再一个优点是用户只专注业务就可以了 云平台会有各自的局限性需要考量   SaaS 除了 Paas 有的优点外，还有即用即付付费模式，用户什么都不用管，作为软件用就好了 软件局限性，用户对特性没有控制权    下图说明了云提供商和云租户之间不同级别的责任。\n   IaaS PaaS SaaS     最灵活的云服务 专注于应用开发 即用即付定价模式   您负责应用程序配置和管理硬件 云提供商负责管理平台 用户为他们在订阅模式中使用的软件付费    什么是无服务器计算\n与 PaaS 一样，无服务器计算使开发人员无需管理基础架构，从而能够更快地构建应用。借助无服务器应用程序，云服务提供商负责自动配置、扩展和管理运行代码所需的基础设施。无服务器架构具有高度可扩展性和事件驱动，仅在特定功能或触发器发生时才使用资源。\n重要的是要注意服务器仍在运行代码。 “无服务器”的名称源于与基础设施供应和管理相关的任务对开发人员是透明的。这种方法使开发人员能够更加关注业务逻辑，并为业务核心提供更多价值。无服务器计算可帮助团队提高生产力并更快地将产品推向市场，并使组织能够更好地优化资源并专注于创新。\n8 Azure 订阅、管理组和资源 Azure 中资源的组织结构有四个级别：管理组、订阅、资源组和资源。\n下图显示了这些级别的自上而下的层次结构。\n了解了自顶向下的组织层次结构后，让我们自底向上描述每个级别：\n  资源\n资源是您创建的服务实例，例如虚拟机、存储或 SQL 数据库。\n  资源组\n资源组合成资源组，这些资源组充当一个逻辑容器，在其中部署和管理 Web 应用、数据库和存储帐户等 Azure 资源。\n  订阅\n订阅将用户帐户和由这些用户帐户创建的资源组合在一起。对于每个订阅，您可以创建和使用的资源数量都有限制或配额。组织可以使用订阅来管理成本以及由用户、团队或项目创建的资源。\n  管理组\n这些组可帮助您管理多个订阅的访问、策略和合规性。管理组中的所有订阅都会自动继承应用于管理组的条件。\n  9 Azure 地域、可用区和地域对 在上一单元中，您了解了 Azure 资源和资源组。资源在地域中创建，这些地域是包含 Azure 数据中心的全球不同的地理位置。\nAzure 由遍布全球的数据中心组成。当您使用服务或创建诸如 SQL 数据库或虚拟机 (VM) 之类的资源时，您正在使用这些位置中的一个或多个位置的物理设备。这些特定的数据中心不会直接向用户公开。相反，Azure 将它们组织成地域。正如你将在本单元后面看到的那样，其中一些地域提供可用区，它们是该地域内的不同 Azure 数据中心。\nAzure 地域（Azure Region）\n地域是地球上的一个地理区域，其中包含至少一个但可能有多个数据中心，这些数据中心临近并通过低延迟网络联网。 Azure 智能地分配和控制每个地域内的资源，以确保工作负载平衡。\n在 Azure 中部署资源时，通常需要选择要部署资源的地域。\n注意：某些服务或 VM 功能仅在某些地域可用，例如特定 VM 规格或存储类型。还有一些全球 Azure 服务不需要您选择特定区域，例如 Azure Active Directory、Azure 流量管理器和 Azure DNS。\n一些地域的例子是美国西部、加拿大中部、西欧、澳大利亚东部和日本西部。以下是截至 2020 年 6 月所有可用地域的视图。\n为什么地域如此重要？：Azure 拥有比任何其他云提供商更多的全球地域。这些地域使您可以灵活地使您的应用更靠近您的用户，无论他们身在何处。全球地域提供更好的可扩展性和冗余性。它们还为您的服务保留数据驻留。\nAzure 还有几个特殊的地域，当您出于合规性或法律目的构建应用时，可以考虑使用。一些例子包括：\n  US DoD Central、US Gov Virginia，US Gov Iowa 等\n这些区域是面向美国政府机构和合作伙伴的 Azure 物理和逻辑网络隔离实例。这些数据中心由经过筛选的美国人员操作，并包括额外的合规认证。\n  中国东部、中国北部等\n由 21Vianet 代理，Microsoft 不直接对数据中心进行维护。\n  地域是您用来标识资源位置的。您还应该注意另外两个术语：地理和可用区。\nAzure 可用区（Availability Zones）\n您希望确保服务和数据是冗余的，以便在发生故障时保护您的信息。托管基础架构时，设置冗余需要创建重复的硬件环境。 Azure 可以通过可用区帮助您的应用高可用。\n可用区是 Azure 地域内物理上独立的数据中心。每个可用区配备一个或多个独立电源、冷却和网络组成的数据中心。可用区设置为隔离边界。如果一个区域出现故障，另一个区域将继续工作。可用区通过高速专用光纤网络连接。\n并非每个地域都支持可用区。请参阅支持可用区的地域列表。\n您可以使用可用区来运行任务关键型应用，通过将计算、存储、网络和数据资源共同置于一个区域，并在其他区域进行复制，将高可用性构建到您的应用架构中。请记住，复制服务和在区域之间传输数据可能会产生额外成本。\n可用区主要用于 VM、托管磁盘、负载均衡器和 SQL 数据库。支持可用区的 Azure 服务分为三类：\n  区域性服务\n您将资源固定到特定区域（例如：VM、托管磁盘、IP 地址）。\n  区域冗余服务\n平台自动跨区域复制（例如：区域冗余存储、SQL 数据库）。\n  非区域服务\n服务始终可在各地访问，并且能够适应区域范围及地域范围的中断。\n  Azure 地域对（Region Pairs）\n可用区是使用一个或多个数据中心创建的。一个地域内至少有三个区域。一场大灾难可能会导致大到足以影响两个数据中心的中断。这就是 Azure 还创建地域对的原因。\n每个 Azure 地域始终与至少 300 英里以外的同一地理区（例如美国、欧洲或亚洲）内的另一个地域配对。这种方法允许跨地理复制资源（例如 VM 存储），这有助于减少由于同时影响两个区域的自然灾害、内乱、停电或物理网络中断等事件而导致中断的可能性。例如，如果一对中的一个地域受到自然灾害的影响，服务将自动故障转移到其地域对中的另一个地域。\nAzure 中的地域对示例包括美国西部与美国东部配对，以及东南亚与东亚配对。\n由于地域对是直接相连的，并且相距足够远，可以免受地域灾难影响，因此您可以使用它们来提供可靠的服务和数据冗余。某些服务通过使用地域对来提供自动异地冗余存储。\n地域对的其他优点：\n 如果发生广泛的 Azure 中断，则会优先考虑对中的一个地域，以确保尽快为该地域对中托管的应用恢复至少一个地域。 计划的 Azure 更新一次升级一个地域，以最大限度地减少停机时间和应用中断的风险。 出于税收和执法管辖目的，数据继续放在同一地理位置的地域对内（巴西南部除外）。  拥有广泛分布的数据中心允许 Azure 提供高可用的保证。\n10 Azure 资源及 Azure 资源管理器 在创建订阅之前，您需要准备好开始创建资源并将它们放在资源组中。考虑到这一点，定义这些术语很重要：\n  资源\n通过 Azure 获得的可管理项目。虚拟机 (VM)、存储帐户、Web 应用程序、数据库和虚拟网络是资源的示例。\n  资源组\n包含 Azure 解决方案相关资源的容器。资源组包括要作为一个组进行管理的资源。您可以根据对您的组织来决定哪些资源属于哪些资源组。\n  Azure 资源组\n资源组是 Azure 平台的基本元素。资源组是部署在 Azure 上的资源的逻辑容器。这些资源是您在 Azure 订阅中创建的任何内容，例如 VM、Azure 应用程序网关实例和 Azure Cosmos DB 实例。所有资源都必须在一个资源组中，并且一个资源只能属于一个资源组。许多资源可以在资源组之间移动，其中一些服务具有特定的限制或移动要求。资源组不能嵌套。在可以预配任何资源之前，您需要一个资源组来放置它。\n资源组是用来管理和组织 Azure 资源的。通过将类似用途、类型或位置的资源放在资源组中，可以为在 Azure 中创建的资源提供顺序和组织。逻辑分组是你在这里最感兴趣的方面，因为我们的资源中有很多混乱。\n如果删除资源组，则其中包含的所有资源也会被删除。按生命周期组织资源在非生产环境中很有用，您可以在其中尝试实验然后将其丢弃。资源组可以轻松地一次性删除一组资源。\n资源组也是应用基于角色的访问控制 (RBAC) 权限的范围。通过将 RBAC 权限应用于资源组，您可以简化管理并将访问权限限制为仅允许需要的内容。\nAzure 资源管理器\nAzure 资源管理器提供了一个管理层，使您能够在 Azure 帐户中创建、更新和删除资源。您可以使用访问控制、锁和标签等管理功能来保护和组织部署后的资源。\n当用户从任何 Azure 工具、API 或 SDK 发送请求时，资源管理器会收到该请求。它对请求进行身份验证和授权。资源管理器将请求发送到 Azure 服务，该服务会执行请求的操作。因为所有请求都通过相同的 API 处理，所以您可以在所有不同的工具中看到一致的结果和功能。\n下图显示了资源管理器在处理 Azure 请求中所起的作用。\nAzure 门户中提供的所有功能也可通过 PowerShell、Azure CLI、REST API 和客户端 SDK 获得。最初通过 API 发布的功能将在初始发布后的 180 天内在门户中呈现。\n使用资源管理器的好处：\n 通过声明性模板而非脚本来管理您的基础架构。资源管理器模板是一个 JSON 文件，用于定义要部署到 Azure 的内容； 将解决方案的所有资源作为一个组进行部署、管理和监控，而不是单独处理这些资源； 在整个开发生命周期中重新部署您的解决方案，并确信您的资源以一致的状态部署； 定义资源之间的依赖关系，以便以正确的顺序部署它们； 对所有服务应用访问控制，因为 RBAC 原生集成到管理平台中； 将标签应用于资源，以逻辑组织订阅中的所有资源； 通过查看相同标签的一组资源的成本来明确组织的计费。  11 Azure 订阅及管理组 开始使用 Azure 前，您的第一步将是创建至少一个 Azure 订阅。使用它在 Azure 中创建基于云的资源。\nAzure 订阅\n使用 Azure 需要 一个 Azure 订阅。订阅用于提供对 Azure 产品和服务的鉴权和授权。它还允许您配置资源。 Azure 订阅是 Azure 服务的逻辑单元，它链接到 Azure 帐户，该帐户是 Azure Active Directory (Azure AD) 或 Azure AD 信任的目录中的一个标识。\n一个帐户可以有一个订阅或多个订阅，这些订阅具有不同的计费模式并且您对其应用不同的访问管理策略。您可以使用 Azure 订阅来定义 Azure 产品、服务和资源的边界。您可以使用两种类型的订阅边界：\n  计费边界\n此订阅类型确定 Azure 帐户使用 Azure 的计费方式。您可以针对不同类型的计费要求创建多个订阅。 Azure 为每个订阅生成单独的计费报告和发票，以便你可以组织和管理成本。\n  访问控制边界\nAzure 在订阅这层应用访问管理策略，您可以创建单独的订阅以反映不同的组织结构。一个示例是，在企业中，您有不同的部门应用不同的 Azure 订阅策略。此计费模型允许您管理和控制对用户通过特定订阅提供的资源的访问。\n  创建其他 Azure 订阅：\n您可能希望为资源或计费管理目的创建其他订阅。例如，您可以选择创建额外的订阅来分隔：\n  环境\n在管理资源时，您可以选择创建订阅以设置单独的环境用于开发和测试、安全性或出于合规性原因隔离数据。这种设计特别有用，因为资源访问控制发生在订阅级别。\n  组织结构\n您可以创建订阅以反映不同的组织结构。例如，您可以限制团队使用成本较低的资源，同时允许 IT 部门使用全方位的资源。此设计允许您管理和控制对用户在每个订阅中提供的资源的访问。\n  计费\n您可能还想为计费目的创建其他订阅。由于成本首先在订阅级别汇总，因此您可能希望创建订阅以根据您的需要管理和跟踪成本。例如，您可能希望为您的生产工作负载创建一个订阅，并为您的开发和测试工作负载创建另一个订阅。\n  订阅限制\n订阅受到一些硬性限制。例如，每个订阅的最大 Azure ExpressRoute 线路数为 10。在您的帐户上创建订阅时应考虑这些限制。如果在特定情况下需要超过这些限制，您可能需要额外订阅。\n  自定义计费以满足您的需求：\n如果您有多个订阅，您可以将它们组织到发票部分。每个发票部分都是发票上的一个行项目，显示当月产生的费用。例如，您可能需要为您的组织提供一张发票，但希望按部门、团队或项目来组织费用。\n根据您的需要，您可以在同一个帐单帐户中设置多个发票。为此，请创建其他计费配置文件。每个计费配置文件都有自己的月度发票和付款方式。\n下图显示了计费结构的概览。如果你之前已注册 Azure，或者你的组织有企业协议，则你的计费设置可能会有所不同。\nAzure 管理组\n如果您的组织有许多订阅，您可能需要一种有效管理这些订阅的访问、策略和合规性的方法。 Azure 管理组提供高于订阅的范围级别。您将订阅组织到称为管理组的容器中，并将您的治理条件应用于管理组。管理组中的所有订阅都会自动继承应用于管理组的条件。无论您拥有何种订阅类型，管理组都能为您提供大规模的企业级管理。单个管理组中的所有订阅都必须信任同一个 Azure AD 租户。\n例如，您可以将策略应用于限制可用于创建 VM 的区域的管理组。通过仅允许在该区域中创建 VM，此策略将应用于该管理组下的所有管理组、订阅和资源。\n您可以构建灵活的管理组和订阅结构，以将资源组织到层次结构中，以实现统一的策略和访问管理。下图显示了使用管理组创建治理层次结构的示例。\n您可以创建应用策略的层次结构。例如，您可以在名为 Production 的组中将 VM 位置限制在美国西部地区。此策略将继承到作为该管理组后代的所有企业协议订阅，并将应用于这些订阅下的所有 VM。资源或订阅所有者无法更改此安全策略，从而可以改进治理。\n您使用管理组的另一种情况是为用户提供对多个订阅的访问权限。通过在该管理组下移动多个订阅，您可以在管理组上创建一个基于角色的访问控制 (RBAC) 分配，该分配将继承对所有订阅的访问权限。管理组上的一项分配可以使用户能够访问他们需要的所有内容，而不是在不同的订阅上编写 RBAC 脚本。\n关于管理组的几个注意点：\n 单个目录可支持 10,000 个管理组； 管理组树最多可支持六个深度级别。此限制不包括根级别或订阅级别； 每个管理组和订阅只能支持一个父级； 每个管理组可以有很多孩子； 所有订阅和管理组都在每个目录的单个层次结构中。   参考资料\n[1] AZ-900: Microfost Azure Fundamentals\n ","permalink":"https://olzhy.github.io/posts/microsoft-azure-fundamentals.html","tags":["Azure"],"title":"AZ-900 Microsoft Azure 基础学习之核心概念"},{"categories":["随笔"],"contents":"要說人間喜樂事，最美莫過中國年。\n我的家位於山西中北部的一個小村子。記憶中小時候的一年總是那麽長，年的味道總是那麽濃。記得小時候問過母親一個現在看來很哭笑不得的問題：「媽，過年好開心啊，為什麼一年不多過幾次呢？」，母親笑著看看我，什麽也沒說。\n一 小年 農村的冬天，大人們早早沒有了農事。孩子們早就放了寒假，盼望著，盼望著，年越來越近了。臘月二十三，小村的街道上，就有人吆喝著賣麻糖了，老人們說，吃了它就表示將嘴糊上了，過年不允許說髒話。要過年了，遇到好吃的東西，再也不用像平時那樣費盡氣力地央求大人們了，父母都會「開恩」一樣給孩子們買著吃。一個個孩子手裏拿著那長長的麻糖，吃著，流着口水，咧著嘴笑著，好不開心！\n二 大年 在我小的時候，父親還是在家過年的，後來隨著他在賭博的路上越走越遠，也就很少回來了。\n我家和二叔同住一個院子。待到除夕那天，兩家人早早就起來忙活了。第一件事是年前最後一次大掃除。我們將院子的各個犄角旮旯都不放過，將那些剩餘的廢紙片，爐子燒燼的焦炭等收集好，推著平車將它們倒到村外去。\n吃過早飯，我負責在爐子上將那白麪漿糊熬的剛剛好，和父親、二叔，以及堂哥迎著上午的暖陽貼對聯。堂妹還小，在屋裏隔著玻璃睜大眼睛看著院子裏邊忙活的我們。鐵鍋裏的漿糊冒著滾燙的熱氣，我和堂哥戴著手套哈著氣，鼻尖凍得通紅，給刷子抹上漿糊並和對聯一起趕緊給大人遞上去，慢了漿糊可就凍住了。父親拿頭，二叔拿尾，我和堂哥退後去看一看，說對正了，就將它拍的嚴嚴實實。我們貼完正房貼西房，貼完西房貼耳房，最後登著梯子去貼街門。這時，路邊走過一個略顯陌生的面孔，問一聲：「忙著貼對子啊」。二叔回頭定睛一看，問一句：「三哥，過年好啊，哪天回來的？」。那人給父親和二叔遞根煙，笑著說：「昨天就回來了」。父親說：「有多年沒回來了吧？」。那人看看我和堂哥，驚訝的說：「是啊，有些年了，孩子們都長這麽大了！」。我們那流傳一句話：「有錢沒錢，回家過年」。所以，過年時，我們可以看到好多平時不多見的面孔，縣城的、外地的，都會帶著老婆孩子回到村裏陪老父母一起過年。\n貼完對聯，我和堂哥又去看二叔怎麼壘旺火，每年的這項事情都是他來負責的。只見他將大塊的炭劈成磚頭狀，認認真真，慢慢悠悠的一塊一塊壘上去，好不用心。二嬸在屋裏等著二叔幹活，見二叔進度慢，難免又小吵兩句。我和堂哥拿著五色彩紙交給母親，看母親怎麽剪出漂亮的波浪花，待二叔旺火壘好了，將它套上去，最後再把最頂頭的一塊方形炭貼上「旺氣衝天」就算大功告成。\n回家洗洗手，天就漸漸暗下來了。母親要準備年夜飯了，炸年糕，炸丸子，炸帶魚，準備餃子餡，一件一件條理清楚，我看著那爐子上的油鍋，等著那第一個丸子炸熟，母親將它餵進我的嘴裏，又燙又香。\n剛剛吞下，父親即喊我將今晚要放的炮仗都放在炕頭，烤的熱乎乎的，只爲晚上它們可以個個響噹噹。\n待母親將好吃的都做的差不多了，就將那準備好的餃子餡兒拿來，一家人坐在热炕上包餃子。父親擀皮兒，母親包餡兒，姐姐喜歡包個可愛的小動物，我不會包，就去找硬幣洗乾凈，待母親包到最後的幾個餃子裏。\n所有事情忙活完畢，興奮又焦急的等待著母親為我和姐姐換上一件只有過年才開始穿的新衣服。走出街門外，前兩天在電線杆上安裝好的路燈已亮了起來，一群孩子們在街道上歡樂的喊叫著，跑著，追趕著。有的哭著喊著想要玩具槍，跑回家要上幾塊錢，去村頭的小賣部買上它，一路上盡情拿著瘋響，可是威風。我和另一些小男孩們則一手拿著一根香，一手拿一把擼下來的鞭炮，點著後眼疾手快的將它扔出去，樂此不疲。小女孩们則躲的遠遠的，捂着耳朵，看著我們肆無忌憚的笑著，鬧著。還有那外村過來的大哥哥大姐姐，成群結隊，個個穿的整齊漂亮，來村裏找他們的同伴，只見他們一波又一波的在那路燈下歡聲笑語，走來走去。\n母親喚我們吃年夜飯了，打開電視，春節聯歡晚會就要開始了，嘴裏吃著玲瑯滿目的好吃的，那「春節序曲」音樂一起，整個晚會隨之拉開序幕。看著那電視裏人們穿著五顏六色的衣服跳著舞蹈就非常喜慶。\n伴著電視里的熱鬧聲，不到零點，我就有點犯瞌睡了。不知何時，母親將我叫醒。只聽見進處遠處密密麻麻的炮聲，零點的鍾聲已經敲響，我們兩家人披著厚衣服全部從屋子裏出來，要發旺火了。我們幾個孩子，捂著耳朵，打著冷顫，嘴裏上邊下邊的牙齒不聽使喚的碰撞著。二叔給旺火倒上柴油，待火一點，我們將那高粱節做成的鍋蓋拿著，每人選一處在那裏拚命的扇風，一會功夫，熊熊燃燒的火焰照亮了每一個通紅的臉。跑去炕頭拿出那所有的炮仗，大人去點那二踢腳，我和堂哥捂著耳朵再去放一通竄天猴。那五彩斑斕的煙花撒滿了整個夜空，那震耳欲聾的炮聲響徹了每個角落。\n發完旺火，放完炮，母親早在炕上倒好了幾碗紅糖水，喻示著一年的甜頭。喝完糖水，母親從爐子上端上來一鍋香噴噴的素菜，這是我們家零點的固定菜。裏邊有白菜，金針，干扁豆角，油炸凍豆腐，是母親從姥姥那邊流傳下來的習俗。\n凌晨一兩點，聽著那依然熱烈的炮聲，我已帶著明天一早拜年領壓歲錢的興奮進入了夢鄉。這一晚，院子裏的旺火，屋裏屋外的燈會一直亮到天明。\n辛丑年臘月廿三\n2022 年 1 月 25 日於大連\n","permalink":"https://olzhy.github.io/posts/chinese-new-year.html","tags":["随笔"],"title":"記憶中兒時的中國年"},{"categories":["随笔"],"contents":"又到歲末，今天有時間坐下來梳理總結即將過去的一年。這一年像往年一樣過的很快，但發生了很多事情，對我的生命產生極大的影響，歸結下來，這一年的大事主要有如下幾個方面。\n一 母親來連 3 月份，母親得知太太照顧孩子很吃力，同時關心我的身體情況，毅然決定過來幫我們分擔家務及照顧孩子。近年，母親患有軀體性精神障礙，身體很弱，服用藥物後，逐漸從臥床多年到能下地活動到現在可以承擔輕量的日常家務，但還是不能太勞累。這一次過來一待就快到一整年，是她強烈的責任心使然。幫我們分擔家務及全心全意照顧孩子，見證了孩子從剛剛站立到會走到會跑的重要成長階段。\n二 房子裝修 3 月底，太太考慮到租房的開銷及一直租房也不是長久之計，決定今年開始裝修我們位於市郊的房子。因這兩年我的精力不佳，太太全權攬起了裝修房子的重任。從學習相關知識到選擇裝修公司，從裝修公司開工到基礎裝修完成到軟裝，一直到家具選購及裝修完成，這整個過程我未操過一點心，全部是由太太一個人負責打理。感謝她的辛勤付出，讓我們全家人在年底住上了自己的房子。\n三 工作拐點 年初即知道公司要進行大裁員，我們所在部門的業務會遷移至其它國家，大部分人員成了被裁對象。領導會帶領極少的幾個人轉至新公司，6 月份，領導問我願不願意隨他到新公司，經過與太太的商量，我決定跟隨領導過去。特別感恩領導及同事對我的欣賞及信任，因近年的焦慮癥，我的精力不佳，自己明白之前的工作做的並不好，這一次領導再次給我機會，給了我極大的鼓舞。這一次，我也有了很大的變化，決心把自己放到最低的姿態，真誠的去與每一個同事相處，隨著精力改善，工作越來越得心應手，同時也讓我發現每一個同事都有自己的優點及專長。\n四 岳母離世 9 月底，一個早晨，睡夢中突然接到小姨子的電話，邊哭邊喘氣的告知了我岳母去世的噩耗。太太得知後，強忍苦痛，變得異常堅強，我們馬上訂票匆匆趕回太太的老家準備處理岳母的後事。初進家門，即看到岳母安靜的躺在冷櫃裏，給我內心極大的震撼。這是我第一次親身近距離經歷親人離世，我當時站在旁邊，感覺岳母就像睡著了，接著的黑夜感覺這個世界靜的只能聽見冷櫃的電流聲。第二天岳母火化，剩下的是一個箱子可以裝下的白骨。第三天岳母安葬，當天的清晨，當我看見太陽照常升起，人們一天的生活照常開始時，使我感受到天地不會為誰也不會因任何事情而停止運轉。這一次的經歷，最直觀的告訴我什麼是人生無常，使我對死亡有了深刻的認識，但我還未真正了解死亡。這是我接下來需要深刻去認識去了解的重要課題。也許真正懂得死的意義，才知道如何生。\n五 孩子成長 這一年，是孩子從 6 個月到 18 個月的關鍵成長期，我見證了一個生命從學步到說話到懂得互動的生動變化。在孩子身上，同樣是太太付出的最多，使我明白一個生命需要很多愛的澆灌才可以自由茁壯的成長。也使我懂得，父母是孩子眼中最大的依靠，兩者缺一不可，一個和諧的家庭是孩子成長的最大心靈營養。王德峰老師說過一句話：「當你孩子出生的那一刻，同時孩子也將你作為父親或母親生了出來」。這句話深富哲理，在我自己身上也深有體會。所有事情都是對立統一的，人生來就不是一個父親，是另一個生命的出現及彼此互動與反射才能促成一個合格父親的產生。\n六 家庭關系 母親過來後，很欣慰太太與母親相處的很融洽，每當我看到她們兩人有說有笑和和睦睦的合作去做一頓好吃的飯菜時，內心非常的欣喜與感動。這一年，我和太太彼此也多了一分理解與包容，彼此比之前更加關愛對方。我更加理解她照顧孩子與處理家庭瑣事的辛苦，及痛失母親的傷痛。他也更加關心我的健康，工作及情緒活動。兩個人一起經歷世事，有難處一起去面對，有幸福一起分享，便多了一分信心與力量。岳母去世後，我們便決定將岳父接來準備長期住下來一起生活。我們到現在相處的都不錯，但每個家庭成員都需要慢慢調整以使對大家庭及孩子成長更加有利。在家庭關系中，太太組織的定期家庭會議是一項不錯的家庭矛盾調和劑。只有多分享內心想法，才能互相體諒與解決問題。\n七 修行進階 這一年，自己的身心狀態及修行體悟均有一定的提升。從時時被欲望驅使而不自知，到慢慢了解意識的不可靠，再到有一定的定力及看清事物本質的微微覺照力，這些成長都是從生活中一件件的事情上磨礪體悟到的。感恩張慶祥講師及同修的遠程教授及勉勵，這是我今生莫大的緣分。這一年，我無一日敢懈怠的堅持修習站樁打坐等日課，學習反觀內心，越來越篤定的相信人人本心的光明。人生的至上意義即是尋回自性的光明，「路漫漫其修遠兮，吾將上下而求索」！\n2021 年 12 月 31 日於大連\n","permalink":"https://olzhy.github.io/posts/2021-summary.html","tags":["随笔"],"title":"2021 年終總結"},{"categories":["计算机"],"contents":"Podman，即 Pod Manager 的缩写，是一个无守护进程容器引擎，用于在 Linux 系统上开发、管理及运行 OCI 容器。\nPodman 兼容 Docker 命令行，只要敲一句 alias docker=podman，即可以使用 Docker 的方式无缝使用 Podman。Podman 提供一组 REST API 供远程应用按需启动容器，该 API 同样兼容 Docker API，支持 Docker Compose 与 Podman 进行交互。\nPodman 管理的容器可以 root 或非特权用户的方式运行。Podman 管理整个容器生态，包括容器镜像、容器、容器卷，及 Pod 等。\nPodman 服务仅可运行在 Linux 平台上，但支持在 MacOS 及 Windows 平台上运行 Podman REST API 客户端，其通过 SSH 的方式与运行在 Linux 主机或虚拟机的 Podman 服务进行通信。\n本文接下来对 Podman 进行安装及简单的使用。\n1 Podman 安装 1.1 MacOS\n从上面知道 Podman 是一个在 Linux 上运行容器的工具，在 MacOS 上可以使用 Podman 客户端，这样即可使用其访问虚拟机或远程运行的 Linux 主机。可以使用 podman machine 命令来管理虚拟机。\n进入 Podman Releases 页面下载最新版 Darwin Podman（podman-remote-release-darwin.zip）。将压缩文件解压后，将可执行文件 podman 拷贝到 /user/local/bin/ 目录下，这样即可在任意目录使用。\n启动虚拟机：\n$ podman machine init $ podman machine start 查看版本信息：\n$ podman version 1.2 Linux\n若是 CentOS，直接使用 yum 进行安装：\n$ sudo yum -y install podman 其它发行版，可查阅官方文档进行安装。\n2 Podman 简单使用 2.1 查看使用帮助\n查看使用帮助文档：\n$ podman --help $ podman \u0026lt;subcommand\u0026gt; --help # 查看子命令使用帮助 2.2 镜像检索、拉取及推送等\n检索 nginx 镜像：\n$ podman search nginx # 检索nginx镜像 $ podman search nginx --filter=is-official # 检索nginx官方镜像 拉取镜像：\n$ podman pull docker.io/library/nginx # 拉取nginx镜像 Trying to pull docker.io/library/nginx:latest... ... 给镜像打 TAG：\n$ podman tag docker.io/library/nginx:latest docker.io/olzhy/nginx:v1.0 # 打 TAG，注意由 library 下打到了自己名下 $ podman images # 查看本地镜像 REPOSITORY TAG IMAGE ID CREATED SIZE docker.io/library/nginx latest 87a94228f133 2 weeks ago 138 MB docker.io/olzhy/nginx v1.0 87a94228f133 2 weeks ago 138 MB 登录 docker.io，推送镜像至个人仓库：\n$ podman login docker.io # 输入 Docker Hub 账号密码，登录 docker.io $ podman push docker.io/olzhy/nginx:v1.0 # 推送 nginx:v1.0 至个人仓库 移除本地镜像：\n$ podman rmi docker.io/olzhy/nginx:v1.0 2.3 运行一个容器\n运行 一个 nginx 容器：\n$ podman run --name mynginx -d -p 8080:80 docker.io/library/nginx # 指定名称，端口映射，以 Detached 方式运行 nginx 容器 查看已创建的或运行中的容器：\n$ podman ps # 加 -a 参数输出包含已退出的容器 CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 77b9c633c895 docker.io/library/nginx:latest nginx -g daemon o... 15 hours ago Up 15 hours ago 0.0.0.0:8080-\u0026gt;80/tcp mynginx 使用 podman inspect 检查容器：\n$ podman inspect mynginx | grep IPAddress 在本机访问 nginx：\n$ curl http://localhost:8080 查看容器访问日志：\n$ podman logs mynginx 10.88.0.2 - - [28/Oct/2021:11:54:29 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;curl/7.64.1\u0026#34; \u0026#34;-\u0026#34; 也可以选择在容器内执行一条命令：\n$ podman exec -it mynginx curl http://localhost # 在容器内访问 nginx 或者直接进入容器，执行一些操作：\n$ podman exec -it mynginx bash # 进入容器 可以拷贝容器内的文件到本机：\n$ podman cp mynginx:/etc/nginx/nginx.conf ~/Downloads 使用 podman top 查看容器内的进程及 CPU 占用率：\n$ podman top mynginx USER PID PPID %CPU ELAPSED TTY TIME COMMAND root 1 0 0.000 8m38.589550934s ? 0s nginx: master process nginx -g daemon off; nginx 26 1 0.000 8m37.590199848s ? 0s nginx: worker process 重启或停止容器：\n$ podman restart mynginx $ podman stop mynginx 不再使用时，可以使用 podman rm 移除容器：\n$ podman rm mynginx # 加 --force 参数可以强行移除容器 2.4 自定义镜像的构建及运行\n假设我们想修改 nginx 首页的内容，使用如下命令新建一个自定义的 index.html：\n$ echo \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;This is my app\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34; \u0026gt; index.html 新建一个 Dockerfile 文件，内容如下：\nFROM docker.io/library/nginx COPY index.html /usr/share/nginx/html/ 基于该 Dockerfile 构建一个 myapp 镜像：\n$ podman build -t myapp -f ./Dockerfile . 然后运行它：\n$ podman run --name myapp -d -p 8081:80 myapp 使用 curl 访问本机，即看到首页内容变了：\n$ curl http://localhost:8081 \u0026lt;html\u0026gt;\u0026lt;body\u0026gt;This is my app\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; 综上，本文对 Podman 作了初探。\n 参考资料\n[1] What is Podman?\n[2] Podman Introduction\n[3] Getting Started with Podman\n ","permalink":"https://olzhy.github.io/posts/podman-getting-started.html","tags":["云原生"],"title":"容器引擎 Podman 初探"},{"categories":["计算机"],"contents":"PGExercises.com 是一个非常不错的 PostgreSQL 在线实践网站。该网站基于一个简单的数据集，设立各类题目，我们可以通过回答这些问题来复习 SQL 知识。\n该站的题目涉及“简单查询及 WHERE 条件”，“连接及 CASE 语句”，“聚集函数，窗口函数及递归查询”等多个门类，是一个不错的测试所学知识的地方。\n下面简单介绍一下该站用到的数据集。\n该数据集针对的是一个刚成立的乡村俱乐部的：有一组会员，一组体育设施，及这些体育设施的预定记录。\n先看一下members表：\n有 ID，基础信息，推荐人 ID，及加入时间等。\nCREATE TABLE cd.members ( memid INTEGER NOT NULL, -- 会员ID  surname CHARACTER VARYING(200) NOT NULL, -- 姓  firstname CHARACTER VARYING(200) NOT NULL, -- 名  address CHARACTER VARYING(300) NOT NULL, -- 地址  zipcode INTEGER NOT NULL, -- 邮政编码  telephone CHARACTER VARYING(20) NOT NULL, -- 电话  recommendedby INTEGER, -- 推荐人  joindate TIMESTAMP NOT NULL, -- 加入时间  CONSTRAINT members_pk PRIMARY KEY (memid), CONSTRAINT fk_members_recommendedby FOREIGN KEY (recommendedby) REFERENCES cd.members(memid) ON DELETE SET NULL ); 接下来，看一下facilities表：\n该表列出可供预定的设施，包含设施 ID，设施名称，会员预定花销，游客预定花销等。\nCREATE TABLE cd.facilities ( facid integer NOT NULL, -- 设施ID  name character varying(100) NOT NULL, -- 设施名称  membercost numeric NOT NULL, -- 会员预定花销  guestcost numeric NOT NULL, -- 游客预定花销  initialoutlay numeric NOT NULL, monthlymaintenance numeric NOT NULL, CONSTRAINT facilities_pk PRIMARY KEY (facid) ); 最后，看一下bookings表：\n该表用于追踪各设施的预定情况，包含设施 ID，预定会员 ID，开始预定时间，及预定了多少个半小时的 slots 等。\nCREATE TABLE cd.bookings ( bookid integer NOT NULL, facid integer NOT NULL, -- 设施ID  memid integer NOT NULL, -- 会员ID  starttime timestamp NOT NULL, -- 开始预定时间  slots integer NOT NULL, -- 预定了多少个半小时  CONSTRAINT bookings_pk PRIMARY KEY (bookid), CONSTRAINT fk_bookings_facid FOREIGN KEY (facid) REFERENCES cd.facilities(facid), CONSTRAINT fk_bookings_memid FOREIGN KEY (memid) REFERENCES cd.members(memid) ); 这三张表的关系如下图所示。\n介绍完数据集，下面就开始我们的练习吧。\n1 简单 SQL 查询 该栏目考察 SQL 基础，题目涵盖 SELECT, WHERE, CASE, UNION 等。\n1 控制取哪些行\n问题描述：\n生成一个设备列表，这些设备对会员收费，且所收的费用不足月度维护费用的 50 分之一。该列表返回设备的 ID，名称，会员费，月度维护费用。\n问题答案：\nSELECT facid, name, membercost, monthlymaintenance FROM cd.facilities WHERE membercost \u0026gt; 0 AND membercost \u0026lt; monthlymaintenance/50; 2 将结果分类\n问题描述：\n生成一个设备列表，若月度维护费用大于 100 就标记为expensive，否则标记为cheap。返回相关设施的名称和月度维护情况。\n问题答案：\nSELECT name, CASE WHEN monthlymaintenance \u0026gt; 100 THEN \u0026#39;expensive\u0026#39; ELSE \u0026#39;cheap\u0026#39; END AS cost FROM cd.facilities; 3 日期处理\n问题描述：\n生成一个会员列表，返回 2012 年 9 月及之后加入的会员。返回会员的 memid，surname，firstname 及 joindate。\n问题答案：\nSELECT memid, surname, firstname, joindate FROM cd.members WHERE joindate \u0026gt;= \u0026#39;2012-09-01\u0026#39;; 4 重复项移除及结果排序\n问题描述：\n生成一个排序后的前 10 位会员的姓氏列表，且不要有重复。\n问题答案：\nSELECT DISTINCT surname FROM cd.members ORDER BY surname LIMIT 10; 5 组合多个查询的结果\n问题描述：\n出于某种原因，您需要一个包含所有姓氏和所有设施名称的组合列表。请生成这个列表。\n问题答案：\n注意使用UNION会移除重复项，而UNION ALL并不会。\nSELECT surname FROM cd.members UNION SELECT name FROM cd.facilities; 6 聚集函数使用\n问题描述：\n您想获取最后一个加入的会员的名字，姓氏，加入时间。该如何做？\n问题答案：\n使用子查询实现。\nSELECT firstname, surname, joindate FROM cd.members WHERE joindate = ( SELECT max(joindate) FROM cd.members); 2 连接及子查询 该栏目主要考察关系型数据库的基础——连接。题目涵盖内连接，外连接，自连接，子查询。\n1 获取会员的预定开始时间\n问题描述：\n获取会员名字为“David Farrell”的预定开始时间。\n问题答案：\n有两种实现方式，一种采用内连接，一种采用子查询。内连接又有两种写法。\na）内连接实现\nSELECT b.starttime FROM cd.bookings b, cd.members m WHERE b.memid = m.memid AND firstname = \u0026#39;David\u0026#39; AND surname = \u0026#39;Farrell\u0026#39;; -- 另一种写法 SELECT b.starttime FROM cd.bookings b INNER JOIN cd.members m ON b.memid = m.memid WHERE firstname = \u0026#39;David\u0026#39; AND surname = \u0026#39;Farrell\u0026#39;; b）子查询实现\nSELECT starttime FROM cd.bookings WHERE memid IN ( SELECT memid FROM cd.members WHERE firstname = \u0026#39;David\u0026#39; AND surname = \u0026#39;Farrell\u0026#39; ); 2 获取网球场的预定开始时间\n问题描述：\n获取2012-09-21这一天预定“Tennis Court”的开始时间列表。返回开始时间及设备名称，按开始时间排序。\n问题答案：\nSELECT b.starttime, f.name FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid AND f.name LIKE \u0026#39;Tennis Court%\u0026#39; AND date(b.starttime) = \u0026#39;2012-09-21\u0026#39; ORDER BY b.starttime; 3 获取推荐过其他会员的所有会员列表\n问题描述：\n获取推荐过其他会员的所有会员列表，确保结果不含重复项，且结果以姓和名排序。\n问题答案：\n采用自连接实现，采用 DISTINCT 去重。\nSELECT DISTINCT m2.firstname, m2.surname FROM cd.members m1, cd.members m2 WHERE m1.recommendedby = m2.memid ORDER BY m2.surname, m2.firstname; 4 获取所有会员及其推荐人\n问题描述：\n获取所有会员及其推荐人（如果有的话），确保结果以姓和名排序。\n问题答案：\nSELECT m1.firstname, m1.surname, m2.firstname, m2.surname FROM cd.members m1 LEFT OUTER JOIN cd.members m2 ON m1.recommendedby = m2.memid ORDER BY m1.surname, m1.firstname; 5 列出所有使用过网球场的会员\n问题描述：\n找出使用过网球场的所有会员的列表。输出包含网球场名，合为一列的会员姓名。确保没有重复数据，并按会员姓名后跟设施名称排序。\n问题答案：\nSELECT DISTINCT (m.firstname || \u0026#39; \u0026#39; || m.surname) AS member, f.name AS facility FROM cd.bookings b, cd.members m, cd.facilities f WHERE b.facid = f.facid AND b.memid = m.memid AND f.name LIKE \u0026#39;Tennis Court%\u0026#39; ORDER BY member, facility; 6 生成一份昂贵的预订清单\n问题描述：\n生成2012-09-14这一天会员或游客花费超过 30 元的预订清单。\n注意：游客和会员的预定费用不同，且游客的 ID 始终为 0。输出中包括设施名称，会员姓名及预定费用。结果按费用降序排序，且不使用任何子查询。\n问题答案：\nSELECT (m.firstname || \u0026#39; \u0026#39; || m.surname) AS member, f.name AS facility, (CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) AS cost FROM cd.bookings b, cd.members m, cd.facilities f WHERE b.memid = m.memid AND b.facid = f.facid AND date(b.starttime) = \u0026#39;2012-09-14\u0026#39; AND ((b.memid = 0 AND b.slots * f.guestcost \u0026gt; 30) OR (b.memid != 0 AND b.slots * f.membercost \u0026gt; 30)) ORDER BY cost DESC; 7 使用子查询生成一份昂贵的预订清单\n问题描述：\n对于上一个问题，实现的有点啰嗦：我们必须在 WHERE 子句和 CASE 语句中两次计算预订成本。尝试使用子查询简化此计算。\n问题答案：\nSELECT * FROM (SELECT (m.firstname || \u0026#39; \u0026#39; || surname) AS member, f.name AS facility, (CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) AS cost FROM cd.bookings b, cd.members m, cd.facilities f WHERE b.memid = m.memid AND b.facid = f.facid AND date(b.starttime) s \u0026#39;2012-09-14\u0026#39;) AS t WHERE t.cost \u0026gt; 30 ORDER BY t.cost DESC; 8 不使用连接生成所有成员及其推荐人列表\n问题描述：\n不使用任何连接的情况下输出所有成员的列表，包括其推荐人（如果有的话）。确保列表中没有重复项，且名字姓氏对被格式化为一列并有序。\n问题答案：\nSELECT DISTINCT (m.firstname || \u0026#39; \u0026#39; || m.surname) AS member, (SELECT (firstname || \u0026#39; \u0026#39; || surname) AS recommender FROM cd.members WHERE memid = m.recommendedby) FROM cd.members m ORDER BY member; 3 数据修改 本栏目涉及插入、更新和删除。像这样更改数据的操作统称为 DML（数据操作语言）。\n1 单行插入\n问题描述：\n俱乐部正在增加一个新设施——SPA。我们需要将它添加到设施表中。值如下。\nfacid: 9, name: \u0026#39;Spa\u0026#39;, membercost: 20, guestcost: 30, initialoutlay: 100000, monthlymaintenance: 800 问题答案：\n可以显示指定列名，也可以省略列名 \u0008 按建表字段顺序插入。\nINSERT INTO cd.facilities (facid, name, membercost, guestcost, initialoutlay, monthlymaintenance) VALUES (9, \u0026#39;Spa\u0026#39;, 20, 30, 100000, 800); -- 按照建表字段顺序插入 INSERT INTO cd.facilities VALUES (9, \u0026#39;Spa\u0026#39;, 20, 30, 100000, 800); 2 多行插入\n问题描述：\n使用一行命令一次加入多个设备。值如下。\nfacid: 9, name: \u0026#39;Spa\u0026#39;, membercost: 20, guestcost: 30, initialoutlay: 100000, monthlymaintenance: 800 facid: 10, name: \u0026#39;Squash Court 2\u0026#39;, membercost: 3.5, guestcost: 17.5, initialoutlay: 5000, monthlymaintenance: 80 问题答案：\nINSERT INTO cd.facilities (facid, name, membercost, guestcost, initialoutlay, monthlymaintenance) VALUES (9, \u0026#39;Spa\u0026#39;, 20, 30, 100000, 800), (10, \u0026#39;Squash Court 2\u0026#39;, 3.5, 17.5, 5000, 80); 3 计算后的数据插入\n问题描述：\n这一次不再指定设备 ID，而是自动计算下一个 facid 值。其它字段值如下。\nname: \u0026#39;Spa\u0026#39;, membercost: 20, guestcost: 30, initialoutlay: 100000, monthlymaintenance: 800. 问题答案：\nINSERT INTO cd.facilities (facid, name, membercost, guestcost, initialoutlay, monthlymaintenance) SELECT (SELECT max(facid)+1 FROM cd.facilities), \u0026#39;Spa\u0026#39;, 20, 30, 100000, 800; 4 根据现有内容作更新\n问题描述：\n我们想改变第二个网球场的价格，使其比第一个网球场贵 10%。尝试在不指定常量值的情况下执行此操作，以便我们可以根据需要重用该语句。\n问题答案：\nUPDATE cd.facilities SET membercost = 1.1 * membercost, guestcost = 1.1 * guestcost WHERE name = \u0026#39;Tennis Court 2\u0026#39;; 5 根据子查询作删除\n问题描述：\n删除所有从未预定过设施的成员。\n问题答案：\nDELETE FROM cd.members WHERE memid NOT IN ( SELECT DISTINCT memid FROM cd.bookings); -- 另一种实现是使用相关子查询 DELETE FROM cd.members m WHERE NOT EXISTS ( SELECT 1 FROM cd.bookings WHERE memid = m.memid); 4 聚合 聚合是一个让人能真正体会到关系型数据库强大能力的功能。该栏目深度覆盖聚合，使用标准分组以及最新的窗口函数来测试我们的掌握情况。\n1 计算各成员的推荐数\n问题描述：\n生成各成员的推荐数列表，以成员 ID 排序。\n问题答案：\nSELECT recommendedby, count(*) FROM cd.members WHERE recommendedby IS NOT NULL GROUP BY ecommendedby ORDER BY recommendedby; 2 列出每个设施的预订总段数\n问题描述：\n生成每个设施的预订总段数。输出设施 ID 和预定总段数，按设施 ID 排序。\n问题答案：\nSELECT facid, sum(slots) FROM cd.bookings GROUP BY facid ORDER BY facid; 3 列出给定月份每个设施的预订总段数\n问题描述：\n生成 2012 年 9 月每个设施的预订总段数。输出设施 ID 和预定总段数，按总段数排序。\n问题答案：\nSELECT facid, sum(slots) AS totalslots FROM cd.bookings WHERE starttime \u0026gt;= \u0026#39;2012-09-01\u0026#39; AND starttime \u0026lt; \u0026#39;2012-10-01\u0026#39; GROUP BY facid ORDER BY totalslots; 4 列出每个设施每月的预订总段数\n问题描述：\n生成 2012 年每个设施每月的预订总段数。输出设施 ID 和预定总段数，按设施 ID 和月份排序。\n问题答案：\nSELECT facid, extract(month from starttime) AS month, sum(slots) FROM cd.bookings WHERE starttime \u0026gt;= \u0026#39;2012-01-01\u0026#39; AND starttime \u0026lt; \u0026#39;2013-01-01\u0026#39; GROUP BY facid, month ORDER BY facid, month; 5 列出预订已超过 1000 个段的设施\n问题描述：\n生成预订已超过 1000 个段的设施列表。输出设施 ID 和预定总段数，按设施 ID 排序。\n问题答案：\n使用HAVING来过滤聚合后的结果。WHERE用于聚合前的数据筛选，而HAVING用于聚合后的数据筛选，这即是两者的区别。\nSELECT facid, sum(slots) FROM cd.bookings GROUP BY facid HAVING sum(slots) \u0026gt; 1000 ORDER BY facid; 6 列出每个设施的总收入\n问题描述：\n列出每个设施的总收入。输出应包括设施名和总收入，按总收入排序。记住，游客和会员的计费是不同的。\n问题答案：\nSELECT f.name, sum(CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) AS revenue FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY f.name ORDER BY revenue; 7 列出总收入低于 1000 的设施\n问题描述：\n列出总收入小于 1000 的设施列表。输出包括设施名称和总收入，按收入排序。记住，游客和会员的计费是不同的。\n问题答案：\nSELECT * FROM (SELECT f.name, sum(CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) AS revenue FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY f.name) AS t WHERE t.revenue \u0026lt; 1000 ORDER BY revenue; 注意如下写法是错误的：\nSELECT f.name, sum( CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) AS revenue FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY f.name HAVING revenue \u0026lt; 1000 -- PostgreSQL不允许在HAVING中直接使用列名 ORDER BY revenue; 8 输出预订段数最多的设施 ID\n问题描述：\n输出预订段数最多的设施 ID。尝试不使用LIMIT来实现（看起来可能会乱一点）。\n问题答案：\n第一种写法，看起来笨一些。\nSELECT facid, sum(slots) FROM cd.bookings GROUP BY facid HAVING sum(slots) = ( SELECT max(totalslots) FROM (SELECT facid, sum(slots) AS totalslots FROM cd.bookings GROUP BY facid) AS t); 第二种写法，使用WITH表达式提取出公用部分，更紧凑。\nWITH t AS ( SELECT facid, sum(slots) AS totalslots FROM cd.bookings GROUP BY facid) SELECT * FROM t WHERE totalslots = ( SELECT max(totalslots) FROM t); 9 输出每个设施的预订总小时数\n问题描述：\n输出每个设施的预订总小时数，注意一个时段为半小时。输出应包含设施 ID、设施名称和预订小时数，按设施 ID 排序。尝试将小时数格式化为两位小数。\n问题答案：\nPostgreSQL 默认是整除的，若需采用浮点除法，需要显式指定一下。\nSELECT b.facid, f.name, round(sum(b.slots)::numeric/2::numeric, 2) FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY b.facid, f.name ORDER BY b.facid; 10 列出每位会员在 2012 年 9 月 1 日之后的首次预订\n问题描述：\n列出每位会员的姓名、ID 和他们在 2012 年 9 月 1 日之后的第一次设施预订时间。按会员 ID 排序。\n问题答案：\nSELECT m.surname, m.firstname, b.memid, min(b.starttime) FROM cd.bookings b, cd.members m WHERE b.memid = m.memid AND b.starttime \u0026gt;= \u0026#39;2012-09-01\u0026#39; GROUP BY m.surname, m.firstname, b.memid ORDER BY b.memid; 11 生成会员名称列表，每行包含会员总数\n问题描述：\n生成会员（包括游客）名称列表，每行包含会员总数。按加入日期排序。\n问题答案：\n使用窗口函数实现。\nSELECT count(*) over (), firstname, surname FROM cd.members ORDER BY joindate; 12 生成一份带编号的会员名单\n问题描述：\n生成一份会员（包括游客）的单调递增编号列表，按加入日期排序。注意，不保证会员 ID 是连续的。\n问题答案：\n使用窗口函数实现。\nSELECT row_number() OVER (ORDER BY joindate), firstname, surname FROM cd.members ORDER BY joindate; 13 查找前三大创收设施\n问题描述：\n列出前三个创收设施（包含排名相同的）。输出设施名称和排名，按排名和设施名称排序。\n问题答案：\nSELECT * FROM (SELECT f.name, rank() OVER (ORDER BY sum( CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) DESC) AS rank FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY f.name) AS t WHERE t.rank \u0026lt;= 3; 14 按营收额对设施进行分类\n问题描述：\n根据营收额将设施等分为高、中和低三类。按分类和设施名称排序。\n问题答案：\n主要考察ntile窗口函数的使用，其会将值尽可能的等分为指定的分组数。\nSELECT name, ( CASE WHEN class = 1 THEN \u0026#39;high\u0026#39; WHEN class = 2 THEN \u0026#39;average\u0026#39; ELSE \u0026#39;low\u0026#39; END) AS revenue FROM (SELECT f.name, ntile(3) over(ORDER BY sum( CASE WHEN b.memid = 0 THEN b.slots * f.guestcost ELSE b.slots * f.membercost END) DESC) AS class FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY f.name) AS t ORDER BY class, name; 5 日期处理 本栏目涉及日期处理，详情请参阅PostgreSQL 日期时间函数文档。\n1 生成 2012 年 8 月 31 日凌晨 1 点的时间戳\n问题描述：\n生成 2012 年 8 月 31 日凌晨 1 点这个时间的时间戳。\n问题答案：\n有三种写法，前两种是 PostgreSQL 的语法，最后一种是 SQL 标准语法。\n-- 第一种写法 SELECT TIMESTAMP \u0026#39;2012-08-31 01:00:00\u0026#39;; -- 第二种写法 SELECT \u0026#39;2012-08-31 01:00:00\u0026#39;::TIMESTAMP; -- 第三种写法 SELECT CAST(\u0026#39;2012-08-31 01:00:00\u0026#39; AS TIMESTAMP); 2 时间戳相减\n问题描述：\n计算时间戳2012-08-31 01:00:00减去时间戳2012-07-30 01:00:00的结果。\n问题答案：\nSELECT TIMESTAMP \u0026#39;2012-08-31 01:00:00\u0026#39; - TIMESTAMP \u0026#39;2012-07-30 01:00:00\u0026#39;; 3 生成 2012 年 10 月的所有日期\n问题描述：\n生成 2012 年 10 月的所有日期。可以输出为时间戳（时间部分为00:00:00）或日期。\n问题答案：\n使用 PostgreSQL 的generate_series函数来生成时间序列。\nSELECT generate_series(TIMESTAMP \u0026#39;2012-10-01\u0026#39;, TIMESTAMP \u0026#39;2012-10-31\u0026#39;, INTERVAL \u0026#39;1 day\u0026#39;); 4 从时间戳获取其属于月份中的哪一天\n问题描述：\n从时间戳2012-08-31中获取其属于月份中的第几天。\n问题答案：\n使用date_part或extract函数实现。\n-- 写法一 SELECT date_part(\u0026#39;day\u0026#39;, TIMESTAMP \u0026#39;2012-08-31\u0026#39;); -- 写法二 SELECT extract(day FROM TIMESTAMP \u0026#39;2012-08-31\u0026#39;); 5 计算时间戳之间的秒数\n问题描述：\n计算时间戳2012-08-31 01:00:00和2012-09-02 00:00:00之间的秒数。\n问题答案：\n-- 手动实现方式 SELECT extract(day FROM t.int) * 24 * 60 * 60 + extract(hour FROM t.int) * 60 * 60 + extract(minute FROM t.int) * 60 + extract(second FROM t.int) FROM (SELECT age(TIMESTAMP \u0026#39;2012-09-02 00:00:00\u0026#39;, TIMESTAMP \u0026#39;2012-08-31 01:00:00\u0026#39;) AS int) AS t; -- 使用PostgreSQL函数 SELECT extract(epoch FROM age(TIMESTAMP \u0026#39;2012-09-02 00:00:00\u0026#39;, TIMESTAMP \u0026#39;2012-08-31 01:00:00\u0026#39;)); 6 输出 2012 年每个月的天数\n问题描述：\n输出 2012 年的每个月及该月的天数。\n问题答案：\nSELECT extract(month FROM t.month) AS month, (t.month + INTERVAL \u0026#39;1 month\u0026#39;) - t.month AS length FROM (SELECT generate_series(DATE \u0026#39;2012-01-01\u0026#39;, DATE \u0026#39;2012-12-31\u0026#39;, interval \u0026#39;1 month\u0026#39;) AS month) AS t; 7 计算给定月的剩余天数\n问题描述：\n给定时间戳2012-02-11 01:00:00，计算其对应月的剩余天数（不论给定的时间戳是几点，都应算作剩余的一整天）。\n问题答案：\nSELECT date_trunc(\u0026#39;month\u0026#39;, t.ts) + INTERVAL \u0026#39;1 month\u0026#39; - date_trunc(\u0026#39;day\u0026#39;, t.ts) FROM (SELECT TIMESTAMP \u0026#39;2012-02-11 01:00:00\u0026#39; AS ts) AS t; 8 计算预订的结束时间\n问题描述：\n在系统中返回最近 10 个预订的开始和结束时间，先按结束时间排序，然后按开始时间排序。\n问题答案：\nSELECT starttime, starttime + slots * (interval \u0026#39;0.5 hour\u0026#39;) AS endtime FROM cd.bookings ORDER BY endtime DESC, starttime DESC LIMIT 10; 9 返回每个月的预订数\n问题描述：\n返回每个月的预订数，结果按月排序。\n问题答案：\nSELECT date_trunc(\u0026#39;month\u0026#39;, starttime) AS month, count(*) FROM cd.bookings GROUP BY month ORDER BY month; 10 按月计算每个设施的利用率\n问题描述：\n按月计算每个设施的利用率，按名称和月份排序，四舍五入到小数点后一位。开门时间是早上 8 点，关门时间是晚上 8:30。您可以将每个月视为整月，无论俱乐部是否有某些日期未开放。\n问题答案：\n每天开门的时间是12.5 * 2个半小时，所以每个设备当月的预定总段数除以这个数就是当月的利用率。\nSELECT name, month, round((totalslots / (extract(day FROM (month + interval \u0026#39;1 month\u0026#39;) - month) * 12.5 * 2) * 100)::NUMERIC, 1) AS utilization FROM (SELECT f.name, date_trunc(\u0026#39;month\u0026#39;, b.starttime) AS month, sum(b.slots) AS totalslots FROM cd.bookings b, cd.facilities f WHERE b.facid = f.facid GROUP BY f.name, MONTH) AS t ORDER BY name, month; 6 字符串操作 本栏目涉及基础字符串操作，LIKE使用，正则表达式使用。详情请参阅PostgreSQL 正则匹配文档。\n1 格式化会员名称\n问题描述：\n输出所有会员的名字，格式为Surname, Firstname。\n问题答案：\nSELECT surname || \u0026#39;, \u0026#39; || firstname FROM cd.members; 2 按名称前缀查找设施\n问题描述：\n查找名称以Tennis开头的所有设施。输出所有列。\n问题答案：\nLIKE中%用于匹配任何字符串，而_用于匹配任何单个字符。\nSELECT * FROM cd.facilities WHERE name LIKE \u0026#39;Tennis%\u0026#39;; 3 执行不区分大小写的搜索\n问题描述：\n不区分大小写以查找名称以tennis开头的所有设施。输出所有列。\n问题答案：\n-- SQL标准写法 SELECT * FROM cd.facilities WHERE LOWER(name) LIKE \u0026#39;tennis%\u0026#39;; -- PostgreSQL独有，使用ILIKE SELECT * FROM cd.facilities WHERE name ILIKE \u0026#39;tennis%\u0026#39;; 4 查找带括号的电话号码\n问题描述：\n您可能已经注意到俱乐部的会员表中的电话号码格式很不一致。查找所有包含括号的电话号码，返回会员 ID 和电话号码，按会员 ID 排序。\n问题答案：\nPostgreSQL 有三种字符串匹配方法：LIKE，SIMILAR TO，及 POSIX 正则表达式。\nSIMILAR TO与LIKE类似，只是其采用 SQL 正则表达式，是一种 LIKE 与 POSIX 正则表达式的结合体。SIMILAR TO不像常规正则表达式一样可以匹配子字符串，其与LIKE一样，想匹配成成功，必须匹配整个字符串。SIMILAR TO与LIKE一样，分别使用_及%表示任意单个字符及任意字符串，而.在SIMILAR TO中不表示任意单个字符。\n-- 使用LIKE SELECT memid, telephone FROM cd.members WHERE telephone LIKE \u0026#39;(%)%\u0026#39;; -- ~~与LIKE等价 SELECT memid, telephone FROM cd.members WHERE telephone ~~ \u0026#39;(%)%\u0026#39;; -- 使用SIMILAR TO SELECT memid, telephone FROM cd.members WHERE telephone SIMILAR TO \u0026#39;\\(%\\)%\u0026#39;; -- 采用POSIX正则表达式 SELECT memid, telephone FROM cd.members WHERE telephone ~ \u0026#39;^\\(\\d*\\)\\s\\d{3}-\\d{4}$\u0026#39;; 5 用前导零填充邮政编码\n问题描述：\n由于存储时zipcode为数值类型，我们示例数据集中的邮政编码已经从它们中删除了前导零。从成员表中检索所有邮政编码，用前导零填充任何少于 5 个字符的邮政编码。\n问题答案：\n-- 使用lpad函数 SELECT lpad(cast(zipcode as char(5)), 5, \u0026#39;0\u0026#39;) FROM cd.members; -- 使用tochar SELECT to_char(zipcode, \u0026#39;FM09999\u0026#39;) FROM cd.members; 6 计算姓氏以每个字母开头的会员数量\n问题描述：\n计算会员姓氏分别以各字母开头的数量。按字母排序，如果计数为 0，就不要打印这个字母。\n问题答案：\n-- 使用substr SELECT substr(surname, 1, 1) AS firstletter, count(*) FROM cd.members GROUP BY firstletter ORDER BY firstletter; -- 使用left SELECT left(surname, 1) AS firstletter, count(*) FROM cd.members GROUP BY firstletter ORDER BY firstletter; -- 使用substring SELECT substring(surname FROM \u0026#39;#\u0026#34;_#\u0026#34;%\u0026#39; FOR \u0026#39;#\u0026#39;) AS firstletter, count(*) FROM cd.members GROUP BY firstletter ORDER BY firstletter; 7 清理电话号码\n问题描述：\n数据库中的电话号码格式非常不一致。您想打印会员 ID 和删除'-'、'('、')'，及' \u0026lsquo;字符后的号码。按会员 ID 排序。\n问题答案：\n使用 regexp_replace 函数实现。\nSELECT memid, regexp_replace(telephone, \u0026#39;[\\s\\-\\(\\)]\u0026#39;, \u0026#39;\u0026#39;, \u0026#39;g\u0026#39;) FROM cd.members ORDER BY memid; 7 递归查询 本栏目涉及递归查询。在 PostgreSQL，可以使用WITH RECURSIVE进行递归查询。这对处理树和图结构数据非常实用。详情请参阅WITH Queries。\n1 追溯会员的上游推荐链\n问题描述：\n寻找会员 ID 为 27 的上游推荐链：即寻找会员 ID 为 27 的推荐人，会员 ID 为 27 的推荐人的推荐人，以此类推。返回会员 ID、名字和姓氏。\n问题答案：\n使用WITH RECURSIVE表达式实现。\nWITH RECURSIVE recommenders(id) AS ( SELECT recommendedby FROM cd.members WHERE memid = 27 UNION ALL SELECT recommendedby FROM cd.members m, recommenders r WHERE m.memid = r.id ) SELECT r.id, m.firstname, m.surname FROM recommenders r, cd.members m WHERE r.id = m.memid; 2 追溯会员的下游推荐链\n问题描述：\n寻找会员 ID 为 1 的下游推荐链：即寻找 ID 为 1 的会员推荐了哪些人，ID 为 1 的会员推荐的这些人又推荐了哪些人，以此类推。返回会员 ID、名字和姓氏，按会员 ID 排序。\n问题答案：\n使用WITH RECURSIVE表达式实现。\nWITH RECURSIVE recommendeds(id) AS ( SELECT memid FROM cd.members WHERE recommendedby = 1 UNION ALL SELECT m.memid FROM cd.members m, recommendeds r WHERE m.recommendedby = r.id ) SELECT r.id, m.firstname, m.surname FROM recommendeds r, cd.members m WHERE r.id = m.memid ORDER BY id;  参考资料\n[1]postgresql exercises\n[2]postgresql date/time functions and operators\n[3]postgresql pattern matching\n[4]postgresql with queries (common table expressions)\n ","permalink":"https://olzhy.github.io/posts/postgres-exercises.html","tags":["PostgreSQL"],"title":"PostgreSQL 基础知识在线练习"},{"categories":["计算机"],"contents":"上一篇已经安装好了 PostgreSQL 环境，本篇会在其上使用 SQL 做一些简单的操作。\n1 基础 SQL 操作 a) 建表\n建两张表：一张是天气表（weather），记录各个城市每天的温度与降水量；一张是城市表（cities），记录城市的坐标。PostgreSQL 推荐关键字采用大写格式，字段名及类型采用小写格式。\n如下为建表语句：\nCREATE TABLE weather ( city varchar(80), -- city name (城市名)  temp_low int, -- low temperature (最低温度)  temp_high int, -- high temperature (最高温度)  prcp real, -- precipitation (降水量)  date date -- date (日期) ); CREATE TABLE cities ( name varchar(80), -- city name (城市名)  location point -- point为PostgreSQL特有类型，该字段表示地理坐标(经度, 纬度) ); b) 插值\n采用如下语句分别为weather表及cities表插入数据。\nINSERT INTO weather (city, temp_low, temp_high, prcp, date) VALUES (\u0026#39;Beijing\u0026#39;, 18, 32, 0.25, \u0026#39;2021-05-19\u0026#39;), (\u0026#39;Beijing\u0026#39;, 20, 30, 0.0, \u0026#39;2021-05-20\u0026#39;), (\u0026#39;Dalian\u0026#39;, 16, 24, 0.0, \u0026#39;2021-05-21\u0026#39;); INSERT INTO cities (name, location) VALUES (\u0026#39;Beijing\u0026#39;, \u0026#39;(116.3, 39.9)\u0026#39;), (\u0026#39;Shanghai\u0026#39;, \u0026#39;(121.3, 31.1)\u0026#39;); c) 简单查询\n在被选列上使用表达式(temp_low + temp_high) / 2，返回城市每天的平均温度。\nSELECT city, (temp_low + temp_high) / 2 AS temp_avg, date FROM weather; city | temp_avg | date ---------+----------+------------ Beijing | 25 | 2021-05-19 Beijing | 25 | 2021-05-20 Dalian | 20 | 2021-05-21 (3 rows) 使用WHERE条件，筛选城市为 Beijing 且降水量大于 0 的记录。\nSELECT * FROM weather WHERE city = \u0026#39;Beijing\u0026#39; AND prcp \u0026gt; 0.0; city | temp_low | temp_high | prcp | date ---------+----------+-----------+------+------------ Beijing | 18 | 32 | 0.25 | 2021-05-19 (1 row) 在被选列上使用DISTINCT关键字，筛选出去重后的城市名，并使用ORDER BY关键字按城市名字段正序返回结果。\nSELECT DISTINCT city FROM weather ORDER BY city; city --------- Beijing Dalian (2 rows) d) 连表查询\n内连接：将weather表及cities表进行内连接（取两表中城市名相同的所有行），返回城市在对应日期的的温度，降水量及地理位置。\nSELECT w.city, w.temp_low, w.temp_high, w.prcp, c.location, w.date FROM weather w, cities c WHERE w.city = c.name; -- 两种写法等价 SELECT w.city, w.temp_low, w.temp_high, w.prcp, c.location, w.date FROM weather w INNER JOIN cities c ON (w.city = c.name); city | temp_low | temp_high | prcp | location | date ---------+----------+-----------+------+--------------+------------ Beijing | 18 | 32 | 0.25 | (116.3,39.9) | 2021-05-19 Beijing | 20 | 30 | 0 | (116.3,39.9) | 2021-05-20 (2 rows) 左外连接：将weather表及cities表进行左外连接（返回左表所有行，若左表的某行在右表没有匹配行，则补空值），返回城市在对应日期的的温度，降水量及地理位置。\nSELECT w.city, w.temp_low, w.temp_high, w.prcp, c.location, w.date FROM weather w LEFT OUTER JOIN cities c ON (w.city = c.name); city | temp_low | temp_high | prcp | location | date ---------+----------+-----------+------+--------------+------------ Beijing | 18 | 32 | 0.25 | (116.3,39.9) | 2021-05-19 Beijing | 20 | 30 | 0 | (116.3,39.9) | 2021-05-20 Dalian | 16 | 24 | 0 | | 2021-05-21 (3 rows) 右外连接：将weather表及cities表进行右外连接（返回右表所有行，若右表的某行在左表没有匹配行，则补空值），返回城市在对应日期的的温度，降水量及地理位置。\nSELECT c.name, w.temp_low, w.temp_high, w.prcp, c.location, w.date FROM weather w RIGHT OUTER JOIN cities c ON (w.city = c.name); name | temp_low | temp_high | prcp | location | date ----------+----------+-----------+------+--------------+------------ Beijing | 20 | 30 | 0 | (116.3,39.9) | 2021-05-20 Beijing | 18 | 32 | 0.25 | (116.3,39.9) | 2021-05-19 Shanghai | | | | (121.3,31.1) | (3 rows) 全外连接：将weather表及cities表进行全外连接（返回两表的所有行，当一表的某行在另一表没有匹配行，则补空值），返回城市在对应日期的的温度，降水量及地理位置。\nSELECT (CASE WHEN w.city IS NOT NULL THEN w.city ELSE c.name END), w.temp_low, w.temp_high, w.prcp, c.location, w.date FROM weather w FULL OUTER JOIN cities c ON (w.city = c.name); name | temp_low | temp_high | prcp | location | date ----------+----------+-----------+------+--------------+------------ Beijing | 18 | 32 | 0.25 | (116.3,39.9) | 2021-05-19 Beijing | 20 | 30 | 0 | (116.3,39.9) | 2021-05-20 Dalian | 16 | 24 | 0 | | 2021-05-21 Shanghai | | | | (121.3,31.1) | (4 rows) 自连接：weather表与自己连接，找出同一城市，某一天的最低温度比另一天低的记录。\nSELECT w1.city, w1.temp_low, w1.date, w2.temp_low, w2.date FROM weather w1, weather w2 WHERE w1.city = w2.city AND w1.temp_low \u0026lt; w2.temp_low; city | temp_low | date | temp_low | date ---------+----------+------------+----------+------------ Beijing | 18 | 2021-05-19 | 20 | 2021-05-20 (1 row) e) 聚集函数使用\n聚集函数针对多行输入计算一个结果。\n下面，找出weather表中的历史最低温度。\nSELECT min(temp_low) FROM weather; min ----- 16 (1 row) 找出拥有这个历史最低温度的是哪个城市哪一天的记录。\n-- 使用子查询 SELECT city, temp_low, date FROM weather WHERE temp_low = (SELECT min(temp_low) FROM weather); -- 错误写法 聚集函数不允许在WHERE条件中使用 -- SELECT city FROM weather WHERE temp_low = min(temp_low); city | temp_low | date --------+----------+------------ Dalian | 16 | 2021-05-21 (1 row) 聚集函数结合GROUP BY找出每个城市的历史最低温度。\nSELECT city, min(temp_low) FROM weather GROUP BY city; city | min ---------+----- Dalian | 16 Beijing | 18 (2 rows) 进一步找出每个城市历史最低温度低于 17 的记录。\nSELECT city, min(temp_low) FROM weather GROUP BY city HAVING min(temp_low) \u0026lt; 17; city | min --------+----- Dalian | 16 (1 row) 从如上示例也看到了WHERE与HAVING使用场景的不同：WHERE用于分组和聚集函数使用前的输入行筛选；而HAVING用于分组和聚集函数使用后的分组行筛选。且WHERE语句中不可以使用聚集函数，而HAVING语句中一般总使用聚集函数（HAVING语句中不使用聚集函数的条件，不如直接将其移到WHERE语句中）。\n如，接着上面，筛选首字母为D的城市，并返回这些城市历史最低温度低于 17 的记录。\nSELECT city, min(temp_low) FROM weather WHERE city like \u0026#39;D%\u0026#39; GROUP BY city HAVING min(temp_low) \u0026lt; 17; -- 不要用这种写法 SELECT city, min(temp_low) FROM weather GROUP BY city HAVING min(temp_low) \u0026lt; 17 AND city like \u0026#39;D%\u0026#39;; city | min --------+----- Dalian | 16 (1 row) f) 更新及删除\n假定2021-05-20及之后的数据录入时温度均比实际值低了 1 度，可以使用如下UPDATE语句进行校正。\nUPDATE weather SET temp_low = temp_low + 1, temp_high = temp_high + 1 WHERE date \u0026gt;= \u0026#39;2021-05-20\u0026#39;; 重新查询数据。\nSELECT * FROM weather; city | temp_low | temp_high | prcp | date ---------+----------+-----------+------+------------ Beijing | 18 | 32 | 0.25 | 2021-05-19 Beijing | 21 | 31 | 0 | 2021-05-20 Dalian | 17 | 25 | 0 | 2021-05-21 (3 rows) 若不再需要城市名为Beijing的数据，可以使用DELETE语句进行删除。\nDELETE FROM weather WHERE city = \u0026#39;Beijing\u0026#39;; SELECT * FROM weather; city | temp_low | temp_high | prcp | date --------+----------+-----------+------+------------ Dalian | 17 | 25 | 0 | 2021-05-21 (1 row) 若整个表的数据都不需要了，确认无误后，可以进行删除。\nDELETE FROM weather; 2 高级特性 a) 视图\n针对上面的场景，若天气与城市坐标总是一起展示，则可以为其创建视图，其使用跟普通的表一样。视图有许多好处，如隐藏表的细节，可以随着应用演进而不必更改接口定义。当然还可以在视图上创建视图。\nCREATE VIEW myview AS SELECT w.city, w.temp_low, w.temp_high, w.prcp, c.location, w.date FROM weather w, cities c WHERE w.city = c.name; SELECT * FROM myview; b) 外健\n针对上述天气表weather与城市表cities，若我们要确保没有人可以在weather表插入cities表中不存在的城市的天气记录。这种约束即是保障数据的参照完整性，可以使用外健来实现。\n新的表定义如下：\nCREATE TABLE cities ( name varchar(80) primary key, location point ); CREATE TABLE weather ( city varchar(80) references cities(name), temp_low int, temp_high int, prcp real, date date ); 现在尝试对weather表插入一个新的城市的天气记录，会报错。\nINSERT INTO weather VALUES (\u0026#39;Tianjin\u0026#39;, 22, 30, 0.0, \u0026#39;2021-05-22\u0026#39;); ERROR: insert or update on table \u0026#34;weather\u0026#34; violates foreign key constraint \u0026#34;weather_city_fkey\u0026#34; DETAIL: Key (city)=(Tianjin) is not present in table \u0026#34;cities\u0026#34;. 在cities表补全该城市后，即可对weather进行插入。\nINSERT INTO cities VALUES (\u0026#39;Tianjin\u0026#39;, \u0026#39;(117.2, 39.1)\u0026#39;); INSERT INTO weather VALUES (\u0026#39;Tianjin\u0026#39;, 22, 30, 0.0, \u0026#39;2021-05-22\u0026#39;); 同理，cities表被参照，所以涉及被参考字段数据的更新及删除等都会受影响。\nDELETE FROM cities WHERE name = \u0026#39;Tianjin\u0026#39;; ERROR: update or delete on table \u0026#34;cities\u0026#34; violates foreign key constraint \u0026#34;weather_city_fkey\u0026#34; on table \u0026#34;weather\u0026#34; DETAIL: Key (name)=(Tianjin) is still referenced from table \u0026#34;weather\u0026#34;. c) 事务\n事务将多步操作看作一个单元，这些操作要么都做，要么都不做。\n现有两张表，accounts与branches，分别用于记录客户余额与分行总余额。现在 Alice 想给 Bob 转 100.00 块钱。可以将 SQL 语句用BEGIN及COMMIT包起来作为一个事务块。\nBEGIN; -- Alice的账户余额减去100.00 UPDATE accounts SET balance = balance - 100.00 WHERE name = \u0026#39;Alice\u0026#39;; -- Alice所在分行总余额减去100.00 UPDATE branches SET balance = balance - 100.00 WHERE name = (SELECT branch_name FROM accounts WHERE name = \u0026#39;Alice\u0026#39;); -- Bob的账户余额加上100.00 UPDATE accounts SET balance = balance + 100.00 WHERE name = \u0026#39;Bob\u0026#39;; -- Bob所在分行总余额加上100.00 UPDATE branches SET balance = balance + 100.00 WHERE name = (SELECT branch_name FROM accounts WHERE name = \u0026#39;Bob\u0026#39;); COMMIT; 此外，在事务中还可以使用SAVEPOINT来细粒度控制执行语句。\n假定从 Alice 的账号给 Bob 的账号打 100.00 块钱，后来才发现收款人应是 Wally。使用SAVEPOINT的语句如下：\nBEGIN; UPDATE accounts SET balance = balance - 100.00 WHERE name = \u0026#39;Alice\u0026#39;; SAVEPOINT my_savepoint; UPDATE accounts SET balance = balance + 100.00 WHERE name = \u0026#39;Bob\u0026#39;; -- oops ... forget that and use Wally\u0026#39;s account ROLLBACK TO my_savepoint; UPDATE accounts SET balance = balance + 100.00 WHERE name = \u0026#39;Wally\u0026#39;; COMMIT; d) 窗口函数\n窗口函数针对与当前行有某种关联的一组行进行计算。然而，窗口函数不会像非窗口聚集函数那样将一组行分组为一个单个的输出行，其会保留独立的输出行。\n窗口函数使用OVER子句来确定如何对行进行分区，以供窗口函数处理。OVER子句中的PARTITION BY用于将行分区。对于每一行，窗口函数会对与其落入同一分区的行进行计算。\n还可以使用OVER中的ORDER BY来控制窗口函数处理行的顺序。若省略PARTITION BY，表示所有行均落入一个分区。若省略ORDER BY，表示默认窗口包含分区中的所有行；若指定ORDER BY，窗口会包含从分区开始到当前行，以及根据ORDER BY子句与当前行相等的行。\n下面演示如何使用窗口函数。\n创建雇员薪资表empsalary：\nCREATE TABLE empsalary ( depname varchar, -- 部门名称  empno bigint, -- 雇员编号  salary int, -- 薪资  enroll_date date -- 入职日期 ); 插入数据：\nINSERT INTO empsalary (depname, empno, salary, enroll_date) VALUES (\u0026#39;develop\u0026#39;,10, 5200, \u0026#39;2021/08/01\u0026#39;), (\u0026#39;sales\u0026#39;, 1, 5000, \u0026#39;2021/10/01\u0026#39;), (\u0026#39;personnel\u0026#39;, 5, 3500, \u0026#39;2021/12/10\u0026#39;), (\u0026#39;sales\u0026#39;, 4, 4800, \u0026#39;2021/08/08\u0026#39;), (\u0026#39;sales\u0026#39;, 6, 5500, \u0026#39;2021/01/02\u0026#39;), (\u0026#39;personnel\u0026#39;, 2, 3900, \u0026#39;2021/12/23\u0026#39;), (\u0026#39;develop\u0026#39;, 7, 4200, \u0026#39;2021/01/01\u0026#39;), (\u0026#39;develop\u0026#39;, 9, 4500, \u0026#39;2021/01/01\u0026#39;), (\u0026#39;sales\u0026#39;, 3, 4800, \u0026#39;2021/08/01\u0026#39;), (\u0026#39;develop\u0026#39;, 8, 6000, \u0026#39;2021/10/01\u0026#39;), (\u0026#39;develop\u0026#39;, 11, 5200, \u0026#39;2021/08/15\u0026#39;); 使用如下 SQL 列出每个雇员的信息及部门平均薪资。\nSELECT *, avg(salary) OVER (PARTITION BY depname) FROM empsalary; depname | empno | salary | enroll_date | avg -----------+-------+--------+-------------+----------------------- develop | 10 | 5200 | 2007-08-01 | 5020.0000000000000000 develop | 7 | 4200 | 2008-01-01 | 5020.0000000000000000 develop | 9 | 4500 | 2008-01-01 | 5020.0000000000000000 develop | 8 | 6000 | 2006-10-01 | 5020.0000000000000000 develop | 11 | 5200 | 2007-08-15 | 5020.0000000000000000 personnel | 2 | 3900 | 2006-12-23 | 3700.0000000000000000 personnel | 5 | 3500 | 2007-12-10 | 3700.0000000000000000 sales | 3 | 4800 | 2007-08-01 | 5025.0000000000000000 sales | 1 | 5000 | 2006-10-01 | 5025.0000000000000000 sales | 4 | 4800 | 2007-08-08 | 5025.0000000000000000 sales | 6 | 5500 | 2007-01-02 | 5025.0000000000000000 (11 rows) 使用如下 SQL 列出每个雇员的信息及部门内薪资排名。\nSELECT *, rank() OVER (PARTITION BY depname ORDER BY salary DESC) FROM empsalary; depname | empno | salary | enroll_date | rank -----------+-------+--------+-------------+------ develop | 8 | 6000 | 2006-10-01 | 1 develop | 10 | 5200 | 2007-08-01 | 2 develop | 11 | 5200 | 2007-08-15 | 2 develop | 9 | 4500 | 2008-01-01 | 4 develop | 7 | 4200 | 2008-01-01 | 5 personnel | 2 | 3900 | 2006-12-23 | 1 personnel | 5 | 3500 | 2007-12-10 | 2 sales | 6 | 5500 | 2007-01-02 | 1 sales | 1 | 5000 | 2006-10-01 | 2 sales | 3 | 4800 | 2007-08-01 | 3 sales | 4 | 4800 | 2007-08-08 | 3 (11 rows) 使用如下 SQL 列出所有部门的雇员信息及全员薪资总和。（未使用PARTITION BY，表示全表为一个分区）\nSELECT *, sum(salary) OVER () FROM empsalary; depname | empno | salary | enroll_date | sum -----------+-------+--------+-------------+------- develop | 10 | 5200 | 2007-08-01 | 52600 sales | 1 | 5000 | 2006-10-01 | 52600 personnel | 5 | 3500 | 2007-12-10 | 52600 sales | 4 | 4800 | 2007-08-08 | 52600 sales | 6 | 5500 | 2007-01-02 | 52600 personnel | 2 | 3900 | 2006-12-23 | 52600 develop | 7 | 4200 | 2008-01-01 | 52600 develop | 9 | 4500 | 2008-01-01 | 52600 sales | 3 | 4800 | 2007-08-01 | 52600 develop | 8 | 6000 | 2006-10-01 | 52600 develop | 11 | 5200 | 2007-08-15 | 52600 (11 rows) 注意，若对如上 SQL 指定了ORDER BY，结果大不同。这是因为指定ORDER BY后，sum针对的是最低薪资到当前行及与当前薪资相等的行的计算。\nSELECT *, sum(salary) OVER (ORDER BY salary) FROM empsalary; depname | empno | salary | enroll_date | sum -----------+-------+--------+-------------+------- personnel | 5 | 3500 | 2007-12-10 | 3500 personnel | 2 | 3900 | 2006-12-23 | 7400 develop | 7 | 4200 | 2008-01-01 | 11600 develop | 9 | 4500 | 2008-01-01 | 16100 sales | 4 | 4800 | 2007-08-08 | 25700 sales | 3 | 4800 | 2007-08-01 | 25700 sales | 1 | 5000 | 2006-10-01 | 30700 develop | 11 | 5200 | 2007-08-15 | 41100 develop | 10 | 5200 | 2007-08-01 | 41100 sales | 6 | 5500 | 2007-01-02 | 46600 develop | 8 | 6000 | 2006-10-01 | 52600 (11 rows) 若查询涉及多个窗口函数，建议将WINDOW子句抽出来，以便在OVER中引用。\nSELECT *, avg(salary) OVER w, sum(salary) OVER w FROM empsalary WINDOW w AS (PARTITION BY depname); depname | empno | salary | enroll_date | avg | sum -----------+-------+--------+-------------+-----------------------+------- develop | 10 | 5200 | 2007-08-01 | 5020.0000000000000000 | 25100 develop | 7 | 4200 | 2008-01-01 | 5020.0000000000000000 | 25100 develop | 9 | 4500 | 2008-01-01 | 5020.0000000000000000 | 25100 develop | 8 | 6000 | 2006-10-01 | 5020.0000000000000000 | 25100 develop | 11 | 5200 | 2007-08-15 | 5020.0000000000000000 | 25100 personnel | 2 | 3900 | 2006-12-23 | 3700.0000000000000000 | 7400 personnel | 5 | 3500 | 2007-12-10 | 3700.0000000000000000 | 7400 sales | 3 | 4800 | 2007-08-01 | 5025.0000000000000000 | 20100 sales | 1 | 5000 | 2006-10-01 | 5025.0000000000000000 | 20100 sales | 4 | 4800 | 2007-08-08 | 5025.0000000000000000 | 20100 sales | 6 | 5500 | 2007-01-02 | 5025.0000000000000000 | 20100 (11 rows) e) 表继承\nPostgreSQL 支持表继承，下面创建城市表cities，及首都表capitals，首都表继承城市表。\nCREATE TABLE cities ( name text, -- 城市名  population real, -- 人口数  elevation int -- 海拔高度 ); CREATE TABLE capitals ( state char(2) UNIQUE NOT NULL -- 州 ) INHERITS (cities); 使用如下 SQL 查询包含首都的所有城市：\nSELECT * FROM cities; 使用如下 SQL 查询不包含首都的所有城市：\nSELECT * FROM ONLY cities; 综上，本文对 PostgreSQL 的基础功能及高级功能进行了初探。\n 参考资料\n[1]postgresql tutorial\n ","permalink":"https://olzhy.github.io/posts/postgres-getting-started.html","tags":["PostgreSQL"],"title":"PostgreSQL 初探"},{"categories":["计算机"],"contents":"PostgreSQL 是一款开源的对象关系型数据库管理系统（Object-Relational Database Management System, ORDBMS），其基于加州大学伯克利分校最初的 POSTGRE 源码开发，支持绝大部分 SQL 标准并提供诸多现代化特性。\nPostgreSQL 采用 C/S 架构。服务端进程（名为postgres）负责管理数据库文件，接收来自客户端的连接，并代表客户端执行数据库操作；客户端进程负责对服务端发起连接并发送数据库操作指令。服务端与客户端进程可以位于同一主机，也可以位于不同主机，若位于不同主机，则通过 TCP/IP 进行网络通信。PostgreSQL 服务端可以同时处理来自客户端的并发连接。其通过为每个连接启动新的进程来实现，且新的进程不会影响原始postgres进程的工作。\n本文将在CentOS 7.6主机上对PostgreSQL 13.3进行源码安装并作简单的使用。\n1 主机要求 常见的现代 Unix 兼容平台均可运行 PostgreSQL。本主机CentOS 7.6满足要求。\n如下软件包是构建 PostgreSQL 所必须的：\na) GNU make 版本须是 3.80+\n本主机满足要求。\n$ make --version GNU Make 3.82 b) ISO/ANSI C 编译器 (推荐使用最新的 GCC)\n本主机满足要求。\n$ gcc --version gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44) c) tar\n用于解压源码压缩文件。本主机满足要求。\n$ tar --version tar (GNU tar) 1.26 d) GNU Readline 库\n被 PostgreSQL 命令行工具psql用来记录键入的每条命令，进而可以使用方向键重用这些命令。本主机满足要求。\n$ rpm -qa | grep readline readline-devel-6.2-11.el7.x86_64 readline-6.2-11.el7.x86_64 e) zlib 压缩库\n支持pg_dump和pg_restore的压缩归档。本主机满足要求。\n$ rpm -qa | grep zlib zlib-devel-1.2.7-18.el7.x86_64 zlib-1.2.7-18.el7.x86_64 2 PostgreSQL 安装 如下命令执行前，当前用户为非rootsudoer 账号larry。\n$ whoami larry a) 获取源码压缩文件\n进入用户根目录，下载PostgreSQL 13.3源码压缩文件，完成后将其解压至当前目录。\n$ cd /home/larry $ wget https://ftp.postgresql.org/pub/source/v13.3/postgresql-13.3.tar.gz $ tar -zxvf postgresql-13.3.tar.gz b) 配置、构建、测试，及安装\n上一步完成后，将在当前目录生成一个目录postgresql-13.3，进入该目录进行配置、构建、测试，及安装。\n$ cd /home/larry/postgresql-13.3 $ ./configure # 源码树配置及依赖变量检查 $ make all # 构建 $ make check # 回归测试 $ sudo make install # 使用root权限进行安装，因默认会安装到/usr/local/pgsql 查看安装目录/usr/local/pgsql/，发现其包含几个文件夹。\n$ ls /usr/local/pgsql/ bin include lib share c) 配置数据目录并启动\n推荐使用一个单独的账号（postgres）运行 PostgreSQL，该账号仅有服务端所管理的数据的权限（特别地，该账号亦不应拥有 PostgreSQL 可执行文件权限，以防被感染服务进程篡改这些可执行文件），且不与其它守护进程共享数据。\n如下命令使用当前 sudoer 用户larry创建了一个新账号postgres，新建了/usr/local/pgsql/data数据文件夹并将控制权限赋予postgres。\n$ sudo adduser postgres $ sudo mkdir /usr/local/pgsql/data $ sudo chown postgres /usr/local/pgsql/data 下面将用户切换为postgres，初始化数据库，并启动服务。\n$ sudo su - postgres $ /usr/local/pgsql/bin/initdb -D /usr/local/pgsql/data # 初始化数据库 $ /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l server.log start # 启动服务，并指定日志输出文件server.log server started 至此，PostgreSQL 服务已启动。\nd) 设置开机自启动\n使用 root 权限编辑/etc/rc.d/rc.local文件，添加启动命令。\n$ sudo vi /etc/rc.d/rc.local su - postgres -c \u0026#39;cd /home/postgres \u0026amp;\u0026amp; /usr/local/pgsql/bin/pg_ctl -D /usr/local/pgsql/data -l server.log start\u0026#39; 3 PostgreSQL 简单使用 创建一个数据库test，并使用 PostgreSQL 交互式命令行程序psql进行连接测试。\n$ sudo su - postgres $ /usr/local/pgsql/bin/createdb test $ /usr/local/pgsql/bin/psql test psql (13.3) Type \u0026#34;help\u0026#34; for help. test=# SELECT version(); version -------------------------------------------------------------------------------------------------------- PostgreSQL 13.3 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44), 64-bit (1 row) test=# \\q 至此，我们完成了对 PostgreSQL 的源码安装及测试。\n 参考资料\n[1]what is postgresql?\n[2]postgresql installation from source code\n[3]postgresql server setup and operation\n ","permalink":"https://olzhy.github.io/posts/install-postgres-on-centos-from-source.html","tags":["PostgreSQL"],"title":"在 CentOS 上以源码安装 PostgreSQL"},{"categories":["读书"],"contents":"一直想花时间了解一下这段离我们最近却不那么熟悉的民国历史。这个四月终于花了一些时间一口气将江城所著的《历史深处的民国》阅读完毕。全书共三部分：晚清的苟延残喘到北洋的初试共和，军阀混战到建立民国，顽强抗日到最终胜利。\n整体来看，这一百年的历史是一个民族受尽苦难又可歌可泣的的新生史。\n晚清的这段历史是多少人愤懑且难以阅读下去的一部分。列强环伺，任人宰割，政府腐败，民不聊生。这个王朝步履蹒跚，昏庸老朽。虽有仁人志士尝试各种革新方法也挽救不了其颓势。\n短暂的共和，虽带来短暂的稳定，但未有坚定的信念与纲领，走到后来或因私心作祟或因利益难衡，终以失败告终。\n接下来的军阀割据，带来较长时间的内乱，都想当老大，都没有足够的实力。终是打来打去，百姓受苦。\n一团革命之火慢慢从广州燃遍全国，北伐成功，东北易帜，国家实现了形式上的统一。未等喘息，蓄谋已久的倭寇即开始侵略中国。多少黄埔男儿，热血儿女舍生忘死，保家卫国，硬是以血肉之躯与倭寇顽强抵抗，粉碎其占领中国的幻想。这一段看的我热血滚滚。\n这段历史是中华民族历史长河中比较低迷的一段。但不可不看，不可不知。从这段历史可以看到，腐败是一个王朝的顽疾。固步自封，内部腐坏的王朝必会被历史所抛弃，内部动荡及外部欺凌只是其内部问题的客观表现。赤子之心救国者方能被历史铭记，牟取私利投机者终不能成就大业。没有一个强有力的中央政府，民主会成为空谈。且在人人为己的本性下，这个国家只会进行无休止的内耗。每当外敌入侵，我们的民族愈能拧成一股绳，平时看着一团散沙，稍稍一击即可攻破的一群人，在国难当头却能形成一股不可战胜之力。这即说明中华之根已潜藏在每个炎黄子孙的内心。中华文化形散神不散。坚韧不拔，自强不息的中国人必将再次站在世界之巅。\n2021年4月14日于大连\n","permalink":"https://olzhy.github.io/posts/reading-one-hundred-year-history.html","tags":["读书"],"title":"读《历史深处的民国》"},{"categories":["计算机"],"contents":"日常编码中离不开字符串拼接，最常用的当属原生的拼接方式（+=）。但其在少量次数拼接中性能还可以，若进行大量的字符串拼接则应使用其它更高效的方式。\n本文首先列出 Golang 中常用的几种字符串拼接方式，然后会对它们进行基准测试，以期阅读完本文，我们能对各种拼接方法的适用场景有一个基本了解。\n1 字符串拼接有几种方法？ 孔乙己问：“回字有几种写法？”。我们在 Golang 使用中也难免会被问到：“字符串拼接有几种方法？”。下面就一一道来。\na) 原生拼接方式（+=）\n原生拼接方式即使用+操作符直接对两个字符串进行拼接。\n如下代码即使用+=来进行字符串拼接及重新赋值。\nvar s string s += \u0026#34;hello\u0026#34; 该种方式为什么不高效呢？因在 Golang 中 string 是不可变的，其拼接时先得将 s 的值取下来（从头遍历复制），然后与一个字符串进行拼接，计算好后再将新值（一个全新的字符串）重新赋给 s，而 s 的旧值会等待垃圾回收器回收。因其每次拼接都会从头遍历复制，会涉及较多的计算与内存分配。\n该方式的时间复杂度为 O(N^2)。\nb) bytes.Buffer\nbytes.Buffer 是一个变长的字节缓存区。其内部使用 slice 来存储字节（buf []byte）。\nbuf := bytes.NewBufferString(\u0026#34;hello\u0026#34;) buf.WriteString(\u0026#34; world\u0026#34;) // fmt.Fprint(buf, \u0026#34; world\u0026#34;) 使用 WriteString 进行字符串拼接时，其会根据情况动态扩展 slice 长度，并使用内置 slice 内存拷贝函数将待拼接字符串拷贝到缓冲区中。因其是变长的 slice，每次拼接时，无须重新拷贝旧有的部分，仅将待拼接的部分追加到尾部即可，所以较原生拼接方式性能高。\n该方式的时间复杂度为 O(N)。\n// WriteString appends the contents of s to the buffer, growing the buffer as // needed. The return value n is the length of s; err is always nil. If the // buffer becomes too large, WriteString will panic with ErrTooLarge. func (b *Buffer) WriteString(s string) (n int, err error) { b.lastRead = opInvalid m, ok := b.tryGrowByReslice(len(s)) if !ok { m = b.grow(len(s)) } return copy(b.buf[m:], s), nil } c) strings.Builder\nstrings.Builder 内部也是使用字节 slice 来作存储。\nvar builder strings.Builder builder.WriteString(\u0026#34;hello\u0026#34;) // fmt.Fprint(\u0026amp;builder, \u0026#34;hello\u0026#34;) 使用 WriteString 进行字符串拼接时，其会调用内置 append 函数仅将待拼接字符串并入缓存区。其效率亦很高。\n// WriteString appends the contents of s to b\u0026#39;s buffer. // It returns the length of s and a nil error. func (b *Builder) WriteString(s string) (int, error) { b.copyCheck() b.buf = append(b.buf, s...) return len(s), nil } d) 内置 copy 函数\n内置 copy 函数支持将一个源 slice 拷贝到一个目标 slice，因字符串的底层表示就是[]byte，所以也可以使用该函数进行字符串拼接。不过限制是需要预先知道字节 slice 的长度。\nbytes := make([]byte, 11) size := copy(bytes[0:], \u0026#34;hello\u0026#34;) copy(bytes[size:], \u0026#34; world\u0026#34;) fmt.Println(string(bytes)) 内置 copy 函数支持将一个 slice 拷贝到另一个 slice（其支持将一个字符串拷贝到[]byte），其返回值为所拷贝元素的长度。\n每次拼接时，其亦只需将待拼接字符串追加到 slice 尾部，效率亦很高。\n// The copy built-in function copies elements from a source slice into a // destination slice. (As a special case, it also will copy bytes from a // string to a slice of bytes.) The source and destination may overlap. Copy // returns the number of elements copied, which will be the minimum of // len(src) and len(dst). func copy(dst, src []Type) int e) strings.Join\n若想将一个 string slice（[]string）的各部分拼成一个字符串，可以使用strings.Join进行操作。\ns := strings.Join([]string{\u0026#34;hello world\u0026#34;}, \u0026#34;\u0026#34;) 其内部也是使用 bytes.Builder 进行实现的。所以也是非常高效的。\n// Join concatenates the elements of its first argument to create a single string. The separator // string sep is placed between elements in the resulting string. func Join(elems []string, sep string) string { ... var b Builder b.Grow(n) b.WriteString(elems[0]) for _, s := range elems[1:] { b.WriteString(sep) b.WriteString(s) } return b.String() } 2 基准测试 下面将如上介绍的几种字符串拼接方法组装为一个测试文件string_test.go进行基准测试。（因strings.Join需要预先生成一个[]string，与其它方法的使用场景不太一样，所以该方法不参与本次测试）\n该基准测试将使用每种方法将一个字符串“s”，拼接 1000 次。\nstring_test.go 源码：\npackage string_test import ( \u0026#34;bytes\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;testing\u0026#34; ) var ( concatSteps = 1000 subStr = \u0026#34;s\u0026#34; expectedStr = strings.Repeat(subStr, concatSteps) ) func BenchmarkConcat(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { var s string for i := 0; i \u0026lt; concatSteps; i++ { s += subStr } if s != expectedStr { b.Errorf(\u0026#34;unexpected result, got: %s, want: %s\u0026#34;, s, expectedStr) } } } func BenchmarkBuffer(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { var buffer bytes.Buffer for i := 0; i \u0026lt; concatSteps; i++ { buffer.WriteString(subStr) } if buffer.String() != expectedStr { b.Errorf(\u0026#34;unexpected result, got: %s, want: %s\u0026#34;, buffer.String(), expectedStr) } } } func BenchmarkBuilder(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { var builder strings.Builder for i := 0; i \u0026lt; concatSteps; i++ { builder.WriteString(subStr) } if builder.String() != expectedStr { b.Errorf(\u0026#34;unexcepted result, got: %s, want: %s\u0026#34;, builder.String(), expectedStr) } } } func BenchmarkCopy(b *testing.B) { for n := 0; n \u0026lt; b.N; n++ { bytes := make([]byte, len(subStr)*concatSteps) c := 0 for i := 0; i \u0026lt; concatSteps; i++ { c += copy(bytes[c:], subStr) } if string(bytes) != expectedStr { b.Errorf(\u0026#34;unexpected result, got: %s, want: %s\u0026#34;, string(bytes), expectedStr) } } } 执行 Benchmark 测试命令：\n$ go test -benchmem -bench . goos: darwin goarch: amd64 pkg: github.com/olzhy/test cpu: Intel(R) Core(TM) i5-7360U CPU @ 2.30GHz BenchmarkConcat-4 7750 148143 ns/op 530274 B/op 999 allocs/op BenchmarkBuffer-4 161848 7151 ns/op 3248 B/op 6 allocs/op BenchmarkBuilder-4 212043 5406 ns/op 2040 B/op 8 allocs/op BenchmarkCopy-4 281827 4208 ns/op 1024 B/op 1 allocs/op PASS ok github.com/olzhy/test 5.773s 可以看到内置 copy 函数与 strings.Builder 的方式是最高效的，bytes.Buffer 次之，原生拼接方式最低效。\n 参考资料\n[1] How to efficiently concatenate strings in Go - Stack Overflow\n[2] Documentation for bytes.Buffer\n[3] Documentation for strings.Builder\n[4] Documentation for builtin.copy\n ","permalink":"https://olzhy.github.io/posts/efficent-string-concatenation-in-golang.html","tags":["Golang"],"title":"Golang 高效的字符串拼接方法"},{"categories":["计算机"],"contents":"本文介绍管理员如何使用根证书、签发证书及秘钥为 Istio 配置 CA（证书颁发机构）。Istio CA 使用由中间 CA 签发的私钥及证书，而中间 CA 由根 CA 签发。这样，Istio CA 即可为工作负载签发根证书及私钥。CA 层次结构图如下。\n图片引自(Plug in CA Certificates)\n接下来即介绍如何为 Istio 生成及植入 CA。\n1 为集群植入证书及私钥 首先，进入 Istio 安装目录/usr/local/istio-1.8.1，创建证书目录certs后进入该目录。\n$ cd /usr/local/istio-1.8.1 $ mkdir certs $ cd certs 然后，使用如下命令生成根证书及私钥。\n$ make -f ../tools/certs/Makefile.selfsigned.mk root-ca 其会生成 4 个文件。\n   FILE DESCRIPTION     root-cert.pem 根证书   root-key.pem 根秘钥   root-ca.conf 生成根证书的openssl配置   root-cert.csr 根证书的CSR    接下来，使用如下命令生成中间证书及私钥。\n$ make -f ../tools/certs/Makefile.selfsigned.mk cluster1-cacerts 其会在cluster1文件夹下生成 4 个文件。\n   FILE DESCRIPTION     ca-cert.pem 中间证书   ca-key.pem 中间秘钥   cert-chain.pem istiod所使用的证书链   root-cert.pem 根证书    最后，创建 namespace istio-system，接着基于cluster1文件夹下生成的文件创建 Secret cacerts。\n$ kubectl create ns istio-system $ kubectl create secret generic cacerts -n istio-system \\  --from-file=cluster1/ca-cert.pem \\  --from-file=cluster1/ca-key.pem \\  --from-file=cluster1/root-cert.pem \\  --from-file=cluster1/cert-chain.pem 2 部署 Istio 及样例服务 指定模式为demo，安装 Istio，Istio CA 将从cacerts读取证书及私钥。\n$ istioctl install --set profile=demo 接着，进入 Istio 安装根目录，创建 namespace istio-demo，然后在该 namespace 下部署样例服务httpbin及用于测试的服务sleep。\n$ cd /usr/local/istio-1.8.1 $ kubectl create ns istio-demo $ kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n istio-demo $ kubectl apply -f \u0026lt;(istioctl kube-inject -f samples/sleep/sleep.yaml) -n istio-demo 然后，使用如下命令指定istio-demo下的工作负载只接受双向 TLS 的流量。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: \u0026#34;security.istio.io/v1beta1\u0026#34; kind: \u0026#34;PeerAuthentication\u0026#34; metadata: name: \u0026#34;default\u0026#34; spec: mtls: mode: STRICT heredoc\u0026gt; EOF 3 校验证书 下面，我们将验证工作负载是否使用了我们所植入的 CA 所签发的证书。\n首先，等待20s，让所配置的 mTLS 规则生效。然后使用如下命令进入sleep的istio-proxy Sidecar 来尝试获取httpbin的证书链。\n$ kubectl exec \u0026#34;$(kubectl get pod -l app=sleep -n istio-demo -o jsonpath={.items..metadata.name})\u0026#34; -c istio-proxy -n istio-demo -- openssl s_client -showcerts -connect httpbin.istio-demo:8000 \u0026gt; httpbin-proxy-cert.txt 然后，得到错误“verify error:num=19:self signed certificate in certificate chain”，符合预期。\n查看文件httpbin-proxy-cert.txt，发现里边有 4 套证书，将其拷贝出来，分别以proxy-cert-i.pem (i=1,2,3,4)命名。\n然后，使用如下命令校验根证书是否与管理员所签发的一致。\n$ openssl x509 -in certs/cluster1/root-cert.pem -text -noout \u0026gt; /tmp/root-cert.crt.txt $ openssl x509 -in ./proxy-cert-3.pem -text -noout \u0026gt; /tmp/pod-root-cert.crt.txt $ diff -s /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt Files /tmp/root-cert.crt.txt and /tmp/pod-root-cert.crt.txt are identical 接着，使用如下命令校验 CA 证书是否与管理员所签发的一致。\n$ openssl x509 -in certs/cluster1/ca-cert.pem -text -noout \u0026gt; /tmp/ca-cert.crt.txt $ openssl x509 -in ./proxy-cert-2.pem -text -noout \u0026gt; /tmp/pod-cert-chain-ca.crt.txt $ diff -s /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt Files /tmp/ca-cert.crt.txt and /tmp/pod-cert-chain-ca.crt.txt are identical 最后，使用如下命令校验根证书的证书链与工作负载的证书链是否一致。\n$ openssl verify -CAfile \u0026lt;(cat certs/cluster1/ca-cert.pem certs/cluster1/root-cert.pem) ./proxy-cert-1.pem ./proxy-cert-1.pem: OK 4 环境清理 使用如下命令移除证书。\n$ cd /usr/local/istio-1.8.1 $ rm proxy-cert-*.pem $ rm httpbin-proxy-cert.txt $ rm -rf certs 使用如下命令移除 Secret cacerts，删除 namespace istio-demo，istio-system。\n$ kubectl delete secret cacerts -n istio-system $ kubectl delete ns istio-demo istio-system 总结本文，介绍了 Istio 的 CA 如何签发，然后使用 httpbin 样例作了测试。\n 参考资料\n[1] Istio Plug in CA Certificates\n ","permalink":"https://olzhy.github.io/posts/istio-plug-in-ca-certificates.html","tags":["服务网格","Istio"],"title":"Istio 证书管理之植入 CA 证书"},{"categories":["计算机"],"contents":"上文“Istio 流量管理之 Ingress Gateway”介绍了如何使用 Gateway 将一个 7 层 HTTP 服务暴露给外部使用。本文将介绍如何为 Gateway 配置单向或双向 TLS 从而暴露一个安全的 HTTPS 服务给外部访问。关于 Istio 安装等环境准备，请参阅“Istio 安装使用”。\n1 部署 httpbin 使用 Istio 安装目录自带的配置文件将 httpbin 部署至istio-demo namespace。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/httpbin/httpbin.yaml 2 生成证书及私钥 使用 openssl 生成用于为服务签发证书的根证书及私钥，如下命令执行后会生成两个文件（example.com.crt，example.com.key）。\n$ openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -subj \u0026#39;/O=example Inc./CN=example.com\u0026#39; -keyout example.com.key -out example.com.crt 为httpbin.example.com生成证书及私钥，如下命令执行后会生成三个文件（httpbin.example.com.csr，httpbin.example.com.key，httpbin.example.com.crt）。\n$ openssl req -out httpbin.example.com.csr -newkey rsa:2048 -nodes -keyout httpbin.example.com.key -subj \u0026#34;/CN=httpbin.example.com/O=httpbin organization\u0026#34; $ openssl x509 -req -days 365 -CA example.com.crt -CAkey example.com.key -set_serial 0 -in httpbin.example.com.csr -out httpbin.example.com.crt 3 配置 TLS Ingress Gateway 使用第 2 步生成的私钥及证书为 Ingress Gateway 创建 secret。\n$ kubectl create -n istio-system secret tls httpbin-credential --key=httpbin.example.com.key --cert=httpbin.example.com.crt 应用 Gateway 配置，端口为 443，hosts 为httpbin.example.com，开启 TLS SIMPLE 模式，并配置 credentialName 为刚刚创建的 secret 名称。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: mygateway spec: selector: istio: ingressgateway # use istio default ingress gateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: httpbin-credential # must be the same as secret hosts: - httpbin.example.com heredoc\u0026gt; EOF 使用 Virtual Service 为 httpbin 配置 Gateway 路由规则。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin.example.com\u0026#34; gateways: - mygateway http: - match: - uri: prefix: /status - uri: prefix: /delay route: - destination: port: number: 8000 host: httpbin heredoc\u0026gt; EOF 使用 curl 对 httpbin 发送 https 请求（本文使用 Docker Desktop Kubernetes 环境，INGRESS_HOST 为 127.0.0.1，SECURE_INGRESS_PORT 为 443），成功返回“418 I’m a Teapot”。\n$ curl -v -HHost:httpbin.example.com --resolve \u0026#34;httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST\u0026#34; \\ --cacert example.com.crt \u0026#34;https://httpbin.example.com:$SECURE_INGRESS_PORT/status/418\u0026#34; ... -=[ teapot ]=- _...._ .\u0026#39; _ _ `. | .\u0026#34;` ^ `\u0026#34;. _, \\_;`\u0026#34;---\u0026#34;`|// | ;/ \\_ _/ `\u0026#34;\u0026#34;\u0026#34;` 4 为多 Host 配置 TLS Gateway 上面配置的 Gateway 仅支持一组 Host 的 TLS 访问。下面再部署一个helloworld-v1服务，然后配置 Ingress Gateway，让其同时支持httpbin.example.com与helloworld-v1.example.com两个 Host 的 TLS 访问。\n部署helloworld-v1样例。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; \u0026gt;.... metadata: name: helloworld-v1 spec: replicas: 1 selector: matchLabels: app: helloworld-v1 version: v1 template: metadata: labels: app: helloworld-v1 version: v1 spec: containers: - name: helloworld image: istio/examples-helloworld-v1 resources: requests: cpu: \u0026#34;100m\u0026#34; imagePullPolicy: IfNotPresent #Always ports: - containerPort: 5000 heredoc\u0026gt; EOF 为helloworld-v1.example.com生成证书及私钥。\n$ openssl req -out helloworld-v1.example.com.csr -newkey rsa:2048 -nodes -keyout helloworld-v1.example.com.key -subj \u0026#34;/CN=helloworld-v1.example.com/O=helloworld organization\u0026#34; $ openssl x509 -req -days 365 -CA example.com.crt -CAkey example.com.key -set_serial 1 -in helloworld-v1.example.com.csr -out helloworld-v1.example.com.crt 为 Ingress Gateway 创建 secret helloworld-credential。\n$ kubectl create -n istio-system secret tls helloworld-credential --key=helloworld-v1.example.com.key --cert=helloworld-v1.example.com.crt 修改 Gateway 配置，增加对helloworld-v1.example.com的 TLS 访问支持。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; \u0026gt;.... name: mygateway spec: selector: istio: ingressgateway # use istio default ingress gateway servers: - port: number: 443 name: https-httpbin protocol: HTTPS tls: mode: SIMPLE credentialName: httpbin-credential hosts: - httpbin.example.com - port: number: 443 name: https-helloworld protocol: HTTPS tls: mode: SIMPLE credentialName: helloworld-credential hosts: - helloworld-v1.example.com heredoc\u0026gt; EOF 使用 Virtual Service 为 Gateway 配置路由规则。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: helloworld-v1 spec: hosts: - helloworld-v1.example.com gateways: - mygateway http: - match: - uri: exact: /hello route: - destination: host: helloworld-v1 port: number: 5000 heredoc\u0026gt; EOF 然后，使用 curl 对helloworld-v1发起 https 请求，发现成功返回 200 状态码。\n$ curl -v -HHost:helloworld-v1.example.com --resolve \u0026#34;helloworld-v1.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST\u0026#34; \\ --cacert example.com.crt \u0026#34;https://helloworld-v1.example.com:$SECURE_INGRESS_PORT/hello\u0026#34; 再次使用刚刚的命令对httpbin发起 https 请求，同样成功返回结果。说明 Gateway 同时支持两组 Host 的 TLS 访问。\n$ curl -v -HHost:httpbin.example.com --resolve \u0026#34;httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST\u0026#34; \\ --cacert example.com.crt \u0026#34;https://httpbin.example.com:$SECURE_INGRESS_PORT/status/418\u0026#34; ... -=[ teapot ]=- _...._ .\u0026#39; _ _ `. | .\u0026#34;` ^ `\u0026#34;. _, \\_;`\u0026#34;---\u0026#34;`|// | ;/ \\_ _/ `\u0026#34;\u0026#34;\u0026#34;` 5 配置双向 TLS Ingress Gateway 为使 Gateway 支持双向 TLS 通信，须将原有 secret 删除，创建新的 secret，并将用于校验客户端的根证书囊括进来。\n$ kubectl -n istio-system delete secret httpbin-credential $ kubectl create -n istio-system secret generic httpbin-credential --from-file=tls.key=httpbin.example.com.key --from-file=tls.crt=httpbin.example.com.crt --from-file=ca.crt=example.com.crt 更新 Gateway 配置，为httpbin开启双向 TLS 模式。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: mygateway spec: selector: istio: ingressgateway # use istio default ingress gateway servers: - port: number: 443 name: https protocol: HTTPS tls: mode: MUTUAL credentialName: httpbin-credential # must be the same as secret hosts: - httpbin.example.com heredoc\u0026gt; EOF 配置生效后，之前请求 httpbin 的方式就不好使了。\n下面使用如下命令尝试为客户端创建证书及私钥。\n$ openssl req -out client.example.com.csr -newkey rsa:2048 -nodes -keyout client.example.com.key -subj \u0026#34;/CN=client.example.com/O=client organization\u0026#34; $ openssl x509 -req -days 365 -CA example.com.crt -CAkey example.com.key -set_serial 1 -in client.example.com.csr -out client.example.com.crt 使用--cert及--key选项将客户端证书及私钥传入后，再次使用 https 方式请求httpbin，这时返回成功，\n$ curl -v -HHost:httpbin.example.com --resolve \u0026#34;httpbin.example.com:$SECURE_INGRESS_PORT:$INGRESS_HOST\u0026#34; \\ --cacert example.com.crt --cert client.example.com.crt --key client.example.com.key \\ \u0026#34;https://httpbin.example.com:$SECURE_INGRESS_PORT/status/418\u0026#34; ... -=[ teapot ]=- _...._ .\u0026#39; _ _ `. | .\u0026#34;` ^ `\u0026#34;. _, \\_;`\u0026#34;---\u0026#34;`|// | ;/ \\_ _/ `\u0026#34;\u0026#34;\u0026#34;` 6 环境清理 测试结束，使用如下命令删除 Gateway，Virtual Service 及 Secret。\n$ kubectl delete gateway mygateway -n istio-demo $ kubectl delete virtualservice httpbin helloworld-v1 -n istio-demo $ kubectl delete --ignore-not-found=true -n istio-system secret httpbin-credential helloworld-credential 使用如下命令卸载 httpbin 及 helloworld-v1 服务。\n$ kubectl delete deploy --ignore-not-found=true httpbin helloworld-v1 -n istio-demo $ kubectl delete svc --ignore-not-found=true httpbin helloworld-v1 -n istio-demo 总结本文，首先介绍了 Istio Ingress Gateway 支持简单及双向 TLS 访问；然后使用 httpbin 样例测试了简单 TLS 访问；引入 helloworld-v1 样例测试了多 Host TLS 访问；最后使用 httpbin 样例测试了双向 TLS 访问。\n 参考资料\n[1] Istio Secure Gateways\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-secure-gateways.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之安全 Gateway"},{"categories":["计算机"],"contents":"Istio Ingress Gateway 是允许外部流量进入 Istio 服务网格的边缘服务。其比 Kubernetes Ingress 更具扩展性。且使用 Istio Ingress Gateway，使得 Istio 对于入口流量同样具有策略控制能力及可观察性。\n本文将使用 Istio 安装目录自带的 httpbin 样例来演示如何配置 Gateway 来实现外部访问。关于 Istio 安装等环境准备，请参阅“Istio 安装使用”。\n1 httpbin 样例部署 进入 Istio 安装目录，应用自带的 httpbin 部署文件，将其部署到istio-demo namespace。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/httpbin/httpbin.yaml 2 httpbin 配置 Gateway 为 httpbin 创建 Gateway。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;httpbin.example.com\u0026#34; heredoc\u0026gt; EOF 为 httpbin 配置 Virtual Service。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;httpbin.example.com\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status route: - destination: port: number: 8000 host: httpbin heredoc\u0026gt; EOF 上述命令为 httpbin 配置 Gateway 与 VirtualService，将其暴露给集群外部访问。且指定访问 httpbin 的 Host 须为httpbin.example.com，且只可访问前缀为/status的 REST 资源。同时我们可以看到，Istio Gateway 与 Kubernetes Ingress 不同的是，无须在 Gateway 部署文件配置路由，而将路由配置移到了 VirtualService。\n下面通过查询用于外部访问的 INGRESS_HOST 与 INGRESS_PORT 来测试我们的配置。\n3 httpbin 外部访问 查询用于外部访问的 INGRESS_HOST 与 INGRESS_PORT。\n$ kubectl get svc istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) istio-ingressgateway LoadBalancer 10.102.158.234 localhost ...80:30841/TCP... 本文使用的是 Docker Desktop 自带的 Kubernetes，可以看到 INGRESS_HOST 即为 localhost，INGRESS_PORT 为 80。\n亦可以使用如下命令查看 INGRESS_HOST 与 INGRESS_PORT，得到同样的结果。\n$ kubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].hostname}\u0026#39; $ kubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].port}\u0026#39; 下面，分别尝试通过 curl 命令及浏览器来访问 httpbin 的 status 接口。\ncurl 命令访问\n通过如下命令访问 httpbin 的 status 接口时，发现报 404 错误。\n$ curl -s -I http://localhost/status/200 HTTP/1.1 404 Not Found date: Fri, 01 Jan 2021 08:27:57 GMT server: istio-envoy transfer-encoding: chunked 原因是我们在第 2 步的 Gateway 中指定访问 Host 必须为httpbin.example.com，加上 Header 后重新访问，发现状态码为 200，访问成功。\n$ curl -s -I -H \u0026#34;Host: httpbin.example.com\u0026#34; http://localhost/status/200 HTTP/1.1 200 OK server: istio-envoy date: Fri, 01 Jan 2021 08:28:02 GMT content-type: text/html; charset=utf-8 access-control-allow-origin: * access-control-allow-credentials: true content-length: 0 x-envoy-upstream-service-time: 20 Web 浏览器访问\n使用浏览器直接打开http://localhost/status/200时，发现同样报 404 错误。因我们仅是在做测试，未真正配置域名解析，所以尝试将 Gateway 与 VirtualService 中 hosts 由httpbin.example.com改为通配符*来实现访问。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: httpbin-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - \u0026#34;*\u0026#34; gateways: - httpbin-gateway http: - match: - uri: prefix: /status route: - destination: port: number: 8000 host: httpbin 这样，再次访问http://localhost/status/200时，发现返回 200 状态码。\n4 环境清理 测试完成，使用如下命令清除 httpbin 的 Gateway 及 VirtualService 配置。\n$ kubectl delete gateway httpbin-gateway -n istio-demo $ kubectl delete virtualservice httpbin -n istio-demo 卸载 httpbin。\n$ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo -f samples/httpbin/httpbin.yaml 总结本文，首先介绍了使用 Istio Gateway 可以实现外部流量进入服务网格，然后为 httpbin 样例配置了 Gateway 并做了外部访问演示。\n 参考资料\n[1] Istio Ingress Gateways\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-ingress-gateways.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之 Ingress Gateway"},{"categories":["计算机"],"contents":"本文介绍一下 Istio 的流量镜像功能，即使用 Istio 可以将某一服务的实时流量拷贝一份并镜像到另一个服务。该特性对线上调试特别有用。\n本文使用 httpbin 样例来做测试，首先部署两个版本的 httpbin 服务，然后将请求流量都打到 v1，最后使用流量镜像功能将打到 v1 的流量同时拷贝一份到 v2。\n关于 Istio 安装等环境准备，请参阅“Istio 安装使用”。\n1 httpbin 样例部署 部署httpbin-v1，且已开启访问日志。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: httpbin-v1 spec: replicas: 1 selector: matchLabels: app: httpbin version: v1 template: metadata: labels: app: httpbin version: v1 spec: containers: - image: docker.io/kennethreitz/httpbin imagePullPolicy: IfNotPresent name: httpbin command: [\u0026#34;gunicorn\u0026#34;, \u0026#34;--access-logfile\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:80\u0026#34;, \u0026#34;httpbin:app\u0026#34;] # 开启访问日志 ports: - containerPort: 80 EOF 部署httpbin-v2，且已开启访问日志。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: httpbin-v2 spec: replicas: 1 selector: matchLabels: app: httpbin version: v2 template: metadata: labels: app: httpbin version: v2 spec: containers: - image: docker.io/kennethreitz/httpbin imagePullPolicy: IfNotPresent name: httpbin command: [\u0026#34;gunicorn\u0026#34;, \u0026#34;--access-logfile\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:80\u0026#34;, \u0026#34;httpbin:app\u0026#34;] # 开启访问日志 ports: - containerPort: 80 EOF 查看 Pod，两个版本已部署成功。\n$ kubectl get pods -n istio-demo | grep httpbin httpbin-v1-75d9447d79-vblbs 2/2 Running 0 2m29s httpbin-v2-fb86d8d46-wgskr 2/2 Running 0 91s 创建 httpbin Service。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF apiVersion: v1 kind: Service metadata: name: httpbin labels: app: httpbin spec: ports: - name: http port: 8000 targetPort: 80 selector: app: httpbin EOF httpbin 部署好了，下面部署一下 sleep，其包含 curl 等命令，用来作测试客户端。\n2 sleep 客户端部署 部署 sleep 服务。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: sleep spec: replicas: 1 selector: matchLabels: app: sleep template: metadata: labels: app: sleep spec: containers: - name: sleep image: tutum/curl command: [\u0026#34;/bin/sleep\u0026#34;,\u0026#34;infinity\u0026#34;] imagePullPolicy: IfNotPresent EOF 进入 Pod，试着给 httpbin 发请求。\n$ kubectl exec sleep-96c4ddd7f-ktjgg -c sleep -n istio-demo -- curl -s http://httpbin:8000/headers 查看 httpbin v1 及 v2 的日志。发现两个版本会随机接收到请求。\n$ kubectl logs -f -l app=httpbin,version=v1 -c httpbin -n istio-demo 127.0.0.1 - - [30/Dec/2020:00:41:30 +0000] \u0026#34;GET /headers HTTP/1.1\u0026#34; 200 559 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; $ kubectl logs -f -l app=httpbin,version=v2 -c httpbin -n istio-demo 127.0.0.1 - - [30/Dec/2020:00:41:28 +0000] \u0026#34;GET /headers HTTP/1.1\u0026#34; 200 559 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; 3 将流量都打到 v1 下面为 httpbin 配置 Virtual Service 及 Destination Rule，将请求流量都打到 v1。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - httpbin http: - route: - destination: host: httpbin subset: v1 weight: 100 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: httpbin spec: host: httpbin subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 EOF 再使用 sleep 给 httpbin 发请求时，只有 httpbin v1 会打印访问日志。\n4 将流量镜像到 v2 下面修改 httpbin Virtual Service 路由配置，将打给 v1 的流量同时拷贝一份镜像给 v2。\n$ kubectl edit virtualservice/httpbin -n istio-demo apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: httpbin spec: hosts: - httpbin http: - route: - destination: host: httpbin subset: v1 weight: 100 mirror: # 增加流量镜像配置 host: httpbin subset: v2 mirror_percent: 100 然后，再使用 sleep 给 httpbin 发请求时，发现 httpbin v1 及 httpbin v2 会同时打印访问日志。\n$ kubectl exec sleep-96c4ddd7f-ktjgg -c sleep -n istio-demo -- curl -s http://httpbin:8000/headers $ kubectl logs -f -l app=httpbin,version=v1 -c httpbin -n istio-demo 127.0.0.1 - - [30/Dec/2020:00:51:27 +0000] \u0026#34;GET /headers HTTP/1.1\u0026#34; 200 559 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; $ kubectl logs -f -l app=httpbin,version=v2 -c httpbin -n istio-demo 127.0.0.1 - - [30/Dec/2020:00:51:27 +0000] \u0026#34;GET /headers HTTP/1.1\u0026#34; 200 599 \u0026#34;-\u0026#34; \u0026#34;curl/7.35.0\u0026#34; 此即验证了 Istio 的流量镜像功能。\n5 环境清理 测试完成，使用如下命令卸载 httpbin，sleep。\n$ kubectl delete deployment httpbin-v1 httpbin-v2 sleep -n istio-demo $ kubectl delete svc httpbin -n istio-demo 删除临时路由。\n$ kubectl delete virtualservice/httpbin -n istio-demo $ kubectl delete destinationrule/httpbin -n istio-demo 总结本文，介绍了 Istio 支持流量镜像功能，然后使用 httpbin 样例对其进行了测试。\n 参考资料\n[1] Istio Mirroring\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-mirroring.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之流量镜像"},{"categories":["计算机"],"contents":"熔断是创建弹性微服务应用的重要特性，使用熔断可以对并发连接太多，请求过频等做出主动防御，避免服务链条因单一故障问题而出现雪崩效应。\n因熔断设置针对的是具体的目标主机，所以 Istio 使用 Destination Rule 来进行配置。\n本文使用 Istio 自带的 httpbin 样例来设定熔断配置，然后使用 fortio 客户端模拟并发请求来触发熔断。关于 Istio 安装等环境准备，请参阅“Istio 安装使用”。\n1 httpbin 样例部署 httpbin 是一个专门用来做 HTTP 请求测试的服务。\n使用 samples 下自带的部署脚本将其部署。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/httpbin/httpbin.yaml 2 fortio 客户端部署 fortio 是一个专门用来做 HTTP 及 gRPC 测试的客户端。\n使用 samples 下自带的部署脚本将其部署。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/httpbin/sample-client/fortio-deploy.yaml 查看 Pod，其已部署完成。\n$ kubectl get pods -n istio-demo | grep fortio 在该 Pod 执行命令，对 httpbin 发起请求，响应显示请求成功。\n$ kubectl exec fortio-deploy-576dbdfbc4-8gr9c -c fortio -n istio-demo -- /usr/bin/fortio curl -quiet http://httpbin:8000/get HTTP/1.1 200 OK server: envoy date: Tue, 29 Dec 2020 00:57:53 GMT content-type: application/json content-length: 628 access-control-allow-origin: * access-control-allow-credentials: true x-envoy-upstream-service-time: 3 { \u0026#34;args\u0026#34;: {}, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Length\u0026#34;: \u0026#34;0\u0026#34;, \u0026#34;Host\u0026#34;: \u0026#34;httpbin:8000\u0026#34;, \u0026#34;User-Agent\u0026#34;: \u0026#34;fortio.org/fortio-1.11.3\u0026#34;, \u0026#34;X-B3-Parentspanid\u0026#34;: \u0026#34;5eaef1e4a496b17b\u0026#34;, \u0026#34;X-B3-Sampled\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;X-B3-Spanid\u0026#34;: \u0026#34;39a6ff187e9d25f3\u0026#34;, \u0026#34;X-B3-Traceid\u0026#34;: \u0026#34;cb07253ba49f9fb05eaef1e4a496b17b\u0026#34;, \u0026#34;X-Envoy-Attempt-Count\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;X-Forwarded-Client-Cert\u0026#34;: \u0026#34;By=spiffe://cluster.local/ns/istio-demo/sa/httpbin;Hash=d7126b5e272db10e8d7fc2e5a68d724fa01b7bd4fbbe3b21c830156d8ac0c647;Subject=\\\u0026#34;\\\u0026#34;;URI=spiffe://cluster.local/ns/istio-demo/sa/default\u0026#34; }, \u0026#34;origin\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;http://httpbin:8000/get\u0026#34; } 设定并发连接数为 2（-c 2），一次发送 20 个请求（-n 20），报告显示 Code 均为 200。\n$ kubectl exec fortio-deploy-576dbdfbc4-8gr9c -c fortio -n istio-demo -- /usr/bin/fortio load -c 2 -qps 0 -n 20 http://httpbin:8000/get ... Code 200 : 20 (100.0 %) ... 3 熔断测试 对 httpbin 配置 Destination Rule，设置熔断参数。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: httpbin spec: host: httpbin trafficPolicy: connectionPool: tcp: maxConnections: 1 http: http1MaxPendingRequests: 1 maxRequestsPerConnection: 1 outlierDetection: consecutiveErrors: 1 interval: 1s baseEjectionTime: 3m maxEjectionPercent: 100 heredoc\u0026gt; EOF 重新使用 fortio 客户端进行测试：设定并发连接数为 2（-c 2），一次发送 20 个请求（-n 20），报告显示 25.0%的请求返回 Code 503。\n$ kubectl exec fortio-deploy-576dbdfbc4-8gr9c -c fortio -n istio-demo -- /usr/bin/fortio load -c 2 -qps 0 -n 20 http://httpbin:8000/get ... Code 200 : 15 (75.0 %) Code 503 : 5 (25.0 %) ... 进入 fortio 的istio-proxy Sidecar，查看pilot-agent状态，显示有 5 个请求发生溢出。\n$ kubectl exec fortio-deploy-576dbdfbc4-8gr9c -c istio-proxy -n istio-demo -- pilot-agent request GET stats | grep httpbin | grep pending cluster.outbound|8000||httpbin.istio-demo.svc.cluster.local.upstream_rq_pending_overflow: 5 此即说明触发了 httpbin 的熔断设置。\n4 环境清理 测试完成，使用如下命令对 httpbin 及 fortio 进行卸载，删除临时 destinationrule。\n$ kubectl delete -n istio-demo -f samples/httpbin/httpbin.yaml $ kubectl delete -n istio-demo -f samples/httpbin/sample-client/fortio-deploy.yaml $ kubectl delete destinationrule/httpbin -n istio-demo 总结本文，首先介绍了 Istio 支持在 Destination Rule 上配置熔断，然后对 httpbin 样例配置了熔断，并使用 fortio 客户端对其进行了测试。\n 参考资料\n[1] Istio Circuit Breaking\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-circuit-breaking.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之熔断"},{"categories":["计算机"],"contents":"可以使用 Istio 在路由中设置请求超时时间。下面使用 Bookinfo 样例测试一下。\n关于环境准备，请参阅“Istio 安装使用”。\n本文，我们将使用 v2 版本的 reviews，然后为 ratings 注入响应延迟，最后修改 reviews 的超时时间来查看 productpage 的变化。\n开始前，先配置默认的 Destination Rule。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/destination-rule-all.yaml 然后，指定 reviews 使用 v2 版本。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 heredoc\u0026gt; EOF 打开http://$GATEWAY_URL/productpage，刷新几次，Review 部分总是显示黑色五星评价，说明 reviews 已使用 v2 版本。\n1 源码浅析 我们知道 productpage、reviews 及 ratings 的调用关系如下：\nproductpage -\u0026gt; reviews -\u0026gt; ratings 在上文“Istio 流量管理之故障注入”中，我们翻阅过 productpage 及 reviews 的源码。reviews 调用 ratings v2 版本的超时时间为 10s；productpage 调用 reviews 的超时时间为 3s，且若调用失败会重试一次。\nLibertyRestEndpoint.java#L132\nprivate JsonObject getRatings(String productId, HttpHeaders requestHeaders) { ... Integer timeout = star_color.equals(\u0026#34;black\u0026#34;) ? 10000 : 2500; ... } productpage.py#L382\ndef getProductReviews(product_id, headers): # Do not remove. Bug introduced explicitly for illustration in fault injection task for _ in range(2): try: ... res = requests.get(url, headers=headers, timeout=3.0) ... return status, {\u0026#39;error\u0026#39;: \u0026#39;Sorry, product reviews are currently unavailable for this book.\u0026#39;} 2 为 ratings 注入响应延迟 下面为 ratings 注入响应延迟，延迟响应时间为 2s。\n$ kubectl apply -n istio-demo -f - \u0026lt;\u0026lt;EOF heredoc\u0026gt; apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: ratings spec: hosts: - ratings http: - fault: delay: percent: 100 fixedDelay: 2s route: - destination: host: ratings subset: v1 heredoc\u0026gt; EOF 刷新 productpage 页面，发现 2s 后返回页面，但功能未受影响。这是因为没有超过代码中设定的超时时间（代码中 reviews 调用 ratings v2 版本的超时时间为 10s，productpage 调用 reviews 的超时时间为 3s。）。\n下面我们尝试使用 Istio 覆盖 reviews 调用 ratings 的超时时间。\n3 覆盖 reviews 的超时时间 下面，使用 Istio 将 reviews 的超时时间更改为 0.5s。\n$ kubectl edit virtualservice/reviews -n istio-demo apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews ... spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 timeout: 0.5s # add 再次刷新 productpage 页面，发现返回页面需要 1s，且报 reviews 无法访问错误。\n这是因为 reviews 实际调用 ratings 完成后返回得需 2s，而现在 0.5 秒即超时了，productpage 接到超时响应后，又重试一次，所以 productpage 页面耗时 1s。\n测试结束，使用如下命令删除 Destination Rule 及临时路由。\n$ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo -f samples/bookinfo/networking/destination-rule-all.yaml $ kubectl delete virtualservice/reviews -n istio-demo $ kubectl delete virtualservice/ratings -n istio-demo 总结本文，介绍了 Istio 可以覆盖代码设置的超时时间，然后使用 Bookinfo 样例对该特性进行了测试。\n 参考资料\n[1] Istio Request Timeouts\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-request-timeouts.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之请求超时"},{"categories":["计算机"],"contents":"在上文“Istio 流量管理之流量转移”中，我们使用 Istio 为 7 层 HTTP 应用作了流量按比例分配测试。本文使用 Istio 自带的 tcp-echo 样例对 4 层 TCP 应用作一下测试。\n关于 Istio 安装等环境准备，请参阅“Istio 安装使用”。\n1 tcp-echo 源码解析 tcp-echo 是一个 4 层应用。其启动后会一直监听所暴露的端口，并等待 TCP 连接，连接成功后提供 ping/pong 请求响应。从源码可以看到，其接收到一串字符后会拼上一个前缀并返回给客户端。\nmain.go\nfunc main() {...} func serve(addr, prefix string) {...} func handleConnection(conn net.Conn, prefix string) { defer conn.Close() reader := bufio.NewReader(conn) for { // read client request data \tbytes, err := reader.ReadBytes(byte(\u0026#39;\\n\u0026#39;)) ... // prepend prefix and send as response \tline := fmt.Sprintf(\u0026#34;%s %s\u0026#34;, prefix, bytes) conn.Write([]byte(line)) } } 下面我们在本地启动运行一下该程序。暴露端口为 9000，前缀为“hello”。\n$ go run main.go 9000 hello listening on [::]:9000, prefix: hello 服务端起来了，我们使用 nc 命令发起 TCP 连接请求并发送字符串“world”。\n$ nc localhost 9000 world hello world 可以看到服务端拼接了前缀“hello”，返回“hello world”。\n本地测试完成，下面我们尝试使用 Istio samples 文件夹下自带的部署文件将其部署到 Docker Desktop Kubernetes 集群。\n2 tcp-echo Kubernetes 部署 使用 samples 文件夹下自带的 tcp-echo 描述文件将其部署至 Kubernetes 集群。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/tcp-echo/tcp-echo-services.yaml 可以看到，该部署文件有两个 Deployment，对应两个版本的 tcp-echo，版本 v1 的输出前缀为“one”，版本 v2 的输出前缀为“two”。每个 Deployment 暴露两个端口 9000 与 9001，通过同一个 Service 对外提供服务。访问 Service 时，会轮训两个版本的 tcp-echo。\nsamples/tcp-echo/tcp-echo-services.yaml\napiVersion: v1 kind: Service metadata: name: tcp-echo labels: app: tcp-echo service: tcp-echo spec: ports: - name: tcp port: 9000 - name: tcp-other port: 9001 selector: app: tcp-echo --- apiVersion: apps/v1 kind: Deployment metadata: name: tcp-echo-v1 labels: app: tcp-echo version: v1 spec: replicas: 1 selector: matchLabels: app: tcp-echo version: v1 template: metadata: labels: app: tcp-echo version: v1 spec: containers: - name: tcp-echo image: docker.io/istio/tcp-echo-server:1.2 imagePullPolicy: IfNotPresent args: [\u0026#34;9000,9001,9002\u0026#34;, \u0026#34;one\u0026#34;] ports: - containerPort: 9000 - containerPort: 9001 --- apiVersion: apps/v1 kind: Deployment metadata: name: tcp-echo-v2 labels: app: tcp-echo version: v2 spec: replicas: 1 selector: matchLabels: app: tcp-echo version: v2 template: metadata: labels: app: tcp-echo version: v2 spec: containers: - name: tcp-echo image: docker.io/istio/tcp-echo-server:1.2 imagePullPolicy: IfNotPresent args: [\u0026#34;9000,9001,9002\u0026#34;, \u0026#34;two\u0026#34;] ports: - containerPort: 9000 - containerPort: 9001 tcp-echo 部署完成，因为我们需要一个带 nc 命令的 Pod 来测试 tcp-echo。所以下面部署一下 Istio 自带的 sleep 应用，该应用包含基础命名 curl、nc 等，就是用来辅助我们做测试的。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/sleep/sleep.yaml 部署完成，进入 sleep Pod，执行测试命令。\n$ kubectl exec -ti sleep-854565cb79-pcgjv -c sleep -n istio-demo -- sh -c \u0026#39;for i in $(seq 1 10); do echo hello | nc tcp-echo 9000; done\u0026#39; two hello two hello one hello one hello two hello two hello one hello one hello two hello two hello 请求 tcp-echo 10 次，前缀有时为“one”，有时为“two”，说明有时请求到版本 v1，有时请求到版本 v2。\n因 Kubernetes 无法做流量按比例分配，下面使用 Istio 来尝试实现一下。\n3 使用 Istio 对 tcp-echo 作流量分配 使用 Istio 自带的描述文件为 tcp-echo 配置 Gateway，Virtual Service，Destination Rule。\n描述文件samples/tcp-echo/tcp-echo-all-v1.yaml内容如下，tcp-echo 会通过 Gateway 以 31400 端口提供 v1 版本的 TCP 服务。\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: tcp-echo-gateway spec: selector: istio: ingressgateway servers: - port: number: 31400 name: tcp protocol: TCP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: tcp-echo-destination spec: host: tcp-echo subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: tcp-echo spec: hosts: - \u0026#34;*\u0026#34; gateways: - tcp-echo-gateway tcp: - match: - port: 31400 route: - destination: host: tcp-echo port: number: 9000 subset: v1 应用该配置文件：\n$ kubectl apply -n istio-demo -f samples/tcp-echo/tcp-echo-all-v1.yaml 查看 Gateway 外部访问 IP 及端口，本文使用 Docker Desktop 内置 Kubernetes，所以外部访问 IP 即为 localhost。\n$ kubectl get service/istio-ingressgateway -n istio-system 以 Gateway 地址请求 tcp-echo 10 次，发现输出前缀均为“one”，命令如下。\n$ for i in $(seq 1 10); do echo hello | nc localhost 31400; done one hello one hello one hello one hello one hello one hello one hello one hello one hello one hello 下面尝试将 80%的流量打到 v1，20%的流量打到 v2。描述文件samples/tcp-echo/tcp-echo-20-v2.yaml内容如下：\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: tcp-echo spec: hosts: - \u0026#34;*\u0026#34; gateways: - tcp-echo-gateway tcp: - match: - port: 31400 route: - destination: host: tcp-echo port: number: 9000 subset: v1 weight: 80 - destination: host: tcp-echo port: number: 9000 subset: v2 weight: 20 应用配置文件命令如下：\n$ kubectl apply -n istio-demo -f samples/tcp-echo/tcp-echo-20-v2.yaml 然后再次以 Gateway 地址请求 tcp-echo 10 次，发现前缀大概率为“one”，命令如下。\n$ for i in $(seq 1 10); do echo hello | nc localhost 31400; done two hello one hello two hello one hello one hello one hello one hello one hello one hello one hello 测试结束，使用如下命令删除 sleep，tcp-echo 应用，及路由配置。\n$ kubectl delete -n istio-demo -f samples/tcp-echo/tcp-echo-services.yaml $ kubectl delete -n istio-demo -f samples/sleep/sleep.yaml $ kubectl delete -n istio-demo -f samples/tcp-echo/tcp-echo-all-v1.yaml 总结本文，首先介绍了 Istio 除了作 7 层流量转移外，还支持 4 层流量转移。然后对 tcp-echo 样例分别进行了本地测试，Kubernetes 部署，及 Istio 流量转移测试。\n 参考资料\n[1] Istio TCP Traffic Shifting\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-tcp-traffic-shifting.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之 TCP 流量转移"},{"categories":["计算机"],"contents":"在日常的持续部署中，我们一般使用滚动升级的方式来进行微服务升级。若使用 Kubernetes 容器编排平台进行微服务滚动升级，其一般通过控制实例数的方式来实现。将旧版本下线，将新版本启动，新实例健康检查通过后，统一将流量打到新版本。\n而使用 Istio，不用控制实例数，且可以更细粒度的控制流量打到各个版本的百分比，从而实现按比例将流量逐渐迁移到新版本来实现升级。\n下面使用 Bookinfo 样例看一下 Istio 的流量转移如何使用。我们知道 reviews 有三个版本，假定我们想从 v1 版本升级到 v3 版本。（关于 Istio 的安装及 Bookinfo 样例的部署，请参看上文“Istio 安装使用”）\n首先，配置默认的 Destination Rule。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/destination-rule-all.yaml 1 将所有流量打到 v1 版本 为 reviews 配置 Virtual Service，指定访问 reviews 的所有流量打到 v1 版本。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/virtual-service-all-v1.yaml 查看配置：\n$ kubectl get virtualservice/reviews -n istio-demo -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews --- spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 访问 productpage，刷新多次发现 Review 部分均无五星等级评价，说明配置已生效。\n2 逐步提升流量比例将 reviews 升级到 v3 版本 下面，为 reviews 配置 Virtual Service，先将 50%的流量打到 v3。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/virtual-service-reviews-50-v3.yaml 查看配置：\n$ kubectl get virtualservice/reviews -n istio-demo -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews --- spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 50 - destination: host: reviews subset: v3 weight: 50 这时，多次刷新 productpage，发现 Review 部分红色五星评价会时而出现。\n下面，提升比例，将 100%的流量都打到 v3。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/virtual-service-reviews-v3.yaml 查看配置：\n$ kubectl get virtualservice/reviews -n istio-demo -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews --- spec: hosts: - reviews http: - route: - destination: host: reviews subset: v3 这时，无论刷新多少次，productpage 的 Review 部分均显示红色的五星评价，说明流量已 100%切了过来。\n测试结束，执行如下命令将测试路由配置清除。\n$ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo -f samples/bookinfo/networking/virtual-service-all-v1.yaml 综上，我们首先介绍了 Istio 流量转移的应用场景，然后使用 Bookinfo 样例对 reviews 作了测试。\n 参考资料\n[1] Istio Traffic Shifting\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-traffic-shifting.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之流量转移"},{"categories":["随笔"],"contents":"2020馬上就要結束了，總結2020，於國家於自己都是不平凡的一年。\n年初，疫情肆虐，讓每個人都心驚膽戰。可敬的是，我們的民族，每當關鍵時刻，總有人不畏艱險，迎難而上，帶領人們走出困境。\n於自己而言，2020將是一生中都難忘的一年：在這一年中，遇到一些事，明白一些道理。\n這一年，太太生產，女兒出生，帶給我不一樣的人生經歷。更重要的是，鞭策我承擔起做一個好丈夫好父親的責任。\n這一年，身體經歷一次非同尋常的考驗，皆因自己多年的不良習慣所致。磨礪我重新審視自己，審視人生。塞翁失馬，焉知非福，也因此結識中醫，結識傳統文化。解決人生的種種問題，皆應內求。—— 這是我至此所明白的最重要的人生道理。而給我這把鑰匙的人即是於網絡空間相識的張慶祥導師，通過導師對「論語」，「心經」，「陽明心學」的解讀，讓我明白自己出問題的根本——即未順應天道。感歎古人的非凡智慧，中華民族的偉大。\n這樣，我便以「遵守天道，修心養德」為核，「站樁打坐，補氣提神」為法，開始了修心驅邪的療愈過程。幾個月下來，身體得到了不小的改善，心靈得到了很大的淨化。大道至簡，感恩老祖宗留下的瑰寶。\n而自己也由此明白修心學道當為一切之本。自我們的先祖堯、舜、禹始，即有十六字心傳——「人心惟危，道心惟微，惟精惟一，允執厥中」。所以，自上古時期起，我們的老祖先即明白了這天地間的「大道」。人於天地間生存，從事一切活動皆應循「道」，不偏不倚，從於最原始的內心。內心之道是自然賦給人的本能，但隨著人們生活的開展，慢慢的沒那麼純粹了，偏離了自然賦予的本真。而古人又給了我們回歸自然的方法，即「格物，致知，正心，修身」，祖先已將道理和方法都告訴了我們，我們只要照著做就可以了。而照著做亦非易事，瞭解、學習以及實踐這些道理和方法是我國歷代欲成大事者一生修煉的功課。\n知道易，守道難。愿2020是自己此生的一個新篇章，從此學道守道，修心養德，期望像古代先賢一樣能真正做到「俯仰無愧」。\n2020年12月24日於大連\n","permalink":"https://olzhy.github.io/posts/2020-summary.html","tags":["随笔"],"title":"2020 年終總結"},{"categories":["计算机"],"contents":"在微服务架构中，若一个服务不可用，会不会导致调用其 API 的上游服务也不可用，上游服务有没有针对该种情形做容错处理，这对应用的整体可用性来说是很关键的。Istio 可以在对微服务无侵入的情况下来模拟其发生故障，以帮助我们测试应用整体的容错能力。\nIstio 主要使用 Virtual Service 提供两种故障注入能力：响应延迟与服务中止。\n 响应延迟  用来模拟被调用服务在高负载情况下造成响应延迟。\n 服务中止  用来模拟被调用服务不可用或宕机，以 HTTP 错误码的形式返回。\n下面使用 Bookinfo 样例来动手测试一下 Istio 的这两种故障注入能力。\n1 响应延迟注入 在上文“Istio 流量管理之请求路由”中，我们知道如何将特定用户的访问流量打到一个服务的一个版本，而将其余用户的访问流量打到另一个版本。这种“探针式的”路由配置对于在实际应用场景中作调试是非常有用的，因我们使用特定用户作调试不会影响到其他用户的正常使用。\n首先，将默认 Destination Rule 配置一下。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/destination-rule-all.yaml 然后，为 reviews 配置 Virtual Service，将登录用户为 jason 的访问流量打到 reviews 的 v2，其他用户或非登录用户的访问流量打到 reviews 的 v1。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml 查看 Virtual Service，确保配置已生效。\n$ kubectl get virtualservice/reviews -n istio-demo -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: reviews --- spec: hosts: - reviews http: - match: - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v1 这时，当我们使用 jason 账户登录，刷新 productpage 页面会发现，Review 部分始终显示黑色的五星评价等级（即 reviews 的 v2 版本），而不登录或使用其他账户登录时，Review 部分无五星评价等级（即 reviews 的 v1 版本）。\n说明 productpage 的访问流量在按我们预想的情形流动：\n   用户 调用链     jason productpage -\u0026gt; reviews:v2 -\u0026gt; ratings:v1   其他 productpage -\u0026gt; reviews:v1 -\u0026gt; ratings:v1    我们翻阅 reviews 的源码，发现 reviews 调用 ratings 时，若调用黑色五星评价时（即 reviews 使用 v2 版本时），超时时间为 10s，否则为 2.5 秒。\nLibertyRestEndpoint.java#L132\nprivate JsonObject getRatings(String productId, HttpHeaders requestHeaders) { ... Integer timeout = star_color.equals(\u0026#34;black\u0026#34;) ? 10000 : 2500; ... } jason 使用的 reviews 正是 v2 版本，尝试将 ratings 的响应时间改为 7s，因其小于 reviews 10s 的超时时间，我们期待本次更改对 jason 的访问不受影响。\n下面，即按照如上设想为 ratings 配置 Virtual Service，若访问用户为 jason，延迟 ratings 的响应时间为 7s，而其他用户访问不受影响。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/virtual-service-ratings-test-delay.yaml 查看 Virtual Service，确保配置已生效。\n$ kubectl get virtualservice/ratings -n istio-demo -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: ratings --- spec: hosts: - ratings http: - fault: # 将用户为jason的所有访问流量延迟响应时间为7s delay: fixedDelay: 7s percentage: value: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: # 其他用户不受影响 - destination: host: ratings subset: v1 这时，使用 jason 账户登录 productpage 进行访问时，发现 Review 部分出错（Sorry, product reviews are currently unavailable for this book.）。\n问题出现在哪里了呢？翻阅 productpage 的源码，发现这里将调用 reviews 的超时时间设置小了（超时时间为 3s，若失败则重试一次，所以总的超时时间为 6s）。\nproductpage.py#L382\ndef getProductReviews(product_id, headers): # Do not remove. Bug introduced explicitly for illustration in fault injection task for _ in range(2): try: ... res = requests.get(url, headers=headers, timeout=3.0) ... return status, {\u0026#39;error\u0026#39;: \u0026#39;Sorry, product reviews are currently unavailable for this book.\u0026#39;} 所以，如上即是使用 Istio 进行响应延时注入及定位 Bug 的全过程。\n2 服务中止注入 下面，看一下 Istio 的服务中止注入。依旧采用 1 中的配置，只对 ratings 的 Virtual Service 配置作少量更改，即若访问用户为 jason，则让 ratings 返回 500 错误，看看前端页面有什么影响。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml 查看配置信息：\n$ kubectl get virtualservice/ratings -n istio-demo -o yaml apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: ratings --- spec: hosts: - ratings http: - fault: # 若访问用户为jason，则返回500错误 abort: httpStatus: 500 percentage: value: 100 match: - headers: end-user: exact: jason route: - destination: host: ratings subset: v1 - route: # 其他用户访问不受影响 - destination: host: ratings subset: v1 使用 jason 账号登录 productpage 页面，发现 Review 部分显示 ratings 无法访问错误（Ratings service is currently unavailable）。\n测试结束，使用如下命令删除临时路由即可。\n$ kubectl delete -n istio-demo -f samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml $ kubectl delete -n istio-demo -f samples/bookinfo/networking/virtual-service-ratings-test-abort.yaml 总结本文，首先介绍了 Istio 支持两种故障注入模式（响应延时注入与服务中止注入），可以帮助我们在无侵入服务的情形下测试应用整体的容错能力。然后使用 Bookinfo 分别测试了如何进行此两种注入。\n 参考资料\n[1] Istio Fault Injection\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-fault-injection.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之故障注入"},{"categories":["计算机"],"contents":"在上文“Istio 安装使用”中，我们对 Istio 进行了安装，并对 Bookinfo 样例进行了部署测试。本文接着上文，对 Istio 流量管理中的请求路由进行概念学习及样例测试。\n我们知道，Istio 通过 Envoy 数据面拦截了所有服务实例的进出流量。这样基于 Istio 服务网格即可以实现诸多常规方式难以实现的流量管理策略，诸如灰度发布，A/B 测试，按比率分流等。\nIstio 主要提供两个通过 YAML 配置的自定义资源来实现流量管理：Virtual Service 及 Destination Rule。这样即做到流量管理与上游请求服务及下游被请求服务解耦。Virtual Service 主要用来配置流量如何流动（即定义符合哪些规则的流量打到哪些服务子集上），而 Destination Rule 则主要用来定义具体的服务子集。\n下面分别看一下 Vistual Service 及 Destination Rule 的概念，最后使用 Bookinfo 样例进行简单的路由配置及测试。\n1 Vistual Service Virtual Service 主要用来配置流量如何流动。典型的使用场景是将流量路由到一个服务的不同版本（通过指定服务子集实现），如实现按比例分配流量，灰度发布等。区别于 Kubernetes 实现的主要优势在于，无须通过调整实例数来实现流量分配，流量路由已与部署实例解耦。另一个使用场景是使用 Virtual Service 为一个 namespace 下所有不同的服务提供统一的路由配置。\n下面通过具体的样例来学习 VirtualService 的配置。\na）为一个服务的不同版本配置路由\n下面使用 VirtualService 为 Bookinfo 的 reviews 服务的几个不同子集配置路由规则，实现将特定的用户访问流量打到特定的版本。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: # 列出Virtual Service的hosts，可以是IP，DNS名称，FQDN或* - reviews http: # 在下面配置Virtual Service的路由规则，指定符合哪些规则的流量打到哪些Destination，支持HTTP/1.1，HTTP2，及gRPC等协议 - match: # 指定具体的匹配规则 - headers: end-user: exact: jason route: - destination: # 指定满足规则后将流量打到哪个具体的Destination host: reviews subset: v2 - route: # 流量规则按从上到下的优先级去匹配，若不满足上述规则时，进入该默认规则 - destination: host: reviews subset: v3 b）为不同的服务提供统一的路由配置\n下面使用 VirtualService 为 Bookinfo 的两个不同服务 reviews 及 ratings 提供路由配置。基于不同的请求 URI 将流量导向不同的服务。支持使用 URI 前缀或正则进行匹配。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - bookinfo.com http: - match: - uri: prefix: /reviews route: - destination: host: reviews - match: - uri: prefix: /ratings route: - destination: host: ratings 除了使用 match 来编写条件，还可以使用 weight 来指定权重。下面使用 VirtualService 指定将 75%的流量打到 reviews 的 v1，25%的流量打到 reviews 的 v2。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 75 - destination: host: reviews subset: v2 weight: 25 2 Destination Rule Destination Rule 主要用来定义服务的不同子集。这样 Virtual Service 即可定义路由规则，将一个服务的哪些流量打到哪些子集。Destination Rule 除了定义服务子集外，还可以为整个目标服务或特定子集的服务设置 Envoy 的流量策略，如负载均衡策略，TLS 安全模式，或熔断设置。\n下面使用 DestinationRule 为 reviews 定义了 3 个子集 v1，v2 及 v3（使用 Kubernetes label 实现）。v1 与 v3 采用 RANDOM 负载均衡策略，v2 采用 ROUND_ROBIN 负载均衡策略。\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews trafficPolicy: loadBalancer: simple: RANDOM subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 trafficPolicy: loadBalancer: simple: ROUND_ROBIN - name: v3 labels: version: v3 3 Bookinfo 样例请求路由配置 在上文“Istio 安装使用”中，我们知道如何部署 Bookinfo 样例应用。且知道 Bookinfo 由如下几个服务组成，除了 reviews 拥有 3 个版本外，其它服务均只有 1 个版本。reviews 的 v1 版本未有五星评价等级，v2 版本的五星评价等级展示颜色为黑色，v3 版本的五星评价等级展示颜色为红色。\n而且，我们只使用如下命令部署了 Bookinfo 的各个服务，及使用 Gateway 与 Virtual Service 配置了简单的路由规则，指定 productpage 为统一的流量入口。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl apply -n istio-demo -f samples/bookinfo/networking/bookinfo-gateway.yaml bookinfo-gateway.yaml配置：\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \u0026#34;*\u0026#34; gateways: - bookinfo-gateway http: # 指定满足productpage入口路径，登录登出路径，以static为前缀的静态资源路径，及以/api/v1/products为前缀的API路径的流量，均打到目标服务productpage:9080 - match: - uri: exact: /productpage - uri: prefix: /static - uri: exact: /login - uri: exact: /logout - uri: prefix: /api/v1/products route: - destination: host: productpage port: number: 9080 未指定具体的路由规则前，productpage 请求各个服务时使用轮训策略，所以我们刷新 productpage 页面可以看到 Review 部分有时为黑色的五星评价等级，有时为红色的五星评价等级，有时无五星评价等级，即流量轮训了 reviews 服务的各个版本。\n下面我们依照上述介绍，对 reviews 服务使用 Virtual Service 及 Destination Rule 配置不同的路由规则并进行验证测试。\na）将访问 reviews 的流量都打到一个版本\n首先，为 reviews 配置 Destination Rule，定义服务的子集并指定负载均衡策略为 RANDOM。\n$ cd /usr/local/istio-1.8.1 $ cat destination-rule-reviews.yaml apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews trafficPolicy: loadBalancer: simple: RANDOM subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 $ kubectl apply -n istio-demo -f destination-rule-reviews.yaml 然后，为 reviews 配置 Virtual Service，将访问 reviews 的所有流量都打到 v1。\n$ cd /usr/local/istio-1.8.1 $ cat virtual-service-all-v1.yaml --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 $ kubectl apply -n istio-demo -f virtual-service-all-v1.yaml 这时，我们刷新 productpage 页面多次会发现，Review 部分始终无五星评价等级。即说明所有访问 reviews 的流量都打到了 v1 版本。\n下面我们看一下如何指定特定用户的访问流量打到特定的版本。\nb）将访问 reviews 的流量按特定用户打到特定版本\n还采用 a）中 Destination Rule 的配置。\n采用如下命令，将 a）中 reviews 的 Virtual Service 配置删除：\n$ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo -f virtual-service-all-v1.yaml 重新为 reviews 配置 Virtual Service，若登录用户为 jason，则将流量打到 v2，否则打到 v3。\n$ cd /usr/local/istio-1.8.1 $ cat virtual-service-reviews-jason-v2-v3.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - match: # header满足特定条件则打到v2 - headers: end-user: exact: jason route: - destination: host: reviews subset: v2 - route: # 不满足如上条件则打到v3 - destination: host: reviews subset: v3 $ kubectl apply -n istio-demo -f virtual-service-reviews-jason-v2-v3.yaml 这时，当我们使用 jason 账户登录，刷新 productpage 页面会发现，Review 部分始终显示黑色的五星评价等级（即 reviews 的 v2 版本）。\n而不登录或使用其他账户登录时，Review 部分始终显示红色的五星评价等级（即 reviews 的 v3 版本）。\n此即验证了 Istio 支持通过配置路由规则将特定用户的访问流量打到特定的版本，其原理是将特定用户标识通过前端一层层传下来，然后 Envoy 根据配置规则实现路由。\n下面看一下如何按比例将流量打到同一服务的不同版本。\nc）将访问 reviews 的流量按比例打到不同的版本\n还采用 a）中 Destination Rule 的配置。\n采用如下命令，将 b）中 reviews 的 Virtual Service 配置删除：\n$ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo -f virtual-service-reviews-jason-v2-v3.yaml 重新为 reviews 配置 Virtual Service，将 90%的流量打到 v1，剩余 10%的流量打到 v2。\n$ cd /usr/local/istio-1.8.1 $ cat virtual-service-reviews-90-10.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: # 将90%的流量打到v1 host: reviews subset: v1 weight: 90 - destination: # 将10%的流量打到v2 host: reviews subset: v2 weight: 10 $ kubectl apply -n istio-demo -f virtual-service-reviews-90-10.yaml 这时，多次刷新 productpage 页面，发现 Review 部分大概率无五星评价等级，小概率显示黑色五星评价等级。\n测试完毕，使用如下命令删除相关路由配置。\n$ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo virtual-service-reviews-90-10.yaml $ kubectl delete -n istio-demo destination-rule-reviews.yaml 若想卸载 Bookinfo 应用或卸载 Istio，可以参看上文“Istio 安装使用”。\n总结本文，我们首先介绍了支持 Istio 流量管理的两个主要的资源 Virtual Service 及 Destination Rule，然后对 Bookinfo 样例使用 Virtual Service 及 Destination Rule 进行配置，测试了几个常用的流量转发场景。\n 参考资料\n[1] Istio Request Routing\n[2] Istio Traffic Management\n ","permalink":"https://olzhy.github.io/posts/istio-request-routing.html","tags":["服务网格","Istio"],"title":"Istio 流量管理之请求路由"},{"categories":["计算机"],"contents":"本文所使用的操作系统为 macOS 11.1，使用 Docker Desktop 3.0.1 自带的 Kubernetes(v1.19.3) 作为部署环境。\n1 Istio 下载及安装 进入 Istio发布页面，下载适配本文操作系统的最新版本istio-1.8.1-osx.tar.gz，然后解压到/usr/local/istio-1.8.1，可以看到下面包含bin及samples文件夹，bin里包含istioctl命令，samples里包含 Istio 自带的样例应用的部署配置。\n$ cd /usr/local/istio-1.8.1 $ tree . ├── bin │ └── istioctl ├── samples │ ├── README.md │ ├── addons │ ├── bookinfo ... 修改/etc/profile，将/usr/local/istio-1.8.1/bin追加到PATH，这样即可以随时随地使用istioctl命令了。\n$ export PATH=/usr/local/istio-1.8.1/bin:$PATH 因我们安装 Istio 主要作样例演示，所以选择profile=demo，安装命令如下：\n$ istioctl install --set profile=demo -y ... ✔ Istio core installed ✔ Istiod installed ✔ Egress gateways installed ✔ Ingress gateways installed ✔ Installation complete 约 1 分钟后，其主要组件 Istiod, Ingress Gateway, Egress Gateway 都安装完成了。可以发现，其将上述组件安装到了istio-system这个 namespace 下。\n$ kubectl get deployments -n istio-system NAME READY UP-TO-DATE AVAILABLE AGE istio-egressgateway 1/1 1 1 14h istio-ingressgateway 1/1 1 1 14h istiod 1/1 1 1 14h 2 Bookinfo 样例应用部署 在部署样例应用前，我们新建一个专门用来演示的 namespace istio-demo，且标记该 namespace 使用 istio 自动注入。\n$ kubectl create namespace istio-demo $ kubectl label namespace istio-demo istio-injection=enabled 接下来先粗略看一下待部署应用 Bookinfo 的几个模块。\n$ cd /usr/local/istio-1.8.1 $ tree -L 1 samples/bookinfo/src . ├── productpage // Bookinfo的页面入口，前后台一体，JavaScript + Python实现 ├── details // 图书详情后台服务，Ruby实现 ├── reviews // 图书评价后台服务，Java实现，采用Liberty部署 └── ratings // 图书评价等级后台服务，nodejs编写，数据库采用mysql或mongodb 下面，使用 Istio samples文件夹下自带的配置部署 Bookinfo 应用：\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -n istio-demo -f samples/bookinfo/platform/kube/bookinfo.yaml ... deployment.apps/reviews-v1 created deployment.apps/reviews-v2 created deployment.apps/reviews-v3 created ... 可以看到reviews组件部署了 3 个版本，除此之外，其他组件均部署了一个版本。\n3 Bookinfo 样例应用访问 查看 deployments 及 pods，发现 Bookinfo 的各个组件已部署完成：\n$ kubectl get deployments -n istio-demo NAME READY UP-TO-DATE AVAILABLE AGE details-v1 1/1 1 1 7m6s productpage-v1 1/1 1 1 7m4s ratings-v1 1/1 1 1 7m6s reviews-v1 1/1 1 1 7m5s reviews-v2 1/1 1 1 7m5s reviews-v3 1/1 1 1 7m5s $ kubectl get pods -n istio-demo NAME READY STATUS RESTARTS AGE details-v1-79c697d759-c8h6k 2/2 Running 0 7m12s productpage-v1-65576bb7bf-5ln54 2/2 Running 0 7m11s ratings-v1-7d99676f7f-2k75j 2/2 Running 0 7m12s reviews-v1-987d495c-njj9f 2/2 Running 0 7m12s reviews-v2-6c5bf657cf-c6x46 2/2 Running 0 7m12s reviews-v3-5f7b9f4f77-mpt9z 2/2 Running 0 7m12s 下面我们试着在 ratings 容器里访问 Bookinfo 的入口页面 productpage。\n使用kubectl describe pod可以发现 ratings pod 除了原有容器 ratings 外，多了两个 Sidecar：istio-init 与 istio-proxy。\n$ kubectl describe pod/ratings-v1-7d99676f7f-2k75j -n istio-demo ... Created container istio-init ... Created container ratings ... Created container istio-proxy 所以，执行命令时，需指定容器为 ratings，curl 请求 productpage，发现页面标题已可正常显示。\n$ kubectl exec ratings-v1-7d99676f7f-2k75j -c ratings -n istio-demo -- curl -s productpage:9080/productpage | grep -o \u0026#34;\u0026lt;title\u0026gt;.*\u0026lt;/title\u0026gt;\u0026#34; \u0026lt;title\u0026gt;Simple Bookstore App\u0026lt;/title\u0026gt; 下面看一下该应用如何在集群外部进行访问。涉及到通过配置 Istio 的 Ingress Gateway，从而将流量打到 productpage。同样，需要执行下samples文件夹下自带的配置文件。\n$ kubectl apply -n istio-demo -f samples/bookinfo/networking/bookinfo-gateway.yaml 然后查看下 Ingress Gateway 的 ip 及端口。\n$ kubectl get service istio-ingressgateway -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.108.227.8 localhost ...80:32008/TCP,443:30895/TCP... 15h 所以，对于本文所采用的 Docker Desktop K8s 本地部署环境来说，外部 IP 就是 localhost。采用http://localhost/productpage即可访问 Bookinfo 的 productpage 页面。\n4 Istio Dashboard 安装 下面安装一下 Istio 的几个插件，初步体验里边的一些功能。\n$ cd /usr/local/istio-1.8.1 $ kubectl apply -f samples/addons ... deployment.apps/kiali created deployment.apps/prometheus created deployment.apps/jaeger created ... a）先看一下 Kiali 面板\n键入如下命令，可以看到，打开了 kiali 面板。\n$ istioctl dashboard kiali 然后，查看istio-demo namespace 的应用拓扑图。\n可以看到，调用关系一目了然，请求由 Istio Ingress Gateway 进来，首先访问 productpage，productpage 访问 details 获取图书详情，productpage 访问 reviews 获取评论，reviews 访问 ratings 获取图书评级。\nb）再看一下 Jaeger 面板\n键入如下命令，打开 jaeger 面板。\n$ istioctl dashboard jaeger 左侧 Service 下拉菜单，选择productpage.istio-demo，从右面的 Traces 里点击 productpage，可以看到如下调用详情。\n调用链以时间序横向展示，同样可以看到请求由 istio-ingressgateway 进来到达 productpage，productpage 调用 details 及 reviews，reviews 调用 ratings，每个调用的时间花费亦显示了出来。\n5 Istio 卸载 Istio 初探结束，按照如下步骤依序进行卸载。\n 卸载 addons  $ cd /usr/local/istio-1.8.1 $ kubectl delete -f samples/addons  卸载 Bookinfo  $ cd /usr/local/istio-1.8.1 $ kubectl delete -n istio-demo -f samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl delete -n istio-demo -f samples/bookinfo/networking/bookinfo-gateway.yaml  卸载 Istio  $ istioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f -  删除 namespace istio-system  $ kubectl delete namespace istio-system  取消对 istio-demo 进行 Istio 自动注入  $ kubectl label namespace istio-demo istio-injection-  删除 namespace istio-demo  $ kubectl delete namespace istio-demo  参考资料\n[1] Get Started - Istio\n ","permalink":"https://olzhy.github.io/posts/istio-get-started.html","tags":["服务网格","Istio"],"title":"Istio 安装使用"},{"categories":["计算机"],"contents":"一般的认为是：API Gateway用来处理南北向流量，Service Mesh用来处理东西向流量。这样的区分方式并不准确。下面会递进式分析两者的使用场景及异同点，以期通过本文可以明白何时使用API Gateway，何时使用Service Mesh？\n1 API Gateway的使用场景 API Gateway一般是一个业务单元以“产品”的方式对外部客户或对内部其它业务单元进行API暴露的统一入口。其一般在网络模型的7层，使用独立于内部系统的中心化部署方式，作为一个Edge服务对外提供访问能力。\n（图片引自The difference between API Gateways and Service Mesh）\n所以从这个场景出发，API Gateway关注的是：\n 隐藏内部实现细节，让API保持稳定，对客户友好  以产品的方式对外提供稳定的统一的API入口，除了作为一个Proxy提供通用的诸如负载均衡等能力外，其可能会涉及一部分业务逻辑，如API聚合，协议转换（如内部使用gRPC，API Gateway使用REST，GraphQL）等。\n 保障边界安全  作为外部客户使用API的统一入口，在API Gateway提供统一的授权鉴权，全局的熔断限速，请求及返回验证等功能。\n 全流程API管理  提供全流程API管理。诸如提供用户注册使用API的入口，提供API文档管理，API测试管理等。\n2 Service Mesh的使用场景 由前文“什么是服务网格？”可知，服务网格解决的是系统内部服务与服务通信的问题。其采用分布式的Sidecar方式与服务实例一同部署，更关注的是内部系统的可信连接，安全通信及可观察能力。\n（图片引自The difference between API Gateways and Service Mesh）\n 与服务解耦，以进程外的方式代理服务进出流量  与服务解耦，以进程外的方式，统一代理服务的进出流量。\n 保障服务与服务通信安全  采用mTLS等，其可保障端到端的通信安全。\n 更细节的可观察性  使用服务网格，可以监控到系统内部所有服务与服务的调用，可以观察到更多细节的度量指标。\n3 何时使用API Gateway？何时使用Service Mesh？ 通过如上简述，我们对API Gateway及Service Mesh的不同使用场景有了一个简单的认识。下面，总结一下，何时使用API Gateway？何时使用Service Mesh？\n决定是否使用API Gateway的一个重要决策点是：是否有需求将内部API以中心化的方式作为一个“产品”来对外提供服务？\n（图片引自The difference between API Gateways and Service Mesh）\n而决定是否使用Service Mesh的一个重要决策点是：是否有需求将所有服务的通信使用分布式的Sidecar来管理，以提供更好的连接，安全，可观察能力？\n一般来说，现代云原生架构下，两者均是需要的，以各自适用的场景及互补的能力来提供系统整体的可用性，可靠性。\n 参考资料\n[1] The difference between API Gateways and Service Mesh\n[2] Do I Need an API Gateway if I Use a Service Mesh?\n ","permalink":"https://olzhy.github.io/posts/api-gateway-and-service-mesh.html","tags":["架构设计","服务网格"],"title":"API Gateway与Service Mesh有什么不同？"},{"categories":["计算机"],"contents":"随着应用规模的不断扩大，单体架构已不能承载企业越来越多的业务需求，微服务架构随之兴起。微服务给我们带来诸多益处的同时也带来诸多挑战，其根源即是复杂性的提升。为了解决微服务带来的诸多问题，其中便催生了服务网格的流行。但2020年初，业内最知名的服务网格实现Istio却反其道而行之，由微服务架构重回单体架构，其原因是什么呢？可能是一个契机，让我们重新审思微服务架构带来的好处及问题。\n1 微服务架构有什么优势？ 将一个复杂的单体应用切分为按领域细分的微服务后，可以让团队聚焦所关注的领域，做到相互独立，彼此不受影响。其带来的优势主要有：\n 彼此独立交付，快速迭代  各自解耦的微服务，可以让彼此间有明确的边界，各自可以采用不同的语言或技术栈，基于轻量协议（HTTP，RPC等）进行交互。每个微服务可以拥有自己的生命周期，无须相互协调或等待，做到彼此独立交付，相互不受影响。因粒度小，迭代快，从总体看，可以做到并行开发，流水线式产出。\n 明确安全边界  不同领域的微服务拥有不同的安全要求。微服务可以让彼此明确安全边界，采用不同等级的防护策略。\n2 微服务架构有什么劣势？ 想必每个团队采用微服务架构的初衷是想降低应用或系统的复杂度，将一个大的问题拆分为几个独立的小问题来解，从而做到分而治之。但拆分后的微服务可能会非常多，调用链会变得非常复杂，随着时间的迁移，甚至变得不可观察。再一个重要的问题即是对运维的要求非常高，之前是一个单一运行时，部署起来非常简单。现在部署一个应用，对应的是一串微服务，相互之间的依赖关系，系统要求均需要考虑清楚。归根结底是微服务化后并没有降低系统的复杂性，反而增加了系统的复杂性。所以为了解决这些问题，衍生出了容器部署技术，容器编排技术等。\n3 Istio之前的架构是什么样的？ 在我们了解了微服务架构的优劣之后，看看本文探讨的重点：Istio之前的架构是什么样的？为什么要进行如此重大调整？\nIstio之前的总体架构如下图所示，与业内其它服务网格采用的架构类似，即分为数据面与控制面两部分。数据面由一组Proxy（或称Sidecar）组成，这些Proxy与服务的实例一同部署，代理了服务的所有进出流量。控制面部署在这些服务的外层，统一负责管理与控制数据面的Proxy。\n（图片引用自Istio as an Example of When Not to Do Microservices）\n最开始，Istio控制面是采用微服务方式实现的，主要有如下几部分：\n  Pilot 负责在运行时对Proxy进行配置\n  Galley 负责监听配置更新，校验及分发配置\n  Citadel 负责证书签发，密钥生成，及与CA的集成\n  Injector 负责对服务的自动注入\n  该种微服务的架构有什么问题？Istio为什么又回归单体应用呢？\n采用微服务架构后，对于Istio开发团队而言，每个服务的确可以独立开发，独立迭代。但对用户而言，不论其内部分几个模块，其需要统一发布，统一提供服务。若使用者在使用过程中遇到涉及Istio内部棘手的问题需要定位，则增加了定位难度。所以Istio开始考虑回归单体，让其使用变得更简单。\n4 Istio现在的架构是什么样的？ 下面即是回归单体后的Istio架构图，可以看到，原先被分割为多个微服务的控制面整合为了一个名为istiod的单体服务。这样即可让其安装，部署，使用，配置，维护，调试变得更简单，同时节省了资源开销。\n（图片引用自Istio as an Example of When Not to Do Microservices）\n5 Istio的架构变迁对我们有什么启发？ 最后，我们谈一下Istio的架构变迁对我们有什么启发？\n首先，这是Istio根据自身特性做的一次架构调整，其场景更适合单体架构，并不能代表整个业界的趋势。其次，微服务架构几乎是现代应用的标配，其给我们带来了诸多的好处，同时也给系统带来极大的复杂性，在系统设计中，我们要根据自己的实际情况对应用面向的客户，使用场景，采用微服务后的性价比做深入的分析与考量。同时微服务的切分要做到粒度恰当，要避免拆的过大，更要避免拆的过小，要结合自己系统的真实情况做选择，不管采用何种架构方式，我们的目的是让系统变得更“简单”。\n 参考资料\n[1] Introducing istiod: simplifying the control plane\n[2] Istio as an Example of When Not to Do Microservices\n ","permalink":"https://olzhy.github.io/posts/why-istio-back-to-monolithic-architecture.html","tags":["架构设计","服务网格","Istio"],"title":"为什么 Istio 重回单体架构？"},{"categories":["计算机"],"contents":"1 什么是服务网格？ 服务网格是分布式软件系统内部用于管理所有“服务到服务”通信的一个系统。\n聊服务网格为什么会出现之前，可以聊聊服务架构的演进过程。起初，我们使用一个单体应用来提供服务。 比如我们在做一个电商系统，采用典型的MVC三层架构，在单体架构中，组成这个系统的购物车功能，库存查询功能，订单功能等都是这个服务内部的一个函数或接口。所以这些操作都是进程内的函数调用，不涉及诸如RPC等服务与服务的跨进程通信。但随着时间的增加，我们发现单体架构越来越不能满足我们的需求，比如用户访问暴增，业务逻辑愈加复杂，一个单体的服务已不能满足功能及性能的要求。我们需要将其按业务领域拆分为几个独立的服务来对外提供服务，这就是微服务架构。比如原来的购物车功能，库存查询功能，订单功能被拆分为独立的服务。这时接收到一个购物请求，我们需要分别查询不同的微服务来进行业务处理，这就涉及跨进程通信。\n而由于服务到服务通信的网络不稳定性，所以如下问题随即出现了：请求失败时如何进行重试？请求超时如何处理？请求太频繁如何限速？服务熔断怎么做？\u0026hellip; 而基于语言或框架的辅助库即是一种解决方案，如Netflix的Eureka，Hystrix等。而在服务里边解决服务与服务通信问题存在多个弊端，如：需要侵入业务代码，受语言或框架限制等。所以，完全解耦的，基于代理模式的服务网格设计理念更符合业界的青睐。\n下图即是一个服务网格部署图，即每个服务的实例旁边都有一个代理，这些代理被形象的称为Sidecar，其来负责对被代理服务的进出流量进行管理。整个系统的所有这些负责服务与服务通信的Sidecar即组成一个服务网格。所以服务只要关注业务逻辑即可，服务与服务通信的逻辑被抽到了服务网格里边，实现了解耦。\n因服务网格代理了分布式系统内所有服务与服务通信的进出流量，相当于将分布式系统中最关键的一些跨进程连接点串联起来，并将其管理，观察。所以其就像系统的眼睛一样，让开发者不再对庞大的调用网络望而生畏，让分布式系统像单体架构一样可以做到可观察。\n2 服务网格发展历史  2010年初，Twitter开始开发基于Scala的Finagle，自此Linkerd服务网格诞生。 2013年末，SmartStack提供一种基于HAProxy的进程外的服务发现机制以满足逐渐兴起的微服务架构。 2014，Netflix发布包括Prana的一组JVM工具包，允许不同语言开发的服务通过HTTP进行调用。 2016，NGINX开发Fabric Model，一个基于NGINX Plus的类服务网格产品。 2017以后，Linkerd，Istio，Maesh，Kuma逐步兴起。  3 服务网格架构 经过上面的介绍，我们对服务网格的衍生及其解决的问题有了一定的了解。本节粗略看一下业内服务网格的通用设计及系统架构，以期对其有一个更好的认识。\n服务网格主要由两部分组成：负责管理服务与服务通信的Sidecar Proxy被称为数据面；以UI，配置及命令等控制数据面行为的被称为控制面。\n4 使用服务网格可以做什么？ 因服务网格代理了系统内所有服务与服务通信的流量，所以其可以做很多事情。\n 流量管理  提供灰度发布，按规则流量分发等功能。\n 熔断重试  提供超时处理，熔断，重试等功能。\n 鉴权授权  提供mTLS安全通信，服务与服务的鉴权授权等功能。\n 观察及监控  提供请求量统计，请求延时统计，请求成功率统计，分布式链路追踪等功能。\n5 服务网格的实现  Linkerd  CNCF孵化项目，100%开源，Rust实现，目标是极简，能用，数据面代理极小(\u0026lt;10mb)，速度极快(\u0026lt;1ms)。\n Istio  最流行的服务网格实现，基于Envoy数据面，背靠谷歌及IBM，功能丰富。\n Consul  HashiCorp出品，基于Envoy数据面，支持的平台多。\n Kuma  基于API Gateway Kong的服务网格。\n Traefik Mesh  基于云原生API Gateway Traefik的服务网格。\n更详细对比，请参考 servicemesh.es。\n6 服务网格的未来 Mecha - 多运行时微服务架构。未来架构的趋势可能是将所有传统的中间件迁移至其它运行时，只在服务中编写业务逻辑。微软Dapr是业界第一个多运行时实践项目。\n 参考资料\n[1] What is a service mesh? - RedHat\n[2] What is a service mesh? - NGINX\n[3] Service Mesh Ultimate Guide: Managing Service-to-Service Communications in the Era of Microservices - InfoQ\n[4] What\u0026rsquo;s a service mesh? And why do I need one? - Buoyant\n ","permalink":"https://olzhy.github.io/posts/what-is-a-service-mesh.html","tags":["架构设计","服务网格"],"title":"什么是服务网格？"},{"categories":["随笔"],"contents":"这是一次难忘的登山旅行。\n来大连三年，除有几次休闲性的登山外，未随驴友真正意义的爬一次山。这次，准备跟随专业的驴友爬一次山，初衷是想捡回时隔三年的登山习惯，感受登山的快乐。\n28号，早上5点起床，穿上登山鞋，冲锋衣，带上太太前日已帮准备好的吃的，带了两瓶热水就出发了。6点半与驴友集合，坐上了去往目的地的小巴。他们多是常年登山的专业驴友，我是他们当中唯一的新人。约行2个半小时，到达目的地盖州青龙山。\n眼前是层峦叠嶂的山峰，不算太高，魏延似青龙，山脚下是一个小湖，晨阳打在里边，波光粼粼。\n刚开始兴致勃勃跟随队伍穿过几个小山坡，当开始真正的上爬模式时，才知道前路艰辛。气息加快，开始用嘴呼吸。常年不运动，身体非常吃力，腿上如绑铅块，难以行进，胳膊上没有力气，抓着石头攀爬，暼到脚下的深谷，感觉就要掉了下去。再看看随行的驴友，个个生龙活虎，如履平地，其中一位65岁的大爷，气息充足，当我毫无气力时，人家放声高歌。这鲜明的对比，真是羞愧难当。我用尽所有力气与全部意志跟随队伍穿越了这两个峰谷。\n终于可以稍作修整，席地而坐，开始吃午餐。午饭过后，开始继续前进。这时的体力比刚刚好了一些。虽上爬时仍觉吃力，但可以跟大家开口说话了，心跳也舒缓了许多。就这样跟随队伍走完了全程。我为自己能克服“厄困”，意志坚强的走下来而喝彩。\n下山后，队友们仍意兴未够，要徒步走到10多公里外的集合点。这次是我此次旅行的第二个挑战，怎么都走不到，看着眼前那不是很陡的长坡，犹如渡劫，腿重的就像绑了沙袋，脚底疼的起了泡。终于走到了小巴车的位置，终于坐下来，感慨万千。\n自己这两年的体力太遭了，要将爬山的习惯捡起来，强健身心。这次随行的驴友都有5到10年的登山龄，而且个个看上去比实际年龄要年轻10岁。健身不是一朝一日的事，须十年几十年如一日，方能将自己真正强健起来。\n 2020年11月29日于大连\n","permalink":"https://olzhy.github.io/posts/qinglongshan.html","tags":["随笔"],"title":"记青龙山之行"},{"categories":["计算机"],"contents":"1 题目描述 给定一棵二叉树的先序遍历及中序遍历，尝试构建该二叉树。\n说明：\n 假定树中不存在值重复的情形  例如：\n 输入  preorder = [3,9,20,15,7] inorder = [9,3,15,20,7]  输出  3 / \\ 9 20 / \\ 15 7 题目出处：LeetCode\n2 解决思路 本文采用递归方式实现：\n 从先序遍历数组中拿出第一个值，其即为根节点值； 从左到右遍历中序遍历数组，找到根节点值出现的位置，其左边的部分构成该根节点的左子树，右边的部分构成该根节点的右子树； 回到先序遍历数组，跳过第一个值，从左往右，拿出与上一步找到的左子树同样数目的值构成该根节点的左子树，剩余部分则构成该根节点的右子树； 按照上述三步递归处理，直至先序遍历数组及中序遍历数组为空。  3 Golang实现代码 https://github.com/olzhy\nfunc buildTree(preorder []int, inorder []int) *TreeNode { if 0 == len(preorder) { return nil } root := preorder[0] i := 0 for ; i \u0026lt; len(inorder); i++ { if root == inorder[i] { break } } return \u0026amp;TreeNode{ Val: root, Left: buildTree(preorder[1:i+1], inorder[:i]), Right: buildTree(preorder[i+1:], inorder[i+1:]), } } 4 Python实现代码 https://github.com/olzhy\nclass Solution: def build_tree(self, preorder: List[int], inorder: List[int]) -\u0026gt; TreeNode: if 0 == len(preorder): return None val = preorder[0] for i in range(len(inorder)): if val == inorder[i]: break node = TreeNode(val=val) node.left = self.build_tree(preorder[1:1 + i], inorder[:i]) node.right = self.build_tree(preorder[1 + i:], inorder[i + 1:]) return node ","permalink":"https://olzhy.github.io/posts/leetcode-construct-binary-tree-from-preorder-and-inorder-traversal.html","tags":["Golang","Python","算法"],"title":"LeetCode 105 以先序遍历及中序遍历构造二叉树"},{"categories":["计算机"],"contents":"1 题目描述 设计链表的实现。您可以选择使用单链表或者双链表来实现。\n单链表中的节点应有val和next两个属性，val为当前节点的值，next为下一个节点的指针或引用。\n若使用双链表实现，则需要一个额外的属性prev来指向当前节点的前一个节点。\n假定链表中节点的索引是从0开始的。现在请实现MyLinkedList类：\n MyLinkedList() 用于实例化一个MyLinkedList对象。 int get(int index) 用于获取链表中第index个节点的值，若index不合法，返回-1。 void addAtHead(int val) 用于在链表的第一个节点前插入一个值为val的节点。插入后，新的节点将成为链表的第一个节点。 void addAtTail(int val) 用于在链表的最后一个节点后增加一个值为val的节点。 void addAtIndex(int index, int val) 用于在链表的第index个节点前新增一个值为val的节点。若index等于链表的长度，该节点将会被加到链表的最后。若index大于链表长度，该节点将不会被插入。 void deleteAtIndex(int index) 用于在index合法的情况下删除链表的第index个节点。  说明：\n 0 \u0026lt;= index, val \u0026lt;= 1000 请勿使用内置链表函数或类库 如下方法最多会调用2000遍 （get，addAtHead，addAtTail，addAtIndex，deleteAtIndex）  例如：\n 输入  [\u0026#34;MyLinkedList\u0026#34;, \u0026#34;addAtHead\u0026#34;, \u0026#34;addAtTail\u0026#34;, \u0026#34;addAtIndex\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;deleteAtIndex\u0026#34;, \u0026#34;get\u0026#34;] [[], [1], [3], [1, 2], [1], [1], [1]]  输出  [null, null, null, null, 2, null, 3]  释义  MyLinkedList myLinkedList = new MyLinkedList(); myLinkedList.addAtHead(1); myLinkedList.addAtTail(3); myLinkedList.addAtIndex(1, 2); // linked list becomes 1-\u0026gt;2-\u0026gt;3 myLinkedList.get(1); // return 2 myLinkedList.deleteAtIndex(1); // now the linked list is 1-\u0026gt;3 myLinkedList.get(1); // return 3 题目出处：LeetCode\n2 解决思路 本文采用单链表实现，但为了优化尾部节点添加效率，链表结构体除了用head记录链表头外，新加一个tail记录链表尾。而且为了判断addAtIndex及deleteAtIndex的合法性，使用len变量记录链表的长度。\n下面看一下针对各个方法的具体实现逻辑：\n MyLinkedList() 初始化一个空链表（head，tail为空，len为0）。 int get(int index) 若index不合法（index \u0026lt; 0 || index \u0026gt;= list.len），返回-1；否则从头往后找到第index个节点，返回其值。 void addAtHead(int val) 新增节点的next指向现在的头，新的头指向新增的节点；注意在空链表第一次新增的情形，须头尾均指向新增节点。最后将链表长度加1。 void addAtTail(int val) 现在的尾的next指向新增节点，新的尾指向新增的节点；同样须注意在空链表第一次新增的情形，须头尾均指向新增节点。最后将链表长度加1。 void addAtIndex(int index, int val) 若index不合法（index \u0026lt; 0 || index \u0026gt; list.len），直接退出；首先判断是否在头前新增，若是，则将新增节点的next指向现在的头，新的头指向新增的节点（若在空链表第一次新增，须同时将尾也指向新增的节点）；其次，需要判断是否在尾部追加（index == list.len），若是，则将现在的尾的next指向新增节点，新的尾指向新增的节点；再次，若是在链表中间某处新增，则找到新增位置的前一个节点，将新增的节点插入并建立新的连接关系。最后将链表长度加1。 void deleteAtIndex(int index) 若index不合法（index \u0026lt; 0 || index \u0026gt;= list.len），直接退出；首先判断是否删除的是第一个节点，若是，则将头指向当前头的下一个节点（若链表仅有一个节点，须同时将尾也指向空）；其它情形，则找到待删除节点的前一个节点，然后将待删节点删除并建立新的连接关系（若删除的是最后一个节点，须同时将尾指向待删除节点的前一个节点）。最后将链表长度减1。  3 Golang实现代码 https://github.com/olzhy\n// MyLinkedList is a struct of singly linked list type MyLinkedList struct { head *Node // the head of the linked list \ttail *Node // the tail of the linked list \tlen int // length of the linked list } // Node is used to construct linked list type Node struct { val int // value of current node \tnext *Node // point to next node } // Constructor is used to construct a empty list func Constructor() MyLinkedList { return MyLinkedList{} } // Get is used to get the vaule of index-th node // If index is invalid, -1 will be returned // Otherwise, It returns the vaule of index-th node in the linked list func (list *MyLinkedList) Get(index int) int { if index \u0026lt; 0 || index \u0026gt;= list.len { return -1 } p := list.head for i := 0; i \u0026lt; index; i++ { p = p.next } return p.val } // AddAtHead is used to add a node of val at the head of the linked list func (list *MyLinkedList) AddAtHead(val int) { node := \u0026amp;Node{val, nil} if 0 == list.len { list.head = node list.tail = node } else { node.next = list.head list.head = node } list.len++ } // AddAtTail is used to add a node of val at the tail of the linked list func (list *MyLinkedList) AddAtTail(val int) { node := \u0026amp;Node{val, nil} if 0 == list.len { list.head = node list.tail = node } else { list.tail.next = node list.tail = node } list.len++ } // AddAtIndex is used to add a node of value val bofore the index-th node of the linked list // If index equals the length of the linked list, the node be added will be the tail of the list // If index is invalid, do nothing func (list *MyLinkedList) AddAtIndex(index int, val int) { if index \u0026lt; 0 || index \u0026gt; list.len { return } node := \u0026amp;Node{val, nil} if 0 == index { // add at head \tnode.next = list.head list.head = node if 0 == list.len { list.tail = node } } else if list.len == index { // add at tail \tlist.tail.next = node list.tail = node } else { p := list.head for i := 0; i \u0026lt; index-1; i++ { p = p.next } node.next = p.next p.next = node } list.len++ } // DeleteAtIndex is used to delete the index-th node of the linked list // If index is invalid, do nothing func (list *MyLinkedList) DeleteAtIndex(index int) { if index \u0026lt; 0 || index \u0026gt;= list.len { return } if 0 == index { // delete head \tlist.head = list.head.next if 1 == list.len { list.tail = nil } } else { p := list.head for i := 0; i \u0026lt; index-1; i++ { p = p.next } p.next = p.next.next if list.len-1 == index { list.tail = p } } list.len-- } 4 Python实现代码 https://github.com/olzhy\nclass Node: \u0026#34;\u0026#34;\u0026#34; Node is used to construct a node of linked list \u0026#34;\u0026#34;\u0026#34; def __init__(self, val: int, next: \u0026#39;__class__\u0026#39; = None): \u0026#34;\u0026#34;\u0026#34; construct a node :param val: value of current node :param next: point to next node \u0026#34;\u0026#34;\u0026#34; self.val = val self.next = next class MyLinkedList: \u0026#34;\u0026#34;\u0026#34; MyLinkedList is used to construct a linked list \u0026#34;\u0026#34;\u0026#34; def __init__(self, head: Node = None, tail: Node = None, size: int = 0): \u0026#34;\u0026#34;\u0026#34; construct a empty linked list :param head: head of linked list :param tail: tail of linked list :param size: size of linked list \u0026#34;\u0026#34;\u0026#34; self.head = head self.tail = tail self.size = size def get(self, index: int) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34; get the value of index-th node of the linked list :param index: index :return: value of the node \u0026#34;\u0026#34;\u0026#34; if index \u0026lt; 0 or index \u0026gt;= self.size: return -1 p = self.head i = 0 while i \u0026lt; index: p = p.next i += 1 return p.val def add_at_head(self, val: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; add a node of val in head of the linked list :param val: val of the node :return: None \u0026#34;\u0026#34;\u0026#34; node = Node(val) if self.head is None: self.head = node self.tail = node else: node.next = self.head self.head = node self.size += 1 def add_at_tail(self, val: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; add a node of val in the tail of the linked list :param val: val of the node :return: None \u0026#34;\u0026#34;\u0026#34; node = Node(val) if self.head is None: self.head = node self.tail = node else: self.tail.next = node self.tail = node self.size += 1 def add_at_index(self, index: int, val: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; add a node of val before the index-th node of the linked list :param index: index :param val: val of the node :return: None \u0026#34;\u0026#34;\u0026#34; if index \u0026lt; 0 or index \u0026gt; self.size: return node = Node(val) if 0 == index: # add at head node.next = self.head self.head = node if 0 == self.size: self.tail = node elif self.size == index: # add at tail self.tail.next = node self.tail = node else: p = self.head i = 0 while i \u0026lt; index - 1: p = p.next i += 1 node.next = p.next p.next = node self.size += 1 def delete_at_index(self, index: int) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34; delete the index-th node of the linked list :param index: index :return: None \u0026#34;\u0026#34;\u0026#34; if index \u0026lt; 0 or index \u0026gt;= self.size: return if 0 == index: self.head = self.head.next if 1 == self.size: self.tail = None else: p = self.head i = 0 while i \u0026lt; index - 1: p = p.next i += 1 p.next = p.next.next if self.size - 1 == index: self.tail = p self.size -= 1 ","permalink":"https://olzhy.github.io/posts/leetcode-design-linked-list.html","tags":["Golang","Python","算法"],"title":"LeetCode 707 设计链表"},{"categories":["计算机"],"contents":"Golang text/template 包是一个数据驱动的模版渲染工具。提供条件判断，数组或 map 遍历；参数赋值，函数或方法调用；自定义函数扩展，模板嵌套及重用等功能。基于该工具，可以轻松实现复杂场景的文本渲染。如Helm Template基于此实现了功能强大的 Kubernetes 配置文件渲染工作。\n本文使用一个样例来演示text/template的使用，代码已托管至GitHub。\n1 样例代码 package main import ( \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;text/template\u0026#34; ) const text = ` {{/* This is a zoo template */}}{{with .Name}}Welcome to {{.}}{{end}}There are {{len .Animals}}animals, they are: {{range .Animals}}{{- . | upper -}}, {{end}}{{if gt (len .Zookeepers) 0}}There are {{len .Zookeepers}}zookeepers, they are: {{range $no, $name := .Zookeepers}}{{printf \u0026#34;%03d\u0026#34; $no}}: {{$name -}}{{end}}{{end}}{{block \u0026#34;Welcome\u0026#34; .Name}}You\u0026#39;re welcome to visit {{.}}next time!{{end}}` type Zoo struct { Name string Animals []string Zookeepers map[int]string } func main() { // template \ttpl := template.Must(template.New(\u0026#34;zoo\u0026#34;).Funcs(template.FuncMap{ \u0026#34;upper\u0026#34;: func(s string) string { // self-defined functions \treturn strings.ToUpper(s) }, }).Parse(text)) // zookeepers \tzooKeepers := map[int]string{ 0: \u0026#34;Alan\u0026#34;, 1: \u0026#34;Larry\u0026#34;, 2: \u0026#34;Alice\u0026#34;, } // zoo \tzoo := \u0026amp;Zoo{ \u0026#34;Beijing Zoo\u0026#34;, []string{\u0026#34;elephant\u0026#34;, \u0026#34;tiger\u0026#34;, \u0026#34;dolphin\u0026#34;}, zooKeepers, } // execute \ttpl.Execute(os.Stdout, zoo) } 2 运行结果 Welcome to Beijing Zoo There are 3 animals, they are: ELEPHANT, TIGER, DOLPHIN, There are 3 zookeepers, they are: 000: Alan 001: Larry 002: Alice You\u0026#39;re welcome to visit Beijing Zoo next time!  参考资料\n[1] https://golang.org/pkg/html/template/\n ","permalink":"https://olzhy.github.io/posts/golang-text-template.html","tags":["Golang"],"title":"Golang text/template 使用样例"},{"categories":["计算机"],"contents":"1 题目描述 给定一棵二叉树，返回其节点值的中序遍历结果。\n例如：\n输入：[1,null,2,3] 1 \\ 2 / 3 输出：[1,3,2] 注：递归较简单，您可否使用循环来实现？\n题目出处：LeetCode\n2 解决思路 参考上图，整体来看，本算法采用多条自左上到右下的线将树根和其左子树的连接“截断”，然后将根节点依次放入一个栈里，当最左下角的根节点已压栈后，然后开始依次出栈。这样的输出顺序即是中序遍历顺序。\n3 Golang实现代码 https://github.com/olzhy\nfunc inorderTraversal(root *TreeNode) []int { if nil == root { return []int{} } vals := []int{} nodes := []*TreeNode{root} for len(nodes) \u0026gt; 0 { node := nodes[len(nodes)-1] if nil != node.Left { nodes = append(nodes, node.Left) node.Left = nil continue } vals = append(vals, node.Val) nodes = nodes[:len(nodes)-1] if nil != node.Right { nodes = append(nodes, node.Right) } } return vals } ","permalink":"https://olzhy.github.io/posts/leetcode-binary-tree-inorder-traversal.html","tags":["Golang","算法"],"title":"LeetCode 94 二叉树中序遍历"},{"categories":["计算机"],"contents":"1 题目描述 对单链表的某一段（自第m个位置起到第n个位置止）进行反转。\n例如：\n输入：1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL, m = 2, n = 4 输出：1-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;5-\u0026gt;NULL 注：1 ≤ m ≤ n ≤ length（链表长度）\n题目出处：LeetCode\n2 解决思路 反转部分的实现逻辑可参考上一题解法LeetCode 206 反转链表，本题的附加难度在于首先要找到待反转部分的起始位置（走m-1步，且要记录原链表起始分割点左半部分的尾节点，便于反转后的连接），然后走n-m步的同时采用上一题方式将该区段反转。最后将原链表左半部分的尾节点连接反转部分的头，反转部分的尾连接原链表右半部分的头即为所求。\n3 Golang实现代码 https://github.com/olzhy\nfunc reverseBetween(head *ListNode, m int, n int) *ListNode { if nil == head || nil == head.Next || m == n { return head } // steps \tstep := n - m // find beginning position \tvar leftTail *ListNode p := head for m \u0026gt; 1 { leftTail = p p = p.Next m-- } // do reverse \tq := p.Next p.Next = nil midTail := p for step \u0026gt; 0 { r := q.Next q.Next = p p = q q = r step-- } // if exists left part? \tif nil == leftTail { midTail.Next = q return p } leftTail.Next = p midTail.Next = q return head } ","permalink":"https://olzhy.github.io/posts/leetcode-reverse-linked-list-ii.html","tags":["Golang","算法"],"title":"LeetCode 92 反转链表 II"},{"categories":["计算机"],"contents":"1 题目描述 对单链表进行反转。\n例如：\n输入: 1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL 输出: 5-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;1-\u0026gt;NULL 注：链表反转可使用循环或者递归方式实现，您能否同时实现此两种算法？\n题目出处：LeetCode\n2 解决思路 使用递归方式实现起来相对比较简单，步骤为：\n 1）用p指向头节点，将从第2个节点起的子链表反转，用q指向其反转后的头； 2）将p的Next指向空，找到q的尾节点，然后将其Next指向p； 3）递归计算，直至子链表为1个节点时触发返回条件。  使用循环方式实现的步骤为：\n 1）p，q，r指向反转前三个连续的节点，q指向当前节点，p指向上一个节点，r指向下一个节点； 2）接下来做反转，将q的Next指向p； 3）移动一步，p指向q，q指向r； 4）重复如上步骤，直至q指向空，从而实现整个链表反转完成。  3 Golang实现代码 使用递归方式实现代码为：\nhttps://github.com/olzhy\nfunc reverseList(head *ListNode) *ListNode { if nil == head || nil == head.Next { return head } p := head q := reverseList(head.Next) p.Next = nil r := q for nil != r.Next { r = r.Next } r.Next = p return q } 使用循环方式实现代码为：\nhttps://github.com/olzhy\nfunc reverseList(head *ListNode) *ListNode { if nil == head || nil == head.Next { return head } p := head q := p.Next p.Next = nil for nil != q { r := q.Next q.Next = p p = q q = r } return p } ","permalink":"https://olzhy.github.io/posts/leetcode-reverse-linked-list.html","tags":["Golang","算法"],"title":"LeetCode 206 反转链表"},{"categories":["计算机"],"contents":"台大林轩田老师《机器学习基石》课程学习笔记（笔记中所有图片均引自林老师的课件）。\n1 关于学习 1.1 课程介绍 该课程将理论与实践相结合，从基础开始切入。\n以讲故事的方式展开如下几个问题：\n 机器何时可以进行学习？ 机器为什么可以进行学习？ 机器如何进行学习？ 机器如何学习的更好？  1.2 何为机器学习 首先我们需要思考何为学习？人或动物通过观察得到技能即为学习。\n何为机器学习？机器通过观察数据学习到技能。\n何为技能？某一种表现的增进。如学习了英语这项技能，则使用其交流则更流畅。\n样例应用场景：通过学习过往股票数据增加投资收益。\n另一个应用场景：自动辨识一棵树。\n 常规程序化实现方式为：列出诸多规则定义何为一棵树，然后进行匹配，但效果不佳。 机器学习的实现方式为：通过观察数据自己进行学习进而进行识别。  所以总结适合使用机器学习的应用场景有：\n 当无法使用人工程序化实现的时候（不容易定义解决方案）； 针对不同用户群体进行个性化服务； \u0026hellip;  可进行机器学习的三要素：\n 存在潜藏的模式； 不易进行程序化实现； 有相关模式对应的数据。  1.3 机器学习的应用 机器学习的应用贯穿我们生活中的诸多方面：\n 衣：推荐系统，为客户推荐时尚穿搭； 食：文本分析，根据推特数据分析餐馆卫生情况； 住：能源消耗，基于现有建筑数据预测建筑耗能； 行：路标识别，识别交通标示牌及交通信号； 育：答题系统，根据历史答题记录推测题目难度与学生能力； 乐：推荐系统，电影推荐，音乐推荐。  电影推荐系统的一个可能的实现方案：\n 电影具有哪些特征：喜剧片，动作片，大片，汤姆·克鲁斯主演… 我喜欢的电影具有哪些特征：有多喜欢喜剧片？有多喜欢动作片？有多喜欢大片？有多喜欢汤姆·克鲁斯？…  根据如上特征值进行匹配度计算。\n1.4 学习的组成部分 样例：是否批准一个申请人的信用卡发放请求。\n申请人信息如下：\n如何描述学习问题：\n x为输入：申请人信息； y为输出：是否发放信用卡； f为未知的目标函数：即潜藏的模式，一个理想的信用卡审批公式$f:x \\rightarrow y$； D为数据：训练样本，历史收集的数据； g为假设函数：越接近f越好，即使用$g:x \\rightarrow y$来衡量是否要发放信用卡。  信用卡是否发放场景的学习过程：\n 未知的目标函数产生了诸多历史数据； 机器学习通过某种学习算法得到最终的假设函数g，我们期待g与f越接近越好。  g为所有假设函数集合的一部分，机器学习算法即是从中找出最优的。 机器学习模型即是基于数据将算法A与允许选择的假设H相结合，得出一个尽可能接近理想目标函数f的假设函数g。\n下面例子是找出歌曲推荐系统中的输入x，输出y，数据D，假设集合H，假设函数g：\n1.5 机器学习及相关领域 从上述可知，机器学习是使用数据来计算一个接近目标函数f的假设g。下面看一下机器学习与相关领域的关系。\n机器学习 vs 数据挖掘：\n 数据挖掘是使用大数据找出一些有趣的事情； 传统的数据挖掘偏重海量数据计算，现两者有诸多相似的部分，可以互相助力。  机器学习 vs 人工智能：\n 人工智能是让机器有一些智能的表现； 机器学习是实现人工智能的一种方法。  机器学习 vs 统计学：\n 统计学是使用数据对未知过程做推论； 传统统计学注重数学推论，现使用统计学相关方法来实现机器学习。  2 学习回答是与非 2.1 感知器假设集合 重温1.4提到的信用卡发放问题。\n申请人信息可用多维向量表示，每个维度有一个对应的权值，假设函数$\\operatorname{h}(x)$为所有维度的权值与对应维度值乘积之和，超过某阈值则同意发放，否则拒绝发放。\n如下推算说明可将阈值看作是第0维的部分。这样$\\operatorname{h}(x)$可看作是第0维到第d维的权重与维度值的乘积之和。也可看作是$\\pmb w$与$\\pmb x$两个向量的乘积。\n在二维空间$\\operatorname{h}(x)$是一条直线，在多维空间$\\operatorname{h}(x)$是一个超平面。感知器即是一个线性分类器。\n2.2 感知器学习算法 感知器学习算法是一个针对数据不断改进的算法，可能需要多轮演算及调整才可能找到一条满足条件的分割线。对于第t轮演算，若在该轮的第n个点发现错判（该轮的某个点的$y$值本来应为+1但算成了-1，说明$\\pmb w$向量与$\\pmb x$向量的夹角太大，造成内积太小；反之，若该轮某个点的$y$值本应为-1但算成了+1，说明$\\pmb w$向量与$\\pmb x$向量的夹角太小，造成内积太大），则将下一轮的$\\pmb w$向量置为$\\pmb w + y\\pmb x$来进行改进（若$y$为+1，则为$\\pmb w + \\pmb x$，表示将$\\pmb w$向量与$\\pmb x$向量的夹角调整的小一点；若$y$为-1，则为$\\pmb w - \\pmb x$，表示将$\\pmb w$向量与$\\pmb x$向量的夹角调整的大一点）。\n该算法的实际运用中，可能需要多轮循环直至所有的点都满足条件。\n下面演示一下该算法的演进过程：\n 原始数据   第1轮：原点到$x_1$构成初始向量   第2轮：根据第1轮找到的法向量对应的直线对数据进行划分，发现$x_9$被错判（本是圈，被错判为叉），则对下一轮$\\pmb w$进行调整（与$x_9$夹角小一点）   第3轮：根据第2轮找到的法向量对应的直线对数据进行划分，发现$x_{14}$被错判（本是叉，被错判为圈），则对下一轮$\\pmb w$进行调整（与$x_{14}$夹角大一点）   以此类推，直至某一轮幸运的找到一条分割线。  但感知器学习算法的问题是并不一定会找到演算停止的情形。\n 参考资料\n[1] https://www.csie.ntu.edu.tw/~htlin/mooc/\n[2] https://www.bilibili.com/video/BV1Cx411i7op?p=2\n ","permalink":"https://olzhy.github.io/posts/machine-learning-foundation-notes.html","tags":["机器学习"],"title":"《机器学习基石》课程学习笔记"},{"categories":["计算机"],"contents":"Istio 使用 Envoy 来代理网格服务的所有进出流量，可在不改变服务代码的情况下自由进行流量控制。 使用 Istio，诸如熔断处理，服务超时，重试等服务级特性，通过简单的几行配置即可实现；同时，诸如 A/B 测试，灰度发布，按比例滚动升级等重要任务亦可以很容易实现。\n所有上述高级特性均可通过使用 Istio 流量管理 API 来实现，该 API 使用 Kubernetes CRDs（custom resource definitions，自定义资源描述）来进行配置。 流量管理 API 的几个重要的资源有：Virtual Service，Destination Rule，Gateway，Service Entry，Sidecar。下面分别进行介绍。\nVirtual Service Virtual Service 与 Destination Rule 一般会结合使用，为 Istio 流量控制的两个重要资源。Virtual Service 用于配置流量如何路由到服务及服务的子集，Destination Rule 则专门用于配置服务的子集。之所以分成两个资源，是这样做 Virtual Service 的路由规则看起来更清爽一点。\nVirtual Service 由一组从上至下按序匹配的路由规则组成，流量进来后会按序遍历所配置的规则，一旦匹配则跳到具体的目标服务上。\nVirtual Service 将发送请求的客户端与实际的目标服务进行了解耦。其典型使用场景是将流量分发到一个服务的不同版本（子集）上。这样，对客户端来说，入口只有一个，具体的分发逻辑则通过 Istio 的 Envoy 实现。基于 Virtual Service 的路由规则可以配置诸如“20%的流量打到新版本”，“这些用户的请求打到某版本”等高级功能。\n而 Virtual Service 的另一个优点是，流量路由控制与实际部署实例已完全分离，这样服务不同版本对应的实例数可以自由伸缩而无需关心路由控制是怎么配的。相比之下，若使用 Kubernetes 来作按比例分流则不得不通过控制实例数实现，变得非常麻烦。\n当然，亦可以使用 Virtual Service 将 Namespace 下所有服务的流量进行代理，即将其作为一个统一的流量入口，这样用也是没问题的。此外，还可与 Gateway 结合使用来作进出流量控制。\n下面就看一下 Virtual Service 的配置吧。如下配置为 reviews 服务定义了两个路由规则，若请求头为end-user: jason则打到 v2 版本，否则打到 v3 版本。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: match: - headers: end-user: exact: jason - destination: host: reviews subset: v2 - route: - destination: host: reviews subset: v3 下面对几个重要字段作一下说明：\n  hosts\n表示 Virtual Service 的 host，即请求方（客户端）调用服务时使用的地址。本例中使用 Kubernetes 中服务的名称。\n  match\n表示匹配条件。本例中，第一个匹配规则使用该字段，指定 headers 来过滤请求。\n  destination\n表示满足条件的流量打到哪里。其下的 host 字段表示一个真实的服务地址（需注册到 Istio 服务注册中心，否则 Envoy 找不着）；subset 字段表示满足规则的流量打到对应服务的哪个子集。\n  因路由规则从上到下逐个匹配，所以前面规则的优先级比后面的高。本例中第二个规则即没有匹配条件，所以不满足第一个规则的流量都会打到它上面。推荐在编写 Virtual Service 的路由规则时，最后均要有一个默认路由。\n上面的例子是对 reviews 服务的两个版本配置路由，下面看一下如何使用 Virtual Service 为两个不同的服务（ratings 与 reviews）配置路由。\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - bookinfo.com http: - match: - uri: prefix: /reviews route: - destination: host: reviews - match: - uri: prefix: /ratings route: - destination: host: ratings 如上路由规则为两个服务设定了不同的 prefix，其会根据请求 URI 来将请求打到不同的服务上。\n除了使用 match 来设定匹配条件外，还可以使用 weight 字段来按比例分流。如下例子为 reviews 服务配置路由规则，将 80%的流量打到了 v1 上，20%的流量打到了 v2 上。\nspec: hosts: - reviews http: - route: - destination: host: reviews subset: v1 weight: 80 - destination: host: reviews subset: v2 weight: 20  参考资料\n[1] https://istio.io/docs/concepts/traffic-management/\n ","permalink":"https://olzhy.github.io/posts/istio-traffic-management.html","tags":["服务网格","Istio"],"title":"Istio 流量管理初探"},{"categories":["计算机"],"contents":"威胁建模是一个识别潜在威胁的过程。通过威胁建模以期找出攻击者的画像及其最可能的攻击路线，以及最易遭受攻击的资产。所以威胁建模做的即是找到最易攻击的地方并制定出应对方案。\n概念上讲，威胁建模就在我们的日常生活中，只是我们未察觉而已。上班早高峰规避危险的操作及地方等以防可能出现的事故。在操场玩耍的孩子们找出最佳路径直奔目的地以规避校霸围追堵截。在更正式的场景，威胁建模从远古起即已用于军事防卫等备战规划上了。\n威胁建模的演进 主要有如下几个。\n1999，微软提出STRIDE模型识别攻击。\n S - Spoofing identity 身份欺骗 T - Tampering with data 数据篡改 R - Repudiation 抵赖 I - Information disclosure 信息暴露 D - Denial of service 拒绝服务 E - Elevation of privilege 特权提升  2014，Ryan提出DML（Detection Maturity Level，检测成熟等级）模型。该模型认为威胁者是一个威胁场景的实例，而一个威胁场景是指一个特定攻击者在脑海有了一个特定的攻击目标后使用各种策略以达到该目标。 目标以及策略表示DML模型的最高语义学等级；而TTP（Tactics, Techniques and Procedures 手段，技术及程序）表示中间的语义学等级；攻击者使用的工具表示DML模型的最低语义学等级。\n威胁建模的方法 威胁建模可以独立的使用如下几个方法，即：以资产为中心，以攻击者为中心，还有以软件为中心。下面是比较著名的四种威胁建模方法。\n STRIDE  微软1999年提出的该方法可为开发者提供找出“我们产品所面临威胁”的一个助记符。与此衍生出诸多模型，实践，数据流图等。\n P.A.S.T.A.  PASTA（The Process for Attack Simulation and Threat Analysis，模拟攻击及威胁分析）是一个七步过程，以风险为中心的方法论。是一个动态威胁识别，威胁列举及评分的过程。\n Trike  Trike是一个将威胁模型看作风险管理工具的方法论。\n VAST  VAST（Visual, Agile, and Simple Threat modeling 可视化，敏捷及简化威胁建模）。是一个将威胁建模贯穿整个SDLC（软件开发生命周期）且与敏捷软件开发无缝集成的方法论。\n常用的威胁建模流程 威胁建模的第一步是进行应用及基础设施的可视化表示。将应用及基础设施梳理划分成各个模块的目的是便于分析。划分完成并可视化表示后，即可以进行威胁识别及进行潜在威胁的列举。再后来进行威胁发生的风险评估，威胁的优先级评估，从而基于各种方法制定出合适的应对措施。 威胁的识别及列举可以从以攻击为中心的方式或从以资产为中心的方式来开展。前者注重找出可能的攻击类型以作应对；后者注重找出受保护的资产以防攻击。当然，两种方法各有利弊。\n基于数据流程图的可视化表示\n上述诸如STRIDE，PASTA，Trike建模方法都使用了DFD（data flow diagrams，数据流程图）来进行可视化表示。DFD是一个让系统工程师从上层来看一个应用的数据是如何流动，存储及操作的工具。DFD有五种特有符号：数据流，数据存储，过程，交互者，信任边界。 一旦一个系统被划分为这五个部分，安全专家即可对所有已知威胁类别进行识别以找出该系统的威胁切入点。一旦找到潜在威胁，即可对其进行分析及控制。\n基于过程流程图的可视化表示\n上述VAST建模方法使用了不同的方式，其将ATM（application threat models，应用威胁模型）与OTM（infrastructure threat models，基础设施威胁模型）进行了区分。ATM使用PFD（process flow diagrams，过程流程图）来进行创建。 过程流程图将应用划分为各种特性或用例，而每个特性又可由页面及代码块来构成，最后特性通过交互协议连接起来。这样，一个应用的过程流程图即是一个用户如何沿着各种特性移动的一张地图。\nOTM则以类似于传统DFD的端到端的数据流图进行创建。端到端的数据流图将一个系统划分成各种独立共用的组件，然后，组件间又通过线路或协议进行通信。\n一旦ATM及OTM构造好，则可进行潜在威胁识别，枚举，优先级划分，从而进行风险评估及安全控制。\n 参考资料\n[1] https://en.wikipedia.org/wiki/Threat_model\n ","permalink":"https://olzhy.github.io/posts/threat-modeling.html","tags":["架构设计"],"title":"威胁建模"},{"categories":["观影"],"contents":"今日看了电影《百鸟朝凤》，有一些感触。\n我是一个土生土长的山西人，对唢呐有一些认识，因为唢呐就生长在这黄土地上。\n村里办白事一般会请一班鼓，其由唢呐，笙，鼓等乐器组成，而唢呐又是这班鼓里的灵魂，班主也一般为唢呐吹奏者所担任。\n我叔家堂兄比我大两岁，从小酷爱唢呐，我在十岁时，我们分别得到一把唢呐。我的是我的二舅给买的，我哥的是他的父亲我二叔给买的。得益于家族的基因，我们在未有任何音乐知识的情况下，能拿起唢呐凭着感觉吹出一些歌曲的调子来。\n只能说唢呐还没有真正走进我的心底，所以无缘在这条道路上继续走下去。相反，我哥却是一个从小就知道自己要什么的人。自这把唢呐开始，从初春的村外静处到夏日的玉米地头，秋日的草垛旁边到冬日的院内树上都是他苦练唢呐的地方。\n一盘盘磁带反复听反复练习，让我二叔明白儿子不是心血来潮。自此，我哥从十四岁起便从自学到拜师，从市里艺校到音乐学府，坚定不移，直至现在成为了一名青年唢呐音乐家。\n从他的经历，我知道学唢呐要比一般的乐器难的多，唢呐的发声需要很高的肺活量，而且换气等都是从小开始日复一日练就的本领。几天不练功，都不敢握唢呐。天赋与几十载苦练方能成为一个唢呐人。\n唢呐声音洪亮，鹤立鸡群，直击人心。这是把经历过世代兴衰的乐器，也吹奏了这块土地上几代人的生命形态与内心世界。\n唢呐作曲者一般就是唢呐人，曲子《黄土情》，《打枣》，《抬花轿》，《百鸟朝凤》等，其情感由作者内心表达出来，听者会感叹这真的是土地上长出来的曲子，触及灵魂且极富生命力。\n纵观县城的唢呐班子，无一不是世家，自老祖宗选择吃这口饭开始，几代人将其传承下来。而今人喜欢逐新，这些老的东西也渐渐被人遗忘，从业者越来越少，唢呐的确面临着传承的问题。也许是今人领略不了这份厚重，到一定岁数，经历一些世事，翻到老祖宗留下来的这些老东西，也许才会感叹它们表达的才是真实的人生。\n","permalink":"https://olzhy.github.io/posts/hundreds-of-birds.html","tags":["观影"],"title":"观《百鸟朝凤》"},{"categories":["计算机"],"contents":"现代软件通常以Web服务的方式交付，称为软件即服务（SaaS），十二因子指导原则即是构建SaaS应用的一套方法论。其不仅是构建SaaS应用的指导原则，也是微服务，云原生应用开发须遵循的指导原则。\n十二因子指导原则或者最佳实践以期达到：\n  使用统一的规范，可以使新进开发人员节省时间成本，按照最佳实践走即可；\n  应用应与底层操作系统解耦，以在各种运行环境之间提供最大的可移植性；\n  应用应适于部署在现代化云平台上，摒弃对服务器和系统管理的依赖；\n  开发环境与生产环境之间不应有很多差异，可以以最大化的敏捷性进行持续部署；\n  可以在不对工具，架构，开发实践进行重大改变的情况下进行自由扩展。\n  十二因子指导原则对应用的开发语言及后端服务的类型均没有限制，是一套统一的方法论。\nI 代码库\n一个应用使用一个代码库，一个代码库支持多个部署环境。\n十二因子应用总是使用版本控制系统（Git，SVN等）来作代码跟踪的。一个应用应有一个代码库，一个代码库存储一套代码，可以有多个版本。不同的部署环境可以使用一个代码库上的不同版本。\n实践中也是这样做的，如我们使用Git来托管代码，一个应用使用一个仓库，不同的应用不应使用一个仓库，而是将依赖关系拆出来，然后分成不同的仓库。每到一个版本开发完毕，我们在仓库上打Tag或者新建Release分支，然后逐步升级开发，测试，类生产，生产环境。\nII 依赖\n显式的声明并抽出依赖。\n绝大多数工具都提供打包功能，依赖可以全局安装或者安装在应用的指定文件夹，如nodejs的site-packages，Golang的vendoring等。\n十二因子指导原则建议应用绝不要隐式依赖系统的全局包。要将应用的所有依赖通过依赖配置文件显式的，完整的，准确的声明出来。且要使用依赖辅助工具将所有依赖抽出来，使用统一的完整的显式的依赖贯穿开发及生产。\n使用显式的依赖声明也简化了新人开发应用的准备步骤。开发者只需将代码检出，并安装必须的语言运行时及依赖管理软件即可。如我们使用Maven构建Java项目，依赖都声明在工程的pom.xml文件了，开发只要将代码拿下来，mvn package即可打包。\n十二因子指导原则建议不要隐式的依赖任何系统工具。如我们应用需要curl，即便大多数系统都自带这个工具，但也保不齐版本或者兼容性不一致，若应用对这个工具强依赖，就该考虑将其打进应用中，或者使用Docker方式构建。\nIII 配置\n将配置存在环境变量中。\n一个应用的配置在不同环境之间是不同的。如：\n  连接数据库，缓存或其它后端服务的配置信息；\n  连接诸如AWS s3等外部服务的密钥信息；\n  部署时需要的特定配置信息，如域名等。\n  应用有时将这些配置信息作为常量保存在代码中，这是违背十二因子指导原则的。应将配置与代码严格的分离，配置随不同的部署环境发生变化，而代码却只需一套。\n一个检测应用是否将所有配置信息从代码中抽出来的方法即是你的代码仓库是否可以随时开源，而无需担心有密钥信息暴露出来。\n当然这里配置的定义并不包含应用内部的配置，如Spring的bean配置信息，这类配置并不随部署环境变化，理应放在代码中。\n另一个方法是使用配置文件，但不将其纳入版本控制。这相比在代码中使用常量已有很大进步，但仍有诸多弊端：\n  易于错将配置文件提到仓库；\n  易将配置文件以不同的格式放置在不同的地方，不便于统一管理；\n  配置文件格式可能与语言或框架相关。\n  根据十二因子指导原则，应将应用配置存在环境变量中。环境变量在不改变代码的情况下可以根据不同部署环境而改变。也不会误将其提到代码仓库，并且其与常规配置文件不同的是其不受语言或操作系统限制。\n另一个办法是将配置文件分组，如建立开发，测试，生产目录，将不同配置放在不同的环境目录下。这样也不好，随着后续环境的增加，管理起来也挺麻烦。\n综上，环境变量是一个粒度恰当的控制办法，其随每次部署独立管理。当环境增多时，可以做到平滑的扩展。\nIV 后端服务\n将后端服务看作附加资源。\n后端服务可以是被应用通过网络来消费的任意服务，这是其常规操作的一部分。诸如MySQL数据库，RabbitMQ队列，Memcached缓存等都是后端服务。\n诸如数据库等的后端服务通常同样由部署应用运行时的系统管理员所管理。除了本地管理的服务以外，也可能有三方组织所提供及管理的服务。诸如指标信息收集服务New Relic，二进制资产服务AWS s3，甚至通过API访问的服务Twitter等。\n十二因子指导原则有一条准则是应用对本地或者三方服务不应有任何区别，都应看作是可以通过URL或者密钥访问的附加资源。应用能够将本地MySQL数据库换成诸如AWS RDS等三方数据库而无需任何代码变更，而仅需改一下配置即可。\n每一个后端服务即是一个资源。两个MySQL实例即是两个资源，其与部署环境是解耦的。资源是可以随部署意愿进行附加或移除的。如生产环境应用使用的一个数据库实例坏掉了，那管理员可以基于其最近一次备份新建一个新的实例顶上去，而无需变更代码。\nV 构建，发布及运行\n严格将构建与运行阶段分离。\n代码库通过如下三步来进行部署：\n 构建阶段是将代码仓库转换为一个可执行包  部署过程从代码的某次提交点拉一个版本出来，构建即是获取该版本的依赖并且将其编译为二进制资产。\n 发布阶段是将构建出来的包与当前部署配置相结合  发布阶段组合可以在运行环境立即执行的包与配置。\n 运行阶段即是将应用在运行环境运行起来  运行应用对应版本的进程。\n十二因子指导原则严格将构建，发布，运行阶段分离。诸如，我们无法在运行阶段修改代码，因我们无法将这些变更传回到构建阶段。\n一些典型的提供发布管理的工具，最显著的能力即是支持回滚到上一个版本。如，Capistrano部署工具将发布版本存储在releases子目录下，当前版本是当前发布文件夹的一个链接，其回滚命令即很容易使其回到上一个版本。\n每次发布应有一个唯一的发布ID，诸如一个发布时间戳（2020-03-20-20:32:17）或一个增长的数值（v100）。发布版本只可叠加且一旦创建即不可修改，任何变更必须新建一个发布版本。\nVI 进程\n以一个或多个无状态进程运行应用。\n先说一个简单的运行场景：代码为一个独立的脚本，运行环境是开发者本机且已安装对应语言的运行时，这样我们即可通过一条命令（如：python start.py）来启动应用进程。其它极端情况下，一个复杂应用的生产部署可能会使用多种进程类型：实例化为0个或多个进程。\n十二因子建议应用为无状态的且不要共享任何资源。任何需要持久化的数据存到有状态的后端服务（通常为数据库）就好了。\n十二因子不会假想任何在内存或硬盘上的缓存数据在后续的请求被使用。因应用运行为多个进程，后续的请求不一定打到哪个进程上，即便只有一个运行进程，也保不齐一次重启即会丢掉所有数据。\n在运行环境使用文件系统的缓存来加速编译也是不建议的。十二因子建议将打包放在构建阶段，这样诸如maven package等工具即可在该阶段将包打好，运行阶段用就好了。\n此外，一些Web系统依赖“粘性Session”，即将用户Session数据缓存到应用进程的内存中。这个与上面一样，多应用进程无法保证下一次请求就正好打到这个节点上。还是建议将Session状态数据存到诸如支持时间过期的Memcached，Redis等数据存储服务上。\nVII 端口绑定\n通过端口绑定暴露服务。\nWeb应用有时在Web容器内运行，诸如Java应用在Tomcat中运行等。\n十二因子建议应用完全自包含。不要依赖运行时注入以创建Web接口服务。即Web应用通过绑定端口来暴露为HTTP服务，以监听打到该端口的请求。\n如在本地环境，开发通过访问http://localhost:5000/来访问应用服务。在生产，通过公共域名来访问端口绑定的Web服务进程。\n当然，不仅HTTP服务可以通过端口绑定来暴露服务以被访问。其它4层服务也可以通过端口绑定来接收请求（如：Redis等）。\n此外，一个应用还可以通过端口绑定成为另一个应用的后端服务，如通过提供URL被其它应用作为资源服务使用。\nVIII 并发\n通过进程模型进行横向扩展。\n任何计算机程序，运行都是以一个或多个进程来表示的。Web应用有多种进程运行方式。如，PHP进程以Apache的子进程方式运行，随请求容量按需启动。而Java进程则相反，JVM启动时保留一块大的系统资源（CPU和内存）以提供一个大的进程，而内部使用线程来进行并发。此两种情况，应用开发者所见的最小单位都仅是进程。\n在十二因子应用中，进程是一等公民。吸纳了Unix守护进程模型的思想。采用该模型，开发者通过对不同类型的工作分配不同的进程类型即可以使应用处理不同的工作载荷。如，Web进程处理HTTP请求，后台进程处理长任务。\n这并不与独立进程进行内部多路复用（运行时虚拟机内部进行线程方式并发，或诸如Node.js的异步事件模型等）相悖，但虚拟机仅能纵向扩展，所以，应用必须同样能够横向扩展，以支持将多个进程运行在多个物理机上。\n进程模型会在横向扩展时大放异彩。对于不共享任何资源还支持水平分区的十二因子应用来说，支持并发是简单可靠的操作。\n十二因子应用不应作为守护进程也不要写PID文件。相反，应该交给操作系统进程管理器（诸如分布式进程管理器systemd）来管理输出流，以处理进程崩溃以及用户发起的重启与停机。\nIX 可便性\n使用快速启动及优雅停止来最大化健壮性。\n十二因子应用的进程是非常可便的，其可在某时按需启停。这样即可支持快速弹性扩展，代码及配置变更后快速部署。\n进程应做到尽量缩短启动时间。理想情况下，从执行命令到进程可用以便接收请求或处理任务只需花费几秒种。这样即可对进程部署及扩容提供更好的敏捷性，而必要时将进程快速移至新的物理机即提供了更强的健壮性。\n当遇到终止信号时进程应优雅的终止。对于Web应用来说，优雅的终止是在接收到终止命令时，当将当前请求处理完毕再退出，然后停止监听服务端口的流量。一般来说HTTP请求极短，一般不超过几秒，而对于长轮询场景，当连接断开后，客户端当尝试重连以实现对用户无感知。\n对于工作进程来说，优雅的停止是通过将当前任务返回到工作队列来实现的。如在Beanstalkd，当一个工作进程断开时，将任务自动返回到队列中。\n进程还当对突然死掉的情形（如硬件故障）作应对以达到更好的健壮性。如使用Beanstalkd后端队列，其可在客户端断开或超时后将任务返回到队列。\nX 开发环境与生产环境的相似性\n开发环境，测试环境及生产环境越相似越好。\n由于历史原因，开发环境与测试环境是有鸿沟的。诸如：\n  时间鸿沟，开发的代码可能很久才上线生产；\n  个人鸿沟，开发写代码，运维部署代码；\n  工具鸿沟，开发环境与生产环境使用的技术栈有差别（开发环境使用Nginx，MySQL，OS X；生产环境使用Apache，SQLite，Linux）。\n  十二因子应用建议设计时当考虑持续部署，将开发与生产的差别保持的越小越好。再看上面的3个鸿沟：\n  解决时间鸿沟，开发者开发了代码，几分钟即部署到生产；\n  解决个人鸿沟，DevOps打通，写代码的人要关注部署；\n  解决工具鸿沟，开发环境与生产环境越接近越好。\n  同时，十二因子应避免开发在开发环境及生产环境使用不同的后端服务。\nXI 日志\n将日志看作事件流。\n日志提供了对一个运行中应用行为的可见性。在基于服务器的环境中日志通常会写到诸如logfile的磁盘文件，但这仅是一种输出方式。\n收集所有运行进程及后端服务的输出流，然后将其按时间序组合起来即为日志流。日志原始即是一行一个事件的文本格式（出现异常堆栈时可能会有多行），其没有固定开头及结尾，但只要应用有操作就会有连续的日志。\n十二因子应用建议不要自己路由或存储输出流。即不要尝试自己写日志文件，而应让每个运行进程将日志写到stdout。在本地开发中，开发者可以在自己的终端来查看日志以观察应用的行为。\n在测试及开发环境，每个进程的输出会被运行环境捕获，然后存档到某些位置以备查看。存档位置不应由应用来配置，而应交给运行环境。开源的日志路由（诸如Fluent）即是做这些事情的。\n将日志发到诸如Splunk的检索分析系统有如下好处：\n  检索之前的特定事件；\n  绘制流量趋势图；\n  按用户定义规则来进行告警（如每分钟错误数超过某阈值即告警）。\n  XII 管理类进程\n将管理类任务作为一次性进程运行。\n除了运行常规任务（处理Web请求）的进程之外，开发者经常有对应用运行管理及维护的意愿，诸如：\n  运行数据库迁移任务；\n  运行控制台任务以执行代码或对线上数据库作检查；\n  运行一次性脚本（如php scripts/fix_bad_records.php）。\n  一次性管理进程当与常规常驻进程使用一样的环境。即管理进程与其它进程使用相同的代码和配置，且管理代码随应用程序一起发布，从而规避不同步问题。\n所有进程类型应使用相同的依赖隔离技术。如，使用bundle exec thin start运行Ruby Web进程，使用bundle exec rake db:migrate执行数据迁移。\n 参考资料\n[1] https://en.wikipedia.org/wiki/Twelve-Factor_App_methodology\n[2] https://12factor.net/\n ","permalink":"https://olzhy.github.io/posts/12-factor-app.html","tags":["架构设计"],"title":"构建SaaS应用的十二因子指导原则"},{"categories":["计算机"],"contents":"Selenium 整合了一揽子工具与依赖库，支持 Web 浏览器自动化，提供一组扩展来模拟人与浏览器交互。我们基于其满足 W3C 标准的 WebDriver 来编写的自动化代码可在各种主流浏览器复用。\n所以这里关键的一个组件即是 WebDriver，其负责与浏览器厂商提供的 API 来与浏览器交互。\n使用其即可做出模拟终端用户的操作，如：文本框输入，下拉框选择，链接点击等。此外还提供鼠标移动，JavaScript 脚本执行等能力。\n1 环境准备 Selenium 提供多种执行方式：如在本机安装 WebDriver 二进制可执行文件，或安装单独的服务，或使用远程 WebDriver 服务，甚至支持多种浏览器多种版本的 Grid 集群方式。\n下面我们使用 docker 方式启动一个拥有 Chrome 环境的单独服务。\ndocker run -d -p 4444:4444 -v /dev/shm:/dev/shm selenium/standalone-chrome:3.141.59-zirconium 查看页面http://localhost:4444/发现已启动成功。\n下面我们用 Golang 写个测试用例试试吧。\n2 Golang Selenium 测试代码 测试场景就选我的博客吧：打开博客首页leileiluoluo.com，点击搜索按钮，搜索框输入istio关键字后回车，应至少有一条结果，此外将搜索结果截图保存。\n本文选择 Golang 的 selenium 包github.com/tebeka/selenium。\n如下为代码说明：\n setup 函数初始化 driver 对象； TestSearch 即为测试搜索功能的函数：打开搜索页，输入关键字，点击搜索按钮，验证搜索结果，保存结果截图； teardown 函数负责 driver 对象的资源释放。  代码已托管至 GitHub：https://github.com/olzhy/go-exercises\npackage blog_test import ( \u0026#34;flag\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/tebeka/selenium\u0026#34; ) var ( browserName = flag.String(\u0026#34;browser\u0026#34;, \u0026#34;chrome\u0026#34;, \u0026#34;browser name\u0026#34;) gridUrl = flag.String(\u0026#34;grid\u0026#34;, \u0026#34;http://localhost:4444/wd/hub\u0026#34;, \u0026#34;grid url\u0026#34;) blogURL = \u0026#34;https://leileiluoluo.com/\u0026#34; searchButtonIdSelector = \u0026#34;searchOpen\u0026#34; keywordInputIdSelector = \u0026#34;search-query\u0026#34; searchResultLoadingCssSelector = \u0026#34;#search-results #loadingDiv\u0026#34; searchResultCssSelector = \u0026#34;#search-results .border-bottom\u0026#34; keyword = \u0026#34;istio\u0026#34; ) var driver selenium.WebDriver func setup() func() { // new remote driver \tcaps := selenium.Capabilities{\u0026#34;browserName\u0026#34;: *browserName} webDriver, err := selenium.NewRemote(caps, *gridUrl) if nil != err { panic(err) } driver = webDriver // teardown \treturn func() { driver.Quit() } } func screenshot(filename string) { bytes, err := driver.Screenshot() if nil != err { log.Printf(\u0026#34;take screenshot error, err: %s\u0026#34;, err) return } err = ioutil.WriteFile(filename, bytes, 0666) if nil != err { log.Printf(\u0026#34;save screenshot error, err: %s\u0026#34;, err) } } func TestSearch(t *testing.T) { // open blog \terr := driver.Get(blogURL) if nil != err { t.Errorf(\u0026#34;search page open error, err: %s\u0026#34;, err) } // click search button \telem, err := driver.FindElement(selenium.ByID, searchButtonIdSelector) if nil != err { t.Errorf(\u0026#34;search button not found, err: %s\u0026#34;, err) } elem.Click() // type keyword and enter \telem, err = driver.FindElement(selenium.ByID, keywordInputIdSelector) if nil != err { t.Errorf(\u0026#34;keyword input element not found, err: %s\u0026#34;, err) } elem.SendKeys(keyword + \u0026#34;\\n\u0026#34;) // wait until search result displayed \tdriver.WaitWithTimeout(func(driver selenium.WebDriver) (bool, error) { elem, err = driver.FindElement(selenium.ByCSSSelector, searchResultLoadingCssSelector) if nil != err { return false, nil } visible, err := elem.IsDisplayed() return !visible, err }, 30*time.Second) // assert \telems, err := driver.FindElements(selenium.ByCSSSelector, searchResultCssSelector) if nil != err || len(elems) \u0026lt; 1 { t.Errorf(\u0026#34;no search result, err: %s\u0026#34;, err) } // save screenshot \tscreenshot(\u0026#34;search.png\u0026#34;) } func TestMain(m *testing.M) { // parse flags \tflag.Parse() // setup / teardown \tteardown := setup() defer teardown() // run tests \tm.Run() } 执行测试：\n$ go test -v 测试结果：\n=== RUN TestSearch\r--- PASS: TestSearch (46.40s)\rPASS\rok github.com/olzhy/test\t91.427s\r至此，我们已可以使用 Selenium 进行自动化测试了。\n分析如上代码，代码编排的有一点粗陋，面对实际 Web 应用的复杂性，测试代码如何落地呢？有一点即是测试代码的编排。\n3 如何编排测试代码 如上测试代码的组织方式在测试逻辑复杂的情况下可能会变得庞杂又混乱。面对一个交互场景稍微复杂些的 Web 应用的时候，我们如何编排测试代码的包结构，或者进而设计一个通用的测试框架呢？\nSelenium 给出一个指导原则——页面对象模型，简单点说即是摒弃直接从测试者的角度想问题，而应从终端用户的视角出发，一个测试场景应是一组动作结合页面上下文的组合。\n所以编写测试用例时重要的是：不要一开始就设想点哪个按钮，选哪个字段，提交哪个表单这么细粒度的问题，而是过一遍真实用户体验。\n所以，写测试用例即如编写业务代码一样，需要考虑重用，封装，单一职责，面向对象，设计模式等知识。\n基于此，自动化测试领域的编码规范或设计模式即页面对象模型应运而生。其采用面向对象原则，将各个页面的选择器标记及行为封装在各自的页面，通过方法提供该页面的服务，且页面模型内不应有断言。\n下面就基于该规范将上边的代码试着改进一下吧。\nblog_test.go 为总测试入口，pages 包下为各页面功能，所以搜索页面的定位标记及功能均封装在 search.go，这样，我们在 blog_test.go 写测试函数调用 pages 下的页面的方法即可进行断言。\n改进后的代码已托管至 GitHub：https://github.com/olzhy/go-exercises\n 代码结构(github.com/olzhy/test)  $ tree . ├─ blog_test.go ├─ pages │ ├─ ... │ └─ search.go ├─ go.mod └─ go.sum  blog_test.go  package blog_test import ( \u0026#34;flag\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/olzhy/test/pages\u0026#34; \u0026#34;github.com/tebeka/selenium\u0026#34; ) var ( browserName = flag.String(\u0026#34;browser\u0026#34;, \u0026#34;chrome\u0026#34;, \u0026#34;browser name\u0026#34;) gridUrl = flag.String(\u0026#34;grid\u0026#34;, \u0026#34;http://localhost:4444/wd/hub\u0026#34;, \u0026#34;grid url\u0026#34;) keyword = \u0026#34;istio\u0026#34; ) var driver selenium.WebDriver func setup() func() { // new remote driver \tcaps := selenium.Capabilities{\u0026#34;browserName\u0026#34;: *browserName} webDriver, err := selenium.NewRemote(caps, *gridUrl) if nil != err { panic(err) } driver = webDriver // teardown \treturn func() { driver.Quit() } } func TestSearch(t *testing.T) { sp := pages.NewSearchPage(driver) count, err := sp.Search(keyword) if nil != err || count \u0026lt; 1 { t.Errorf(\u0026#34;search error, count: %d, err: %s\u0026#34;, count, err) } } func TestMain(m *testing.M) { // parse flags \tflag.Parse() // setup / teardown \tteardown := setup() defer teardown() // run tests \tm.Run() }  pages/search.go  package pages import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/tebeka/selenium\u0026#34; ) const ( blogURL = \u0026#34;https://leileiluoluo.com/\u0026#34; searchButtonIdSelector = \u0026#34;searchOpen\u0026#34; keywordInputIdSelector = \u0026#34;search-query\u0026#34; searchResultLoadingCssSelector = \u0026#34;#search-results #loadingDiv\u0026#34; searchResultCssSelector = \u0026#34;#search-results .border-bottom\u0026#34; ) var drv selenium.WebDriver type SearchPage struct { } // initializer func NewSearchPage(driver selenium.WebDriver) *SearchPage { drv = driver return \u0026amp;SearchPage{} } // open blog and click search button func (sp *SearchPage) openBlogAndClickSearchButton() error { // open blog \terr := drv.Get(blogURL) if nil != err { return errors.New(fmt.Sprintf(\u0026#34;search page open error, err: %s\u0026#34;, err)) } // click search button \telem, err := drv.FindElement(selenium.ByID, searchButtonIdSelector) if nil != err { return errors.New(fmt.Sprintf(\u0026#34;search button not found, err: %s\u0026#34;, err)) } return elem.Click() } // type keyword and enter func (sp *SearchPage) typeKeyword(keyword string) error { elem, err := drv.FindElement(selenium.ByID, keywordInputIdSelector) if nil != err { return errors.New(fmt.Sprintf(\u0026#34;keyword input element not found, err: %s\u0026#34;, err)) } return elem.SendKeys(keyword + \u0026#34;\\n\u0026#34;) } // wait until search result displayed func (sp *SearchPage) waitUntilResultDisplayed() error { return drv.WaitWithTimeout(func(driver selenium.WebDriver) (bool, error) { elem, err := driver.FindElement(selenium.ByCSSSelector, searchResultLoadingCssSelector) if nil != err { return false, nil } visible, err := elem.IsDisplayed() return !visible, err }, 30*time.Second) } // Search by keyword // return count of search result func (sp *SearchPage) Search(keyword string) (int, error) { // open blog and click search button \terr := sp.openBlogAndClickSearchButton() if nil != err { return 0, err } // type keyword and enter \terr = sp.typeKeyword(keyword) if nil != err { return 0, err } // wait until search result displayed \terr = sp.waitUntilResultDisplayed() if nil != err { return 0, err } // return \telems, err := drv.FindElements(selenium.ByCSSSelector, searchResultCssSelector) if nil != err { return 0, errors.New(fmt.Sprintf(\u0026#34;search element error, err: %s\u0026#34;, err)) } return len(elems), nil }  参考资料\n[1] https://www.selenium.dev/\n[2] https://github.com/tebeka/selenium\n[3] https://github.com/SeleniumHQ/docker-selenium\n ","permalink":"https://olzhy.github.io/posts/golang-selenium.html","tags":["Golang","自动化测试","Selenium"],"title":"Golang 使用 Selenium 实现自动化测试初探"},{"categories":["计算机"],"contents":"云平台给我们带来大量好处的同时也给 DevOps 团队带来很多负担。开发使用微服务来架构，与此同时，运维需要管理超多环境及超复杂的部署。\nIstio 即是为解放微服务带来的复杂性而生，其是一个开源的服务网格，可以与现有分布式应用无缝集成。以便为您的分布式微服务架构提供一个统一的方式来连接、管理，保护及监控微服务。\n服务网格是什么？\n组成应用的微服务网络及服务间的交互统称为服务网格。服务网格会愈加庞大也愈加复杂，会变得难以理解且难以管理。其需求通常包括：服务发现，负载均衡，故障恢复，度量监控等。更复杂点的需求可能会有：A/B 测试，灰度发布，速率限制，访问控制，端到端鉴权等。\nIstio 即是将服务网格看作一个整体来管理，提供完整解决方案以满足微服务应用各类需求。\n为何使用 Istio?\nIstio 支持服务以无代码修改或少量代码修改即可达到对部署服务具有负载均衡，服务间鉴权及监控等能力。这是怎么做到的呢？\n这是通过在运行环境部署 Istio 边车代理，从而拦截微服务之间的所有网络请求，这样即可通过 Istio 控制面板对服务进行配置及管理。\n下面为 Istio 提供的能力：\n  可针对不同协议作负载均衡\n如 HTTP，gRPC，WebSocket，TCP 等。\n  细粒度流量行为控制\n使用强大的路由规则进行流量控制，重试，容错等。\n  访问控制\n可插拔规则层及 API 配置以支持流量控制，速率限制，定额限制等。\n  进出流量监控\n可对集群中所有进出流量进行自动化度量，监控，跟踪等。\n  服务到服务鉴权及授权\n可以通过基于身份的鉴权及授权保障集群中服务到服务的安全通信。\n  此外，Istio 的设计支持很高的扩展性，可满足各种部署需求。\nIstio 核心特性\n  流量管理\nIstio 轻便的规则配置及路由设置可以让我们控制服务调用流量。通过简单配置即可实现服务级熔断，超时，重试等能力，也可以实现 A/B 测试，灰度发布，按比例滚动发布等。更好的流量可视化以及开箱即用的错误恢复特性，可将问题在发生前捕获。\n  安全管理\n因 Istio 提供底层安全通信管道，从而可管理鉴权，授权，服务加密通信等，其可在各种协议及运行时之上提供更加安全的服务通信。\n因其是平台独立组件，可与 Kubernetes 结合使用，提供在网络或应用层上的 pod 到 pod 或 service 到 service 的安全通信。\n  监控管理\nIstio 强大的跟踪，监控，日志特性可使您深入洞察您的服务网格。使用其监控特性可使我们真实理解上下游服务的调用性能，结合其可视化面板发现性能瓶颈所在。\n  平台支持\nIstio 平台独立，可在各种环境运行，包括自建集群，Kubernetes，Mesos 平台等。\n 参考资料\n[1] https://istio.io/docs/concepts/what-is-istio/\n ","permalink":"https://olzhy.github.io/posts/what-is-istio.html","tags":["服务网格","Istio"],"title":"Istio 是什么？"},{"categories":["计算机"],"contents":"1 题目描述\n该题目需要您找出二叉树中每一层的最大值，然后以数组返回。\n例子：\n输入：\n 1\r/ \\\r3 2\r/ \\ \\\r5 3 9\r输出：[1, 3, 9]\n题目出处： https://leetcode.com/problems/find-largest-value-in-each-tree-row/\n2 解决思路\n层次遍历二叉树，计算完一层，计算下一层，初始 root 即为一层。\n3 Golang 实现代码\nhttps://github.com/olzhy/leetcode\nfunc largestValues(root *TreeNode) []int { if nil == root { return []int{} } largestVals := []int{} children := []*TreeNode{root} for len(children) \u0026gt; 0 { tmp := children[:] children = []*TreeNode{} largest := -(1 \u0026lt;\u0026lt; 32) for _, child := range tmp { if child.Val \u0026gt; largest { largest = child.Val } if nil != child.Left { children = append(children, child.Left) } if nil != child.Right { children = append(children, child.Right) } } largestVals = append(largestVals, largest) } return largestVals } ","permalink":"https://olzhy.github.io/posts/leetcode-find-largest-value-in-each-tree-row.html","tags":["Golang","算法"],"title":"LeetCode 515 寻找二叉树每层的最大值"},{"categories":["计算机"],"contents":"Kubernetes 是一个开源的容器编排系统。支持将应用的部署、扩展及管理自动化。其设计思路深受谷歌 Borg 系统的影响。\nKubernetes 定义了一组构建块，提供了基于 CPU，内存及自定义指标来部署，维护及扩展应用的机制。Kubernetes 是松耦合的且可对不同的工作载荷进行扩展。其扩展性绝大部分是通过 Kubernetes API 来实现的。\n1 概览\nkubernetes 为了便于控制计算及存储资源，将资源定义为了对象。主要对象有：\nPod pod 是一组容器化组件的上层抽象。pod 是由一个或多个容器组成的可以共享主机及资源的基础调度单元。Kubernetes 集群中的每个 pod 被分配了一个单独的内网 IP 地址。pod 内的多个容器可以以localhost引用彼此，但不同 pod 内的容器不支持使用 pod IP 进行直接访问。 这是由于 pod IP 地址是临时的，当 pod 重启后有可能发生变化。那一个 pod 的容器怎么访问另一个 pod 的容器呢？这可以通过访问另一个 pod 的 service 来解决，因 service 以一个特殊的 pod IP 地址持有到目标 pod 的引用。 访问方式如下图所示，Pod1 内的容器 A 可以通过 Service2 访问到 Pod2 内的容器 C。\npod 可以定义一个卷，该卷可以是本地磁盘，也可以是网络磁盘，以曝露给 pod 内的多个容器使用。这些卷是 Kubernetes ConfigMap的基础。\npod 可通过 Kubernetes API 进行手动管理，也可代理给 controller 进行自动管理。\nReplica Set Replica Set 使用选择器将 pod 分组。\nService Kubernetes service 是一组一起工作的 pod。可以通过标签选择器定义一组组成 service 的 pod。Kubernetes 使用环境变量或 DNS 提供服务发现。服务发现会给 service 分配稳定的 IP 地址及 DNS 名称。service 默认仅对集群内部曝露，当然也可以曝露到集群外。\nVolume Kubernetes 容器的文件系统默认仅支持临时存储，这样 pod 一重启，数据就丢了。Kubernetes 卷即是提供持久化存储的，该存储可以作为 pod 内容器的共享盘使用。同一块卷可以被不同容器挂载到不同挂载点上。\nNamespace Namespace 是 Kubernetes 提供的一种可将资源划分成不重叠集合的管理方式。使用其可以将不同团队，不同项目的用户进行环境划分，也可以用来划分开发，测试，生产等环境。\nConfigMap 及 Secret 一个应用常有存储及管理配置信息的需求，其中有的的信息还可能是敏感数据。配置数据可以是一些字段信息，也可以是整个 JSON 或 XML 文件。Kubernetes 提供两个相关的机制来满足该需求：configmap 及 secret。我们可以使用 deployment 来为应用配置 configmap 及 secret。configmap 及 secret 仅会发送到需要它们的 node 上，Kubernetes 会将其存在 node 的内存中。 一旦依赖 configmap 或 secret 的 pod 被删除，这些内存中的 configmap 或 secret 也会随之删除。这些数据可以通过环境变量（pod 启动时创建）或容器文件系统被 pod 访问。这些数据均存储在 master 节点上，configmap 与 secret 最大的不同是 secret 数据以 base64 加密存储。新版本 k8s，secrets 以加密方式存储在 etcd 上。\nDaemonSet 通常，pod 运行在哪个 node 是由 Kubernetes 调度器算法来决定的。而有些情形下，可能需要将 pod 运行在集群中的每个 node 上，诸如日志搜集，存储服务等。进行诸类调度的特性即是 DaemonSet。\n我们可以使用如下机制对 Kubernetes 对象进行管理。\n标签及选择器 我们可以给 Kubernetes 中的对象打标签，然后使用标签选择器进行查询。例如，我们可以给 pod 打标签，然后在 service 定义标签选择器，以便负载均衡器或路由分发器将流量打到 pod 实例上。这样给一组 pod 打上不同标签，然后结合在 service 上使用标签选择器即可进行蓝绿部署，这是一个既松耦合又轻便的动态解决方案。 如：一个应用的一组 pod 被打了 tier 标签（有两值，front-end 与 back-end 及 release_track 标签（也有两值，canary 与 production），然后我们可以使用诸如tier=back-end AND release_track=canary进行 pod 筛选。\n字段选择器 字段选择器也可以用来筛选 Kubernetes 资源，但其不是自定义分类标签，而是基于资源本来有的属性。如，可使用metadata.name与metadata.namespace作字段选择器。\n副本控制器及部署任务 根据如上可知，Replica Set声明了想要的实例数，而副本控制器即是保障系统中存活的 pod 数与此一致。部署任务是管理部署的，如升级或回滚，当部署任务扩展或收缩完成后，这即导致Replica Set发生变化，保证理想状态则交给了副本控制。\n集群 API Kubernetes 的设计原则即是程序化创建，配置及管理 Kubernetes 集群。该功能即是通过调用集群 API 实现的。API 设计理念是，集群也如 Kubernetes 其它资源一样可以当作对象进行管理，同样，组成集群的机器也被当作 Kubernetes 资源。\n2 Kubernetes 架构\nKubernetes 遵循主从架构。架构图如下，其组件一部分用来管理 node，另一部分组成控制面板。\nKubernetes 控制面板 Kubernetes master 为集群主要控制单元，管理工作载荷及整个系统通信。Kubernetes 控制面板由多个组件组成，这些组件可以运行在同一个 master 节点上，也可以以高可用集群方式运行在多个 master 上。Kubernetes 的核心组件如下。\n  etcd 轻量级分布式持久化健值存储服务 用于存储集群配置信息，代表了集群在某一时间点的总体状态。如 ZooKeeper 设计理念一样，在网络分区下，etcd 偏重一致性胜过可用性（CAP 理论）。其一致性是正确调度及操作服务的关键。 Kubernetes API 服务使用 etcd 的检测 API 来监控集群以便作出关键的配置变更及状态恢复。如开发对某一类 pod 定义了须有 3 个实例处于运行中，该配置会存在 etcd 中，若某时发现与配置产生了偏差，仅有两个实例在运行，则 Kubernetes 会调度再额外创建 1 个实例。\n  API 服务 提供 Kubernetes API 的核心组件 以 REST 方式提供 Kubernetes 内部及外部接口。API 服务校验及处理 REST 请求从而更新 etcd 中的对象。从而允许客户端在工作节点上配置工作载荷及容器。\n  调度器 一个可插拔的组件 用于基于资源可用额度来为未调度的 pod 选择可用节点。调度器知道资源需求，资源可用额度，及其它诸如服务质量、偏好及非偏好需求等用户提供的约束及规则，从而管理可用资源与工作载荷的供求。\n  控制器管理器 控制器使用 API 服务来对其管理的资源进行增删改以将实际集群状态接近理想集群状态。控制器管理器是将一组核心的 Kubernetes 控制器进行管理的进程。 一类控制器是副本控制器，其用于管理副本，如在集群中运行指定数目的 pod 副本来支持水平扩展。同时，若 pod 所在 node 坏掉时，会创建替代 pod。 其它控制器为 Kubernetes 系统的核心部分，如DaemonSet控制器（在每个机器上运行且只运行一个 pod ，或 Job 控制器。控制器所管理的一组 pod 是由标签选择器决定的（标签选择器为控制器定义的部分）。\n  Kubernetes 节点\n节点 node 是工作载荷部署的机器。集群中的每个节点都包含一个诸如 Docker 的容器运行时，此外，还有 kubelet 及 kube-proxy。\n  kubelet 负责搜集每个节点的运行状态，确保节点上的所有容器运行正常。关注启动，停止及将容器组织为 pod 以被控制面板管理。 kubelet 监控 pod 状态，若不在理想状态，则将 pod 在该节点重新部署。节点状态每几秒通过一次心跳传到 master，一旦 master 检测到节点故障，副本控制器即会将 pod 迁移到其它健康节点上运行。\n  kube-proxy kube-proxy 是网络代理与负载均衡器的实现，其负责将进入流量路由到合适的容器上。\n  容器运行时 寄居在 pod 内的容器，容器是微服务的最小单元，其包含运行时应用，库及其它依赖。\n   参考资料\n[1] https://en.wikipedia.org/wiki/Kubernetes\n ","permalink":"https://olzhy.github.io/posts/kubernetes-introduction.html","tags":["Kubernetes","云原生"],"title":"Kubernetes 概览"},{"categories":["练字"],"contents":"“道可道，非常道。名可名，非常名。无，名天地之始；有，名万物之母。此两者同出而异名，同谓之玄。玄之又玄，众妙之门。”——《道德经》\n下图为太太所作：\n大连\n庚子年二月初七\n","permalink":"https://olzhy.github.io/posts/handwriting-tao.html","tags":["练字"],"title":"练字《道可道》"},{"categories":["计算机"],"contents":"OpenID Connect 1.0协议是基于OAuth 2.0授权框架之上的一个身份鉴别层。其使得客户端可以基于授权服务的鉴权能力来验证及识别终端用户的身份。此外，还可以一种类REST的方式来获取终端用户的基本画像信息。\nOpenID Connect 1.0使用Claims来获取终端用户信息，其还描述了一些安全及隐私方面的考量。\n我们知道，OAuth 2.0授权框架为三方应用获得对受保护资源的访问提供了通用标准。其定义了以访问令牌获取受保护资源的机制，但未定义身份鉴别方面的标准。\nOpenID Connect 1.0协议即是为此而生，即其为OAuth 2.0授权框架扩展了鉴权能力。使用起来也很简单，客户端只需在发起授权请求时将scope值设为openid即可。其返回一个JWT格式的身份令牌（ID Token）即具有鉴权能力。\nOAuth 2.0中，实现了OpenID Connect的授权服务（Authentication Server）被叫作开放身份认证提供商（OpenID Providers），使用了OpenID Connect的客户端（Client）被叫作依赖方（Relying Parties）。\n1 概览\nOpenID Connect协议的概览图如下。\n(1) 客户端向授权服务发送授权请求。\n(2) 授权服务对终端用户进行鉴权并获得其授权。\n(3) 授权服务对客户端响应以身份令牌及访问令牌。\n(4) 客户端对身份令牌进行校验，并携带访问令牌向授权服务的用户信息端点请求用户信息。\n(5) 授权服务用户信息端点返回终端用户的Claims。\nOpenID Connect对OAuth 2.0作的主要扩展即是引入以JWT格式表示的身份令牌，使用其即可对终端用户作鉴权。下面列出一个身份令牌（ID Token）的样例。\n{ \u0026quot;iss\u0026quot;: \u0026quot;https://server.example.com\u0026quot;, \u0026quot;sub\u0026quot;: \u0026quot;mail@leileiluoluo.com\u0026quot;, \u0026quot;aud\u0026quot;: \u0026quot;s6BhdRkqt3\u0026quot;, \u0026quot;nonce\u0026quot;: \u0026quot;n-0S6_WzA2Mj\u0026quot;, \u0026quot;exp\u0026quot;: 1311281970, \u0026quot;iat\u0026quot;: 1311280970, \u0026quot;auth_time\u0026quot;: 1311280969, \u0026quot;acr\u0026quot;: \u0026quot;urn:mace:incommon:iap:silver\u0026quot; } 如下是身份令牌的必须字段：\n  iss 令牌签发者，以https打头的一串URL。\n  sub 终端用户唯一标识。\n  aud 身份另牌的使用对象，其必包含客户端ID（Client ID）。\n  exp 身份令牌失效截止时间，表示为自1970-01-01 00:00:00的秒数。\n  iat 签发时间，单位同样为秒。\n  如下是一个非常重要的可选字段：\n nonce 联系客户端及身份令牌会话的一个字符串。客户端若收到该字段，须校验是否为其请求授权时所携带。  2 鉴权\n2.1 使用授权码模式进行鉴权\n我们知道，在OAuth 2.0中，授权码模式的一大优点是令牌的签发由授权服务直接给到客户端，未暴露给用户代理。且在客户端携带授权码请求访问令牌前，授权服务还可对其进行鉴权。授权码模式使得客户端可与授权服务间维护一个客户端密钥（Client Secret）。\n下图为OAuth 2.0授权码模式流程图。\n下面结合该图的每一步，阐释OpenID Connect授权码模式的鉴权流程。\n(1) 初始由客户端发起，将资源所有者的用户代理导向授权服务的授权端点。\n授权服务会对客户端信息进行校验，若校验通过，会对终端用户进行鉴权。鉴权方法有用户名密码，sesson cookie等。\n样例请求如下：\nHTTP/1.1 302 Found Location: https://server.example.com/authorize? response_type=code \u0026amp;scope=openid%20profile%20email \u0026amp;client_id=s6BhdRkqt3 \u0026amp;state=af0ifjsldkj \u0026amp;redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb 必须字段有：\n  scope 要支持OpenID Connect，该字段必须包含openid。\n  response_type 授权码模式，值为code。\n  client_id 客户端ID\n  redirect_uri 在OpenID Provider注册客户端时所填。\n  推荐字段有：\n state 维护请求及回调状态，一般为与浏览器cookie绑定后的加密字段。  可选字段有：\n prompt 若为login，请求授权服务对终端用户重新鉴权。若为consent，请求授权服务获取终端用户许可。  (2) 授权服务对终端用户鉴权通过后，须在返回信息给客户端前获取资源所有者的授权结果。可能会建立一个交互式窗口让终端用户决定授权哪些权限。\n(3) 授权服务对客户端及资源所有者鉴权完成后，会携带结果将请求重定向至客户端所指定的回调URI。\n若鉴权成功，会生成一个一次性授权码，同时将客户端请求时携带的state参数一并通过用户代理返回到客户端。\n鉴权成功的响应样例如下：\nHTTP/1.1 302 Found Location: https://client.example.org/cb? code=SplxlOBeZQQYbYS6WxSbIA \u0026amp;state=af0ifjsldkj 若终端用户拒绝授权或者授权服务对终端用户验证失败，在客户端回调地址正确的情况下，授权服务会将错误信息返回。\n鉴权失败的响应样例如下：\nHTTP/1.1 302 Found Location: https://client.example.org/cb? error=invalid_request \u0026amp;error_description= Unsupported%20response_type%20value \u0026amp;state=af0ifjsldkj 错误码有：\n  interaction_required\n  login_required\n  consent_required\n  \u0026hellip;\n(4) 客户端携带授权码及回调地址向授权服务令牌端点请求令牌。\n客户端对授权服务返回的授权码等信息进行校验，同时验证state参数是否与自己发请求时携带的一致等。若验证通过，会携带授权码及回调地址向授权服务请求访问令牌。\n这次携带的回调地址只是用于授权服务端的验证。\n客户端获取令牌的样例请求如下：\nPOST /token HTTP/1.1 Host: server.example.com Content-Type: application/x-www-form-urlencoded Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW grant_type=authorization_code\u0026amp;code=SplxlOBeZQQYbYS6WxSbIA \u0026amp;redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb 注意客户端密钥信息通过Authorization: Basic ...的方式进行传递。授权码模式的grant_type字段值须设为authorization_code。\n(5) 授权服务为客户端签发令牌\n签发令牌前，授权服务需要校验客户端密钥信息，校验授权码是否有效且是否已使用，校验回调地址是否与初始请求授权码时一致等。\n验证通过，则签发身份令牌及访问令牌。\n授权服务成功签发令牌的样例响应如下。\nHTTP/1.1 200 OK Content-Type: application/json Cache-Control: no-store Pragma: no-cache { \u0026quot;access_token\u0026quot;: \u0026quot;SlAV32hkKG\u0026quot;, \u0026quot;token_type\u0026quot;: \u0026quot;Bearer\u0026quot;, \u0026quot;refresh_token\u0026quot;: \u0026quot;8xLOxBtZp8\u0026quot;, \u0026quot;expires_in\u0026quot;: 3600, \u0026quot;id_token\u0026quot;: \u0026quot;eyJhbGciOiJSUzI1NiIsImtpZCI6IjFlOWdkazcifQ.ewogImlzc yI6ICJodHRwOi8vc2VydmVyLmV4YW1wbGUuY29tIiwKICJzdWIiOiAiMjQ4Mjg5 NzYxMDAxIiwKICJhdWQiOiAiczZCaGRSa3F0MyIsCiAibm9uY2UiOiAibi0wUzZ fV3pBMk1qIiwKICJleHAiOiAxMzExMjgxOTcwLAogImlhdCI6IDEzMTEyODA5Nz AKfQ.ggW8hZ1EuVLuxNuuIJKX_V8a_OMXzR0EHR9R6jgdqrOOF4daGU96Sr_P6q Jp6IcmD3HP99Obi1PRs-cwh3LO-p146waJ8IhehcwL7F09JdijmBqkvPeB2T9CJ NqeGpe-gccMg4vfKjkM8FcGvnzZUN4_KSP0aAp1tOJ1zZwgjxqGByKHiOtX7Tpd QyHE5lcMiKPXfEIQILVq0pc_E2DzL7emopWoaoZTF_m0_N0YzFC6g6EJbOEoRoS K5hoDalrcvRYLSrQAZZKflyuVCyixEoV9GfNQC3_osjzw2PAithfubEEBLuVVk4 XUVrWOLrLl0nx7RkKU8NXNHq-rvKMzqg\u0026quot; } 可以看到，响应体为json格式。令牌类型为Bearer，身份令牌为JWT格式。且特别注意响应头Cache-Control及Pragma，说明了签发的令牌为敏感信息。\n若鉴权失败，授权服务令牌签发失败的样例响应如下：\nHTTP/1.1 400 Bad Request Content-Type: application/json Cache-Control: no-store Pragma: no-cache { \u0026quot;error\u0026quot;: \u0026quot;invalid_request\u0026quot; } 客户端接收到响应后，须按如下步骤校验身份令牌：\n  使用公钥（注册OpenID Connect Provider时所生成）及约定算法解码身份令牌。\n  校验身份令牌iss字段，查看签发者是否有效。\n  校验aud字段是否包含自身客户端ID。\n  \u0026hellip;\n按如下步骤校验访问令牌：\n 若身份令牌包含at_hash字段，须按如下步骤校验其是否合法。  取身份令牌头字段alg所指定哈希算法，计算访问令牌的八进制哈希值；将哈希值左半部分使用base64url加密；其应与at_hash字段值相等。\n2.2 使用隐式授权模式进行鉴权\n我们知道OAuth 2.0中，隐式授权主要针对在浏览器脚本语言实现的客户端，会暴露给用户代理，具有一定安全风险。使用隐式授权，令牌直接从授权端点返回，并未用到令牌端点。隐式授权的鉴权部分大部分与授权码模式一致，仅对nonce字段的校验是必须的。\n下图为OAuth 2.0隐式授权模式流程图。\n下面对图中每一步作解释。\n(1) 初始，客户端将资源所有者的用户代理导向授权端点。\n指定client_id，scope，state参数，同时指定回调地址以便授权服务将用户代理重定向回来。\n鉴权方式与采用授权码模式一致。\n样例请求如下：\nGET /authorize? response_type=id_token%20token \u0026amp;client_id=s6BhdRkqt3 \u0026amp;redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb \u0026amp;scope=openid%20profile \u0026amp;state=af0ifjsldkj \u0026amp;nonce=n-0S6_WzA2Mj HTTP/1.1 Host: server.example.com 必须字段：\n  response_type 可传id_token或id_token token。\n  redirect_uri 客户端回调地址，须与注册时一致。\n  nonce 用于建立客户端会话与令牌的对应关系，将被授权服务原封不动传回。\n  (2) 取得资源所有者授权。\n授权服务对资源所有者鉴权（通过用户代理），获取是否准许授权的结果。\n鉴权方式与采用授权码模式一致。\n(3) 若准许授权，授权服务将访问令牌拼在URL上，然后将用户代理重定向至客户端回调地址。\n鉴权方式与采用授权码模式一致。\n成功响应样例如下：\nHTTP/1.1 302 Found Location: https://client.example.org/cb# access_token=SlAV32hkKG \u0026amp;token_type=bearer \u0026amp;id_token=eyJ0 ... NiJ9.eyJ1c ... I6IjIifX0.DeWt4Qu ... ZXso \u0026amp;expires_in=3600 \u0026amp;state=af0ifjsldkj 错误响应样例与采用授权码模式类似。\n(4) 用户代理接到重定向指令，并请求服务端静态资源（用于解码令牌）。\n(5) 客户端服务器部分返回内置脚本的资源，可以用来将URI中的令牌取出。\n(6) 用户代理使用上述脚本将令牌取出，并给到客户端。\n使用隐式授权获得的身份令牌须包含如下字段：\n nonce 用于客户端验证响应合法性。  可能包含如下字段：\n at_hash 用于验证访问令牌  2.3 使用混合模式进行鉴权\nOpenID Connect混合模式的response_type的组合方式有如下几种，code id_token，code token，code id_token token。\n所以混合模式的授权与鉴权流程大致与授权码模式一致。只是在获取授权码的时候可以顺带获取令牌，也多了一些校验。\n样例请求及响应：\nGET /authorize? response_type=code%20token \u0026amp;client_id=s6BhdRkqt3 \u0026amp;redirect_uri=https%3A%2F%2Fclient.example.org%2Fcb \u0026amp;state=af0ifjsldkj HTTP/1.1 Host: server.example.com HTTP/1.1 302 Found Location: https://client.example.org/cb# access_token=2YotnFZFEjr1zCsicMWpAA \u0026amp;token_type=Bearer \u0026amp;code=SplxlOBeZQQYbYS6WxSbIA \u0026amp;state=af0ifjsldkj \u0026amp;expires_in=3600 详细流程不再赘述。\n 参考资料\n[1] https://openid.net/specs/openid-connect-core-1_0.html\n ","permalink":"https://olzhy.github.io/posts/openid-connect-core-1-0.html","tags":["架构设计"],"title":"OpenID Connect 1.0 协议要点梳理"},{"categories":["计算机"],"contents":"OAuth 2.0是一个委托访问授权框架。\n即，若有三方应用想访问我在某网站的一些资源，我不必将用户名密码给它，而是采用OAuth 2.0授权流程，让资源网站通过我的授权给其下发一个访问令牌来实现该功能。\n这样即省去了直接将密码交给三方网站的诸多风险，还可以很好的实现资源限制，令牌过期等细粒度控制。\nOAuth 2.0框架应用广泛，如使用GitHub账号登录CSDN，这即是CSDN三方网站想借助GitHub账号能力，通过我的授权，使用我在GitHub的头像名称等基本信息，而实现一键登录。\nOAuth 定义了4个角色。\n  Resource Owner（资源所有者） 对受保护资源进行访问授权的实体，若资源所有者是人，则指的是终端用户。\n  Resource Server（资源服务器） 对受保护资源提供服务，接收携带访问令牌的请求，并对其作出响应。\n  Client（客户端） 代表资源所有者及其授权发送请求，可以是跑在桌面，服务端或其它设备上的应用。\n  Authorization Server（授权服务器） 在对资源所有者鉴权成功后，并在取得资源所有者授权后，对客户端签发访问令牌。\n  授权服务器与资源服务器可能是一个服务，也可能是分开的两个服务。一个授权服务器可以给多个资源服务器签发令牌。\n如图1，看一下OAuth 2.0大致的授权流程。\n图1：OAuth 2.0大致授权流程（引自RFC6749）\n(A) 客户端请求资源所有者授权，授权请求虽可以直接发给资源所有者，但最好经过授权服务器中转。\n(B) 客户端接收到了授权准许，授权准许类型取决于客户端的请求方式及授权服务器所支持的类型。\n(C) 客户端携带授权准许请求授权服务器\n(D) 授权服务器对客户端进行鉴权，包括对客户端的鉴权及对其所携带的资源所有者授权准许的校验，校验成功则发放访问令牌。\n(E) 客户端携带访问令牌请求资源服务器上的受保护资源\n(F) 资源服务器校验客户端携带的访问令牌，若令牌有效，则提供服务。\n关于授权准许\n授权准许为资源所有者同意客户端访问其受保护资源的证明，也是客户端用来获取访问令牌的基础。\nOAuth 2.0定义了4种授权类型：授权码（authorization code），隐式授权（implicit），资源所有者密码证明（resource owner password credentials），客户端证明（client credentials），此外还支持扩展类型。\n授权码\n授权码的获取是通过将授权服务器作为客户端及资源所有者的中间层来实现的。\n避免客户端直接向资源所有者获得授权。客户端通过浏览器等用户代理将资源所有者引导至授权服务器。\n授权服务器对资源所有者进行鉴权并获得资源所有者对客户端要求访问的授权。\n拿到授权码后，用户代理又将资源所有者引导回客户端。\n因资源所有者仅被授权服务器鉴权，所以资源所有者的密钥从没有与客户端分享。\n授权码模式还有一些优点，即提供了授权服务器对客户端进行鉴权的能力，而且访问令牌直接给了客户端，并没有经过资源所有者的用户代理。\n隐式授权\n隐式授权是授权码模式的简化版。用于客户端为诸如JS脚本语言实现的浏览器应用的优化模式。\n隐式授权绕过了对客户端签发授权码，直接给客户端签发访问令牌。\n在隐式授权模式中，签发访问令牌时，授权服务器未对客户端进行校验。某些情况下，客户端身份可以通过对回调路径（用于传递访问令牌）的校验进行识别。隐式授权中，访问令牌可能暴露给资源所有者及有权访问资源所有者用户代理的其它应用。隐式授权减少了请求次数，带来了便捷，但需要权衡其安全问题。\n资源所有者密码证明\n资源所有者密码证明（如用户名密码）可以直接用作授权准许以获取访问令牌。\n该模式仅可在资源所有者充分信任客户端及其它授权类型不可用时使用。\n客户端证明\n客户端证明可以用在授权范围仅限于客户端控制的受保护资源，或者受保护资源之前已被授权服务器分配过。\n客户端证明典型场景是用在客户端即是资源所有者的情况下，或者是获取对之前已被授权过的受保护资源的授权访问。\n访问令牌\n访问令牌是访问受保护资源的证明。访问令牌对客户端透明，用于标识访问范围，访问时段等。访问令牌可以是一个id，也可以是一个自包含验证信息的字符串。\n更新令牌\n更新令牌用于当访问令牌失效时获取新的访问令牌。不同于访问令牌的是，更新令牌只会发给授权服务器，不会与资源服务器交互。\n下面参考图2，看一下更新令牌的使用。\n图2：更新失效的访问令牌（引自RFC6749）\n(A) 客户端携带授权准许向授权服务器请求访问令牌。\n(B) 授权服务器对客户端及授权准许进行校验，若校验通过，发放访问令牌及更新令牌。\n(C) 客户端携带访问令牌对资源服务器进行受保护资源访问。\n(D) 资源服务器校验访问令牌，若有效，则提供服务。\n(E) 重复(C)及(D)两步直至访问令牌失效，若客户端已知访问令牌失效，则跳到步骤(G)，否则进行另一次访问。\n(F) 因访问令牌已失效，资源服务器返回访问令牌失效错误。\n(G) 客户端携带更新令牌向授权服务器请求新的访问令牌。\n(H) 授权服务器对客户端及更新令牌进行鉴权，若校验通过，则签发新的访问令牌及更新令牌。\n 参考资料\n[1] https://en.wikipedia.org/wiki/OAuth\n[2] https://oauth.net/2/\n[3] https://tools.ietf.org/html/rfc6749\n ","permalink":"https://olzhy.github.io/posts/oauth2-authorization-framework.html","tags":["架构设计"],"title":"OAuth 2.0 授权框架梳理"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉搜索树（BST）的根节点及待插入值。请将该值插入到该二叉搜索树，然后返回值插入后的二叉搜索树。（注：待插入值在原二叉搜索树中不存在）\n可能存在多种有效的插入方式，即只要在值插入后仍旧是二叉搜索树即可。您可以返回有效结果的任意一种。\n例子：\n输入：\n给定树：\n 4 / \\ 2 7 / \\ 1 3 及插入值：5\n输出：\n您可以返回如下二叉搜索树：\n 4 / \\ 2 7 / \\ / 1 3 5 返回如下二叉搜索树也是有效的：\n 5 / \\ 2 7 / \\ 1 3 \\ 4 题目出处：LeetCode\n2 解决思路\n判断插入值与根节点的大小，进而决定将该值插入到左子树还是右子树，递归调用，直至找到最终位置。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc insertIntoBST(root *TreeNode, val int) *TreeNode { if nil == root { return \u0026amp;TreeNode{Val: val} } if val \u0026gt; root.Val { root.Right = insertIntoBST(root.Right, val) } else { root.Left = insertIntoBST(root.Left, val) } return root } ","permalink":"https://olzhy.github.io/posts/leetcode-insert-into-a-binary-search-tree.html","tags":["Golang","算法"],"title":"LeetCode 701 二叉搜索树插入"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，返回其值的Z字形层次遍历。（如，先从左到右，下一层从右到左，以此类推，直至最后一层遍历完成）\n例子：\n输入：\n[3,9,20,null,null,15,7]  3 / \\ 9 20 / \\ 15 7 输出：\n[[3], [20,9], [15,7]] 题目出处：LeetCode\n2 解决思路\n将根节点设为第0层，采用迭代算法，每一次遍历一层，针对每层的遍历，判断该层是奇数层还是偶数层，偶数层正序追加节点，奇数层逆序追加节点。遍历完成即得到结果。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc zigzagLevelOrder(root *TreeNode) [][]int { if nil == root { return [][]int{} } var allVals [][]int nodes := []*TreeNode{root} level := 0 for len(nodes) \u0026gt; 0 { var vals []int tmp := nodes[:] nodes = []*TreeNode{} for _, p := range tmp { if 0 == level%2 { vals = append(vals, p.Val) } else { vals = append([]int{p.Val}, vals...) } if nil != p.Left { nodes = append(nodes, p.Left) } if nil != p.Right { nodes = append(nodes, p.Right) } } allVals = append(allVals, vals) level++ } return allVals } ","permalink":"https://olzhy.github.io/posts/leetcode-binary-tree-zigzag-level-order-traversal.html","tags":["Golang","算法"],"title":"LeetCode 103 二叉树Z字形层次遍历"},{"categories":["计算机"],"contents":"1 题目描述\n以先序遍历构建二叉搜索树，并返回其根节点。\n二叉搜索树是满足如下条件的二叉树：\n对于每个节点，左子树node.left任意节点的值均小于node.val；右子树node.right任意节点的值均大于node.val。\n先序遍历先展示根节点值，然后遍历左子树，最后遍历右子树。\n注：\na）1 \u0026lt;= preorder.length \u0026lt;= 100；\nb）preorder的值均是不同的。\n例子：\n输入：[8,5,1,7,10,12]\n输出：[8,5,10,1,7,null,12]\n 8 / \\ 5 10 / \\ \\ 1 7 12 题目出处：LeetCode\n2 解决思路\n采用递归思路构建二叉搜索树。\na）从先序遍历数组取第一个元素作为根节点；\nb）从第二个节点起自左向右遍历该先序遍历数组，寻找根节点左右子树的分界点，即寻找第一个出现大于根节点值的位置，将该数组第2个节点至该位置上一个节点的元素组成的子数组作为根节点左子树先序遍历数组；该位置直到末尾的元素组成的子数组作为右子树先序遍历数组。\nc）递归调用构建函数，直至构建完成，返回整个二叉搜索树。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc bstFromPreorder(preorder []int) *TreeNode { if 0 == len(preorder) { return nil } val := preorder[0] preorder = preorder[1:] i := 0 for i \u0026lt; len(preorder) \u0026amp;\u0026amp; preorder[i] \u0026lt; val { i++ } return \u0026amp;TreeNode{ val, bstFromPreorder(preorder[:i]), bstFromPreorder(preorder[i:]), } } ","permalink":"https://olzhy.github.io/posts/leetcode-construct-binary-search-tree-from-preorder-traversal.html","tags":["Golang","算法"],"title":"LeetCode 1008 以先序遍历构建二叉搜索树"},{"categories":["计算机"],"contents":"1 题目描述\n对于给定的先序及后序遍历，返回满足条件的任意二叉树。\n注：\na）1 \u0026lt;= pre.length == post.length \u0026lt;= 30；\nb）pre[]及post[]均是1, 2, \u0026hellip;, pre.length的排列；\nc）输入保证有解，对于有多个解的情形，返回任意一个即可。\n例子：\n输入：pre = [1,2,4,5,3,6,7], post = [4,5,2,6,7,3,1]\n输出：[1,2,3,4,5,6,7]\n题目出处：LeetCode\n2 解决思路\n采用递归思路，先序遍历数组的第一个元素为根，后序遍历最后一个元素为根。\n根节点有了，然后将先序遍历与后序遍历数组分别掐头去尾。接下来构建左右子树。\n掐头后的先序遍历数组的第一个元素即为左子树的根，以该节点自左向右到去尾后的后序遍历数组寻找其出现的位置，找到后，在该位置将后序遍历数组切割为两部分，该节点及其前面的部分为左子树后序遍历数组，该节点后面的部分为右子树后序遍历数组。同样，先序遍历数组也切割为两部分，自左向右取与左子树后序遍历数组相同数目的节点作为左子树先序遍历数组，剩下的为右子树先序遍历数组。\n递归调用构造方法构建左右子树。最后，整个树即构建完成了。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc constructFromPrePost(pre []int, post []int) *TreeNode { if 0 == len(pre) { return nil } if 1 == len(pre) { return \u0026amp;TreeNode{Val: pre[0]} } root := \u0026amp;TreeNode{Val: pre[0]} pre = pre[1:] post = post[:len(post)-1] i := 0 for i \u0026lt; len(post) { if pre[0] == post[i] { break } i++ } root.Left = constructFromPrePost(pre[:i+1], post[:i+1]) root.Right = constructFromPrePost(pre[i+1:], post[i+1:]) return root } ","permalink":"https://olzhy.github.io/posts/leetcode-construct-binary-tree-from-preorder-and-postorder-traversal.html","tags":["Golang","算法"],"title":"LeetCode 889 以先序及后序遍历构建二叉树"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，返回其节点值的后序遍历。\n注：递归实现较简单，可以通过迭代实现吗。\n例子：\n输入：[1,null,2,3]\n 1 \\ 2 / 3 输出：[3,2,1]\n题目出处：LeetCode\n2 解决思路\n后序遍历顺序为左右根，从根节点起，我们可以使用根右左来遍历，最后将遍历数组逆转即可。\n该思路，因左子树迟迟得不到遍历，需要先记录下来，所以申请一个存放左子树的数组，初始时将根节点放入该数组。\n又因根右左最后才是左，所以当右子树遍历完，后记录的左子树先遍历。\n综上，算法步骤总结如下。\n当左子树数组不为空时：\na）从末尾取一个节点（数组len-1）；然后循环遍历该节点及其右孩子，将这些节点值记录，若有左子树，将其放入左子树数组。\nb）重复a）直至左子树数组为空。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc postorderTraversal(root *TreeNode) []int { if nil == root { return []int{} } var vals []int leftNodes := []*TreeNode{root} for len(leftNodes) \u0026gt; 0 { node := leftNodes[len(leftNodes)-1] leftNodes = leftNodes[:len(leftNodes)-1] for nil != node { vals = append([]int{node.Val}, vals...) if nil != node.Left { leftNodes = append(leftNodes, node.Left) } node = node.Right } } return vals } ","permalink":"https://olzhy.github.io/posts/leetcode-binary-tree-postorder-traversal.html","tags":["Golang","算法"],"title":"LeetCode 145 二叉树后序遍历"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树的中序遍历与后序遍历，请以此构造出该二叉树。\n注：您可以假定该二叉树中不存在重复节点值。\n例子：\n输入：inorder = [9,3,15,20,7], postorder = [9,15,7,20,3]\n输出：\n 3 / \\ 9 20 / \\ 15 7 题目出处：LeetCode\n2 解决思路\n对于给定的后序遍历，其最后的一个值是根节点值，然后以该值将中序遍历分割为两部分，前半部分为根节点的左子树中序遍历，后半部分为根节点的右子树中序遍历；因节点个数不因遍历方式改变，从移除根节点后的后序遍历数组中自左向右取出与左子树中序遍历个数相同个数的节点，即为左子树后序遍历，剩余部分为右子树后序遍历。\n这样递归调用buildTree即可得到结果。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc buildTree(inorder []int, postorder []int) *TreeNode { if 0 == len(inorder) { return nil } val := postorder[len(postorder)-1] root := \u0026amp;TreeNode{Val: val} i := 0 for ; i \u0026lt; len(inorder); i++ { if inorder[i] == val { break } } root.Left = buildTree(inorder[:i], postorder[:i]) root.Right = buildTree(inorder[i+1:], postorder[i:len(postorder)-1]) return root } ","permalink":"https://olzhy.github.io/posts/leetcode-construct-binary-tree-from-inorder-and-postorder-traversal.html","tags":["Golang","算法"],"title":"LeetCode 106 根据中序遍历与后序遍历构造二叉树"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，找出最后一行最左边节点的值。\n注：您可以假定给定的树非空。\n例子1：\n输入：\n 2 / \\ 1 3 输出：1\n例子2：\n输入：\n 1 / \\ 2 3 / / \\ 4 5 6 / 7 输出：7\n题目出处：LeetCode\n2 解决思路\n采用层次遍历来遍历二叉树，首次进入某一层时，记录当前深度及对应的第一个节点值，遍历完成后得到最大深度及对应的第一个节点值，即为所求。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc findBottomLeftValue(root *TreeNode) int { nodes := []*TreeNode{root} maxDepth := -1 val := 0 for depth := 0; len(nodes) \u0026gt; 0; depth++ { copy := nodes[:] nodes = []*TreeNode{} for _, node := range copy { if depth \u0026gt; maxDepth { maxDepth = depth val = node.Val } if nil != node.Left { nodes = append(nodes, node.Left) } if nil != node.Right { nodes = append(nodes, node.Right) } } } return val } ","permalink":"https://olzhy.github.io/posts/leetcode-find-bottom-left-tree-value.html","tags":["Golang","算法"],"title":"LeetCode 513 找出二叉树左下角节点的值"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个字符串，请基于字符出现的频次将其倒序排列。\n例子1：\n输入：\u0026quot;tree\u0026quot; 输出：\u0026quot;eert\u0026quot; 释义：'e'出现2次，而'r'及't'各出现1次，所以'e'应出现在'r'及't'的前面，因此\u0026quot;eetr\u0026quot;是一个有效的答案。 例子2：\n输入：\u0026quot;cccaaa\u0026quot; 输出：\u0026quot;cccaaa\u0026quot; 释义：'c'与'a'均各出现3次，所以\u0026quot;aaaccc\u0026quot;是一个有效的答案，注意\u0026quot;cacaca\u0026quot;是不正确的，相同的字符应连在一起。 例子3：\n输入：\u0026quot;Aabb\u0026quot; 输出：\u0026quot;bbAa\u0026quot; 释义：'c'与'a'均各出现3次，所以\u0026quot;aaaccc\u0026quot;是一个有效的答案，注意\u0026quot;cacaca\u0026quot;是不正确的，相同的字符应连在一起。 题目出处：LeetCode\n2 解决思路\na）遍历一遍字符串，得到key为字符value为其出现次数的map charCounts；\nb）遍历一遍charCounts，得到key为出现次数value为字符数组的map countChars；同时搜集到出现次数数组；\nc）将出现次数数组倒序排好，然后遍历该数组，针对每个出现次数，查询countChars得到字符，知道该字符应重复几次，依序排好，直至构造出一个新的字符串返回即可。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc frequencySort(s string) string { charCounts := make(map[rune]int) for _, c := range s { charCounts[c] += 1 } var counts []int countChars := make(map[int][]rune) for c, count := range charCounts { if chars, ok := countChars[count]; ok { countChars[count] = append(chars, c) } else { countChars[count] = []rune{c} counts = append(counts, count) } } sort.Slice(counts, func(i, j int) bool { return counts[i] \u0026gt; counts[j] }) var chars []rune for _, count := range counts { for _, c := range countChars[count] { i := count for i \u0026gt; 0 { chars = append(chars, c) i-- } } } return string(chars) } ","permalink":"https://olzhy.github.io/posts/leetcode-sort-characters-by-frequency.html","tags":["Golang","算法"],"title":"LeetCode 451 以出现频次排序字符"},{"categories":["计算机"],"contents":"1 题目描述\n一段包含A-Z的文字使用如下映射关系加密为数字。\n'A' -\u0026gt; 1 'B' -\u0026gt; 2 ... 'Z' -\u0026gt; 26 给定一个仅包含数字的字符串，计算其有几种解码方式。\n例子1：\n输入：\u0026quot;12\u0026quot; 输出：2 释义：可以被解码为\u0026quot;AB\u0026quot; (1 2) 或 \u0026quot;L\u0026quot; (12) 例子2：\n输入：\u0026quot;226\u0026quot; 输出：3 释义：可以被解码为\u0026quot;BZ\u0026quot; (2 26)，\u0026quot;VF\u0026quot; (22 6)，或\u0026quot;BBF\u0026quot; 题目出处：LeetCode\n2 解决思路\n声明函数decode(string, int)，第1个参数传字符串，第2个传当前遍历到的标号，总体采用递归思路，初始标号为0，从头至尾遍历字符串（初始为 decode(s, 0)）：\na）若已无字符遍历，则说明该种解码情形满足要求，返回1；\nb）若当前字符为'0'，则说明该种解码情形无法进行下去，返回0；\nc）若为非'0\u0026rsquo;字符，则至少可以推进一位，解码总数先置为decode(s, i+1)；若还满足推进2位的情形，则解码总数加上decode(s, i+2)； 递归完成，即可得到结果。\n因某些标号对应的值可能造成重复递归计算，程序使用一个map来记录之前算过的标号来实现加速。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc decode(s string, i int, table map[int]int) int { if len(s) == i { return 1 } if \u0026#39;0\u0026#39; == s[i] { return 0 } // if calculated before, return value directly \tif v, ok := table[i]; ok { return v } num := decode(s, i+1, table) if i \u0026lt; len(s)-1 { if \u0026#39;1\u0026#39; == s[i] { num += decode(s, i+2, table) } else if \u0026#39;2\u0026#39; == s[i] \u0026amp;\u0026amp; s[i+1] \u0026lt;= \u0026#39;6\u0026#39; { num += decode(s, i+2, table) } } table[i] = num return num } func numDecodings(s string) int { table := make(map[int]int) return decode(s, 0, table) } ","permalink":"https://olzhy.github.io/posts/leetcode-decode-ways.html","tags":["Golang","算法"],"title":"LeetCode 91 解码方式"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个字符串，将该字符串依序按单词进行翻转。\n注：\na）单词被定义为连续的非空字符；\nb）输入字符串首尾可能有空格，但您翻转后的字符串首尾不应有空格；\nc）翻转后的字符串应将源字符串中两个单词间的多个空格减为一个。\n例子1：\n输入：\u0026quot;the sky is blue\u0026quot; 输出：\u0026quot;blue is sky the\u0026quot; 例子2：\n输入：\u0026quot; hello world! \u0026quot; 输出：\u0026quot;world! hello\u0026quot; 释义：翻转后的字符串首尾不应有空格 例子3：\n输入：\u0026quot;a good example\u0026quot; 输出：\u0026quot;example good a\u0026quot; 释义：翻转时应将两个单词间的多个空格减为一个 题目出处：LeetCode\n2 解决思路\n声明变量lastIndex，用来表示遍历到新的单词的末尾字符的位置。\n从后向前一个字符一个字符遍历字符串，i表示当前位置：\na）若i对应的当前字符为空格，那么判断i是否与lastIndex相等，若相等，则lastIndex前移一位，i也前移一位；若不相等，则拼接上这个单词（i+1至lastIndex+1位置对应的字符串）及一个空格，并将lastIndex置为i-1。\nb）若i对应的当前字符非空格，则判断是否抵达头部，若抵达头部并且i\u0026lt;=lastIndex，则把最后一个单词拼接上。\n最后trim一下尾部可能的空格并返回结果。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc reverseWords(s string) string { reversed := \u0026#34;\u0026#34; lastIndex := len(s) - 1 for i := len(s) - 1; i \u0026gt;= 0; i-- { if \u0026#39; \u0026#39; == s[i] { if i == lastIndex { lastIndex-- } else { reversed += s[i+1:lastIndex+1] + \u0026#34; \u0026#34; lastIndex = i - 1 } } else { if 0 == i \u0026amp;\u0026amp; i \u0026lt;= lastIndex { reversed += s[i : lastIndex+1] } } } if len(reversed) \u0026gt; 1 \u0026amp;\u0026amp; \u0026#39; \u0026#39; == reversed[len(reversed)-1] { reversed = reversed[:len(reversed)-1] } return reversed } ","permalink":"https://olzhy.github.io/posts/leetcode-reverse-words-in-a-string.html","tags":["Golang","算法"],"title":"LeetCode 151 将字符串中的单词翻转"},{"categories":["计算机"],"contents":"Go 1.11，在 Go 1.10 发布半年后如期而至。多数变化在工具链实现、运行时及库上面。该版本继续秉承 Go 1 兼容性准则。期待几乎所有的程序均可像之前一样编译及运行。\n1 移植\n WebAssembly Go 1.11 对 WebAssembly（js/wasm）加入试验性支持。 目前，编译为一个 WebAssembly 模块的 Go 程序，包括支持 goroutine 调度的 go 运行时、垃圾收集器，map 等。这样即造成目标二进制文件最小接近 2 MB（压缩后 500 KB）。Go 程序可以使用 syscall/js 试验性包来调用 JavaScript。二进制大小的优化及与其它语言的交互还未排进优先事项，但可能会在后续的版本解决。  所有新加了 GOOS 变量值\u0026quot;js\u0026quot;及 GOARCH 变量值\u0026quot;wasm\u0026quot;,被命名为*_js.go 或*_wasm.go 的 go 文件将被 Go 工具忽略（除了 GOOS/GOARCH 设定的情况）。若现有程序包含满足该命名方式的文件，需要重命名。\n关于 Go WebAssembly 的使用，请移步“Go WebAssembly 初探”。\n2 工具\n  模块、包版本化，及依赖管理 Go 1.11 对模块增加了初步支持。使用模块，开发者不再限定于在 GOPATH 内开发，版本依赖信息显式且轻量，构建更可信且更可重现。详情请参阅“Golang Modules”。\n  引用路径限制 因 Go 模块支持对命令行操作中的@符号赋予了特殊含义，所以，go 命令目前不允许引用路径中使用@符号。此类引用路径已不被 go get 所允许，该限制仅会影响构建自定义引用路径的场景。\n  包加载 新包golang.org/x/tools/go/packages对源码包的定位及加载提供一个简单的 API。尽管其非标准库的一部分，对许多任务，其已有效替代了 go/build 包（API 无法全力支持模块）。因其运行诸如 go list 的额外查询命令来获取包信息，使得分析工具的构建与诸如 Bazel 及 Buck 等可选构建系统的合力工作表现更佳。\n  构建缓存 Go 1.11 将会是最后一个支持设置环境变量 GOCACHE=off（取消构建缓存选项，由 Go 1.10 引入）的版本。自 Go 1.12 起，作为趋向移除$GOPATH/pkg 的一步，构建缓存是需要的。如上描述的模块及包加载支持已需要构建缓存开启。\n  编译器工具链 目前，更多函数中意于默认内敛，包含调用 panic 的函数。\n  编译器工具链目前支持line 原语的列信息。\n一个新的包导出数据格式已被引入。除了对大型 Go 工程的构建次数加速，其对终端用户应是透明的。若其引起问题，可以在使用 go tool 构建二进制时，传入gcflags=all=-iexport=false来关闭该功能。\n编译器目前禁止在类型选择语句中有未使用的变量声明。（诸如如下的 x 变量）\nfunc f(v interface{}) { switch x := v.(type) { } }  调试 编译器目前对优化后的二进制生成更精确的调试信息，包含变量位置信息、行号及断点位置。其可用来调试不使用-N -l 编译的二进制。调试信息的质量仍然有限，有一些是基础的，还有一些会在后续版本改进。  因调试器会生成更扩展更精确的调试信息，DWARF 部分目前已默认压缩。其对绝大多数 ELF 工具（诸如位于 Linux 及*BSD 的调试器）是透明的，且被 Delve 调试器的各平台所支持，但 macOS 及 Windows 的本地工具支持有限。取消 DWARF 压缩，构建二进制时，可以对 go tool 传入-ldflags=-compressdwarf=false。\nGo 1.11 对调试器内部调用 Go 函数增加了试验性支持。这是很有用的，诸如停在某个断点调用 String 方法。该功能目前仅支持 Delve 1.1.0 及以上版本。\n 测试 自 Go 1.10 起，go test 命令在即将被测试的包上运行 go vet，以检测测试前的问题。因 vet 在运行前对代码进行类型检查，类型检查不通过的用例将会失败。特别是，使用 Go 1.10 编译的（编译器错误的接受了该场景），闭包内部含有未使用变量的用例，目前会失败。  用在 go test 的-memprofile 标记目前默认为“allocs”分析，记录自测试开始的总分配字节（包含垃圾收集字节）。\n Vet 当被分析的包未通过类型检查时，go vet 会报一个致命错误（之前仅打印一句警告）。  此外，go vet 对 printf 的格式检查更健壮。诸如，如下代码会报错。\n// test.go func wrapper(s string, args ...interface{}) { fmt.Printf(s, args...) } func main() { wrapper(\u0026#34;%s\u0026#34;, 42) } $ go vet test.go # command-line-arguments ./test.go:10:2: wrapper format %s has arg 42 of wrong type int   Trace 使用新包 runtime/trace 的用户 API，用户可以记录 Trace 执行时的应用级信息，且可给相关的 goroutine 分组。go tool trace 还可将这些信息可视化。\n  Cgo 自 Go 1.10 起，cgo 已将一些 C 指针类型转换为 Go 类型 uintptr。这些类型包含 Darwin 核心框架的 CFTypeRef 层次结构及 Java JNI 接口的 jobject 层次结构。在 Go 1.11，已进行多项改进来检测这些类型。使用这些类型的代码可能需要更新。\n  Go 命令 GOFLAGS 环境变量目前可能会被用于设置 go 命令的默认标记。其在有些场景是很有用的。在有些因 DWARF 而链接很慢的系统，用户可能会默认设置-ldflags=-w。对于模块，用户或持续集成系统若想使用 vendor 模式，可以默认设置-mod=vendor。\n  Godoc Go 1.11 将会是支持 godoc 命令行的最后一个版本。未来版本，godoc 仅是一个 web 服务，用户可以使用 go doc 作命令行帮助。\n  Run go run 命令目前允许传入一个单一的引用路径，一个目录名称，或匹配一个包的模式。这样即允许 go run pkg 或 go run dir，甚至 go run .。\n  3 运行时\n目前，运行时使用一个稀疏堆的布局，所以没有了 Go 堆大小的上限（之前上限为 512GiB）。这样修复了少数情形“地址空间冲突”问题（混合 Go/C 二进制或使用-race 编译的二进制情形）。\n在 macOS 及 iOS 系统，运行时目前使用 libSystem.dylib 代替直接调用内核。这将使 Go 二进制与未来的 macOS 及 iOS 的版本更兼容。syscall 包仍然采用直接系统调用，计划在未来修复。\n4 性能\nmath/big 包有多项性能改进，同时，针对 GOARCH=arm64 有多项性能改进。\n 编译器工具链 编译器目前优化了针对如下方式的 map 清理操作：  for k := range m { delete(m, k) } 同时，编译器还优化了针对如下方式的 slice 扩展：\nappend(s, make([]T, n)...) 编译器目前在边界检查及分支淘汰上表现更佳。目前编译器会识别传递性联系，如：若 i\u0026lt;j 且 j\u0026lt;len(s)，则其会使用该事实略过对 s[i]的检查。其还会懂一点诸如 s[i-10]的简单算术，进而识别循环中更多归纳的情形。进而，编译器目前使用边界信息对移位操作作更积极的优化。\n 参考资料\n[1] https://golang.org/doc/go1.11\n ","permalink":"https://olzhy.github.io/posts/go1dot11-release-notes.html","tags":["Golang"],"title":"Go 1.11 Release Notes 要点整理"},{"categories":["计算机"],"contents":"WebAssembly（简写为 wasm）是一种新的可以运行在现代 web 浏览器的二进制格式。其采用底层类汇编语言将高级语言（如 C++/Rust/Go）编译为二进制然后运行在 web 浏览器上，其性能接近原生，且可与 JS 互相调用，这样即可以一种新的方式（WebAssembly 的性能结合 JS 的表达能力）来实现一个应用。\nGo 自 1.11 起即开始试验性的支持 WebAssembly，虽截止目前还处在初级阶段，存在诸如编译的二进制文件太大，不好调试等诸多问题，但不影响我们尝鲜，这些问题期待官方在后续的版本可以逐步丰富与优化。\n1 Hello WebAssembly 下面的test.go是一个最简单的 Go 程序，如何将其以 WebAssembly 方式运行在浏览器上呢？\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello Assembly!\u0026#34;) } 编译为.wasm 文件\n需要指定 GOOS 为 js，GOARCH 为 wasm，然后编译。\n$ GOOS=js GOARCH=wasm go build -o test.wasm test.go 发现当前目录下多了一个 test.wasm 文件。\n$ ls -lht ... 2.2M Oct 4 15:36 test.wasm ... 76B Oct 4 15:31 test.go 编写 index.html\n下面编写一个 html 文件，目的是让其以 WebAssembly 方式加载 test.wasm。\n\u0026lt;html\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;utf-8\u0026quot;/\u0026gt;\r\u0026lt;script src=\u0026quot;wasm_exec.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script\u0026gt;\rconst go = new Go();\rWebAssembly.instantiateStreaming(fetch(\u0026quot;test.wasm\u0026quot;), go.importObject).then((result) =\u0026gt; {\rgo.run(result.instance);\r});\r\u0026lt;/script\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r可以看到这里用到一个 wasm_exec.js 依赖文件，其位于$GOROOT/misc/wasm下，将其拷到当前文件夹下。\n$ cp $GOROOT/misc/wasm/wasm_exec.js . 浏览器运行.wasm\n现在当前目录下共有 4 个文件。\n$ ls -lht ... 14K Oct 4 15:45 wasm_exec.js ... 354B Oct 4 15:43 index.html ... 2.2M Oct 4 15:36 test.wasm ... 76B Oct 4 15:31 test.go 使用goexec（一个执行 go 函数的命令行工具）将当前目录下的资源以 web 方式启动起来并提供服务。\n$ goexec \u0026#39;http.ListenAndServe(\u0026#34;:8080\u0026#34;, http.FileServer(http.Dir(\u0026#34;.\u0026#34;)))\u0026#39; 这样打开浏览器访问http://localhost:8080/，即可以看到在 Console 打印的“Hello Assembly!”。\n以 node.js 方式运行\n位于$GOROOT/misc/wasm/下的go_js_wasm_exec提供以node.js的方式测试及运行.wasm的能力。\n$ GOOS=js GOARCH=wasm go run -exec=\u0026#34;$GOROOT/misc/wasm/go_js_wasm_exec\u0026#34; test.go Hello Assembly! 将$GOROOT/misc/wasm/go_js_wasm_exec添加到PATH环境变量即可以不指定-exec参数的方式直接运行.wasm。\n$ export PATH=\u0026#34;$PATH:$GOROOT/misc/wasm\u0026#34; $ GOOS=js GOARCH=wasm go run test.go Hello Assembly! $ GOOS=js GOARCH=wasm go run test.go ? command-line-arguments [no test files] 2 实现一个简单的计算器 实现一个简单的加法计算器，这样即涉及到 DOM 操作。index.html 加 3 个 input 标签（前两个用来输入数字，最后一个用来显示结果），1 个 button 标签（点击 button 时计算结果并显示）。\nindex.html\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;/\u0026gt; \u0026lt;script src=\u0026#34;wasm_exec.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const go = new Go(); WebAssembly.instantiateStreaming(fetch(\u0026#34;test.wasm\u0026#34;), go.importObject).then((result) =\u0026gt; { go.run(result.instance); }); \u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;input id=\u0026#34;num1\u0026#34; type=\u0026#34;number\u0026#34; /\u0026gt; + \u0026lt;input id=\u0026#34;num2\u0026#34; type=\u0026#34;number\u0026#34; /\u0026gt; = \u0026lt;input id=\u0026#34;rlt\u0026#34; type=\u0026#34;number\u0026#34; readonly=\u0026#34;readonly\u0026#34; /\u0026gt; \u0026lt;button id=\u0026#34;compute\u0026#34;\u0026gt;compute\u0026lt;/button\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; test.go\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall/js\u0026#34; ) func registerCallbackFunc() { cb := js.FuncOf(func(this js.Value, args []js.Value) interface{} { fmt.Println(\u0026#34;button clicked\u0026#34;) num1 := getElementById(\u0026#34;num1\u0026#34;).Get(\u0026#34;value\u0026#34;).String() v1, err := strconv.Atoi(num1) if nil != err { panic(err) } num2 := getElementById(\u0026#34;num2\u0026#34;).Get(\u0026#34;value\u0026#34;).String() v2, err := strconv.Atoi(num2) if nil != err { panic(err) } rlt := v1 + v2 getElementById(\u0026#34;rlt\u0026#34;).Set(\u0026#34;value\u0026#34;, rlt) return nil }) getElementById(\u0026#34;compute\u0026#34;).Call(\u0026#34;addEventListener\u0026#34;, \u0026#34;click\u0026#34;, cb) } func getElementById(id string) js.Value { return js.Global().Get(\u0026#34;document\u0026#34;).Call(\u0026#34;getElementById\u0026#34;, id) } func main() { done := make(chan struct{}, 0) registerCallbackFunc() \u0026lt;-done } 编译及浏览器运行\n$ GOOS=js GOARCH=wasm go build -o test.wasm test.go $ goexec \u0026#39;http.ListenAndServe(\u0026#34;:8080\u0026#34;, http.FileServer(http.Dir(\u0026#34;.\u0026#34;)))\u0026#39; 运行结果如下图所示。\n3 问题与展望 因 Go 是 gc 型语言，所以编译后的单个 wasm 二进制文件需要携带 gc 及运行时，会很大（最小也得\u0026gt;2m）。这对于一个 web 应用还是非常致命的，社区涌现了一些针对优化压缩算法的工具，致力做到最大化压缩，以降低文件大小。还有即是不使用标准的 Go SDK，使用更轻的基础环境，如TinyGo即采用此种思路。\n再一个就是 Go WebAssembly 相关的 API 还不够丰富，期待后续的版本可以丰富一些简单易用的 WebAssembly 包。\n 参考资料\n[1] https://webassembly.org/\n[2] https://developer.mozilla.org/en-US/docs/WebAssembly\n[3] https://github.com/golang/go/wiki/WebAssembly\n ","permalink":"https://olzhy.github.io/posts/golang-webassembly.html","tags":["Golang"],"title":"Golang WebAssembly 初探"},{"categories":["计算机"],"contents":"之前写过一篇Golang Modules，是 Go Module 的入门篇，介绍了 Module 的设计初衷及工作方式。本文结合 Go 1.13 最新 Module 官网文档，进一步梳理 Module 的使用方式。\nModule 是一组相关包的组合，是方便被引用及版本化的单元。自 Go 1.13 起，内置命令已默认支持基于 Module 的依赖及构建方式，自此，设置 GOPATH 已不再成为必须。\n1 GO111MODULE 变量\nGo 1.13 继续使用 GO111MODULE 临时变量。根据该值的不同设置来决定使用 Module 模式还是原有的 GOPATH 模式。\na) GO111MODULE=auto （默认模式）\n当 GO111MODULE 被设置为 auto 或空时，go 命令会判断当前文件夹来决定使用何种模式。若当前文件夹或其父文件夹（或者多级上层文件夹）包含 go.mod 文件时，使用 Module 模式；否则使用 GOPATH 模式。\nb) GO111MODULE=on\nGOPATH 将不起作用，go 命令将不再使用 GOPATH，将以 Module 模式工作。\nc) GO111MODULE=off\n我们将其称作 GOPATH 工作模式。go 命令会在 GOPATH 目录或 vendor 目录寻找依赖。\n注：若开启 Module 模式，则 GOPATH 变量不再对构建依赖起作用，仅用来标识下载的 Module 依赖的存储位置（GOPATH/pkg/mod）或安装命令的位置（GOPATH/bin，若设置 GOBIN，则会覆盖）。\n2 go.mod 文件\n包含 go.mod 文件的文件夹为模块根文件夹，位于该文件夹下的 Go 源码包或者子目录同属该模块，当然，位于该文件夹下的代码子树也可拥有自己的 go.mod 文件，定义一个独立的子模块。\ngo.mod 文件定义了当前模块的路径且列出了其所依赖的模块路径及版本。\n如下 go.mod 定义了 go.mod 所在的路径为 Module 根路径，且引用路径为github.com/olzhy/test。require 语句则声明了该模块依赖的两个包的指定版本（golang.org/x/text v0.3.0与gopkg.in/yaml.v2 v2.1.0）。\nmodule github.com/olzhy/test require ( golang.org/x/text v0.3.0 gopkg.in/yaml.v2 v2.1.0 ) 当然，go.mod 文件亦可指定要替换（replace）或要排除（exclude）的依赖包版本。\n创建 Module 仅须一条命令，如：go mod init github.com/olzhy/test。\n一旦 go.mod 文件存在，go 命令（诸如 go build、go test，甚至 go list）会自动加入新的依赖。\n3 主模块及构建列表\n主模块是包含 go 命令运行所在文件夹的模块。go 命令会依次从当前文件夹、其父文件夹、其父文件夹的父文件夹等递归直至找到 go.mod 文件所在的模块根目录。\ngo.mod 文件通过 require、replace，及 exclude 等语句描述确定的依赖包路径及版本集合。go 命令从主模块的 go.mod 文件的 require 语句可以找到所有依赖模块，而依赖模块所依赖的模块同样会成为当前主模块的依赖，但 go 命令仅会扫描依赖模块 go.mod 文件的 require 语句，replace 及 exclude 语句将被忽略。因此，replace 及 exclude 语句即允许在主模块自己的构建中完全控制。\n提供用于构建的一组包的模块列表叫作构建列表。初始时，构建列表仅有主模块，然后根据模块所需依赖指定版本的模块来更新构建列表，递归直至没有新的模块依赖为止。若存在对某一模块的多个版本的依赖，构建时仅使用最近的版本（根据semver版本排序）。\ngo list 命令提供关于主模块及构建列表的信息。\ngo list -m # 打印主模块引用路径 go list -m -f={{.Dir}} # 打印主模块根目录 go list -m all # 打印构建列表 4 所需模块维护\ngo.mod 文件对程序员或工具均是可读写的。go 命令会自动更新该文件以维护标准的格式及准确的 require 语句。\n任何 go 命令执行时，若发现新的引用路径，会自动将该引用所属模块的最新版本加入 go.mod 文件。因此，在绝大多数开发场景，我们在源文件引用一个新的包，然后运行 go build、go test，甚至 go list，则 go 命令即会自动发现并解决依赖且更新 go.mod 文件。\n运行 go mod tidy 命令会自动加入缺失的模块引用并移除不再使用的模块引用。\n作为在 go.mod 文件维护所需模块的一部分，go 命令会跟踪哪些包被当前模块直接引用，哪些包被间接引用（即被当前模块的依赖模块所引用）。间接引用的包在 go.mod 会被标记“// indirect”注释。间接引用的包一旦在 go.mod 文件被直接引用，则间接引用语句即会被移除。\n使用 go get 命令可以更改所需依赖模块的版本。升级一个模块可能会意味着升级关联的其他模块，降级一个模块也可能意味着降级关联的其它模块。go get 命令会作这些隐式的更新。同样，go.mod 被手动编辑了，使用 go build 或 go list 命令也会隐式的自动作一些关联更新。\n使用-mod 构建标记会对 go.mod 文件的更新或使用提供额外的控制。\n构建时，使用-mod=readonly 标记，则不允许 go 命令以诸如如上情形的方式隐式更新 go.mod 文件，其会在需要更新 go.mod 文件时报错。该项设置可用于 go.mod 不需要更新的场景，诸如用在持续集成或测试系统。例外是，当设置-mod=readonly 时，仍然准许 go get 命令更新 go.mod 文件。而 go mod 命令没有-mod 标记。\n若调用时，开启-mod=vendor，go 命令会假定所有的依赖均已拷贝到 vendor 文件夹下，忽略 go.mod 中的依赖描述。\n5 伪版本号\ngo.mod 及 go 命令一般是使用 semver 标准版本格式来描述模块版本的，所以可以进行版本比较以判断哪个早于哪个，哪个晚于哪个。诸如 v1.2.3 这样的模块版本可以通过对对应的源码仓库的某次修正打一个标签来实现。未打标签的修正可以通过诸如“v0.0.0-yyyymmddhhmmss-abcdefabcdef”（这里的时间为 UTC 提交时间，后缀为提交 hash）的“伪版本号”来引用。这样提交时间可以用来比较版本先后，提交 hash 可以指向某次提交，而前缀（本例中的 v0.0.0-）表示在本次提交前最近的标签版本。\n总结一下，有三种“伪版本号”格式：\na）vX.0.0-yyyymmddhhmmss-abcdefabcdef\n表示在目标提交之前没有带有合适主版本的版本标签。（这是原先的仅有格式，所以一些旧的 go.mod 文件甚至对在打了标签后的提交仍使用这种格式）\nb）vX.Y.Z-pre.0.yyyymmddhhmmss-abcdefabcdef\n表示在目标提交前最近的版本提交为 vX.Y.Z-pre。\nc）vX.Y.(Z+1)-0.yyyymmddhhmmss-abcdefabcdef\n表示在目标提交前最近的版本提交为 vX.Y.Z。\n伪版本号切勿手敲，go 命令会进行自动转换。\n6 模块查询\n支持在 go 命令行或者编辑 go.mod 文件的方式（若发现文件有查询语句，go 命令会将查询结果替换文件中的查询语句）进行模块查询。如 go.mod 文件现在引用 v1.4.0 的github.com/gorilla/mux：\nrequire ( github.com/gorilla/mux v1.4.0 // indirect ) 若想引用比其新一点的版本，可以直接编辑文件：\nrequire ( github.com/gorilla/mux \u0026gt;v1.4.0 // indirect ) 然后运行 go get，文件内容变为：\nrequire ( github.com/gorilla/mux v1.5.0 // indirect ) 查询语句支持采用确定版本号，版本号前缀及指定范围等查询。\na）如获取一个确定的版本，如 v1.6.2\n$ go get github.com/gorilla/mux@v1.6.2 b）按版本前缀获取携带该前缀的最新标签版本\n$ go get github.com/gorilla/mux@v1 // 获取v1前缀的最新版本 $ go get github.com/gorilla/mux@v1.5 // 获取v1.5前缀的最新版本 c）指定范围查询\n$ go get github.com/gorilla/mux@\u0026#39;\u0026gt;v1.5.0\u0026#39; // 获取\u0026gt;v1.5.0的最近版本 d）获取最新版本\nlatest 可用来获取最新标签版本，没有标签即获取仓库最新无标签版本。\n$ go get github.com/gorilla/mux@latest // 获取最新版本 e）获取最新补丁版本\npatch 用来获取与当前版本大小版本号一致的最新补丁版本，若未指定当前版本，则等于 latest。\n$ go get github.com/gorilla/mux@patch // 获取最新版本 原始为 v1.6.0，执行获取 patch 版本后版本为 v1.6.2。\nf）按提交 hash 获取版本\n$ go get github.com/gorilla/mux@e3702bed2 注意，版本选择更喜欢发布版本而非预发布版本，如“\u0026lt;v1.2.3”获取到的是“v1.2.2”，而非“v1.2.3-pre1”，尽管“v1.2.3-pre1”比“v1.2.2”更近。\n综上，如下查询命令均是有效的。\n$ go get github.com/gorilla/mux@latest # go get默认即@latest $ go get github.com/gorilla/mux@v1.6.2 # 指向v1.6.2 $ go get github.com/gorilla/mux@e3702bed2 # 指向v1.6.2 $ go get github.com/gorilla/mux@c856192 # 指向v0.0.0-20180517173623-c85619274f5d $ go get github.com/gorilla/mux@master # 指向master $ go get github.com/gorilla/mux@\u0026#39;\u0026gt;v1.6.2\u0026#39; # 查询v1.6.2以上的版本 7 模块下载及校验\ngo 命令可以根据 GOPROXY 环境变量的设置，从而从 proxy 服务或者从源码服务直接获取模块。GOPROXY 默认设置为“https://proxy.golang.org,direct”，即首先从运行在谷歌的go模块代理寻找，没有的话（HTTP状态码404或410）直接回源到vcs系统下载。若GOPROXY直接设置为“direct”，表示不经过代理，直接从vcs下载。此外还可以对GOPROXY设置一组代理，即输入一组代理URL，按逗号分割。go命令会依序尝试每个代理，仅当一个代理的HTTP状态码返回404或410时才尝试下一个。若proxy列表有“direct”，则其之后的代理不会被遍历到。\nGOPRIVATE 及 GONOPROXY 环境变量可用来设置跳过使用代理的模块。\ngo 命令会对下载的模块进行校验和检查，校验和用来检查指定版本的模块是否发生了改变，以确保可重复构建及进行恶意更新检测。\n与 go.mod 一起，同样位于模块根目录的 go.sum 文件包含了依赖模块的加密校验和信息。\ngo.sum 的每行有三个字段：\n\u0026lt;module\u0026gt; \u0026lt;version\u0026gt;[/go.mod] \u0026lt;hash\u0026gt; 每个依赖模块在 go.sum 文件有两行记录。\n第一行给出了模块版本文件树的 hash 值。第二行在版本后加了“/go.mod”，给出了模块对应该版本 go.mod 的 hash 值。\n校验和检查首先查询当前模块的 go.sum 文件，然后再回到 Go 校验和数据库。校验和数据库可以通过 GOSUMDB 及 GONOSUMDB 环境变量设置。\n若一个下载的模块在 go.sum 文件未出现过且该模块是一个开放访问的模块，go 命令将会从 Go 校验和数据库查询期望的 go.sum 相关行。若下载的代码校验和与查询的校验和不匹配，go 命令会报不匹配错误并退出。注意，若模块已在 go.sum 文件列出，将不会再查数据库。\nGOSUMDB 环境变量用来指定校验和数据库的名称及公共 key 及 URL 两个可选字段。\n诸如：\nGOSUMDB=\u0026quot;sum.golang.org\u0026quot; GOSUMDB=\u0026quot;sum.golang.org+\u0026lt;publickey\u0026gt;\u0026quot; GOSUMDB=\u0026quot;sum.golang.org+\u0026lt;publickey\u0026gt; https://sum.golang.org\u0026quot; GOSUMDB 默认为sum.golang.org，运行在 Google 上的校验和数据库，go 命令知道 sum.golang.org 的公共 key，使用其它数据库需要显式给出公共 key，URL 默认为“https://”加数据库名称。\n若将 GOSUMDB 设置为 off，或使用“go get”时加“-insecure”标签，那么将不使用校验和数据库查询，即接受所有未识别模块且放弃安全保障。对特定模块略过校验的一个更好的方式是使用 GOPRIVATE 或 GONOSUMDB 环境变量。\n“go mod verify”命令用来检查模块缓存的校验和是否与 go.sum 中的记录匹配。\n8 私有模块配置\n由上述可知，go 命令默认从 Go 模块镜像 proxy.golang.org 下载模块，且默认从 Go 校验和数据库 sum.golang.org 校验下载模块。\nGOPRIVATE 环境变量用来控制哪些模块是私有的且不使用模块代理及校验和数据库。GOPRIVATE 变量是一组按逗号分割的匹配模式.如：\nGOPRIVATE=*.corp.example.com,rsc.io/private 对于模块下载及校验，更细粒度的控制方式是结合使用 GONOPROXY 及 GONOSUMDB 环境变量。因这两个变量的配置会覆盖 GOPRIVATE。\n如，一个公司采用如下配置为私有模块提供服务：\nGOPRIVATE=*.corp.example.com GOPROXY=proxy.example.com GONOPROXY=none 这样，即告诉 go 命令及其它工具匹配 corp.example.com 子域名的模块是私有的，而 GONOPROXY 配置了 none，覆盖了 GOPRIVATE，这样即采用 GOPROXY 的设置来下载公共及私有模块。\n 参考资料\n[1] https://golang.org/cmd/go/#hdr-Modulesmodule_versionsand_more\n ","permalink":"https://olzhy.github.io/posts/go1dot13-using-go-modules.html","tags":["Golang"],"title":"Go 1.13 Module使用说明"},{"categories":["计算机"],"contents":"1 二进制整数表示\n使用前缀0b或0B来表示二进制数，如0b0010。\n示例代码：\nnum := 0b0010 fmt.Printf(\u0026#34;binary: %#b, decimal: %d\\n\u0026#34;, num, num) Go 1.13 fmt使用\u0026quot;%#b\u0026quot;将整数格式化为二进制格式，原有的\u0026quot;%b\u0026quot;用来将整数格式化为不带进制前缀的二进制格式。\n2 八进制整数表示\n使用前缀0o或0O来表示八进制数，如0o70，现有的八进制数表示法（首位为0，后面跟八进制数）仍然保留。\n示例代码：\nnum := 0o70 fmt.Printf(\u0026#34;octal: 0o%o, decimal: %d\\n\u0026#34;, num, num) 注意，Go 1.13 fmt使用\u0026quot;0o%o\u0026quot;将整数格式化为八进制格式，原有的\u0026quot;%#o\u0026quot;用来将整数格式化为不带进制前缀的八进制格式。\n3 十六进制浮点数表示\n我们知道，使用前缀0x或0X来表示十六进制数，如0x0a01。Go 1.13引入了使用十六进制表示浮点数的方式，如0x10.a1p+2。\n表示规则：\nhex_float_lit = \u0026#34;0\u0026#34; ( \u0026#34;x\u0026#34; | \u0026#34;X\u0026#34; ) hex_mantissa hex_exponent . hex_mantissa = [ \u0026#34;_\u0026#34; ] hex_digits \u0026#34;.\u0026#34; [ hex_digits ] | [ \u0026#34;_\u0026#34; ] hex_digits | \u0026#34;.\u0026#34; hex_digits . hex_exponent = ( \u0026#34;p\u0026#34; | \u0026#34;P\u0026#34; ) [ \u0026#34;+\u0026#34; | \u0026#34;-\u0026#34; ] decimal_digits . 即前面是一个十六进制数，然后是小数点，然后是p或P，最后面跟十进制表示的指数（表示2^exp）。\n如上例子0x10.a1p+2，表示(0x10 + 0xa1/0x100) * 2^2。\n示例代码：\nnum := 0.1234 fmt.Printf(\u0026#34;floating point: %x, decimal: %f\\n\u0026#34;, num, num) 输出为：\nfloating point: 0x1.f972474538ef3p-04, decimal: 0.123400 注意，Go 1.13 fmt使用\u0026quot;%x\u0026quot;将浮点数格式化为十六进制浮点数格式。\n4 虚数表示\nGo 1.13，末位为i的虚数可以用在任意进制数中。如：\n0o123i // == 0o123 * 1i == 83i 0xabci // == 0xabc * 1i == 2748i 0.i 2.71828i 1.e+0i 6.67428e-11i 1E6i .25i .12345E+5i 0x1p-2i // == 0x1p-2 * 1i == 0.25i 5 数字分割\nGo 1.13，可以使用下划线分割数字。如：\n1000_000_000 3.1415_926 0x0000_0001 注意，两组数字间最多使用一个分割符，且勿将分割符用于数字开头或结尾。\n_42 // an identifier, not an integer literal 42_ // invalid: _ must separate successive digits 4__2 // invalid: only one _ at a time 0_xBadFace // invalid: _ must separate successive digits 6 移位\n之前限定移动位数必须是无符号类型，Go 1.13取消该限制，即有符号类型无须额外类型转换亦可作为移动位数，但该数必须非负。\n如下代码示例之前操作数在用作移动位数时需要转换为无符号类型。\nn := uint(bitSize) // uint cast x := (r \u0026lt;\u0026lt; (64 - n)) \u0026gt;\u0026gt; (64 - n) 现在无须转换。\nvar bitSize int = 2 fmt.Println(1 \u0026lt;\u0026lt; bitSize) // 4 但仍须操作数非负，如上代码将bitSize写作-2即会报错。\npanic: runtime error: negative shift amount goroutine 1 [running]: main.main() /Users/larry/Documents/WORKSPACE/go/test.go:7 +0x11 exit status 2  参考资料\n[1] https://golang.org/ref/spec#Integer_literals\n[2] https://golang.org/ref/spec#Floating-point_literals\n[3] https://golang.org/ref/spec#Imaginary_literals\n[4] https://github.com/golang/proposal/blob/master/design/19113-signed-shift-counts.md\n ","permalink":"https://olzhy.github.io/posts/go1dot13-number-literals.html","tags":["Golang"],"title":"Go 1.13 数字表示、移位及分割"},{"categories":["计算机"],"contents":"Go 1.10，在 Go 1.9 发布半年后如期而至。其主要变化在工具链实现、运行时及库上面。一如既往，该版本秉承 Go 1 兼容性准则。以期所有的 Go 程序如之前一样编译及运行。\n该版本改进了包构建缓存，增加了成功测试结果缓存，在测试时自动进行校验，且准许使用 cgo 在 Go 及 C 程序间直接传递 string 类型的值。\n1 语言方面\n语言基准未有大的变化。\n阐明了未指定类型的常量移位，编译器已允许诸如 x[1.0 \u0026laquo; s]（s 是一个无符号整型变量）的索引表达式。\nmethod expression 语法已放宽到允许任何类型表达式作为接收器；如 struct{io.Reader}.Read 是有效的。\n2 工具方面\n 默认 GOROOT 及 GOTMPDIR 若未设置$GOROOT，之前 go tool 会在工具链编译时使用默认的 GOROOT 设置。现在，在使用默认设置之前，go tool 会从其自己的执行路径推断 GOROOT。这样，即允许二进制文件解压到文件系统的任意位置，且可在未显式设置 GOROOT 的情况下使用。  默认情况下，go tool 会在系统临时文件夹下创建文件夹及文件（如 Unix 的$TMPDIR），若设置了新的变量$GOTMPDIR，即会改为在该文件夹下创建。\n Build \u0026amp; Install go build 命令目前可以基于源文件内容与构建标记及存储在编译包上的元数据来检测出过期的包。修改时间不再会考量。当修改时间产生误导的时候，之前增加-a 标记以强制构建的建议已没有必要：构建目前总会检查包何时需要重新构建。  go build的-asmflags、-gcflags、-gccgoflags、及-ldflags选项目前默认仅应用于命令行直接列出的包。例如，go build -gcflags=-m mypkg会在对 mypkg 包而非其依赖构建时给编译器传入-m 标记。新的更通用的方式-asmflags=pattern=flags仅会将标记应用到匹配了模式的包。例如：go install -ldflags=cmd/gofmt=-X=main.version=1.2.3 cmd/...会对满足cmd/...的包安装所有命令，但仅会对 cmd/gofmt 应用-X 选项。更多详情请参阅go help build。\ngo build 命令目前维护了一块最近构建包的缓存（不同于包安装路径$GOROOT/pkg或$GOPATH/pkg）。缓存应会对未显式安装包或当在源码的不同副本中切换时（如在同一版本控制系统的不同分支上前后切换）起到加速构建的作用。之前添加-i 标记以加速（诸如go build -i或go test -i）的建议已没有必要：有没有-i 都会一样快，更多详情请参阅go help cache。\ngo install 命令目前仅安装直接在命令行列出的包与命令。例如go install cmd/gofmt安装了 gofmt 程序，但没有安装其任何依赖。新的构建缓存使得未来的命令仍可运行的好似安装了依赖一样快。使用新的go install -i标记可以强制安装依赖。\ngo build 的诸多实现细节已对这些改进作了支持。一个新的改变是纯二进制包的引用必须在其被引用源码中声明准确的引用块。这样，当使用纯二进制包链接一个程序的时候以让这些引用是可用的。更多详情请查阅go help filetype。\n Test go test 目前缓存了测试结果：若 test 可执行且命令行匹配了之前的一次执行，且检测到文件及环境变量未发生改变，那么 go test 将会打印之前的测试结果（时间耗费字段将会被替换为字符串“(cached)”）。Test 缓存仅应用于成功的测试结果；仅应用在显式列出包的 go test 命令；仅应用在使用-cpu、-list、-parallel、-run、-short 及-v 的测试标记子集的命令行。理想的略过测试缓存的方式是使用-count=1。  go test 命令目前会对将要测试的包自动运行 go vet，以在运行测试前识别重要问题。此类问题会造成构建错误及阻止测试执行。使用 go test -vet=off 可以关闭该检测。\ngo test -coverpkg 标记目前将其参数解释为一个按冒号分割的模式列表以匹配每个测试的依赖，并非作为一个包列表以重新加载。如 go test -coverpkg=all 目前是一个对测试包及其所有依赖开启覆盖率测试的有趣方式。同样，go test -coverprofile 选项目前也支持运行多组测试。\n对于超时的错误情形，测试更可能在退出前写入画像。\ngo test 目前会从给定的二进制执行中将标准错误并入标准输出然后写到 go test 的标准输出中。而之前 go test 仅会在多数时间应用该合并。\n目前，当并行测试停顿或继续的时候，go test -v输出会包括 PAUSE 及 CONT 状态标记行。\n新的go test -failfast标记在测试失败时将不会运行剩余的测试。（注：以并行方式运行的测试在测试运行失败时允许测试执行完成）\n最后，新的go test -json标记通过新的go tool test2json命令过滤测试输出以生成机器可读的 JSON 格式的测试执行描述。这样会允许在 IDE 及其它工具中创建更多丰富的说明信息。\n更多详情请参阅go help test及test2json 文档。\n Doc go doc 工具目前增加了对于类型 T 的[]T 或[]T 等 slice 结果返回函数的显示（类似于现有的对单个 T 或T 返回函数的显示机制）。例如：  $ go doc mail.Address type Address struct { Name string // Proper name; may be empty. Address string // user@domain } Address represents a single mail address. An address such as \u0026#34;Barry Gibbs \u0026#34; is represented as Address{Name: \u0026#34;Barry Gibbs\u0026#34;, Address: \u0026#34;bg@example.com\u0026#34;}. func ParseAddress(address string) (*Address, error) func ParseAddressList(list string) ([]*Address, error) func (a *Address) String() string 之前 ParseAddressList 函数仅会在包预览时显示（go doc mail）。\n Pprof 通过 runtime/pprof 包生成的阻塞及互斥的画像目前包含了符号信息，这些符号信息可以在没有产生画像的二进制中使用 go tool pprof 查看。（所有其它画像类型已在 Go 1.9 更新为包含符号信息）  go tool pprof 画像可视化工具已更新至 git 版本 9e20b5b（github.com/google/pprof），该版本包含一个更新了的 web 接口。\n  Vet go vet 命令目前在检查包（甚至对使用 cgo 或 vendored 方式引入的包）的时候总是使用完整的最新的类型信息。结果报告应该会更准确一些。注意仅 go vet 有权访问这些信息，应避免使用 go tool vet。（截至 Go 1.9，go vet 已提供类似 go tool vet 所有标记的访问）\n  Gofmt Go 源码的默认格式化有两处小的细节上的改动。第一个，对于三索引 slice 表达式之前会被格式化为 x[i+1 : j:k]，而目前会被使用定长空格格式化为 x[i+1 : j : k]。第二个，写作单行的单个方法接口（有时会在类型断言时使用）将不再格式化为多行。\n  注意这类针对 gfmt 的小的更新将会不定期进行。一般讲，检查源码的构建系统应匹配指定版本的 gofmt 的输出。\n 编译器工具链 编译器对代码生成的性能上作了诸多改进（在所支持的体系结构上广泛着手）。  记录于二进制的 DWARF 调试信息有几项改进：常量值目前会被记录；行号信息会更精确，使得源码级调试更好一些；并且目前每个包会呈现其自己的 DWARF 编译单元。\n各种构建模式已移植到更多的系统。特别是 c-shared 目前工作在 linux/ppc64le、windows/386，及 windows/amd64 上；pie 目前工作在 darwin/amd64，且同样在所有系统上强制使用外部链接；plugin 目前工作在 linux/ppc64le 及 darwin/amd64 上。\nlinux/ppc64le 端目前需要对使用 cgo 的任何程序（甚至被标准库使用时）使用外部链接。\n3 运行时\n嵌套调用 LockOSThread 及 UnlockOSThread 的行为已发生改变。这些函数用来控制是否一个 goroutine 被锁定在一个操作系统线程上，以让该 goroutine 仅在那个线程上运行，且那个线程仅运行该 goroutine。之前在一行调用 LockOSThread 多次相当于调用一次，而仅调用一次 UnlockOSThread 即可解锁线程。现在调用是嵌套的：调用 LockOSThread 多次，须调用 UnlockOSThread 同样次数才能解锁。没有嵌套调用的现有代码仍可正确运行。多数使用这些函数的公开 Go 源码被分入了第二类。\n因通常使用 LockOSThread 与 UnlockOSThread 来允许 Go 源码可靠的修改本地线程状态（如 Linux 或 Plan 9 命名空间）。运行时目前认为被锁的线程不适于重用或用来创建新线程。 除非包装器自己出现了错误，堆栈信息不再包含隐式的包装函数（之前被标记为）。 结果，传给诸如 Caller 函数的跳过次数目前应匹配所写代码的结构，而非依赖优化决策及实现细节。 垃圾收集器已减少对分配延迟的影响。当运行时，其目前使用更少部分的总体 CPU，但可能会运行更长的时间。被垃圾收集器使用的总体 CPU 没有发生显著改变。\nGOROOT 函数目前实际上是在调用程序编译后，默认使用 GOROOT 或 GOROOT_FINAL。而之前，是在编译调用程序的工具链编译后，使用 GOROOT 或 GOROOT_FINAL。\nGOMAXPROCS 设置目前已无上限。（在 Go 1.9，上限为 1024）\n4 性能\n一如既往，该版本变化较广，较难对性能作精确陈述。因垃圾收集器加速、更好的代码生成及核心库的优化，多数程序应会运行的快一些。\n5 垃圾收集器\n当垃圾收集器活跃时，多数程序应会感受到更低的分配延迟及总体的性能提升。\n6 核心库\n标准库的所有改动都比较小，bytes 包及 net/url 包的变化可能需要更新现有的程序。\n 参考资料\n[1] https://golang.org/doc/go1.10\n ","permalink":"https://olzhy.github.io/posts/go1dot10-release-notes.html","tags":["Golang"],"title":"Go 1.10 Release Notes 要点整理"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个链表及一个值x，请以x分割链表以让小于x的节点出现在大于等于x的节点之前。\n您须保证分割后的两部分仍保持原始链表的节点顺序。\n例子：\n输入：head = 1-\u0026gt;4-\u0026gt;3-\u0026gt;2-\u0026gt;5-\u0026gt;2, x = 3 输出：1-\u0026gt;2-\u0026gt;2-\u0026gt;4-\u0026gt;3-\u0026gt;5 题目出处：LeetCode\n2 解决思路\n声明两个链表变量left与right，遍历原始链表，判断节点Val，若小于x，追加该节点到left尾部，否则追加该节点到right尾部。\n最后，将left与right两个链表拼接起来返回即可。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc partition(head *ListNode, x int) *ListNode { if nil == head || nil == head.Next { return head } var left, right, p, q *ListNode for ; nil != head; head = head.Next { if head.Val \u0026lt; x { if nil == left { left = \u0026amp;ListNode{Val: head.Val} p = left } else { p.Next = \u0026amp;ListNode{Val: head.Val} p = p.Next } } else { if nil == right { right = \u0026amp;ListNode{Val: head.Val} q = right } else { q.Next = \u0026amp;ListNode{Val: head.Val} q = q.Next } } } if nil == left { return right } if nil != right { p.Next = right } return left } ","permalink":"https://olzhy.github.io/posts/leetcode-partition-list.html","tags":["Golang","算法"],"title":"LeetCode 86 分割链表"},{"categories":["计算机"],"contents":"1 题目描述\n给您一颗二叉树，求出现次数最多的子树和。\n一个节点的子树和的定义：根为该节点的所有子树节点值的总和（包含该根节点本身）。\n所以，求一下出现次数最多的子树和是多少？若出现次数最多的子树和不唯一，请以任意顺序返回这些子树和的全部。\n例子1：\n输入：\n 5 / \\ 2 -3 输出：返回[2, -3, 4]，因子树和的所有值仅出现一次，返回全部。\n例子2：\n输入：\n 5 / \\ 2 -5 输出：返回[2]，因2出现两次，而-5仅出现一次。\n注：\n您可以假定任意子树的子树和在32位有符号整型所表示范围之内。\n题目出处：LeetCode\n2 解决思路\n计算所有节点的子树和可以采用后序遍历（如下代码treeSum函数），递归完成后，得到一个key为子树和value为出现次数的map。\n然后遍历该map，将最多出现次数的子树和记录，返回即可。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TreeNode struct { Val int Left *TreeNode Right *TreeNode } func treeSum(root *TreeNode, frequents map[int]int) int { if nil == root { return 0 } sum := root.Val + treeSum(root.Left, frequents) + treeSum(root.Right, frequents) if v, ok := frequents[sum]; ok { frequents[sum] = v + 1 } else { frequents[sum] = 1 } return sum } func findFrequentTreeSum(root *TreeNode) []int { if nil == root { return []int{} } frequents := make(map[int]int) treeSum(root, frequents) vMax := 1 maxMap := make(map[int][]int) for k, v := range frequents { if v \u0026lt; vMax { continue } if v \u0026gt; vMax { vMax = v } maxMap[vMax] = append(maxMap[vMax], k) } return maxMap[vMax] } ","permalink":"https://olzhy.github.io/posts/leetcode-most-frequent-subtree-sum.html","tags":["Golang","算法"],"title":"LeetCode 508 高频子树和"},{"categories":["计算机"],"contents":"1 题目描述\n创建一个基于时间的“键-值”存储类TimeMap，其支持两类操作：\n  a）set(string key, string value, int timestamp) 与timestamp一起，存储key, value。\n  b）get(string key, int timestamp)\n  返回一个之前设置的值，如set(key, value, timestamp_prev)，满足timestamp_prev \u0026lt;= timestamp；\n  若满足情况的值有多个，返回对应最大的timestamp_prev；\n  若没有满足情况的值，返回一个空字符串\u0026quot;\u0026quot;。\n    例子1：\n输入： inputs = [\u0026quot;TimeMap\u0026quot;,\u0026quot;set\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;set\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;get\u0026quot;], inputs = [[],[\u0026quot;foo\u0026quot;,\u0026quot;bar\u0026quot;,1],[\u0026quot;foo\u0026quot;,1],[\u0026quot;foo\u0026quot;,3],[\u0026quot;foo\u0026quot;,\u0026quot;bar2\u0026quot;,4],[\u0026quot;foo\u0026quot;,4],[\u0026quot;foo\u0026quot;,5]] 输出： [null,null,\u0026quot;bar\u0026quot;,\u0026quot;bar\u0026quot;,null,\u0026quot;bar2\u0026quot;,\u0026quot;bar2\u0026quot;] 释义：\nTimeMap kv; kv.set(\u0026quot;foo\u0026quot;, \u0026quot;bar\u0026quot;, 1); // 与timestamp = 1一起，存储键\u0026quot;foo\u0026quot;，值\u0026quot;bar\u0026quot; kv.get(\u0026quot;foo\u0026quot;, 1); // 输出\u0026quot;bar\u0026quot; kv.get(\u0026quot;foo\u0026quot;, 3); // 输出\u0026quot;bar\u0026quot;，因键\u0026quot;foo\u0026quot;在timestamp 3及timestamp 2没有值，仅有的值是在timestamp 1的\u0026quot;bar\u0026quot; kv.set(\u0026quot;foo\u0026quot;, \u0026quot;bar2\u0026quot;, 4); kv.get(\u0026quot;foo\u0026quot;, 4); // 输出\u0026quot;bar2\u0026quot; kv.get(\u0026quot;foo\u0026quot;, 5); // 输出\u0026quot;bar2\u0026quot; 例子2：\n输入： inputs = [\u0026quot;TimeMap\u0026quot;,\u0026quot;set\u0026quot;,\u0026quot;set\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;get\u0026quot;,\u0026quot;get\u0026quot;], inputs = [[],[\u0026quot;love\u0026quot;,\u0026quot;high\u0026quot;,10],[\u0026quot;love\u0026quot;,\u0026quot;low\u0026quot;,20],[\u0026quot;love\u0026quot;,5],[\u0026quot;love\u0026quot;,10],[\u0026quot;love\u0026quot;,15],[\u0026quot;love\u0026quot;,20],[\u0026quot;love\u0026quot;,25]] 输出： [null,null,null,\u0026quot;\u0026quot;,\u0026quot;high\u0026quot;,\u0026quot;high\u0026quot;,\u0026quot;low\u0026quot;,\u0026quot;low\u0026quot;] 注：\na）所有的键值字符串均为小写字母；\nb）所有的键值字符串长度位于区间[1, 100]；\nc）对于所有TimeMap.set操作的timestamp是严格递增的；\nd）1 \u0026lt;= timestamp \u0026lt;= 10^7；\ne）TimeMap.set及TimeMap.get函数在每个测试用例将被调用120000次。\n题目出处：LeetCode\n2 解决思路\n使用两个map，一个是timestamps，key存键，value是一个slice，存时间戳（递增）。\n另一个map是values，key是时间戳，value存值。\n这样，对于给定key与timestamp进行Get时，先遍历第一个map找到满足条件的时间戳，然后用第二个map直接读出值即可。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TimeMap struct { timestamps map[string][]int values map[int]string } func Constructor() TimeMap { return TimeMap{make(map[string][]int), make(map[int]string)} } func (this *TimeMap) Set(key string, value string, timestamp int) { this.timestamps[key] = append(this.timestamps[key], timestamp) this.values[timestamp] = value } func (this *TimeMap) Get(key string, timestamp int) string { if timestamps, ok := this.timestamps[key]; ok { for i := len(timestamps) - 1; i \u0026gt;= 0; i-- { if timestamp \u0026gt;= timestamps[i] { return this.values[timestamps[i]] } } } return \u0026#34;\u0026#34; } ","permalink":"https://olzhy.github.io/posts/leetcode-time-based-key-value-store.html","tags":["Golang","算法"],"title":"LeetCode 981 基于时间的“键-值”存储"},{"categories":["计算机"],"contents":"1 题目描述\n假定一个以升序排好的数组在您预先未知的某个支点被旋转了（如：[0,0,1,2,2,5,6]可能变成了[2,5,6,0,0,1,2]）。\n给您一个target值来搜索，若在数组中找到了，请返回true，否则返回false。\n例子1：\n输入：nums = [2,5,6,0,0,1,2], target = 0\n输出：true\n例子2：\n输入：nums = [2,5,6,0,0,1,2], target = 3\n输出：false\n注：\na）该题是“在旋转的有序数组搜索”的一个变种，nums可能包含重复的数。\nb）其会影响运行时复杂度吗，为什么？\n题目出处：LeetCode\n2 解决思路\n整体还是采用二分搜索，begin，end初始分别标识第一个及最后一个数。\n当begin\u0026lt;=end时，按如下步骤循环：\na）每次循环时有一个额外的处理：若第一个数与最后一个数相等，则先trim掉头部这些相等的数；\nb）计算mid，若mid对应的数与target相等，直接返回true；\nc）若整个数组是升序的，采用常规二分搜索计算；\nd）若是旋转过的，说明数组的组成有两部分，较大的前半部分（升序），较小的后半部分（升序）；这样需要分开判断target与mid位置的数的大小，同时还需判断是mid处在前半部分，还是后半部分来决定怎样折半。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc search(nums []int, target int) bool { begin, end := 0, len(nums)-1 for begin \u0026lt;= end { // trim begin nums of begin == end  if nums[begin] == nums[end] { if target == nums[begin] { return true } for begin \u0026lt; end \u0026amp;\u0026amp; nums[begin] == nums[end] { begin++ continue } } mid := (begin + end) / 2 if target == nums[mid] { return true } // all is asending  if nums[begin] \u0026lt; nums[end] { if target \u0026gt; nums[mid] { begin = mid + 1 } else { end = mid - 1 } continue } // rotated asending  if target \u0026gt; nums[mid] { if nums[mid] \u0026gt;= nums[begin] { begin = mid + 1 } else { if target \u0026gt;= nums[begin] { end = mid - 1 } else { begin = mid + 1 } } } else { if nums[mid] \u0026gt;= nums[begin] { if target \u0026gt;= nums[begin] { end = mid - 1 } else { begin = mid + 1 } } else { end = mid - 1 } } } return false } ","permalink":"https://olzhy.github.io/posts/search-in-rotated-sorted-array-ii.html","tags":["Golang","算法"],"title":"LeetCode 81 在旋转的排序数组搜索 II"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，返回以层次序遍历的节点值（从左到右，遍历完一层遍历下一层）。\n例子：\n 3 / \\ 9 20 / \\ 15 7 输入：\n[3,9,20,null,null,15,7] 输出：\n[[3],[9,20],[15,7]] 题目出处：LeetCode\n2 解决思路\n初始时将根节点放入队列，然后只要队列不为空，即重复如下步骤：\n将队列内所有节点拷贝出来，然后将队列清空。然后依次遍历该拷贝队列内的所有节点，对于每个节点，记录其节点值，然后先后判断该节点的左右子树是否为空，若左子树非空，则将左子树放入队列末尾，若右子树非空，则将右子树放入队列末尾。\n重复如上步骤，当队列为空时，即记录了每一层节点的值。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TreeNode struct { Val int Left *TreeNode Right *TreeNode } func levelOrder(root *TreeNode) [][]int { if nil == root { return [][]int{} } var vals [][]int nodes := []*TreeNode{root} for len(nodes) \u0026gt; 0 { currLevel := []int{} copy := nodes[:] nodes = []*TreeNode{} for _, node := range copy { currLevel = append(currLevel, node.Val) if nil != node.Left { nodes = append(nodes, node.Left) } if nil != node.Right { nodes = append(nodes, node.Right) } } vals = append(vals, currLevel) } return vals } ","permalink":"https://olzhy.github.io/posts/leetcode-binary-tree-level-order-traversal.html","tags":["Golang","算法"],"title":"LeetCode 102 二叉树层次遍历"},{"categories":["计算机"],"contents":"1 题目描述\n比较两个版本号，version1与version2。 若version1 \u0026gt; version2返回1，若version1 \u0026lt; version2返回-1，相等返回0。 您可以假设版本号非空，并且只包含数字和“.”字符。 “.”字符不代表小数点，而是用于分隔数字序列。\n例如，2.5不是“两个半”，也不是“差一半到三”，而是第2版中的第5个小版本。 你可以假设版本号的每一级的默认修订版号为0。例如，版本号3.4的第1级（大版本）和第2级（小版本）修订号分别为3和4。其第3级和第4级修订号均为0。\n例子1：\n输入：version1 = \u0026ldquo;0.1\u0026rdquo;, version2 = \u0026ldquo;1.1\u0026rdquo;\n输出：-1\n例子2：\n输入：version1 = \u0026ldquo;1.0.1\u0026rdquo;, version2 = \u0026ldquo;1\u0026rdquo;\n输出：1\n例子3：\n输入：version1 = \u0026ldquo;7.5.2.4\u0026rdquo;, version2 = \u0026ldquo;7.5.3\u0026rdquo;\n输出：-1\n例子4：\n输入：version1 = \u0026ldquo;1.01\u0026rdquo;, version2 = \u0026ldquo;1.001\u0026rdquo;\n输出：0\n释义：忽略0前缀，“01”与“001”均是1。\n例子5：\n输入：version1 = \u0026ldquo;1.0\u0026rdquo;, version2 = \u0026ldquo;1.0.0\u0026rdquo;\n输出：0\n释义：第1个版本号没有第3位，表示第3位默认为0。\n题目出处：LeetCode\n2 解决思路\n思路比较简单：用i，j两个指针分别指向version1及version2的起始位置。\ni，j同时向右移动，找到一个以“.”分割的小版本则比较是否相等，不相等则跳出并返回结果。相等则i，j继续向右移动，寻找下一个小版本，重复如上比较步骤，直至跳出或两个版号均遍历到最后。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc compareVersion(version1 string, version2 string) int { i, j := 0, 0 v1, v2 := 0, 0 for i \u0026lt; len(version1) || j \u0026lt; len(version2) { for ; i \u0026lt; len(version1); i++ { if \u0026#39;.\u0026#39; == version1[i] { i++ break } if 0 == v1 \u0026amp;\u0026amp; \u0026#39;0\u0026#39; == version1[i] { continue } v1 = 10*v1 + int(version1[i]-\u0026#39;0\u0026#39;) } for ; j \u0026lt; len(version2); j++ { if \u0026#39;.\u0026#39; == version2[j] { j++ break } if 0 == v2 \u0026amp;\u0026amp; \u0026#39;0\u0026#39; == version2[j] { continue } v2 = 10*v2 + int(version2[j]-\u0026#39;0\u0026#39;) } if v1 != v2 { break } v1, v2 = 0, 0 } if v1 \u0026gt; v2 { return 1 } if v1 \u0026lt; v2 { return -1 } return 0 } ","permalink":"https://olzhy.github.io/posts/leetcode-compare-version-numbers.html","tags":["Golang","算法"],"title":"LeetCode 165 版本号比对"},{"categories":["计算机"],"contents":"1 题目描述\n假设您是一个老练的盗贼，计划沿着街道进行打家劫舍。该街道每家都存着一定数额的钱，而阻止您进行盗窃的唯一屏障是相邻两家的防盗系统是连接的，若相邻两家在同一晚都发生了盗窃案，该系统会自动通知到警察。\n现在给定一个非负整数数组，代表该街道沿线每家的金钱数额，请计算在不触发系统通知警察的情况下，您今晚能盗窃的最大金钱数额。\n例子1：\n输入：[1,2,3,1]\n输出：4\n释义：先盗第1家，盗窃金钱1，然后再盗第3家，盗窃金钱3，总数为1 + 3 = 4。\n例子2：\n输入：[2,7,9,3,1]\n输出：12\n释义：分别去盗第1、3、5家，盗窃总金额为2 + 9 + 1 = 12。\n题目出处：LeetCode\n2 解决思路\n逆向思考，先站在最后一家门口算一算，盗与不盗的最大利益。\n这家盗的最大金额为：截至到上上家盗取的最大金额 + 这家的金额\n这家不盗的最大金额为：截至到上一家盗取的最大金额\n以此类推，直至递归到第1家，或第0家（空）。\n因递归时用到的很多子计算是重复的，我们使用table存取截至到第i家盗取的最大金额，首次计算后写入，再次需要时直接返回，实现加速。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc recusiveRob(nums []int, len int, table map[int]int) int { if 0 == len { return 0 } if 1 == len { return nums[0] } if v, ok := table[len]; ok { return v } max := recusiveRob(nums, len-2, table) + nums[len-1] pre := recusiveRob(nums, len-1, table) if pre \u0026gt; max { max = pre } table[len] = max return max } func rob(nums []int) int { table := make(map[int]int) return recusiveRob(nums, len(nums), table) } ","permalink":"https://olzhy.github.io/posts/leetcode-house-robber.html","tags":["Golang","算法"],"title":"LeetCode 198 入室抢劫者"},{"categories":["计算机"],"contents":"1 题目描述\n给定一组区间，合并所有重叠的区间。\n例子1：\n输入：\n[[1,3],[2,6],[8,10],[15,18]] 输出：\n[[1,6],[8,10],[15,18]] 释义：\n[1,3]与[2,6]重叠，合并为[1,6] 例子2：\n输入：\n[[1,4],[4,5]] 输出：\n[[1,5]] 释义：\n[1,4]与[4,5]重叠 题目出处：LeetCode\n2 解决思路\n对于两个坐标x[0,1]与y[0,1]，若最小的右边界大于等于最大的左边界则说明重叠，即若min(x[1], y[1]) \u0026gt;= max(x[0], y[0])，则重叠。 整体的比较步骤为：\na）第1个区间与其后2\u0026hellip;n个区间比较，合并重叠的部分，并更新到第1个区间；\nb）第2个区间与其后3\u0026hellip;n个区间比较，合并重叠的部分，并更新到第2个区间；\n\u0026hellip;\ni）第i个区间与其后i+1\u0026hellip;n个区间比较，合并重叠的部分，并更新到第i个区间；\n\u0026hellip;\n一般地，若在i）步没有找到合并的部分，则进入i+1步，否则重复i）步。 这样，遍历到最后即完成合并。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc min(x, y int) int { if x \u0026lt; y { return x } return y } func max(x, y int) int { if x \u0026gt; y { return x } return y } func merge(intervals [][]int) [][]int { i, j := 0, 0 for i \u0026lt; len(intervals) { merged := false for j = i + 1; j \u0026lt; len(intervals); { x, y := intervals[i], intervals[j] if min(x[1], y[1]) \u0026gt;= max(x[0], y[0]) { merged = true // fix intervals[i]  intervals[i][0], intervals[i][1] = min(x[0], y[0]), max(x[1], y[1]) // remove intervals[j]  intervals[j] = intervals[len(intervals)-1] intervals = intervals[:len(intervals)-1] continue } j++ } if merged { continue } i++ } return intervals } ","permalink":"https://olzhy.github.io/posts/leetcode-merge-intervals.html","tags":["Golang","算法"],"title":"LeetCode 56 合并区间"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个已按升序排好的整数数组nums，对于一个目标值target，寻找其在数组中的起始位置及结束位置。\n您算法的运行时时间复杂度须满足O(log n)。\n若target不存在，返回[-1, -1]。\n例子1：\n输入：nums = [5,7,7,8,8,10], target = 8\n输出：[3,4]\n例子2：\n输入：nums = [5,7,7,8,8,10], target = 6\n输出：[-1,-1]\n题目出处：LeetCode\n2 解决思路\n首先使用折半查找，直至target出现，然后向前找，找到首次出现位置，再向后找，找到最后一次出现位置，然后返回结果。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc searchRange(nums []int, target int) []int { start, end := 0, len(nums)-1 for start \u0026lt;= end { mid := (start + end) / 2 if target == nums[mid] { r := make([]int, 2) i := mid // find first position  for i \u0026gt;= 0 \u0026amp;\u0026amp; target == nums[i] { i-- } i++ r[0] = i // find last position  for i \u0026lt;= len(nums)-1 \u0026amp;\u0026amp; target == nums[i] { i++ } r[1] = i - 1 return r } if target \u0026gt; nums[mid] { start = mid + 1 continue } end = mid - 1 } return []int{-1, -1} } ","permalink":"https://olzhy.github.io/posts/leetcode-find-first-and-last-position-of-element-in-sorted-array.html","tags":["Golang","算法"],"title":"LeetCode 34 在有序数组寻找元素的出现范围"},{"categories":["计算机"],"contents":"1 题目描述\n设计一个简单的推特版本。支持用户发推，支持用户关注或取消关注其他用户，且用户可以在动态里看到最近的10条推文。\n您的设计应支持如下几个方法：\n  a）postTweet(userId, tweetId)：发表新推文；\n  b）getNewsFeed(userId)：在用户动态里展示最近的10条推文id，动态里的每条推文须是用户自己发的或是其关注者发的，推文须按时间由近及远排序；\n  c）follow(followerId, followeeId)：关注；\n  d）unfollow(followerId, followeeId)：取消关注。\n  例子：\nTwitter twitter = new Twitter();\r// User 1 posts a new tweet (id = 5).\rtwitter.postTweet(1, 5);\r// User 1's news feed should return a list with 1 tweet id -\u0026gt; [5].\rtwitter.getNewsFeed(1);\r// User 1 follows user 2.\rtwitter.follow(1, 2);\r// User 2 posts a new tweet (id = 6).\rtwitter.postTweet(2, 6);\r// User 1's news feed should return a list with 2 tweet ids -\u0026gt; [6, 5].\r// Tweet id 6 should precede tweet id 5 because it is posted after tweet id 5.\rtwitter.getNewsFeed(1);\r// User 1 unfollows user 2.\rtwitter.unfollow(1, 2);\r// User 1's news feed should return a list with 1 tweet id -\u0026gt; [5],\r// since user 1 is no longer following user 2.\rtwitter.getNewsFeed(1);\r题目出处：LeetCode\n2 解决思路\n动态的实现一般使用“拉模式”或者“推模式”，即用户可以看到的动态可以采用查询的时候直接计算（拉）也可以在用户的关注者发推的时候直接“推”到用户的动态列表。\n本文使用“推模式”实现，如下是用到的几个数据结构：\n  a）tweets用来存放用户发表的推文；\n  b）feeds用来存放每个用户可以看到的动态；\n  c）fans用来存放用户的粉丝（关注者）列表。\n  接下来看一下几个方法的实现逻辑：\nPostTweet：当用户发送一条推文时，tweets存一下该推文的id与时间，feeds把该动态append到末尾；\nGetNewsFeed：从末尾开始遍历feeds，返回最近的10条推文id；\nFollow：有用户a关注用户b，则把a放入b的fans列表，且把b的tweets推文并入a的feeds，因合并的两部分均是按时间升序排列的数组，所以避免使用常规排序算法，使用自写的merge函数可以加速合并；\nUnfollow：用用户a取消关注b，则将a从b的fans列表移除，还要从a的feeds中移除b的tweets。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype Twitter struct { tweets map[int][]tweet feeds map[int][]tweet fans map[int][]int } type tweet struct { id int time time.Time } func Constructor() Twitter { tweets := make(map[int][]tweet) feeds := make(map[int][]tweet) fans := make(map[int][]int) return Twitter{tweets, feeds, fans} } func (this *Twitter) PostTweet(userId int, tweetId int) { time.Sleep(time.Nanosecond) now := time.Now() newTweet := tweet{tweetId, now} this.tweets[userId] = append(this.tweets[userId], newTweet) for _, followerId := range this.fans[userId] { this.feeds[followerId] = append(this.feeds[followerId], newTweet) } this.feeds[userId] = append(this.feeds[userId], tweet{tweetId, now}) } func (this *Twitter) GetNewsFeed(userId int) []int { var feedIds []int feeds := this.feeds[userId] count := 0 for i := len(feeds) - 1; i \u0026gt;= 0; i-- { if count \u0026gt;= 10 { break } feedIds = append(feedIds, feeds[i].id) count++ } return feedIds } func (this *Twitter) Follow(followerId int, followeeId int) { if followerId == followeeId { return } found := false for _, item := range this.fans[followeeId] { if item == followerId { found = true break } } if !found { this.fans[followeeId] = append(this.fans[followeeId], followerId) this.feeds[followerId] = merge(this.feeds[followerId], this.tweets[followeeId]) } } func merge(left, right []tweet) []tweet { var r []tweet if 0 == len(left) || 0 == len(right) { return append(left, right...) } i, j := 0, 0 for i \u0026lt; len(left) \u0026amp;\u0026amp; j \u0026lt; len(right) { for i \u0026lt; len(left) \u0026amp;\u0026amp; left[i].time.Before(right[j].time) { r = append(r, left[i]) i++ } for j \u0026lt; len(right) \u0026amp;\u0026amp; i \u0026lt; len(left) \u0026amp;\u0026amp; right[j].time.Before(left[i].time) { r = append(r, right[j]) j++ } } for i \u0026lt; len(left) { r = append(r, left[i]) i++ } for j \u0026lt; len(right) { r = append(r, right[j]) j++ } return r } func (this *Twitter) Unfollow(followerId int, followeeId int) { if followerId == followeeId { return } for i := 0; i \u0026lt; len(this.fans[followeeId]); i++ { item := this.fans[followeeId][i] if item == followerId { this.fans[followeeId] = append(this.fans[followeeId][:i], this.fans[followeeId][i+1:]...) for _, tweet := range this.tweets[followeeId] { for i, item := range this.feeds[followerId] { if item.id == tweet.id { this.feeds[followerId] = append(this.feeds[followerId][:i], this.feeds[followerId][i+1:]...) break } } } break } } } ","permalink":"https://olzhy.github.io/posts/leetcode-design-twitter.html","tags":["Golang","算法"],"title":"LeetCode 355 设计推特"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，判断其是否为一个完全二叉树。\n来自Wikipedia的完全二叉树定义：\n在一个完全二叉树中，除了最后一层可能未被完全填充外，其它所有层均被完全填充，且最后一层的节点尽可能靠左。\n最后一层h的节点数介于区间[1, 2^h]。\n注：节点数介于[1, 100]。\n例子1：\n 1 / \\ 2 3 / \\ / 4 5 6 输入：[1,2,3,4,5,6]\n输出：true\n例子2：\n 1 / \\ 2 3 / \\ \\ 4 5 7 输入：[1,2,3,4,5,null,7]\n输出：false\n题目出处：LeetCode\n2 解决思路\n给节点编号，使用层次遍历方式从编号为1的根节点开始遍历二叉树。\n针对每次遍历，判断上一个兄弟节点的编号与当前编号是否连续，若不连续则说明破坏了完全二叉树的规则，返回false；\n若遍历到最后一个节点仍未发现破坏完全二叉树规则的情况，则返回true。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TreeNode struct { Val int Left *TreeNode Right *TreeNode } type withNo struct { *TreeNode No int } func isCompleteTree(root *TreeNode) bool { var nodes []withNo preNo := 0 no := 1 nodes = append(nodes, withNo{root, no}) for len(nodes) \u0026gt; 0 { node := nodes[0] if preNo+1 != node.No { return false } nodes = nodes[1:] no++ if nil != node.Left { nodes = append(nodes, withNo{node.Left, no}) } no++ if nil != node.Right { nodes = append(nodes, withNo{node.Right, no}) } preNo = node.No } return true } ","permalink":"https://olzhy.github.io/posts/leetcode-check-completeness-of-a-binary-tree.html","tags":["Golang","算法"],"title":"LeetCode 958 检查二叉树的完整性"},{"categories":["计算机"],"contents":"1 题目描述\n假定一个按升序排好的数组在您预先不可知的某个支点被旋转了。如[0,1,2,4,5,6,7]被旋转为了[4,5,6,7,0,1,2])。 给您一个目标值来搜索，若在数组中找到了，返回其标号，否则返回-1。您可以假定该数组中元素没有重复。您的运行时复杂度须为O(log n)。\n例子1：\n输入：nums = [4,5,6,7,0,1,2], target = 0\n输出：4\n例子2：\n输入：nums = [4,5,6,7,0,1,2], target = 3\n输出：-1\n题目出处：LeetCode\n2 解决思路\n总体思路还是折半查找，判断数组的两种情况：\n  1）标准升序数组（头值小于等于尾值），直接折半查找；\n  2）被旋转的升序数组（头值大于尾值），判断的情形可能会多一点；\n  2.1）若目标值大于等于头值；\n  2.1.1）若mid值比头值大且目标值大于mid值，去后半部分查找；\n  2.1.1）若mid值比头值小，去前半部分查找；\n    2.2）若目标值小于头值；\n  2.2.1）若mid值比头值大，mid之前的部分可以排除了，去后半部分查找；\n  2.2.2）若mid值比头值小，判断目标值若比mid值还小，则去排除头值的左半部分查找；否则去后半部分查找。\n      3 Golang实现代码\nhttps://github.com/olzhy/\nfunc search(nums []int, target int) int { start := 0 end := len(nums) - 1 for start \u0026lt;= end { mid := (start + end) / 2 if target == nums[mid] { return mid } if nums[start] \u0026lt;= nums[end] { if target \u0026lt; nums[mid] { end = mid - 1 } else { start = mid + 1 } } else { if target \u0026gt;= nums[start] { if nums[mid] \u0026gt;= nums[start] \u0026amp;\u0026amp; target \u0026gt; nums[mid] { start = mid + 1 } else { end = mid - 1 } } else { if nums[mid] \u0026gt;= nums[start] { start = mid + 1 } else { if target \u0026lt; nums[mid] { start++ end = mid - 1 } else { start = mid + 1 } } } } } return -1 } ","permalink":"https://olzhy.github.io/posts/leetcode-search-in-rotated-sorted-array.html","tags":["Golang","算法"],"title":"LeetCode 33 在旋转的有序数组搜索"},{"categories":["计算机"],"contents":"GitHub: github.com/olzhy/leetcode\n   题号 题目 难度 实现     2 Add Two Numbers 中 Golang   3 Longest Substring 中 Golang   5 Longest Palindromic Substring 中 Golang   6 ZigZag Conversion 中 Golang   8 String to Integer 中 Golang   12 Integer to Roman 中 Golang   17 Letter Combinations of A\u0026hellip; 中 Golang   19 Remove Nth Node From End of List 中 Golang   24 Swap Nodes In Pairs 中 Golang   33 Search in Rotated Sorted Array 中 Golang   34 Find First and Last Position… 中 Golang   48 Rotate Image 中 Golang   54 Spiral Matrix 中 Golang   55 Jump Game 中 Golang   56 Merge Intervals 中 Golang   59 Spiral Matrix II 中 Golang   60 Permutation Sequence 中 Golang   61 Rotate List 中 Golang   71 Simplify Path 中 Golang   73 Set Matrix Zeroes 中 Golang   74 Search a 2D Matrix 中 Golang   75 Sort Colors 中 Golang   77 Combinations 中 Golang   81 Search in Rotated Sorted Array II 中 Golang   86 Partition List 中 Golang   91 Decode Ways 中 Golang   92 Reverse Linked List II 中 Golang   93 Restore IP Addresses 中 Golang   94 Binary Tree Inorder Traversal 中 Golang   95 Unique Binary Search Trees II 中 Golang   96 Unique Binary Search Trees 中 Golang   98 Validate Binary Search Tree 中 Golang   102 Binary Tree Level Order Traversal 中 Golang   103 Binary Tree Zigzag Level Order Traversal 中 Golang   105 Construct Binary Tree from Preorder\u0026hellip; 中 Golang, Python   106 Construct Binary Tree from Inorder\u0026hellip; 中 Golang   130 Surrounded Regions 中 Golang   143 Reorder List 中 Golang   144 Binary Tree Preorder Traversal 中 Golang   145 Binary Tree Postorder Traversal 难 Golang   151 Reverse Words in a String 中 Golang   165 Compare Version Numbers 中 Golang   198 House Robber 易 Golang   199 Binary Tree Right Side View 中 Golang   206 Reverse Linked List 易 Golang   224 Basic Calculator 难 Golang   338 Couting Bits 中 Golang   355 Design Twitter 中 Golang   393 UTF-8 Validation 中 Golang   413 Arithmetic Slices 中 Golang   451 Sort Characters By Frequency 中 Golang   477 Total Hamming Distance 中 Golang   508 Most Frequent Subtree Sum 中 Golang   513 Find Bottom Left Tree Value 中 Golang   515 Find Largest Value in Each Tree Row 中 Golang   641 Design Circular Deque 中 Golang   701 Insert into a Binary Search Tree 中 Golang   707 Design Linked List 中 Golang, Python   807 Max Increase to Keep City\u0026hellip; 中 Golang   856 Score of Parentheses 中 Golang   885 Spiral Matrix III 中 Golang   889 Construct Binary Tree from\u0026hellip; 中 Golang   911 Online Election 中 Golang   946 Validate Stack Sequences 中 Golang   958 Check Completeness of a Binary Tree 中 Golang   981 Time Based Key-Value Store 中 Golang   1008 Construct Binary Search Tree\u0026hellip; 中 Golang    ","permalink":"https://olzhy.github.io/leetcode-golang-implementations/","tags":["Golang","Python","算法"],"title":"LeetCode 题目Golang实现汇总"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个整数n，生成节点为1\u0026hellip;n的所有的二叉搜索树（BST）。\n例子1：\n输入：3\n输出：\n[[1,null,3,2],[3,2,null,1],[3,1,null,null,2],[2,1,3],[1,null,2,null,3]] 释义：\n如上输出为对应n为的5的所有的二叉搜索树: 1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 题目出处：LeetCode\n2 解决思路\n写一个generate函数，其负责生成包含begin\u0026hellip;end节点的BST。\n初始时，begin为1，end为n。\n欲求所有的BST，可以按如下步骤来计算：\na）当root为begin时，root的左子树为空，右子树为包含begin+1\u0026hellip;end的BST；\nb）当root为begin+1时，root的左子树为begin，右子树为包含begin+2\u0026hellip;end的BST；\n\u0026hellip;\nx）当root为i时，root的左子树为包含begin\u0026hellip;i-1的BST，右子树为包含i+1\u0026hellip;end的BST；\n\u0026hellip;\n递归调用如上步骤，直至begin大于end，则返回空树数组，或者begin=end，返回仅包含begin一个根节点树的数组。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TreeNode struct { Val int Left *TreeNode Right *TreeNode } func generate(begin, end int) []*TreeNode { if begin \u0026gt; end { return []*TreeNode{nil} } if begin == end { return []*TreeNode{{Val: begin}} } var trees []*TreeNode for i := begin; i \u0026lt;= end; i++ { left := generate(begin, i-1) right := generate(i+1, end) for _, j := range left { for _, k := range right { root := \u0026amp;TreeNode{Val: i, Left: j, Right: k} trees = append(trees, root) } } } return trees } func generateTrees(n int) []*TreeNode { if 0 == n { return []*TreeNode{} } return generate(1, n) } ","permalink":"https://olzhy.github.io/posts/leetcode-unique-binary-search-trees-ii.html","tags":["Golang","算法"],"title":"LeetCode 95 不同的二叉搜索树 II"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个整数n，求以1 \u0026hellip; n为节点所组成的二叉搜索树（BST）共有多少种情形？\n例子1：\n输入：3\n输出：5\n释义：\n对n=3，共有如下5种满足BST的情形: 1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 题目出处：LeetCode\n2 解决思路\n先拿n=3时举个例：\na）root为1时，2、3仅可放在右子树；\nb）root为2时，1放左子树，3放右子树；\nc）root为3时，1、2仅可放在左子树。\n总数为此三种加起来。\n至于子树为两个节点的情况，再拿n=2时按上述步骤去处理。\n所以可以看到规律，对于n的情形，采用如下步骤计算：\na）root为1时，2\u0026hellip;n仅可放在右子树；\nb）root为2时，1放左子树，3\u0026hellip;n放右子树；\n\u0026hellip;\nx）root为i时，1\u0026hellip;i-1这i-1个节点放左子树，i+1\u0026hellip;n这n-i个节点放右子树；\n\u0026hellip;\n对于上述情况，递归计算即可，最后将各结果加起来，返回条件为n=0或n=1，返回为1。\n改进：\n上述递归计算时，对用到的子树计算结果有大量重复计算的情形，因递归较深，这样非常耗时。我们可以借助一个map来存储算过的值，这样不同的值仅计算一次。\n3 Golang实现代码\nhttps://github.com/olzhy/\nvar table = make(map[int]int) func numTrees(n int) int { if 0 == n || 1 == n { return 1 } if v, ok := table[n]; ok { return v } num := 0 for i := 1; i \u0026lt;= n; i++ { num += numTrees(i-1) * numTrees(n-i) } table[n] = num return num } ","permalink":"https://olzhy.github.io/posts/leetcode-unique-binary-search-trees.html","tags":["Golang","算法"],"title":"LeetCode 96 不同的二叉搜索树"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，判断其是否为一个有效的二叉搜索树（BST）。\n假定一个二叉搜索树的定义为：\na）一个节点的左子树包含的节点的key小于该节点的key；\nb）一个节点的右子树包含的节点的key大于该节点的key；\nc）左右子树均须是二叉搜索树。\n例子1：\n 2 / \\ 1 3 输入：[2,1,3]\n输出：true\n例子2：\n 5 / \\ 1 4 / \\ 3 6 输入：[5,1,4,null,null,3,6]\n输出：false\n释义：根节点值为5，而右节点值为4，小于根节点值。\n题目出处：LeetCode\n2 解决思路\n按照二叉搜索树定义，若对其进行先序遍历，则应满足节点key递增原则。\n本文采用递归方式对二叉树进行先序遍历，使用一个变量记录上一个遍历到的节点的key，若当前key不大于上一个key，则退出遍历，返回false。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc preOrderTraversal(root *TreeNode, preVal *int) bool { if nil == root { return true } ok := preOrderTraversal(root.Left, preVal) if !ok { return false } if root.Val \u0026lt;= *preVal { return false } *preVal = root.Val return preOrderTraversal(root.Right, preVal) } func isValidBST(root *TreeNode) bool { preVal := -(1 \u0026lt;\u0026lt; 32) return preOrderTraversal(root, \u0026amp;preVal) } ","permalink":"https://olzhy.github.io/posts/leetcode-validate-binary-search-tree.html","tags":["Golang","算法"],"title":"LeetCode 98 校验二叉搜索树"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，想象站在其右侧，返回以该视角看到的自上而下的节点值。\n例子1：\n输入：[1,2,3,null,5,null,4]\n输出：[1, 3, 4]\n释义：\n 1 \u0026lt;--- / \\ 2 3 \u0026lt;--- \\ \\ 5 4 \u0026lt;--- 题目出处：LeetCode\n2 解决思路\n该题目要求的是列出二叉树每一层的最右节点。\n我们递归对树进行后序遍历，若当前层未遍历过，则将该节点记录。这样每一层最先遍历到的即是最右节点。\n当整个树遍历完成时，即返回所有最右节点。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TreeNode struct { Val int Left *TreeNode Right *TreeNode } func postOrderTraversal(root *TreeNode, depth int, r *[]int) { if len(*r) \u0026lt; depth { *r = append(*r, root.Val) } if nil != root.Right { postOrderTraversal(root.Right, depth+1, r) } if nil != root.Left { postOrderTraversal(root.Left, depth+1, r) } } func rightSideView(root *TreeNode) []int { if nil == root { return []int{} } r := []int{root.Val} postOrderTraversal(root, 1, \u0026amp;r) return r } ","permalink":"https://olzhy.github.io/posts/leetcode-binary-tree-right-side-view.html","tags":["Golang","算法"],"title":"LeetCode 199 二叉树右侧视角图"},{"categories":["计算机"],"contents":"1 题目描述\n给定两个整数n与k，返回出自区间[1, n]的所有可能的k个数的组合。\n例子1：\n输入：n = 4, k = 2\n输出：[[2,4],[3,4],[2,3],[1,2],[1,3],[1,4],]\n题目出处：LeetCode\n2 解决思路\n采用递归思路：\na）先拿出一个数，与之后的k-1个数的组合进行组合；\nb）再拿出下一个数，与之后的k-1个数的组合进行组合；\n直至计算到最后一个数。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc comb(begin, end, k int) [][]int { var r [][]int for i := begin; i \u0026lt;= end; i++ { if 1 == k { r = append(r, []int{i}) continue } suf := comb(i+1, end, k-1) for _, j := range suf { r = append(r, append([]int{i}, j...)) } } return r } func combine(n int, k int) [][]int { return comb(1, n, k) } ","permalink":"https://olzhy.github.io/posts/leetcode-combinations.html","tags":["Golang","算法"],"title":"LeetCode 77 组合"},{"categories":["计算机"],"contents":"1 题目描述\n在一次选举中，定义第i次投票为在时间times[i]给人persons[i]投票。现在，我们想实现如下查询函数：\nTopVotedCandidate.q(int t) 其会返回在给定时间t的领先者编号。在t时刻的投票也会计入查询。在有平局的情况下，最近被投票的为领先者。\n例子1：\n输入：[\u0026ldquo;TopVotedCandidate\u0026rdquo;,\u0026ldquo;q\u0026rdquo;,\u0026ldquo;q\u0026rdquo;,\u0026ldquo;q\u0026rdquo;,\u0026ldquo;q\u0026rdquo;,\u0026ldquo;q\u0026rdquo;,\u0026ldquo;q\u0026rdquo;], [[[0,1,1,0,0,1,0],[0,5,10,15,20,25,30]],[3],[12],[25],[15],[24],[8]]\n输出：[null,0,1,1,0,0,1]\n释义：\n在时间3，投票为[0]，0领先；\n在时间12，投票为[0,1,1]，1领先；\n在时间25，投票为[0,1,1,0,0,1]，1领先（因平局时，最近被投票的被认为领先）；\n依此继续计算时间15、24与8即可。\n题目出处：LeetCode\n2 解决思路\n为方便查询，需构造出一个key为时间，value为领先者编号的map。\n因决定领先者可能发生变化的时间只能是输入参数times中的各时间，即投票时间数组。\n所以该map的key即为times，value需要在Constructor中按以下逻辑计算得出，构造好后，对于给定输入时间t，我们将其向下靠拢（即例子1中查询时间t为3即可看作t为0，t为12即可看作t为10等），这个t的靠拢计算可以使用折半查询。\n计算该map中value的过程可以使用如下步骤。\n声明orders用来记录按投票顺序编排的person数组，遍历投票时间数组times及被投人persons：\n针对时间t及被投人p\na）若投过p，则将p现在的票数取出，然后将其从orders移除，将票数加1后重新放至orders尾部；若没投过，票数设为1，直接append至尾部即可；\nb）设当前最多票数为max，从尾至头遍历orders数组，若有票数大于该值，则将max替换并记录领先者编号，至遍历完成，即得到领先者；\n遍历完成，即得到领先者map，供查询即可。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype TopVotedCandidate struct { times []int leadings map[int]int } func Constructor(persons []int, times []int) TopVotedCandidate { var orders [][]int leadings := make(map[int]int) for i := 0; i \u0026lt; len(times); i++ { t := times[i] p := persons[i] count := 1 // orders  for j, order := range orders { if p == order[0] { count = order[1] + 1 orders = append(orders[:j], orders[j+1:]...) } } orders = append(orders, []int{p, count}) // leadings map  max := 1 for i := len(orders) - 1; i \u0026gt;= 0; i-- { count := orders[i][1] if count \u0026gt; max { p = orders[i][0] max = count } } leadings[t] = p } return TopVotedCandidate{times, leadings} } func (this *TopVotedCandidate) Q(t int) int { // binary search  q := this.times[0] start, end := 0, len(this.times)-1 for start \u0026lt;= end { mid := (start + end) / 2 if t \u0026lt; this.times[mid] { end = mid - 1 continue } q = this.times[mid] start = mid + 1 } return this.leadings[q] } ","permalink":"https://olzhy.github.io/posts/leetcode-online-election.html","tags":["Golang","算法"],"title":"LeetCode 911 在线选举"},{"categories":["计算机"],"contents":"1 题目描述\n在一个R行C列的二维网格上，我们起始在(r0, c0)位置，且面朝东。\n这样，矩阵的西北角在第一行第一列，东南角在最后一行最后一列。\n现在，我们以顺时针螺旋形状来访问网格的每个位置。\n当走到网格边界之外时，则继续在边界之外走（可能稍后会回到网格的边界）。\n最终，我们访问了全部R * C个空间。\n返回一个代表网格访问顺序的列表。\n注：\na）1 \u0026lt;= R \u0026lt;= 100\nb）1 \u0026lt;= C \u0026lt;= 100\nc）0 \u0026lt;= r0 \u0026lt; R\nd）0 \u0026lt;= c0 \u0026lt; C\n例子1：\n输入：R = 1, C = 4, r0 = 0, c0 = 0\n输出：[[0,0],[0,1],[0,2],[0,3]]\n释义：\n例子2：\n输入：R = 5, C = 6, r0 = 1, c0 = 4\n输出：[[1,4],[1,5],[2,5],[2,4],[2,3],[1,3],[0,3],[0,4],[0,5],[3,5],[3,4],[3,3],[3,2],[2,2],[1,2],[0,2],[4,5],[4,4],[4,3],[4,2],[4,1],[3,1],[2,1],[1,1],[0,1],[4,0],[3,0],[2,0],[1,0],[0,0]]\n释义：\n题目出处：LeetCode\n2 解决思路\n  a）首先访问当前位置空间；\n  b）用circle表示当前访问到第几圈，从第1圈开始直至还有未触达的边界，即扩大圈半径进行如下循环：\n  i）首先walk方向为朝下，从i为max(0, r0-circle+1)，j为c0 + circle开始，若j未跨过边界，则自上到下直至i抵达r0+circle的上一个空间或边界；\n  ii）然后walk方向为朝左，从i为r0 + circle，j为min(c-1, c0+circle)开始，若i未跨过边界，则自右到左直至j抵达c0-circle的上一个空间或边界；\n  iii）然后walk方向为朝上，从i为min(r-1, r0+circle)，j为c0 - circle开始，若j未跨过边界，则自下到上直至i抵达r0-circle的上一个空间或边界；\n  iv）最后walk方向为朝右，从i为r0 - circle，j为max(0, c0-circle)开始，若i未跨过边界，则自左到右直至j抵达c0+circle或边界。\n    c）循环退出即遍历完了所有的空间。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc min(a, b int) int { if a \u0026lt; b { return a } return b } func max(a, b int) int { if a \u0026gt; b { return a } return b } func spiralMatrixIII(r int, c int, r0 int, c0 int) [][]int { walk := make([][]int, r*c) i, j := r0, c0 index := 0 walk[index] = []int{i, j} index++ circle := 1 for r0+circle \u0026lt; r || r0-circle \u0026gt;= 0 || c0+circle \u0026lt; c || c0-circle \u0026gt;= 0 { // down direction  i = max(0, r0-circle+1) j = c0 + circle for j \u0026lt; c \u0026amp;\u0026amp; i \u0026lt; r0+circle \u0026amp;\u0026amp; i \u0026lt; r { walk[index] = []int{i, j} index++ i++ } // left direction  i = r0 + circle j = min(c-1, c0+circle) for i \u0026lt; r \u0026amp;\u0026amp; j \u0026gt; c0-circle \u0026amp;\u0026amp; j \u0026gt;= 0 { walk[index] = []int{i, j} index++ j-- } // up direction  i = min(r-1, r0+circle) j = c0 - circle for j \u0026gt;= 0 \u0026amp;\u0026amp; i \u0026gt; r0-circle \u0026amp;\u0026amp; i \u0026gt;= 0 { walk[index] = []int{i, j} index++ i-- } // right direction  i = r0 - circle j = max(0, c0-circle) for i \u0026gt;= 0 \u0026amp;\u0026amp; j \u0026lt;= c0+circle \u0026amp;\u0026amp; j \u0026lt; c { walk[index] = []int{i, j} index++ j++ } circle++ } return walk } ","permalink":"https://olzhy.github.io/posts/leetcode-spiral-matrix-iii.html","tags":["Golang","算法"],"title":"LeetCode 885 螺旋矩阵 III"},{"categories":["计算机"],"contents":"delve是一款专门针对 Golang 程序调试而开发的命令行调试器，该工具功能强大，简单易用。\n本文从安装开始，使用一个实际的 Golang 程序调试例子，学习一下 delve 的主要调试方式及常用调试命令。\n本文所使用的是 Windows 环境。\n1 安装\n使用 go get 命令安装构建 delve。\n$ go get -u github.com/go-delve/delve/cmd/dlv 其会在$GOPATH/bin下生成二进制可执行文件dlv.exe，将$GOPATH/bin 添加到 PATH 环境变量即可在任意目录使用 dlv。\n2 主要调试方式\n键入 dlv help 可以看到 dlv 的使用帮助文档。\n$ dlv help Delve is a source level debugger for Go programs. Delve enables you to interact with your program by controlling the execution of the process, evaluating variables, and providing information of thread / goroutine state, CPU register state and more. The goal of this tool is to provide a simple yet powerful interface for debugging Go programs. Pass flags to the program you are debugging using `--`, for example: `dlv exec ./hello -- server --config conf/config.toml` Usage: dlv [command] Available Commands: attach Attach to running process and begin debugging. connect Connect to a headless debug server. core Examine a core dump. debug Compile and begin debugging main package in current directory, or the package specified. exec Execute a precompiled binary, and begin a debug session. help Help about any command run Deprecated command. Use \u0026#39;debug\u0026#39; instead. test Compile test binary and begin debugging program. trace Compile and begin tracing program. version Prints version. Flags: ... 下面我们会结合一个实际的例子看一下主要的几种调试方式。\n我们就以 Golang官方的例子 着手吧。\n$ go get -u github.com/golang/example/hello 如上命令会将github.com/golang/example/hello下载到$GOPATH/src下。\n进入到$GOPATH/src/github.com/golang/example 下，查看文件目录结构：\n$ cd $GOPATH/src/github.com/golang/example $ tree . ├─ hello │ └─ hello.go ├─ stringutil │ ├─ reverse.go │ └─ reverse_test.go └─ template ... 我们后面仅会涉及到 hello.go 与 reverse.go 这两个文件。\na）dlv debug\n使用 dlv debug 可以在 main 函数文件所在目录直接对 main 函数进行调试，也可以在根目录以指定包路径的方式对 main 函数进行调试。\nb）dlv test\n使用 dlv test 可以对 test 包进行调试。\nc）dlv attach\n使用 dlv attach 可以附加到一个已在运行的进程进行调试。\nd）dlv connect\n使用 dlv connect 可以连接到调试服务器进行调试。\ne）dlv trace\n使用 dlv trace 可以追踪程序。\nf）dlv exec\n使用 dlv exec 可以对编译好的二进制进行调试。\n3 常用调试命令\n下面对github.com/golang/example/hello包进行调试，学习一下常用的调试命令。\n首先看一下该包下唯一的文件 hello.go 的内容。\n$ cat hello.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/golang/example/stringutil\u0026#34; ) func main() { fmt.Println(stringutil.Reverse(\u0026#34;!selpmaxe oG ,olleH\u0026#34;)) } 可以看到 hello.go 程序主要在 main 函数调用了一下 stringutil 包的 Reverse 方法。\n下面进入调试。\n$ cd $GOPATH/src/github.com/golang/example/hello $ dlv debug a）使用 break 或 b 对 main.main 设置断点\n$ b main.main 显示已设置 OK。\nBreakpoint 1 set at 0x4af2aa for main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:25 b）使用 continue 或 c 进入断点\n$ c 显示已进入。\n\u0026gt; main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:25 (hits goroutine(1):1 total:1) (PC: 0x4af2aa) 20: \u0026quot;fmt\u0026quot; 21: 22: \u0026quot;github.com/golang/example/stringutil\u0026quot; 23: ) 24: =\u0026gt; 25: func main() { 26: fmt.Println(stringutil.Reverse(\u0026quot;!selpmaxe oG ,olleH\u0026quot;)) 27: } c）使用 next 或 n 移至下一步\n$ n 显示已移至下一步。\n\u0026gt; main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:26 (PC: 0x4af2bb) 21: 22: \u0026quot;github.com/golang/example/stringutil\u0026quot; 23: ) 24: 25: func main() { =\u0026gt; 26: fmt.Println(stringutil.Reverse(\u0026quot;!selpmaxe oG ,olleH\u0026quot;)) 27: } d）使用 step 或 s 进入函数\n显示已进入 Reverse 函数。\n\u0026gt; github.com/golang/example/stringutil.Reverse() E:/workspace/go/src/github.com/golang/example/stringutil/reverse.go:21 (PC: 0x4af0b2) 16: 17: // Package stringutil contains utility functions for working with strings. 18: package stringutil 19: 20: // Reverse returns its argument string reversed rune-wise left to right. =\u0026gt; 21: func Reverse(s string) string { 22: r := []rune(s) 23: for i, j := 0, len(r)-1; i \u0026lt; len(r)/2; i, j = i+1, j-1 { 24: r[i], r[j] = r[j], r[i] 25: } 26: return string(r) e）在指定行设置断点\nb 24 对 24 行设置了一个断点，输入 c 进入该断点。\nf）打印变量\np i 打印 i 的值，输出为 0。\n查看局部变量的值可以使用 locals。\nlocals r = []int32 len: 19, cap: 32, [...] j = 18 i = 0 g）使用 breakpoint 或 bp 查看所有断点\nbp Breakpoint unrecovered-panic at 0x42f4b0 for runtime.fatalpanic() D:/soft/go/location/src/runtime/panic.go:690 (0) print runtime.curg._panic.arg Breakpoint 1 at 0x4af2aa for main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:25 (1) Breakpoint 2 at 0x4af162 for github.com/golang/example/stringutil.Reverse() E:/workspace/go/src/github.com/golang/example/stringutil/reverse.go:24 (1) 可以看到我们先后设置的两个断点 hello.go:25 与 reverse.go:24 的详细信息，且它们的 id 分别为 1 和 2。\nh）对指定断点设置执行脚本\n对于一个循环，想查看其局部变量值，如 i 的值，每次都 print 比较麻烦，这时可以使用 on 对断点设置一个执行脚本。\non 2 print i on 2 print j 如上命令对 reverse.go:24 这个断点设置了执行脚本，接下来每次触发该断点的时候，即会打印 i 与 j 的值。\n下面执行 continue，其再一次进入该断点，输出为：\nc \u0026gt; github.com/golang/example/stringutil.Reverse() E:/workspace/go/src/github.com/golang/example/stringutil/reverse.go:24 (hits goroutine(1):2 total:2) (PC: 0x4af162) i: 1 j: 17 19: 20: // Reverse returns its argument string reversed rune-wise left to right. 21: func Reverse(s string) string { 22: r := []rune(s) 23: for i, j := 0, len(r)-1; i \u0026lt; len(r)/2; i, j = i+1, j-1 { =\u0026gt; 24: r[i], r[j] = r[j], r[i] 25: } 26: return string(r) 27: } 可以看到额外输出了 i 与 j 的值。\ni）对指定断点设置条件\n对于一个循环，若整个迭代比较多，我们调试时要走到想要的位置，一直输入 continue 也不是办法，这时可以使用 cond 给断点设置条件。\ncond 2 5==i 如上命令对 reverse.go:24 这个断点设置了条件，即 5==i，这次执行 c 的时候会直接走到该条件触发的位置。\nc \u0026gt; github.com/golang/example/stringutil.Reverse() E:/workspace/go/src/github.com/golang/example/stringutil/reverse.go:24 (hits goroutine(1):4 total:4) (PC: 0x4af162) i: 5 j: 13 19: 20: // Reverse returns its argument string reversed rune-wise left to right. 21: func Reverse(s string) string { 22: r := []rune(s) 23: for i, j := 0, len(r)-1; i \u0026lt; len(r)/2; i, j = i+1, j-1 { =\u0026gt; 24: r[i], r[j] = r[j], r[i] 25: } 26: return string(r) 27: } j）使用 stepout 跳出当前函数\n该函数调试的差不多了，可以使用 stepout 直接跳出到上层函数。\nstepout \u0026gt; main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:26 (PC: 0x4af2d4) Values returned: ~r1: \u0026quot;Hello, Go examples!\u0026quot; 21: 22: \u0026quot;github.com/golang/example/stringutil\u0026quot; 23: ) 24: 25: func main() { =\u0026gt; 26: fmt.Println(stringutil.Reverse(\u0026quot;!selpmaxe oG ,olleH\u0026quot;)) 27: } 这样，又回到了我们的 main 函数。\nk）使用 clear 清除指定断点\n想清除某个断点，可以使用 clear 命令，下面我们清除 reverse.go:24 这个断点，然后再查看所有断点。\nclear 2 Breakpoint 2 cleared at 0x4af162 for github.com/golang/example/stringutil.Reverse() E:/workspace/go/src/github.com/golang/example/stringutil/reverse.go:24 bp Breakpoint unrecovered-panic at 0x42f4b0 for runtime.fatalpanic() D:/soft/go/location/src/runtime/panic.go:690 (0) print runtime.curg._panic.arg Breakpoint 1 at 0x4af2aa for main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:25 (1) 可以看到只剩下 hello.go:25 这一个手动设置的断点了。\n若想清除所有断点，可以使用 clearall。\nclearall Breakpoint 1 cleared at 0x4af2aa for main.main() E:/workspace/go/src/github.com/golang/example/hello/hello.go:25 bp Breakpoint unrecovered-panic at 0x42f4b0 for runtime.fatalpanic() D:/soft/go/location/src/runtime/panic.go:690 (0) print runtime.curg._panic.arg l）使用 restart 或 r 重新进入调试\n若想重新进入一次新的调试，无须退出程序再次执行 dlv debug。\n可以使用 restart 或 r 命令：\nr Process restarted with PID 8008 这样即可以重新开始了。\n本文仅介绍了一些主要的调试命令，全部调试命令可以参阅该地址。\n 参考资料\n[1] https://github.com/go-delve/delve\n[2] https://www.jamessturtevant.com/posts/Using-the-Go-Delve-Debugger-from-the-command-line/\n[3] https://blog.gopheracademy.com/advent-2015/debugging-with-delve/\n ","permalink":"https://olzhy.github.io/posts/debugging-golang-programs-with-delve.html","tags":["Golang"],"title":"使用delve调试Golang程序"},{"categories":["计算机"],"contents":"在前两篇文章（Golang 模块获取包modfetch研读，Golang模块代理goproxy.io源码研读），我们学习了Golang Module Proxy的工作原理以及实现原理。\n本文尝试独立实现一个Golang Module Proxy服务。\n实现逻辑主要涉及这几块内容：\n  a）main.go负责服务启动，服务优雅终止；\n  b）generate.sh负责将$GOROOT中的internal包拷贝至当前项目并替换引用路径；\n  c）proxy.go核心逻辑部分，负责工作目录设定，路径检查，Module请求处理。\n下面详细看一下这几部分的代码。\n1 main.go\n头部的//go:generate注释指定脚本generate.sh，当执行go generate时，其会调用generate.sh将modfetch包及其依赖包从$GOROOT中的internal文件夹拷贝至当前项目，然后即可以在当前项目直接使用了。\n初始化一个http.Server，其Handler使用proxy.go的proxy.Proxy函数。\n启动一个goroutine监听中断信号，以便优雅的终止服务（如何优雅的终止一个服务？）。\n//go:generate sh generate.sh package main import ( ... \u0026#34;github.com/olzhy/goproxy/pkg/proxy\u0026#34; ) var port = flag.String(\u0026#34;serverPort\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;server port\u0026#34;) func main() { // server  srv := http.Server{ Addr: *port, Handler: proxy.Proxy(), } // server startup / gracefully shutdown  ... srv.ListenAndServe() ... } 2 generate.sh\ngenerate.sh负责将$GOROOT中的internal包拷贝至当前项目并将引用路径替换为新的引用路径。\n#!/bin/bash  mkdir internal # copy dependencies cp -r $GOROOT/src/cmd/go/internal/modfetch ./internal/ ... cp -r $GOROOT/src/cmd/internal/sys ./internal/ ... # replace import paths find . -type f -name \u0026#34;*.go\u0026#34; -exec sed -i \u0026#39;\u0026#39; \u0026#39;s#cmd/go/internal/#github.com/olzhy/goproxy/internal/#g\u0026#39; {} \\; ... 3 proxy.go\npkg/proxy/proxy.go提供proxy.Proxy函数。proxy.go首先会设置工作目录，启动后对于一个GET请求，首先会校验请求路径，对不满足规则的请求直接返回404，然后仅对这几类符合Module请求格式的请求作处理：\na）后缀为“/@v/list”\n如GET github.com/olzhy/quote/@v/list\n从请求路径截取mod名称，调用modfetch.Lookup函数返回所有可用版本。\nb）后缀为“/@latest”\n如GET github.com/olzhy/quote/@latest\n从请求路径截取mod名称，调用modfetch.Lookup函数获取最近一次提交信息。\nc）后缀为“.info”\n如GET github.com/olzhy/quote/@v/v1.0.0.info\n从请求路径截取mod及version信息，调用modfetch.Stat函数获取info。\nd）后缀为“.mod”\n如GET github.com/olzhy/quote/@v/v1.0.0.mod\n从请求路径截取mod及version信息，调用modfetch.GoMod函数获取mod内容。\ne）后缀为“.zip”\n如GET github.com/olzhy/quote/@v/v1.0.0.zip\n从请求路径截取mod及version信息，调用modfetch.DownloadZip函数获取zip文件路径名称并提供下载。\npackage proxy import ( ... \u0026#34;github.com/olzhy/goproxy/internal/modfetch\u0026#34; ... ) const ( ListSuffix = \u0026#34;/@v/list\u0026#34; LatestSuffix = \u0026#34;/@latest\u0026#34; InfoSuffix = \u0026#34;.info\u0026#34; ModSuffix = \u0026#34;.mod\u0026#34; ZipSuffix = \u0026#34;.zip\u0026#34; VInfix = \u0026#34;/@v/\u0026#34; ) func init() { modfetch.PkgMod = ... codehost.WorkRoot = ... } func Proxy() http.HandlerFunc { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { path := strings.Trim(r.RequestURI, \u0026#34;/\u0026#34;) // req path validation  if err := pathValidation(path); nil != err { w.WriteHeader(http.StatusBadRequest) fmt.Fprintln(w, err) return } switch { // suffix is /@v/list  case strings.HasSuffix(path, ListSuffix): ... // call modfetch.Lookup(mod)  lookupVersions(mod) ... return // suffix is /@latest  case strings.HasSuffix(path, LatestSuffix): ... // modfetch.Lookup(mod)  lookupLatestRev(mod) ... return // suffix is .info  case strings.HasSuffix(path, InfoSuffix): ... // call modfetch.Stat(mod, rev)  loadRev(mod, ver) ... return // suffix is .mod  case strings.HasSuffix(path, ModSuffix): ... // call modfetch.GoMod(mod, rev) \tloadModContent(mod, ver) ... return // suffix is .zip  case strings.HasSuffix(path, ZipSuffix): ... // call modfetch.DownloadZip(module.Version{Path: mod, Version: rev})  loadZip(mod, ver) ... return default: w.WriteHeader(http.StatusBadRequest) fmt.Fprintln(w, \u0026#34;please give me a correct module query\u0026#34;) } }) } 完整实现代码已提交至GitHub（github.com/olzhy/goproxy），欢迎大家关注。\n此外该服务已部署至服务器，欢迎大家使用https://golangcenter.com。\n 参考资料\n[1] https://github.com/olzhy/goproxy\n ","permalink":"https://olzhy.github.io/posts/implement-a-golang-module-proxy.html","tags":["Golang"],"title":"实现一个Golang Module Proxy"},{"categories":["计算机"],"contents":"goproxy.io是一款很好用的Golang Module Proxy，解决了国内用户无法直接下载Golang模块依赖的问题。\n本文准备研读一下其开源代码github.com/goproxyio/goproxy，了解下其实现原理。\ngoproxy工程的主要目录结构如下：\ngoproxy ├ build │ └ generate.sh ├ pkg │ └ proxy │ └ proxy.go └ main.go main.go头部有一行代码：\n//go:generate ./build/generate.sh 下面会解释其作用。\n1 建立一个Golang Proxy服务需要实现哪些功能\n$ go help goproxy ... A Go module proxy is any web server that can respond to GET requests for URLs of a specified form. The requests have no query parameters, so even a site serving from a fixed file system (including a file:/// URL) can be a module proxy. The GET requests sent to a Go module proxy are: GET $GOPROXY/\u0026lt;module\u0026gt;/@v/list returns a list of all known versions of the given module, one per line. GET $GOPROXY/\u0026lt;module\u0026gt;/@v/.info returns JSON-formatted metadata about that version of the given module. GET $GOPROXY/\u0026lt;module\u0026gt;/@v/.mod returns the go.mod file for that version of the given module. GET $GOPROXY/\u0026lt;module\u0026gt;/@v/.zip returns the zip archive for that version of the given module. ... 从如上说明可知，使用诸如go get等go命令会从vcs下载所需模块，设置GOPROXY环境变量即可指定Go Module Proxy服务，其为不带参数URL的GET请求提供服务，为模块下载作了一层包装。\n其主要提供如下几个接口：\n  a）GET $GOPROXY/\u0026lt;module\u0026gt;/@v/list 该接口返回给定模块的已知版本号列表，每行一个。\n  b）GET $GOPROXY/\u0026lt;module\u0026gt;/@v/\u0026lt;version\u0026gt;.info 该接口返回给定模块在给定版本的JSON格式的元数据信息。\n  c）GET $GOPROXY/\u0026lt;module\u0026gt;/@v/\u0026lt;version\u0026gt;.mod 该接口返回给定模块在给定版本的go.mod的内容。\n  d）GET $GOPROXY/\u0026lt;module\u0026gt;/@v/\u0026lt;version\u0026gt;.zip 该接口返回给定模块在给定版本的.zip压缩包。\n  为避免大小写敏感型文件系统服务问题，\u0026lt;module\u0026gt;及\u0026lt;version\u0026gt;会对大写字母加密，即将大写字母转换为!加小写字母的方式。\n如github.com/Azure编码为github.com/!azure。\nJSON格式的元数据信息对应的Go内置结构体为：\ntype Info struct { Version string // version string  Time time.Time // commit time } 给定模块在给定版本的.zip压缩包内的文件树结构与模块源码树结构一致，且模块及版本未使用大写字母加密。\n不论go命令直接访问vcs还是从Proxy下载，其均会将模块的info、mod及zip文件组合到一起，置于$GOPATH/pkg/mod/cache/download本地缓存。\n因缓存路径与请求路径对应，所以对$GOPATH/pkg/mod/cache/download文件夹以https://example.com/proxy提供服务即可访问到缓存的模块。\n2 goproxy/build/generate.sh\n当执行go generate命令时，Go会扫描当前包相关的源码文件，找出所有包含//go:generate的特殊注释，并执行注释指定的脚本。\n因goproxy.io工程需要使用内置的模块获取包，而由上一篇文章“Golang 模块获取包modfetch研读”可知，modfetch、modload等模块获取包是位于cmd/go/internal路径下的，所以无法直接引用，main.go所指向的goproxy/build/generate.sh这个脚本即是用来将modfetch、modload等内部包及其依赖包拷贝至当前工作目录下，以便直接引用。\n3 goproxy/main.go\nmain.go负责接收http请求，此外检查git有没有安装（因Go从诸如github.com等vcs下载新的依赖包需要依赖git工具），构造Golang Module的工作目录cacheDir等。\n4 goproxy/pkg/proxy/proxy.go\nproxy.go是该工程的核心代码，其会根据main.go传入的cacheDir设置工作目录。然后分别为后缀为.info，.mod，.zip，/@v/list，/@latest的几种类型的请求提供服务。\n对如上类型的请求，会调用modfetch等包的内置函数来辅助实现，关于modfetch中几个核心函数的功能及用法，请参考上一篇博文：Golang 模块获取包modfetch研读。\na）若请求后缀为/@v/list，/@latest\n因请求的是所有可用版本号或最新版本，所以必须请求vcs系统，所以可以使用modfetch.Lookup函数。若是/@v/list，返回repo.Versions(\u0026quot;\u0026quot;)，若是/@latest，返回repo.Latest()。\nb）若请求后缀为.info\n使用modfetch.Stat函数，传入模块路径及版本号即可获取到修订信息，然后返回即可。\nc）若请求后缀为.mod\n使用modfetch.GoMod函数（源码中使用的是GoModFile，其实因需返回go.mod内容，使用GoMod即可），传入模块路径及版本号返回go.mod信息即可。\nd）若请求后缀为.zip\n使用modfetch.DownloadZip函数，其会返回文件路径，然后http.ServeFile即可。\n如上即为goproxy.io对Go Module Proxy的实现，总体思路较清晰，代码较简洁。使用goproxy.io或将该开源代码搭建至一台国外ECS上，基本可以解决国内用户对Go Module无法直接下载的问题。\n不过对于深度用户，有诸如权限管理、私有依赖管理等更高的需求。微软的工程师针对这些行业通用需求，实现了一个叫athens的工具，有机会可以学习一下。\n","permalink":"https://olzhy.github.io/posts/goproxyio.html","tags":["Golang"],"title":"Golang模块代理goproxy.io源码研读"},{"categories":["计算机"],"contents":"自Go 1.11引入Modules以来，其内置命令已集成包查询、下载等功能。\n之前专门写过一篇Golang Modules的文章，介绍了Module的使用方式。\n如一个Module工程，使用命令构建时会自动获取依赖，如：\n$ go build\rgo: finding github.com/olzhy/quote latest\rgo: downloading github.com/olzhy/quote v0.0.0-20190510033103-5cb7d4598cfa\rgo: extracting github.com/olzhy/quote v0.0.0-20190510033103-5cb7d4598cfa\r使用命令亦可查询最新可用版本:\n$ go get -u\rgo: finding github.com/olzhy/quote v1.0.0\rgo: downloading github.com/olzhy/quote v1.0.0\rgo: extracting github.com/olzhy/quote v1.0.0\r这些均是因为内置命令已集成了模块查询、获取的能力。支撑模块获取的一个关键的包即是“cmd/go/internal/modfetch”，本文将研读一下该包的几个关键的接口、结构体及函数。\n1 Repo接口与Lookup函数\nRepo表示一个仓库的一个模块存储的所有版本。\n$ go doc modfetch.Repo\r其接口定义如下，ModulePath返回模块路径；Versions列出给定前缀的语义学版本；Stat返回修订信息（可以是提交哈希、分支、标签等）；Latest返回默认分支的最新修订（仅用于没有标签的修订）； GoMod返回给定版本的go.mod信息；Zip将指定版本的压缩文件写到目标位置。\ntype Repo interface {\r// ModulePath returns the module path.\rModulePath() string\r// Versions lists all known versions with the given prefix.\r// Pseudo-versions are not included.\r// Versions should be returned sorted in semver order\r// (implementations can use SortVersions).\rVersions(prefix string) (tags []string, err error)\r// Stat returns information about the revision rev.\r// A revision can be any identifier known to the underlying service:\r// commit hash, branch, tag, and so on.\rStat(rev string) (*RevInfo, error)\r// Latest returns the latest revision on the default branch,\r// whatever that means in the underlying source code repository.\r// It is only used when there are no tagged versions.\rLatest() (*RevInfo, error)\r// GoMod returns the go.mod file for the given version.\rGoMod(version string) (data []byte, err error)\r// Zip writes a zip file for the given version to dst.\rZip(dst io.Writer, version string) error\r}\r接下来看一下如何获取到一个Module的Repo信息。\n$ go doc modfetch.Lookup\r其go doc如下，Lookup可以返回一个Module的Repo信息。\nfunc Lookup(path string) (Repo, error)\rLookup returns the module with the given module path. A successful return\rdoes not guarantee that the module has any defined versions.\r下面，我们使用其获取一下“github.com/olzhy/quote”这个Go Module的Repo信息。\n首先我的工作空间为workspace，在工作空间下，test.go文件位于github.com/olzhy/test下，目录结构为：\nworkspace\r└ github.com\r└ olzhy\r└ test\r└ test.go\r因modfetch包是internal包，不可直接引用，需将其拷贝至当前模块目录（github.com/olzhy/test）下，然后将codefetch包及其相关依赖拷贝进来，并将引用路径替换。\nshell脚本github.com/olzhy/test/copy_replace.sh内容如下：\n#!/bin/bash  mkdir internal # copy dependencies cp -r $GOROOT/src/cmd/go/internal/modfetch ./internal/ cp -r $GOROOT/src/cmd/go/internal/modfile ./internal/ cp -r $GOROOT/src/cmd/go/internal/modinfo ./internal/ cp -r $GOROOT/src/cmd/go/internal/base ./internal/ cp -r $GOROOT/src/cmd/go/internal/cache ./internal/ cp -r $GOROOT/src/cmd/go/internal/lockedfile ./internal/ cp -r $GOROOT/src/cmd/go/internal/module ./internal/ cp -r $GOROOT/src/cmd/go/internal/par ./internal/ cp -r $GOROOT/src/cmd/go/internal/renameio ./internal/ cp -r $GOROOT/src/cmd/go/internal/semver ./internal/ cp -r $GOROOT/src/cmd/go/internal/cfg ./internal/ cp -r $GOROOT/src/cmd/go/internal/str ./internal/ cp -r $GOROOT/src/cmd/go/internal/dirhash ./internal/ cp -r $GOROOT/src/cmd/go/internal/get ./internal/ cp -r $GOROOT/src/cmd/go/internal/web ./internal/ cp -r $GOROOT/src/cmd/go/internal/web2 ./internal/ cp -r $GOROOT/src/cmd/go/internal/load ./internal/ cp -r $GOROOT/src/cmd/go/internal/search ./internal/ cp -r $GOROOT/src/cmd/go/internal/work ./internal/ cp -r $GOROOT/src/cmd/internal/sys ./internal/ cp -r $GOROOT/src/cmd/internal/objabi ./internal/ cp -r $GOROOT/src/cmd/internal/buildid ./internal/ cp -r $GOROOT/src/cmd/internal/browser ./internal/ cp -r $GOROOT/src/internal/testenv ./internal/ cp -r $GOROOT/src/internal/singleflight ./internal/ cp -r $GOROOT/src/internal/xcoff ./internal/ # replace import paths find . -type f -name \u0026#34;*.go\u0026#34; -exec sed -i \u0026#39;\u0026#39; \u0026#39;s#cmd/go/internal/#github.com/olzhy/test/internal/#g\u0026#39; {} \\; find . -type f -name \u0026#34;*.go\u0026#34; -exec sed -i \u0026#39;\u0026#39; \u0026#39;s#cmd/internal/#github.com/olzhy/test/internal/#g\u0026#39; {} \\; find . -type f -name \u0026#34;*.go\u0026#34; -exec sed -i \u0026#39;\u0026#39; \u0026#39;s#internal/testenv#github.com/olzhy/test/internal/testenv#g\u0026#39; {} \\; find . -type f -name \u0026#34;*.go\u0026#34; -exec sed -i \u0026#39;\u0026#39; \u0026#39;s#internal/singleflight#github.com/olzhy/test/internal/singleflight#g\u0026#39; {} \\; find . -type f -name \u0026#34;*.go\u0026#34; -exec sed -i \u0026#39;\u0026#39; \u0026#39;s#internal/xcoff#github.com/olzhy/test/internal/xcoff#g\u0026#39; {} \\; 拷贝并替换完成后，我们在test.go（github.com/olzhy/test/test.go）使用一下modfetch.Lookup，代码如下：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;github.com/olzhy/test/internal/modfetch\u0026#34; \u0026#34;github.com/olzhy/test/internal/modfetch/codehost\u0026#34; ) func main() { // mod dir is $GOPATH/pkg/mod  modfetch.PkgMod = filepath.Join(os.Getenv(\u0026#34;GOPATH\u0026#34;), \u0026#34;pkg\u0026#34;, \u0026#34;mod\u0026#34;) // work dir is $GOPATH/pkg/mod/cache/vcs  codehost.WorkRoot = filepath.Join(modfetch.PkgMod, \u0026#34;cache\u0026#34;, \u0026#34;vcs\u0026#34;) repo, err := modfetch.Lookup(\u0026#34;github.com/olzhy/quote\u0026#34;) if nil != err { panic(err) } fmt.Println(repo.Latest()) } 使用modfetch.Lookup时需设置codehost.WorkRoot变量，即vcs下载的模块工作路径，一般为$GOPATH/pkg/mod/cache/vcs，如上代码获取“github.com/olzhy/quote”模块的最新提交信息，运行test.go，输出为：\ngo: finding github.com/olzhy/quote latest\r\u0026amp;{v0.0.0-20190515022821-f8e0536df3d4 2019-05-15 02:28:21 +0000 UTC } 然后看一下codehost.WorkRoot下新下载了什么：\n$ ls $GOPATH/pkg/mod/cache/vcs\r274f0d09769743d2dea3632161aca27cae4d90c87432a7984a434e7deeb6a244\r274f0d09769743d2dea3632161aca27cae4d90c87432a7984a434e7deeb6a244.info\r274f0d09769743d2dea3632161aca27cae4d90c87432a7984a434e7deeb6a244.lock\r可以看到modfetch.Lookup会请求vcs，获取包在master分支最新修正信息，并且下载至本地。\n2 RevInfo结构体及Stat函数\n$ go doc modfetch.RevInfo\rRev表示Module仓库的一个修订。其有版本名称Version及提交时间Time两个重要属性。\nStat函数可以返回指定Module路径的某次修订的具体信息。\ntype RevInfo struct {\rVersion string // version string\rTime time.Time // commit time\r// These fields are used for Stat of arbitrary rev,\r// but they are not recorded when talking about module versions.\rName string `json:\u0026quot;-\u0026quot;` // complete ID in underlying repository\rShort string `json:\u0026quot;-\u0026quot;` // shortened ID, for use in pseudo-version\r}\rA Rev describes a single revision in a module repository.\rfunc Stat(path, rev string) (*RevInfo, error)\r下面，我们使用其获取一下“github.com/olzhy/quote”这个Go Module版本v1.0.0的信息。\ntest.go main函数如下：\nfunc main() {\r// mod dir is $GOPATH/pkg/mod\rmodfetch.PkgMod = filepath.Join(os.Getenv(\u0026quot;GOPATH\u0026quot;), \u0026quot;pkg\u0026quot;, \u0026quot;mod\u0026quot;)\r// work dir is $GOPATH/pkg/mod/cache/vcs\rcodehost.WorkRoot = filepath.Join(modfetch.PkgMod, \u0026quot;cache\u0026quot;, \u0026quot;vcs\u0026quot;)\rstat, err := modfetch.Stat(\u0026quot;github.com/olzhy/quote\u0026quot;, \u0026quot;v1.0.0\u0026quot;)\rif nil != err {\rpanic(err)\r}\rfmt.Println(stat)\r}\r输出为：\n\u0026amp;{v1.0.0 2019-05-10 03:40:59 +0000 UTC }\r且若之前未获取过这个版本，其会将对应版本代码下载至$GOPATH/pkg/mod下。\n$ ls $GOPATH/pkg/mod/github.com/olzhy/\rquote@v1.0.0\r3 GoMod、DownloadZip函数\n$ go doc modfetch.GoMod\rGoMod类似于Lookup(path).GoMod(rev)，但其不会解析仓库路径从而请求网络而从版本控制网站来获取，而会先看本地缓存有没有。\nfunc GoMod(path, rev string) ([]byte, error)\rGoMod is like Lookup(path).GoMod(rev) but avoids the repository path\rresolution in Lookup if the result is already cached on local disk.\r下面，我们使用其获取一下“github.com/olzhy/quote”这个Module的go.mod内容。\ntest.go main函数如下：\nfunc main() { // mod dir is $GOPATH/pkg/mod  modfetch.PkgMod = filepath.Join(os.Getenv(\u0026#34;GOPATH\u0026#34;), \u0026#34;pkg\u0026#34;, \u0026#34;mod\u0026#34;) mod, err := modfetch.GoMod(\u0026#34;github.com/olzhy/quote\u0026#34;, \u0026#34;v1.0.0\u0026#34;) if nil != err { panic(err) } fmt.Println(string(mod)) } 输出为：\nmodule github.com/olzhy/quote\r即该模块go.mod的内容。\n最后看一下modfetch.DownloadZip的使用。\n$ go doc modfetch.DownloadZip\rDownloadZip有一个参数module.Version，其有两个属性Path与Version。\n对于指定模块，传入模块路径及版本信息，DownloadZip首先会看本地有没有，本地有直接返回文件名，否则会下载该模块至本地缓存并返回文件名。\nfunc DownloadZip(mod module.Version) (zipfile string, err error)\rDownloadZip downloads the specific module version to the local zip cache and\rreturns the name of the zip file.\r下面，我们使用其下载“github.com/olzhy/quote”这个Module的在版本v1.0.0的zip文件。\ntest.go main函数如下：\nfunc main() { // mod dir is $GOPATH/pkg/mod  modfetch.PkgMod = filepath.Join(os.Getenv(\u0026#34;GOPATH\u0026#34;), \u0026#34;pkg\u0026#34;, \u0026#34;mod\u0026#34;) zipfile, err := modfetch.DownloadZip(module.Version{Path: \u0026#34;github.com/olzhy/quote\u0026#34;, Version: \u0026#34;v1.0.0\u0026#34;}) if nil != err { panic(err) } fmt.Println(zipfile) } 输出为：\n/Users/larry/Documents/workspace/pkg/mod/cache/download/github.com/olzhy/quote/@v/v1.0.0.zip\r 参考资料\n[1] https://golang.org/pkg/cmd/go/internal/modfetch/\n ","permalink":"https://olzhy.github.io/posts/golang-modfetch-package.html","tags":["Golang"],"title":"Golang 模块获取包modfetch研读"},{"categories":["计算机"],"contents":"采用常规方式启动一个 Golang http 服务时，若服务被意外终止或中断，即未等待服务对现有请求连接处理并正常返回且亦未对服务停止前作一些必要的处理工作，这样即会造成服务硬终止。这种方式不是很优雅。\n参看如下代码，该 http 服务请求路径为根路径，请求该路径，其会在 2s 后返回 hello。\nvar addr = flag.String(\u0026#34;server addr\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;server address\u0026#34;) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { time.Sleep(2 * time.Second) fmt.Fprintln(w, \u0026#34;hello\u0026#34;) }) http.ListenAndServe(*addr, nil) } 若服务启动后，请求http://localhost:8080/，然后使用 Ctrl+C 立即中断服务，服务即会立即退出（exit status 2），请求未正常返回（ERR_CONNECTION_REFUSED），连接即马上断了。\n接下来介绍使用 http.Server 的 Shutdown 方法结合 signal.Notify 来优雅的终止服务。\n1 Shutdown 方法 Golang http.Server 结构体有一个终止服务的方法 Shutdown，其 go doc 如下。\nfunc (srv *Server) Shutdown(ctx context.Context) error Shutdown gracefully shuts down the server without interrupting any active connections. Shutdown works by first closing all open listeners, then closing all idle connections, and then waiting indefinitely for connections to return to idle and then shut down. If the provided context expires before the shutdown is complete, Shutdown returns the context\u0026#39;s error, otherwise it returns any error returned from closing the Server\u0026#39;s underlying Listener(s). When Shutdown is called, Serve, ListenAndServe, and ListenAndServeTLS immediately return ErrServerClosed. Make sure the program doesn\u0026#39;t exit and waits instead for Shutdown to return. Shutdown does not attempt to close nor wait for hijacked connections such as WebSockets. The caller of Shutdown should separately notify such long-lived connections of shutdown and wait for them to close, if desired. See RegisterOnShutdown for a way to register shutdown notification functions. Once Shutdown has been called on a server, it may not be reused; future calls to methods such as Serve will return ErrServerClosed. 由文档可知：\n使用 Shutdown 可以优雅的终止服务，其不会中断活跃连接。\n其工作过程为：首先关闭所有开启的监听器，然后关闭所有闲置连接，最后等待活跃的连接均闲置了才终止服务。\n若传入的 context 在服务完成终止前已超时，则 Shutdown 方法返回 context 的错误，否则返回任何由关闭服务监听器所引起的错误。\n当 Shutdown 方法被调用时，Serve、ListenAndServe 及 ListenAndServeTLS 方法会立刻返回 ErrServerClosed 错误。请确保 Shutdown 未返回时，勿退出程序。\n对诸如 WebSocket 等的长连接，Shutdown 不会尝试关闭也不会等待这些连接。若需要，需调用者分开额外处理（诸如通知诸长连接或等待它们关闭，使用 RegisterOnShutdown 注册终止通知函数）。\n一旦对 server 调用了 Shutdown，其即不可再使用了（会报 ErrServerClosed 错误）。\n有了 Shutdown 方法，我们知道在服务终止前，调用该方法即可等待活跃连接正常返回，然后优雅的关闭。\n关于上面用到的 Golang Context 参数，之前专门写过一篇文章介绍了 Context 的使用场景（请参考：Golang Context 使用小结）。\n但服务启动后的某一时刻，程序如何知道服务被中断了呢？服务被中断时如何通知程序，然后调用 Shutdown 作处理呢？接下来看一下系统信号通知函数的作用。\n2 signal.Notify 函数 signal 包的 Notify 函数提供系统信号通知的能力，其 go doc 如下。\nfunc Notify(c chan\u0026lt;- os.Signal, sig ...os.Signal) Notify causes package signal to relay incoming signals to c. If no signals are provided, all incoming signals will be relayed to c. Otherwise, just the provided signals will. Package signal will not block sending to c: the caller must ensure that c has sufficient buffer space to keep up with the expected signal rate. For a channel used for notification of just one signal value, a buffer of size 1 is sufficient. It is allowed to call Notify multiple times with the same channel: each call expands the set of signals sent to that channel. The only way to remove signals from the set is to call Stop. It is allowed to call Notify multiple times with different channels and the same signals: each channel receives copies of incoming signals independently. 由文档可知：\n参数 c 是调用者的信号接收通道，Notify 可将进入的信号转到 c。sig 参数为需要转发的信号类型，若不指定，所有进入的信号都将会转到 c。\n信号不会阻塞式的发给 c：调用者需确保 c 有足够的缓冲空间，以应对指定信号的高频发送。对于用于通知仅一个信号值的通道，缓冲大小为 1 即可。\n同一个通道可以调用 Notify 多次：每个调用扩展了发送至该通道的信号集合。仅可调用 Stop 来从信号集合移除信号。\n允许不同的通道使用同样的信号参数调用 Notify 多次：每个通道独立的接收进入信号的副本。\n综上，有了 signal.Notify，传入一个 chan 并指定中断参数，这样当系统中断时，即可接收到信号。\n参看如下代码，当使用 Ctrl+C 时，c 会接收到中断信号，程序会在打印“program interrupted”语句后退出。\nfunc main() { c := make(chan os.Signal) signal.Notify(c, os.Interrupt) \u0026lt;-c log.Fatal(\u0026#34;program interrupted\u0026#34;) } $ go run main.go Ctrl+C 2019/06/11 17:59:11 program interrupted exit status 1 3 Server 优雅的终止 接下来我们使用如上 signal.Notify 结合 http.Server 的 Shutdown 方法实现服务优雅的终止。\n如下代码，Handler 与文章开始时的处理逻辑一样，其会在 2s 后返回 hello。\n创建一个 http.Server 实例，指定端口与 Handler。\n声明一个 processed chan，其用来保证服务优雅的终止后再退出主 goroutine。\n新启一个 goroutine，其会监听 os.Interrupt 信号，一旦服务被中断即调用服务的 Shutdown 方法，确保活跃连接的正常返回（本代码使用的 Context 超时时间为 3s，大于服务 Handler 的处理时间，所以不会超时）。\n处理完成后，关闭 processed 通道，最后主 goroutine 退出。\n代码同时托管在 GitHub，欢迎关注（github.com/olzhy/go-exercises）。\nvar addr = flag.String(\u0026#34;server addr\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;server address\u0026#34;) func main() { // handler  handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { time.Sleep(2 * time.Second) fmt.Fprintln(w, \u0026#34;hello\u0026#34;) }) // server  srv := http.Server{ Addr: *addr, Handler: handler, } // make sure idle connections returned  processed := make(chan struct{}) go func() { c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt) \u0026lt;-c ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) defer cancel() if err := srv.Shutdown(ctx); nil != err { log.Fatalf(\u0026#34;server shutdown failed, err: %v\\n\u0026#34;, err) } log.Println(\u0026#34;server gracefully shutdown\u0026#34;) close(processed) }() // serve  err := srv.ListenAndServe() if http.ErrServerClosed != err { log.Fatalf(\u0026#34;server not gracefully shutdown, err :%v\\n\u0026#34;, err) } // waiting for goroutine above processed  \u0026lt;-processed } ","permalink":"https://olzhy.github.io/posts/golang-shutdown-server-gracefully.html","tags":["Golang"],"title":"Golang 优雅的终止一个服务"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个非负整数数组，您初始位于数组的第一个位置。\n数组中的每个元素，代表您在该位置可以跳跃的最大长度。\n请判断您能否抵达数组的最后一个位置。\n例子1：\n输入：[2,3,1,1,4]\n输出：true\n释义：从位置0跳一步到位置1，然后跳3步抵达最终位置。\n例子2：\n输入：[3,2,1,0,4]\n输出：false\n释义：不论怎么跳都会抵达位置3，因位置3可跳跃的最大长度为0，所以到不了最终位置。\n题目出处：LeetCode\n2 解决思路\n声明一个变量reach，表示当前可以抵达的最远位置。\n从左到右遍历数组，判断当前位置加上当前值（即最大跳跃长度）是否大于reach，大于则扩展reach，否则跳到下一个（特殊情况是元素为0，则判断reach是否大于当前元素所在位置，大于则下一个，否则退出）。\n在遍历的时候，若reach已能抵达最后一个位置，则跳出遍历，直接返回true。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc canJump(nums []int) bool { reach := 0 for i, num := range nums { if reach \u0026gt;= len(nums)-1 { return true } if 0 == num \u0026amp;\u0026amp; reach \u0026lt;= i { return false } if i+num \u0026gt; reach { reach = i + num } } return false } ","permalink":"https://olzhy.github.io/posts/leetcode-jump-game.html","tags":["Golang","算法"],"title":"LeetCode 55 跳跃游戏"},{"categories":["计算机"],"contents":"1 题目描述\n请设计对双端队列的实现。\n实现需支持如下操作：\n  a）MyCircularDeque(k): 构造器，设置双端队列的容量\n  b）insertFront(): 在头部插入元素，若操作成功则返回true\n  c）insertLast(): 在尾部插入元素，若操作成功则返回true\n  d）deleteFront(): 删除头部元素，若操作成功则返回true\n  e）deleteLast(): 删除尾部元素，若操作成功则返回true\n  f）getFront(): 查询头部元素，若队列为空返回-1\n  g）getRear(): 查询尾部元素，若队列为空返回-1\n  h）isEmpty(): 队列是否为空\n  i）isFull(): 队列是否已满\n  例子：\nMyCircularDeque circularDeque = new MycircularDeque(3); // set the size to be 3\rcircularDeque.insertLast(1);\t// return true\rcircularDeque.insertLast(2);\t// return true\rcircularDeque.insertFront(3);\t// return true\rcircularDeque.insertFront(4);\t// return false, the queue is full\rcircularDeque.getRear(); // return 2\rcircularDeque.isFull();\t// return true\rcircularDeque.deleteLast();\t// return true\rcircularDeque.insertFront(4);\t// return true\rcircularDeque.getFront();\t// return 4\r注：\n  a）所有元素值位于区间[0,1000]；\n  b）所有操作个数位于区间[1,1000]；\n  c）请勿直接使用内置双端队列实现。\n  题目出处：LeetCode\n2 简版解决思路及代码\n使用内置slice数据结构，取头或取尾、在头插入，在尾插入都有现成函数，实现起来简单，但性能不佳。\ntype MyCircularDeque struct { stores []int cap int } func Constructor(k int) MyCircularDeque { return MyCircularDeque{cap: k} } func (this *MyCircularDeque) InsertFront(value int) bool { if this.cap == len(this.stores) { return false } this.stores = append([]int{value}, this.stores...) return true } func (this *MyCircularDeque) InsertLast(value int) bool { if this.cap == len(this.stores) { return false } this.stores = append(this.stores, value) return true } func (this *MyCircularDeque) DeleteFront() bool { if 0 == len(this.stores) { return false } this.stores = this.stores[1:] return true } func (this *MyCircularDeque) DeleteLast() bool { if 0 == len(this.stores) { return false } this.stores = this.stores[:len(this.stores)-1] return true } func (this *MyCircularDeque) GetFront() int { if 0 == len(this.stores) { return -1 } return this.stores[0] } func (this *MyCircularDeque) GetRear() int { if 0 == len(this.stores) { return -1 } return this.stores[len(this.stores)-1] } func (this *MyCircularDeque) IsEmpty() bool { return 0 == len(this.stores) } func (this *MyCircularDeque) IsFull() bool { return this.cap == len(this.stores) } 3 优化版解决思路及代码\n因插入仅在头部或尾部，查询也仅在头部或尾部，所以使用双向链表数据结构实现较为合适。\n双端队列只要记录链表的头指针和尾指针即可，这样查询或者插入非常效率。\nhttps://github.com/olzhy/\ntype node struct { val int pre *node next *node } type MyCircularDeque struct { front *node rear *node len int cap int } func Constructor(k int) MyCircularDeque { return MyCircularDeque{\u0026amp;node{}, \u0026amp;node{}, 0, k} } func (this *MyCircularDeque) InsertFront(value int) bool { if this.cap == this.len { return false } if 0 == this.len { this.front = \u0026amp;node{val: value} this.rear = this.front } else { front := this.front this.front = \u0026amp;node{val: value, next: front} front.pre = this.front } this.len++ return true } func (this *MyCircularDeque) InsertLast(value int) bool { if this.cap == this.len { return false } if 0 == this.len { this.rear = \u0026amp;node{val: value} this.front = this.rear } else { rear := this.rear this.rear = \u0026amp;node{val: value, pre: rear} rear.next = this.rear } this.len++ return true } func (this *MyCircularDeque) DeleteFront() bool { if 0 == this.len { return false } next := this.front.next if nil == next { this.front = nil this.rear = nil } else { this.front = this.front.next this.front.pre = nil } this.len-- return true } func (this *MyCircularDeque) DeleteLast() bool { if 0 == this.len { return false } pre := this.rear.pre if nil == pre { this.rear = nil this.front = nil } else { this.rear = this.rear.pre this.rear.next = nil } this.len-- return true } func (this *MyCircularDeque) GetFront() int { if 0 == this.len { return -1 } return this.front.val } func (this *MyCircularDeque) GetRear() int { if 0 == this.len { return -1 } return this.rear.val } func (this *MyCircularDeque) IsEmpty() bool { return 0 == this.len } func (this *MyCircularDeque) IsFull() bool { return this.cap == this.len } ","permalink":"https://olzhy.github.io/posts/leetcode-design-circular-deque.html","tags":["Golang","算法"],"title":"LeetCode 641 设计循环双端队列"},{"categories":["计算机"],"contents":"1 题目描述\n给定两个序列pushed与popped，每个序列内的值均是不同的。对于一个空的栈，当前仅当其是有效的push与pop操作序列时返回true。\n例子1：\n输入：pushed = [1,2,3,4,5], popped = [4,5,3,2,1]\n输出：true\n释义：\n我们可能做如下操作：\npush(1)，push(2)，push(3)，push(4)，pop()-\u0026gt;4,\npush(5)，pop()-\u0026gt;5，pop()-\u0026gt;3，pop()-\u0026gt;2，pop()-\u0026gt;1。\n例子2：\n输入：pushed = [1,2,3,4,5], popped = [4,3,5,1,2]\n输出：false\n释义：\n1不可在2之前弹出。\n注：\n  a）0 \u0026lt;= pushed.length == popped.length \u0026lt;= 1000；\n  b）0 \u0026lt;= pushed[i], popped[i] \u0026lt; 1000；\n  c）pushed是popped的一个排列；\n  d）pushed与popped的值均是不同的。\n  题目出处：LeetCode\n2 解决思路\n首先实现一个栈，判断压栈与弹栈序列是否有效需要借助栈数据结构。\n遍历弹栈序列popped：\na）对于当前popped序列元素pop，若其与栈顶元素相等，则弹栈；\nb）若其与栈顶元素不相等，则将pushed元素压栈，直至遇到与pop相等的元素，然后跳到popped遍历的下一次循环；\nc）若对popped的某次循环，pop与栈顶元素不相等且pushed序列已遍历完，则说明是一个无效的序列，直接返回false。若遍历直至popped完成且这时栈也刚好为空，说明是有效序列，返回true。\n3 Golang实现代码\nhttps://github.com/olzhy/\ntype stack struct { stores []int } func (s *stack) top() int { return s.stores[len(s.stores)-1] } func (s *stack) pop() { s.stores = s.stores[:len(s.stores)-1] } func (s *stack) push(e int) { s.stores = append(s.stores, e) } func (s *stack) len() int { return len(s.stores) } func validateStackSequences(pushed []int, popped []int) bool { s := new(stack) i, j := 0, 0 for ; i \u0026lt; len(popped); i++ { pop := popped[i] if s.len() \u0026gt; 0 \u0026amp;\u0026amp; s.top() == pop { s.pop() continue } if j == len(pushed) { return false } for j \u0026lt; len(pushed) { e := pushed[j] if pop != e { s.push(e) j++ continue } j++ break } } return 0 == s.len() \u0026amp;\u0026amp; len(popped) == i } ","permalink":"https://olzhy.github.io/posts/leetcode-validate-stack-sequences.html","tags":["Golang","算法"],"title":"LeetCode 946 校验栈序列"},{"categories":["计算机"],"contents":"1 题目描述\n在二维数组grid中，每个值grid[i][j]代表位于此的建筑物高度。我们允许对其中的任意建筑物增长不等的高度。高度0仍为一个有效的建筑物。 增高后的建筑群，从其四个方向来看，必须与之前建筑群的天际线保持一致。城市天际线是从远处观看时，由所有建筑组成的外形轮廓。请看如下例子。\n请计算所有建筑物可以增长的最大总和。\n例子：\n输入：\ngrid = [[3,0,8,4],[2,4,5,7],[9,2,6,3],[0,3,1,0]] 输出：35\n释义：\n二维矩阵为： [ [3, 0, 8, 4], [2, 4, 5, 7], [9, 2, 6, 3], [0, 3, 1, 0] ] 从左到右看，天际线为：[9, 4, 8, 7] 从前到后看，天际线为：[8, 7, 9, 3] 在不影响天际线情况下，增长高度后的新矩阵为： gridNew = [ [8, 4, 8, 7], [7, 4, 7, 7], [9, 4, 8, 7], [3, 3, 3, 3] ] 总增长为35。 题目出处：LeetCode\n2 解决思路\n欲保持天际线不变，对于每个建筑来说，其最大高度为其所在行最高与所在列最高的较小值。\n所以如下算法，先遍历一遍矩阵，计算出行最高数组与列最高数组。\n然后再遍历一遍矩阵，根据如上两个数组计算各个建筑的最大可增长高度，最后返回总增加高度。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc min(x, y int) int { if x \u0026lt; y { return x } return y } func maxIncreaseKeepingSkyline(grid [][]int) int { rowMaxs := make([]int, len(grid)) colMaxs := make([]int, len(grid[0])) for i := 0; i \u0026lt; len(grid); i++ { for j := 0; j \u0026lt; len(grid[0]); j++ { e := grid[i][j] if e \u0026gt; rowMaxs[i] { rowMaxs[i] = e } if e \u0026gt; colMaxs[j] { colMaxs[j] = e } } } increments := 0 for i := 0; i \u0026lt; len(grid); i++ { for j := 0; j \u0026lt; len(grid[0]); j++ { e := grid[i][j] increments += (min(rowMaxs[i], colMaxs[j]) - e) } } return increments } ","permalink":"https://olzhy.github.io/posts/leetcode-max-increase-to-keep-city-skyline.html","tags":["Golang","算法"],"title":"LeetCode 807 求保持城市现有天际线的最大增高"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个二叉树，返回节点值先序遍历数组。\n注：勿使用递归，请使用循环解决。\n例子：\n输入：[1,null,2,3]\n 1 \\ 2 / 3 输出：[1,2,3]\n题目出处：LeetCode\n2 解决思路\n使用循环遍历时，按先序遍历规则，遍历根后，需先遍历其左子树，这样循环对左子树递进时，右子树暂时得不到遍历，所以需要将待遍历的右子树先记录下来，而这些右子树中，先记录的后遍历。\n如下代码使用nodes slice记录待遍历的右子树，首先，将root整个认为是一个右子树放进去。\n若nodes slice不为空，先取nodes slice最后一个节点，遍历其左子树，若右子树不为空，仅将右子树加入nodes slice，循环直至所有左子树遍历完成，然后进入slice的下一次循环，直至slice为空。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc preorderTraversal(root *TreeNode) []int { var vals []int if nil == root { return vals } // represent for right nodes waiting for traversal  nodes := []*TreeNode{root} for len(nodes) \u0026gt; 0 { node := nodes[len(nodes)-1] for p := node; nil != p; p = p.Left { vals = append(vals, p.Val) if node == p { nodes = nodes[:len(nodes)-1] } if nil != p.Right { nodes = append(nodes, p.Right) } } } return vals } ","permalink":"https://olzhy.github.io/posts/leetcode-binary-tree-preorder-traversal.html","tags":["Golang","算法"],"title":"LeetCode 144 二叉树先序遍历"},{"categories":["计算机"],"contents":"Go 1.9，在1.8发布6个月后如约而至，其是Go 1.x系列的第10个版本。\n该版本有两项语言级变化：增加了类型别名支持以及对可能熔断浮点操作实现的定义。\n多数变化在工具链、运行时及库的实现上。一如既往，Go 1.9坚持Go 1兼容性准则。\n该版本增加了透明的单调时间支持，同一包内的函数并行编译，以及对测试工具函数的更好支持，引入了一个新的位操作包，引入了一个新的并发Map类型。\n1 语言方面\nGo目前引入了类型别名，当在包间移动一个类型时，以支持渐进代码修补。\n类型别名定义：\ntype T1 = T2\r该声明为T2类型增加了一个新的别名T1，两者指向同一类型。\n另一个小的语言级变化：当实现允许熔断浮点操作时，诸如通过使用体系结构“熔断乘与加”（FMA）指令来计算xy + z，而未对xy直接结果取整。欲强制直接取整，写作float64(x*y) + z。\n2 工具方面\n 并行编译 目前Go编译器支持对一个包内的函数利用多核进行并行编译。  之前Go命令已支持对不同包的并行编译。并行编译默认开启，若想关闭，可设置环境变量GO19CONCURRENTCOMPILATION为0。\n 文档 参数列表太长，会被截断，这样提升了某些自动生成代码的go doc可读性。  目前支持结构体字段文档查看，如：go doc http.Client.Jar。\n  环境信息 使用go env -json可以将环境信息以json格式输出。\n  测试 使用go test -list，然后跟一个正则表达式，可以列出与之匹配的测试名称。\n  3 运行时方面\n调用栈包含内联帧。\n使用runtime.Caller可以获取单次调用的信息，使用runtime.CallersFrames可以获取调用栈完整视图。\n不建议使用runtime.Callers查询PC slice，然后遍历其调用runtime.FuncForPC等来单独查询PC，这样可能丢失调用栈的内联帧。\n接下来看一个使用的例子：\n封装一个err函数，用于记录错误发生时的堆栈信息，其内部使用runtime.Callers、runtime.CallersFrames来获取调用栈的帧信息。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;runtime\u0026#34; ) func f2() { f3 := func() { err(\u0026#34;error\u0026#34;) } f3() } func f1() { f2() } func err(err string) { fmt.Printf(\u0026#34;err: %s, call stack:\\n\u0026#34;, err) ptrs := make([]uintptr, 32) count := runtime.Callers(2, ptrs) frames := runtime.CallersFrames(ptrs[:count]) blanks := \u0026#34; \u0026#34; for { frame, more := frames.Next() if !more { break } fmt.Printf(\u0026#34;%s%s:%d, func: %s\\n\u0026#34;, blanks, frame.File, frame.Line, frame.Function) blanks += \u0026#34; \u0026#34; } } func main() { f1() } 运行结果为：\nerr: error, call stack:\r~/workspace/src/github.com/larry/test/test.go:10, func: main.f2.func1\r~/workspace/src/github.com/larry/test/test.go:12, func: main.f2\r~/workspace/src/github.com/larry/test/test.go:16, func: main.f1\r~/workspace/src/github.com/larry/test/test.go:36, func: main.main\r~/go/src/runtime/proc.go:200, func: runtime.main\r若err函数使用不推荐的方式实现（使用runtime.Callers获取PC slice，再遍历其调用runtime.FuncForPC获取具体信息），代码如下：\nfunc err(err string) { fmt.Printf(\u0026#34;err: %s, call stack:\\n\u0026#34;, err) ptrs := make([]uintptr, 32) count := runtime.Callers(2, ptrs) blanks := \u0026#34; \u0026#34; for _, ptr := range ptrs[:count] { funcForPC := runtime.FuncForPC(ptr) f, line := funcForPC.FileLine(ptr) fmt.Printf(\u0026#34;%s%s:%d, func: %s\\n\u0026#34;, blanks, f, line, funcForPC.Name()) blanks += \u0026#34; \u0026#34; } } 运行结果为：\nerr: error, call stack:\r~/workspace/src/github.com/larry/test/test.go:10, func: main.f2\r~/workspace/src/github.com/larry/test/test.go:12, func: main.f2\r~/workspace/src/github.com/larry/test/test.go:16, func: main.main\r~/workspace/src/github.com/larry/test/test.go:16, func: main.f1\r~/go/src/runtime/proc.go:209, func: runtime.main\r~/go/src/runtime/asm_amd64.s:1338, func: runtime.goexit\r可以看到运行结果是不一样的。\n4 性能方面\n一如既往，因变动较广，所以较难对性能作精准陈述。因垃圾收集器优化、代码生成的更好以及核心库优化，绝大多数程序会运行的快一点。\n用于触发“世界静止”的垃圾收集库函数，目前用于触发并发垃圾收集。特别地， runtime.GC、debug.SetGCPercent及debug.FreeOSMemory目前用于触发并发垃圾收集，仅会阻塞调用goroutine直至收集完成。\n大对象的分配性能有较大提升。runtime.ReadMemStats函数目前即便对非常大的堆，耗时仍低于100µs。\n5 核心库方面\n  透明的单调时间支持 time包目前透明地在每个Time值追踪单调时间，这样即便在时钟调整时，计算两个Time值的时间间隔是一个安全操作。\n  新的位操作包 Go 1.9引入了一个新包math/bits，包含对位操作的优化实现。\n  在多数体系结构上，该包的函数会被编译器特别对待，且被认为是内部指令，所以有很高的性能。\n使用方式参看如下一个例子：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math/bits\u0026#34; ) func main() { fmt.Printf(\u0026#34;len: %d\\n\u0026#34;, bits.Len(33)) fmt.Printf(\u0026#34;onesCount: %d\\n\u0026#34;, bits.OnesCount(7)) fmt.Printf(\u0026#34;leadingZeros: %d\\n\u0026#34;, bits.LeadingZeros(7)) fmt.Printf(\u0026#34;reverse: %d\\n\u0026#34;, bits.Reverse8(1)) } 输出为：\nlen: 6\ronesCount: 3\rleadingZeros: 61\rreverse: 128\r 并发Map sync.Map是一个并发map，存、取以及删仅使用常数时间。该Map的方法对多goroutine并发调用是安全的。  使用方式参看如下一个例子：\n该代码申明一个sync.Map类型的m，启动10个goroutine同时给m赋值，然后遍历将key，value打印。\n并发赋值未报错误。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { var m sync.Map var wg sync.WaitGroup for i := 0; i \u0026lt; 10; i++ { wg.Add(1) go func(k int) { m.Store(k, k) wg.Done() }(i) } wg.Wait() m.Range(func(key, value interface{}) bool { fmt.Printf(\u0026#34;key: %v, value: %v\\n\u0026#34;, key, value) return true }) } 输出为：\nkey: 0, value: 0\rkey: 5, value: 5\rkey: 3, value: 3\rkey: 2, value: 2\rkey: 1, value: 1\rkey: 4, value: 4\rkey: 6, value: 6\rkey: 7, value: 7\rkey: 8, value: 8\rkey: 9, value: 9\r若该代码使用基础map类型，可以写作：\n运行时，会报并发赋值错误。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) func main() { m := make(map[int]int, 10) var wg sync.WaitGroup for i := 0; i \u0026lt; 10; i++ { wg.Add(1) go func(k int) { m[k] = k wg.Done() }(i) } wg.Wait() for i := 0; i \u0026lt; 10; i++ { fmt.Printf(\u0026#34;key: %v, value: %v\\n\u0026#34;, i, m[i]) } } 输出为：\nfatal error: concurrent map writes\rgoroutine 19 [running]:\rruntime.throw(0x4c9a1f, 0x15)\r~/go/src/runtime/panic.go:617 +0x79 fp=0xc00008df58 sp=0xc00008df28 pc=0x42b419\rruntime.mapassign_fast64(0x4abbc0, 0xc00006c300, 0x0, 0x0)\r~/go/src/runtime/map_fast64.go:176 +0x34b fp=0xc00008df98 sp=0xc00008df58 pc=0x40f6bb\rmain.main.func1(0xc00006c300, 0xc000054080, 0x0)\r~/workspace/go/src/github.com/larry/test/test.go:14 +0x48 fp=0xc00008dfc8 sp=0xc00008df98 pc=0x4931e8\rruntime.goexit()\r~/go/src/runtime/asm_amd64.s:1337 +0x1 fp=0xc00008dfd0 sp=0xc00008dfc8 pc=0x4520e1\rcreated by main.main\r~/workspace/go/src/github.com/larry/test/test.go:13 +0xcd\r 参考资料\n[1] https://golang.org/doc/go1.9\n ","permalink":"https://olzhy.github.io/posts/go1dot9-release-notes.html","tags":["Golang"],"title":"Go 1.9 Release Notes 要点整理"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个数组，该数组有N个对象，每个对象被标记为红、白、蓝三种颜色中的某一种。对该对象数组进行排序，使相同颜色的对象连在一起，分别为红色部分，白色部分，蓝色部分。\n这里，我们将数字1，2，3分别代表红，白，蓝。\n注：请勿使用sort库函数。\n例子:\n输入：[2,0,2,1,1,0]\n输出：[0,0,1,1,2,2]\n进阶：\n  a）一个直接的解法是先计数再排序，使用了两次遍历来实现，即先遍历一次数组算出所有1，2，3的个数，然后再遍历一次，分别将对应个数的1，2，3填进去；\n  b）您能否提出一种一次遍历且使用常数空间的算法。\n  题目出处：LeetCode\n2 解决思路\n总体思路为：遍历数组，遇到0，将其排到头部，遇到2，则将其排到尾部，这样遍历一次即可将3类数都排好。\n下面为具体步骤：\n  a）首先申请3个变量，i、start与end，i为当前元素指针，用start标记头部1的头，end标记尾部2的头。\n  b）从头遍历数组，若i指的元素为0，若其前边有1（即i\u0026gt;start），则与start对应的1的头交换，然后start后移，i也后移；若其前边没有1，无需处理，start和i后移即可。\n  c）若i指的元素为2，则看其与end对应元素比谁大，若其比end对应元素大，则两元素交换，然后end前移一位；若其与end对应元素相等，end亦前移一位。\n  d）若i指的元素为1，仅i后移，去查看下一个元素。\n  e）i遍历至end的位置，则完成排序。\n  整个算法时间复杂度为O(N)，空间复杂度为O(1)。\n3 Golang实现代码\nhttps://github.com/olzhy/\nfunc sortColors(nums []int) { swap := func(nums []int, i, j int) { nums[i], nums[j] = nums[j], nums[i] } start, end := 0, len(nums)-1 for i := start; i \u0026lt;= end; { switch nums[i] { case 0: if start \u0026lt; i { swap(nums, i, start) } start++ i++ case 1: i++ case 2: if nums[end] \u0026lt; 2 { swap(nums, i, end) } end-- } } } ","permalink":"https://olzhy.github.io/posts/leetcode-sort-colors.html","tags":["Golang","算法"],"title":"LeetCode 75 颜色排序"},{"categories":["计算机"],"contents":"1 题目描述\n对m x n整数矩阵，写一个对某值进行高效搜索的算法。该矩阵有如下特征：\na）每行数值自左向右是有序的；\nb）每行第一个数比上一行最后一个数大。\n例子1：\n输入：\nmatrix = [\n[1, 3, 5, 7],\n[10, 11, 16, 20],\n[23, 30, 34, 50]\n]\ntarget = 3\n输出：true\n例子2：\n输入：\nmatrix = [\n[1, 3, 5, 7],\n[10, 11, 16, 20],\n[23, 30, 34, 50]\n]\ntarget = 13\n输出：false\n题目出处：LeetCode\n2 解决思路\na）对于给定目标值，首先对二维矩阵每行在左边第一列进行二分搜索，以确定目标值在哪一行；\nb）确定目标值在哪一行后，然后在该行进行二分搜索即可。\n该算法的整体复杂度为O(log(M))+O(log(N))。\n下面先温故一下对整数数组的二分搜索。\n3 温故二分搜索\nfunc binarySearch(nums []int, target int) int { start, end := 0, len(nums)-1 for start \u0026lt;= end { mid := (start + end) / 2 if target == nums[mid] { return mid } if target \u0026lt; nums[mid] { end = mid - 1 continue } start = mid + 1 } return -1 } 这样针对本题目，进行两次二分搜索即可。\n4 算法Golang完整实现代码\nhttps://github.com/olzhy/\nfunc searchMatrix(matrix [][]int, target int) bool { if len(matrix) \u0026lt; 1 || len(matrix[0]) \u0026lt; 1 { return false } // binary search for row\u0026#39;s first elems  rowsStart, rowsEnd := 0, len(matrix)-1 for rowsStart \u0026lt;= rowsEnd { rowsMid := (rowsStart + rowsEnd) / 2 if target \u0026lt; matrix[rowsMid][0] { rowsEnd = rowsMid - 1 continue } if target \u0026gt; matrix[rowsMid][len(matrix[0])-1] { rowsStart = rowsMid + 1 continue } // binary search for current row  colsStart, colsEnd := 0, len(matrix[0])-1 for colsStart \u0026lt;= colsEnd { colsMid := (colsStart + colsEnd) / 2 switch { case target == matrix[rowsMid][colsMid]: return true case target \u0026lt; matrix[rowsMid][colsMid]: colsEnd = colsMid - 1 case target \u0026gt; matrix[rowsMid][colsMid]: colsStart = colsMid + 1 } } return false } return false } ","permalink":"https://olzhy.github.io/posts/leetcode-search-a-2d-matrix.html","tags":["Golang","算法"],"title":"LeetCode 74 二维矩阵搜索"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个类Unix绝对路径，请将其简化，换言之，请将其转换为“canonical”路径。 在类Unix文件系统中，“.”代表当前目录，“..”代表移至上层目录。注意返回的简化路径需以“/”开头，而且两个文件夹名称之间须有分隔符“/”，最后一个文件夹名称（若有的话），不能以“/”结尾。此外，简化路径须是绝对路径的最短表示。\n例子1：\n  输入：\u0026quot;/home/\u0026quot;\n  输出：\u0026quot;/home\u0026quot;\n  释义：最后一个文件夹名称不能以“/”结尾。\n  例子2：\n  输入：\u0026quot;/../\u0026quot;\n  输出：\u0026quot;/\u0026quot;\n  释义：根目录“/”没有上层目录，无需处理，返回根目录即可。\n  例子3：\n  输入：\u0026quot;/home//foo/\u0026quot;\n  输出：\u0026quot;/home/foo\u0026quot;\n  释义：两个连续的“/”分隔符仅表示一个“/”。\n  例子4：\n  输入：\u0026quot;/a/./b/../../c/\u0026quot;\n  输出：\u0026quot;/c\u0026quot;\n  例子5：\n  输入：\u0026quot;/a/../../b/../c//.//\u0026quot;\n  输出：\u0026quot;/c\u0026quot;\n  例子6：\n  输入：\u0026quot;/a//b////c/d//././/..\u0026quot;\n  输出：\u0026quot;/a/b/c\u0026quot;\n  例子7（重要）：\n  输入：\u0026quot;/\u0026hellip;../\u0026quot;\n  输出：\u0026quot;/\u0026hellip;../\u0026quot;\n  释义：“\u0026hellip;..”是一个符合规范的文件夹名称。\n  题目出处：LeetCode\n2 解决思路\n简化该path，需处理几种情形：\n  a）“./”代表当前目录；\n  b）“../”或者最末尾的“..”代表移至上层目录；\n  c）“//”只表示一个“/”；\n  d）文件夹名称，如“a”，“aaa”等，特别注意“a..a”以及前后没有分隔符的“\u0026hellip;”，“\u0026hellip;.”等也是满足要求的；\n  e）“x/”，分割符。\n  如下代码，简化路径首先append头字符，然后从第二个字符开始遍历path，然后判断邻近的两个字符，满足如上4种情形的哪一种：\n  a）若是“//”或“./”，不作处理；\n  b）若是“/.”，则判断接下来的字符是否为“/”或“.”，若是，交给下一次循环，其会落入“./”或“..”；若不是，则append该字符到简化路径；\n  c）若是“..”，则判断接下来的字符，若接下来的字符不是“/”，则说明是正常文件夹名称，append该字符到简化路径直至遇到分隔符“/”；反之，若接下来的字符是“/”或者走到了最后，则说明需返回上层目录，处理时需找到上一个分隔符，也要注意当前路径是否可以返回上层目录，特殊情况作特殊处理；\n  d）其他情形，直接append即可。\n  3 Golang实现代码\nhttps://github.com/olzhy/\nfunc simplifyPath(path string) string { if len(path) \u0026lt; 2 { return path } canonicalPath := path[:1] for i := 1; i \u0026lt; len(path); { switch path[i-1 : i+1] { case \u0026#34;//\u0026#34;, \u0026#34;./\u0026#34;: case \u0026#34;/.\u0026#34;: if i+1 \u0026lt; len(path) \u0026amp;\u0026amp; \u0026#39;/\u0026#39; != path[i+1] \u0026amp;\u0026amp; \u0026#39;.\u0026#39; != path[i+1] { canonicalPath += \u0026#34;.\u0026#34; } case \u0026#34;..\u0026#34;: // valid directory name  if i+1 \u0026lt; len(path) \u0026amp;\u0026amp; \u0026#39;/\u0026#39; != path[i+1] { canonicalPath += \u0026#34;..\u0026#34; i++ for ; i \u0026lt; len(path) \u0026amp;\u0026amp; \u0026#39;/\u0026#39; != path[i]; i++ { canonicalPath += string(path[i]) } continue } // move directory up a level  if len(canonicalPath) \u0026gt; 1 \u0026amp;\u0026amp; \u0026#39;/\u0026#39; == canonicalPath[len(canonicalPath)-1] { j := len(canonicalPath) - 2 for ; j \u0026gt; 0; j-- { if \u0026#39;/\u0026#39; == canonicalPath[j] { break } } canonicalPath = canonicalPath[:j+1] } default: canonicalPath += string(path[i]) } i++ } // trim last \u0026#39;/\u0026#39;  if len(canonicalPath) \u0026gt; 1 \u0026amp;\u0026amp; \u0026#39;/\u0026#39; == canonicalPath[len(canonicalPath)-1] { canonicalPath = canonicalPath[:len(canonicalPath)-1] } return canonicalPath ","permalink":"https://olzhy.github.io/posts/leetcode-simplify-path.html","tags":["Golang","算法"],"title":"LeetCode 71 简化路径"},{"categories":["计算机"],"contents":"Go 2的总体目标是在辅助工程扩展为大的代码基线时做到游刃有余。\n通常，我们的Go程序有很多错误检查，但缺少错误处理。我们通常使用如下代码所示的赋值判断语句进行错误检查。\nif _, err := io.Copy(w, r); nil != err { return err } 这样写起来较繁琐，设计草案旨在引入一种轻量的语法来进行错误检查以解决当前的这些问题。\n1 当前问题\nGo 使用的是对显式错误结果的显式错误检查，而其他异常处理型语言（诸如C++，C#，Java等）使用的是对隐式结果进行隐式检查。对于异常处理型语言的处理方式，因我们全然看不到隐式检查，所以难以验证程序是否正确恢复到检查失败时的状态。\n下面是一个错误检查较完整的文件拷贝代码，其错误处理的重点在于当io.Copy或w.Close失败时，应移除写了一半的dst文件。\nfunc CopyFile(src, dst string) error { r, err := os.Open(src) if err != nil { return fmt.Errorf(\u0026#34;copy %s %s: %v\u0026#34;, src, dst, err) } defer r.Close() w, err := os.Create(dst) if err != nil { return fmt.Errorf(\u0026#34;copy %s %s: %v\u0026#34;, src, dst, err) } if _, err := io.Copy(w, r); err != nil { w.Close() os.Remove(dst) return fmt.Errorf(\u0026#34;copy %s %s: %v\u0026#34;, src, dst, err) } if err := w.Close(); err != nil { os.Remove(dst) return fmt.Errorf(\u0026#34;copy %s %s: %v\u0026#34;, src, dst, err) } } 该代码较健壮，但不够整洁，也不够优雅。\n2 目标\n减少大量错误检查代码，使错误检查更轻量，使错误处理更便捷。\n不重蹈异常处理的覆辙，错误检查及错误处理应继续保持显式的方式。\n兼容现有代码。\n3 草案概览\n设计草案引入了两个新的关键字，check与handle，分别进行错误检查与错误处理。使用check f(x, y, z)或check err进行显式错误检查。使用hande语句进行错误处理器的定义。当错误检查失败时，其转向到最里边的Handler，最里边的Handler又转向到其上的下一个Handler，直至某一个Handler执行了return语句。\n例如，依照设计草案，如上代码可以改进为更简短的方式：\nfunc CopyFile(src, dst string) error { handle err { return fmt.Errorf(\u0026#34;copy %s %s: %v\u0026#34;, src, dst, err) } r := check os.Open(src) defer r.Close() w := check os.Create(dst) handle err { w.Close() os.Remove(dst) // (only if a check fails)  } check io.Copy(w, r) check w.Close() return nil } 4 草案详情\n check check可用于error类型的表达式或者返回一组值且最后一个值为error类型的函数调用。  给定变量v1, v2, …, vN, vErr，\nv1, ..., vN := check /expr/ 其等价于：\nv1, ..., vN, vErr := /expr/ if vErr != nil { /error result/ = handlerChain(vn) return } vErr必须为error类型。\n类似，\nfoo(check /expr/) 等价于：\nv1, ..., vN, vErr := /expr/ if vErr != nil { /error result/ = handlerChain(vn) return } foo(v1, ..., vN) 如下是一段常规的错误处理代码：\nfunc printSum(a, b string) error { x, err := strconv.Atoi(a) if err != nil { return err } y, err := strconv.Atoi(b) if err != nil { return err } fmt.Println(\u0026#34;result:\u0026#34;, x + y) return nil } 其可被改写为：\nfunc printSum(a, b string) error { handle err { return err } fmt.Println(\u0026#34;result:\u0026#34;, check strconv.Atoi(x) + check strconv.Atoi(y)) return nil } 通常需要包装下错误信息的上下文，代码可以写作：\nfunc printSum(a, b string) error { handle err { return fmt.Errorf(\u0026#34;printSum(%q + %q): %v\u0026#34;, a, b, err) } fmt.Println(\u0026#34;result:\u0026#34;, check strconv.Atoi(x) + check strconv.Atoi(y)) return nil }  Handler Handler用来处理check所发现的错误，Handler使用return语句可以使对应函数即刻退出。不带返回值的return仅可用于无返回值函数或变量声明式函数，对变量声明式函数，其返回这些变量的当前值。Handler链函数带一个error类型的参数且与对应函数的返回值定义相同。每个check对应哪个Handler链函数取决于check所定义的范围。  拿如下代码举例：\nfunc process(user string, files chan string) (n int, err error) { handle err { return 0, fmt.Errorf(\u0026#34;process: %v\u0026#34;, err) } // handler A  for i := 0; i \u0026lt; 3; i++ { handle err { err = fmt.Errorf(\u0026#34;attempt %d: %v\u0026#34;, i, err) } // handler B  handle err { err = moreWrapping(err) } // handler C  check do(something()) // check 1: handler chain C, B, A  } check do(somethingElse()) // check 2: handler chain A } Check 1：在循环内，依序运行Handler C、B及A。不同于defer，定义在循环内的Handler不会因每次新的迭代而累积。\nCheck 2：在函数末尾，仅运行Handler A。\n几个重要点：\ncheck到错误，即会落入Handler，无法再回到对应函数的控制；\nHandler执行总是在defer语句之前；\n若对应函数需要有返回值，但check的Handler链函数没有return语句会引起编译错误。\n 默认Handler 默认Handler隐式定义在最后一个参数是error类型的函数的头部。  依赖默认Handler，printSum函数可以写作：\nfunc printSum(a, b string) error { x := check strconv.Atoi(a) y := check strconv.Atoi(b) fmt.Println(\u0026quot;result:\u0026quot;, x + y) return nil }  总结    1）Handler\n  a）仅需一个error类型的参数；\n  b）与对应函数的返回参数相同。\n    2）handle语句\n  a）Handler使用return会将对应函数返回；\n  b）对应函数使用参数声明式返回，一个空的return语句会返回这些参数的当前值。\n    3）check表达式\n  a）若check用在仅返回一个error值的函数前面，check会消费该值，且不会生产任何结果；\n  b）一个check的Handler链会依Handler在当前作用的域的定义序的反序执行，直至某个Handler return；\n  c）check表达式不可用于Handler。\n    4）默认Handler\n  a）对应函数非参数声明式返回，默认Handler会返回排头参数的0值及最后参数的error值；\n  b）对应函数为参数声明式返回，默认Handler会返回排头参数的当前值及最后参数的error值；\n  c）因默认Handler定义在函数头部，其是Handler链的最后一环。\n    重点：\nHandler链调用类似于函数调用，check到错误的位置被保存为Handler的调用者栈帧。\n 参考资料\n[1] https://github.com/golang/proposal/blob/master/design/go2draft-error-handling-overview.md\n[2] https://github.com/golang/proposal/blob/master/design/go2draft-error-handling.md\n ","permalink":"https://olzhy.github.io/posts/go2-error-handling-draft-design.html","tags":["Golang"],"title":"Go 2 错误处理设计草案预览"},{"categories":["计算机"],"contents":"Go 1.8，在Go 1.7发布半年后如约而至。该版本的绝大多数变化是在工具链、运行时及库的实现上。有两项小的语言规范上的变化。一如既往，该版本遵守Go 1兼容性准则，期待所有程序像之前一样编译及运行。\n1 语言方面\n在Go 1.8，两个仅tag不同结构体可以执行转换。\n例如，如下代码，是合法的：\ntype T1 struct { Hello string `json:\u0026#34;hello\u0026#34;` } type T2 struct { Hello string `json:\u0026#34;hi\u0026#34;` } func convert() { var v1 T1 var v2 T2 v2 = T2(v1) // now legal } 2 工具方面\n Pprof 目前，分析TLS服务时，pprof工具可以使用“https+insecure”模式的URL来略过证书校验。  接下来验证一下，首先使用如下命令生成证书。\n$ openssl genrsa -out ca.key 1024 $ openssl req -new -key ca.key -out ca.csr $ openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt 然后，写一个简单的嵌入pprof的TLS服务。\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/echo\u0026#34;, func(w http.ResponseWriter, r *http.Request) { io.WriteString(w, \u0026#34;hello\u0026#34;) }) log.Fatal(http.ListenAndServeTLS(\u0026#34;:8080\u0026#34;, \u0026#34;ca.crt\u0026#34;, \u0026#34;ca.key\u0026#34;, nil)) } 最后，pprof tool使用“https+insecure”模式访问即可略过证书校验作分析。\n$ go tool pprof https+insecure://localhost:8080/debug/pprof/goroutine?debug=1  Vet Vet校验在有些方面会更严格，而在之前引起误报的方面则放宽了一些。  Vet目前可以用来检查加锁数据拷贝、重复JSON及XML结构体字段tag，非空格分割结构体tag，检查error之前的HTTP Response.Body.Close延迟调用及Printf的索引参数等问题。\n例如，如下代码，使用Vet即可提示参数错误所在。\nfunc main() { fmt.Printf(\u0026#34;Hello %d\\n\u0026#34;, \u0026#34;World\u0026#34;) } $ go vet test.go # command-line-arguments ./test.go:6:2: Printf format %d has arg \u0026quot;world\u0026quot; of wrong type string  GOPATH默认值 未设置GOPATH环境变量时，在Unix上其默认为$HOME/go，在Windows上默认为%USERPROFILE%/go。  3 运行时方面\n  参数存活 垃圾收集器不再认为参数在整个函数生命周期都是存活的。更多信息及如何强制一个变量维持存活，请参看在Go 1.7加入的runtime.KeepAlive函数。\n  并发情况下的Map误用 在Go 1.6，运行时增加了轻量的及全力的Map误用检查，该版本对检测器作了改进，支持检测程序中Map的并发写及并发迭代。\n  一如既往，若一个goroutine正在对Map作写操作，其他任何goroutine不应并发写或并发读（包括迭代）。\n4 性能方面\n因垃圾收集器加速及标准库优化，多数程序应比之前运行快一点。\n 垃圾收集器 垃圾收集器停顿时间应该显著低于其在Go 1.7上的时间，通常低于100微秒并且经常低至10微秒。  5 标准库方面\n Sort sort包目前引入一个便捷的函数Slice，其可以传入一个less函数而对slice进行排序。这意味着在多数情况下无须写一个新的可排序类型。  SliceStable及SliceIsSorted也是新引入的。\n下面例子为对Slice函数的使用。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strings\u0026#34; ) type Person struct { Name string } func main() { persons := []Person{ {\u0026#34;Larry\u0026#34;}, {\u0026#34;Jacky\u0026#34;}, {\u0026#34;Alex\u0026#34;}, } sort.Slice(persons, func(i, j int) bool { return strings.Compare(persons[i].Name, persons[j].Name) \u0026lt; 0 }) fmt.Println(persons) }  HTTP/2 Push net/http包目前包含一个从Handler发送HTTP/2服务端推送的机制。类似现有的Flusher与Hijacker接口，一个HTTP/2 ResponseWriter目前实现了新的Pusher接口。  如下为一个Go 1.8服务器推送的代码示例。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) const ( html = ` \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34; href=\u0026#34;/style.css\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;hello\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ` css = `p { color: red }` ) func main() { http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { if pusher, ok := w.(http.Pusher); ok { err := pusher.Push(\u0026#34;/style.css\u0026#34;, nil) if nil != err { fmt.Printf(\u0026#34;fail to push, err: %s\\n\u0026#34;, err) } } fmt.Fprintln(w, html) }) http.HandleFunc(\u0026#34;/style.css\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/css\u0026#34;) fmt.Fprintln(w, css) }) log.Fatal(http.ListenAndServeTLS(\u0026#34;:8080\u0026#34;, \u0026#34;ca.crt\u0026#34;, \u0026#34;ca.key\u0026#34;, nil)) }  参考资料\n[1] https://golang.org/doc/go1.8\n ","permalink":"https://olzhy.github.io/posts/go1dot8-release-notes.html","tags":["Golang"],"title":"Go 1.8 Release Notes 要点整理"},{"categories":["计算机"],"contents":"1 场景 我们知道，在 Go 服务端，每个进入的请求会被其所属 goroutine 处理。\n例如，如下代码，每次请求，Handler 会创建一个 goroutine 来为其提供服务，而且连续请求 3 次，r 的地址也是不同的。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/echo\u0026#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Println(\u0026amp;r) w.Write([]byte(\u0026#34;hello\u0026#34;)) }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } $ go run test.go $ curl http://localhost:8080/echo $ curl http://localhost:8080/echo $ curl http://localhost:8080/echo 0xc000072040 0xc000072048 0xc000072050 而每个请求对应的 Handler，常会启动额外的的 goroutine 进行数据查询或 PRC 调用等。\n而当请求返回时，这些额外创建的 goroutine 需要及时回收。而且，一个请求对应一组请求域内的数据可能会被该请求调用链条内的各 goroutine 所需要。\n例如，在如下代码中，当请求进来时，Handler 会创建一个监控 goroutine，其会每隔 1s 打印一句“req is processing”。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/echo\u0026#34;, func(w http.ResponseWriter, r *http.Request) { // monitor  go func() { for range time.Tick(time.Second) { fmt.Println(\u0026#34;req is processing\u0026#34;) } }() // assume req processing takes 3s  time.Sleep(3 * time.Second) w.Write([]byte(\u0026#34;hello\u0026#34;)) }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } 假定请求需耗时 3s，即请求在 3s 后返回，我们期望监控 goroutine 在打印 3 次“req is processing”后即停止。但运行发现，监控 goroutine 打印 3 次后，其仍不会结束，而会一直打印下去。\n问题出在创建监控 goroutine 后，未对其生命周期作控制，下面我们使用 context 作一下控制，即监控程序打印前需检测r.Context()是否已经结束，若结束则退出循环，即结束生命周期。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func main() { http.HandleFunc(\u0026#34;/echo\u0026#34;, func(w http.ResponseWriter, r *http.Request) { // monitor  go func() { for range time.Tick(time.Second) { select { case \u0026lt;-r.Context().Done(): fmt.Println(\u0026#34;req is outgoing\u0026#34;) return default: fmt.Println(\u0026#34;req is processing\u0026#34;) } } }() // assume req processing takes 3s  time.Sleep(3 * time.Second) w.Write([]byte(\u0026#34;hello\u0026#34;)) }) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } 基于如上需求，context 包应运而生。\ncontext 包可以提供一个请求从 API 请求边界到各 goroutine 的请求域数据传递、取消信号及截止时间等能力。\n2 Context 类型 // A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this Context is canceled  // or times out.  Done() \u0026lt;-chan struct{} // Err indicates why this context was canceled, after the Done channel  // is closed.  Err() error // Deadline returns the time when this Context will be canceled, if any.  Deadline() (deadline time.Time, ok bool) // Value returns the value associated with key or nil if none.  Value(key interface{}) interface{} } Done 方法返回一个 channel，当 Context 取消或到达截止时间时，该 channel 即会关闭。Err 方法返回 Context 取消的原因。\nContext 自己没有 Cancel 方法，而且 Done channel 仅用来接收信号：接收取消信号的函数不应同时是发送取消信号的函数。父 goroutine 启动子 goroutine 来做一些子操作，而子 goroutine 不应用来取消父 goroutine。\nContext 是安全的，可被多个 goroutine 同时使用。一个 Context 可以传给多个 goroutine，而且可以给所有这些 goroutine 发取消信号。\n若有截止时间，Deadline 方法可以返回该 Context 的取消时间。\nValue 允许 Context 携带请求域内的数据，该数据访问必须保障多个 goroutine 同时访问的安全性。\n3 衍生 Context // Background returns an empty Context. It is never canceled, has no deadline, // and has no values. Background is typically used in main, init, and tests, // and as the top-level Context for incoming requests. func Background() Context // WithCancel returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed or cancel is called. func WithCancel(parent Context) (ctx Context, cancel CancelFunc) // A CancelFunc cancels a Context. type CancelFunc func() // A CancelFunc cancels a Context. type CancelFunc func() // WithTimeout returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed, cancel is called, or timeout elapses. The new // Context\u0026#39;s Deadline is the sooner of now+timeout and the parent\u0026#39;s deadline, if // any. If the timer is still running, the cancel function releases its // resources. func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) // WithValue returns a copy of parent whose Value method returns val for key. func WithValue(parent Context, key interface{}, val interface{}) Context context 包提供从已有 Context 衍生新的 Context 的能力。这样即可形成一个 Context 树，当父 Context 取消时，所有从其衍生出来的子 Context 亦会被取消。\nBackground 是所有 Context 树的根，其永远不会被取消。\n使用 WithCancel 及 WithTimeout 可以创建衍生的 Context，WithCancel 可用来取消一组从其衍生的 goroutine，WithTimeout 可用来设置截止时间。\nWithValue 提供给 Context 赋予请求域数据的能力。\n下面来看几个对如上方法使用的例子。\n1）首先，看一下 WitchCancel 的使用\n在如下代码中，main 函数使用 WithCancel 创建一个基于 Background 的 ctx。\n然后启动一个 monitor goroutine，该 monitor 每隔 1s 打印一句“monitor woring”，main 函数在 3s 后执行 cancel，那么 monitor 检测到取消信号后即会退出。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { ctx, cancel := context.WithCancel(context.Background()) defer cancel() // monitor  go func() { for range time.Tick(time.Second) { select { case \u0026lt;-ctx.Done(): return default: fmt.Println(\u0026#34;monitor woring\u0026#34;) } } }() time.Sleep(3 * time.Second) } 2）再看一个使用 WithTimeout 的例子\n如下代码中使用 WithTimeout 创建一个基于 Background 的 ctx，其会在 3s 后取消。\n注意，虽然到截止时间会自动 cancel，但 cancel 代码仍建议加上。\n到截止时间而被取消还是被 cancel 代码所取消，取决于哪个信号发送的早。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { ctx, cancel := context.WithTimeout(context.Background(), 3*time.Second) defer cancel() select { case \u0026lt;-time.After(4 * time.Second): fmt.Println(\u0026#34;overslept\u0026#34;) case \u0026lt;-ctx.Done(): fmt.Println(ctx.Err()) } } WithDeadline 的使用与 WithTimeout 相似。\n没想好 Context 的具体使用，可以使用 TODO 来占位，也便于工具作正确性检查。\n3）最后看一下 WithValue 的使用\n如下代码基于 Background 创建一个带值的 ctx，然后可以根据 key 来取值。\n注意：避免多个包同时使用 context 而带来冲突，key 不建议使用 string 或其他内置类型，而建议自定义 key 类型。\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; ) type ctxKey string func main() { ctx := context.WithValue(context.Background(), ctxKey(\u0026#34;a\u0026#34;), \u0026#34;a\u0026#34;) get := func(ctx context.Context, k ctxKey) { if v, ok := ctx.Value(k).(string); ok { fmt.Println(v) } } get(ctx, ctxKey(\u0026#34;a\u0026#34;)) get(ctx, ctxKey(\u0026#34;b\u0026#34;)) } 最后列一下 Context 使用规则：\n 勿将 Context 作为 struct 的字段使用，而是对每个使用其的函数分别作参数使用，其需定义为函数或方法的第一个参数，一般叫作 ctx； 勿对 Context 参数传 nil，未想好的使用那个 Context，请传context.TODO； 使用 context 传值仅可用作请求域的数据，其它类型数据请不要滥用； 同一个 Context 可以传给使用其的多个 goroutine，且 Context 可被多个 goroutine 同时安全访问。   参考资料\n[1]https://blog.golang.org/context\n[2]https://golang.org/pkg/context/\n ","permalink":"https://olzhy.github.io/posts/golang-context.html","tags":["Golang"],"title":"Golang Context使用小结"},{"categories":["计算机"],"contents":"Go 1.7在1.6发布6个月后如约而至，绝大多数的变化在工具链、运行时及核心库的实现上。语言规格上有一项小变化。一如既往，该版本遵守Go 1兼容性准则。\n1 语言方面\n该版本有一项小的语言级变化，即阐明了结束语句的定义。与现有gc及gccgo工具链规则相符，“最后的非空语句”被认为是结束语句。之前的定义（最后一句即是结束语句）可能会有空语句的问题，是不明确的。\n2 工具方面\n  Go 命令 go命令的基础操作未有变化。Go 1.6已声明过，在Go 1.7移除了GO15VENDOREXPERIMENT环境变量，vendoring支持目前是go命令及工具链的标准特性。\n  Go tool dist go tool list list会打印出所有支持的操作系统及体系结构对。\n  Go tool trace Go 1.5引入的go tool trace有几项修整。Go 1.7搜集trace信息较之前更高效。trace文件目前包含文件及行号信息。\n  参看如下代码，在your code之前插入trace语句：\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;runtime/trace\u0026#34; ) func main() { f, err := os.Create(\u0026#34;trace.out\u0026#34;) if nil != err { panic(err) } trace.Start(f) defer trace.Stop() // your code } 然后即可使用工具来对trace.out文件作分析。\n$ go tool trace trace.out 3 性能方面\n跟之前一样，该版本变化较广，难以对性能作准确陈述。因垃圾收集器加速及核心库的优化，使用该版本的绝大多数程序应比之前运行的快一点。在x86-64系统上，因使用新的编译器后端来生成代码，许多程序会运行的更快。\n对于拥有大量闲置goroutine、栈尺寸波动较大及大量包级别变量的程序，Go 1.7垃圾收集器停顿时间相较于Go 1.6会显著低一些。\n4 核心库方面\n  Context Go 1.7将golang.org/x/net/context包移入了标准库。这样即可在其它诸如net、net/http及os/exec的标准包使用context来处理连接取消、超时及request级数据等问题。\n  HTTP跟踪 Go 1.7引入net/http/httptrace包，可以使用其来跟踪HTTP请求事件。\n  测试 testing包目前支持子测试及子基准测试。使用其可以编写表-驱动测试及层级测试，同样还可以复用setup及tear-down代码。\n  参看如下代码：\npackage test import ( \u0026#34;fmt\u0026#34; \u0026#34;testing\u0026#34; ) func TestSubtests(t *testing.T) { // setup  fmt.Println(\u0026#34;setup\u0026#34;) // sub tests  t.Run(\u0026#34;A=1\u0026#34;, func(t *testing.T) { fmt.Println(\u0026#34;A=1\u0026#34;) }) t.Run(\u0026#34;A=2\u0026#34;, func(t *testing.T) { fmt.Println(\u0026#34;A=2\u0026#34;) }) t.Run(\u0026#34;B=1\u0026#34;, func(t *testing.T) { fmt.Println(\u0026#34;B=1\u0026#34;) }) t.Run(\u0026#34;B=2\u0026#34;, func(t *testing.T) { fmt.Println(\u0026#34;B=2\u0026#34;) }) // tear down  fmt.Println(\u0026#34;tear down\u0026#34;) } 对go test -run传入不同参数，可以控制执行哪些子测试。\n如采用如下命令可以指定运行TestSub*测试。\n$ go test -run Sub setup A=1 A=2 B=1 B=2 tear down PASS ok github.com/olzhy/test 0.006s 采用如下命令可以指定运行TestSub*测试的A组测试。\n$ go test -run Sub/A setup A=1 A=2 tear down PASS ok github.com/olzhy/test 0.005s  参考资料\n[1] https://golang.org/doc/go1.7\n ","permalink":"https://olzhy.github.io/posts/go1dot7-release-notes.html","tags":["Golang"],"title":"Go 1.7 Release Notes 要点整理"},{"categories":["计算机"],"contents":"Go 1.6在1.5发布半年后如约而至，该版本主要变化在语言、运行时及库上面，语言规范未有变化。同理，其保持Go 1兼容性准则。\n1 工具方面\n Cgo 一个大点：定义了程序与C代码共享Go指针的规则，以确保C代码与Go垃圾收集器可以共存。简言之，使用cgo调用时，可能会将一块内存传给C，这样，Go代码和C代码可能会共享由Go分配的内存（规则规定内存自身不包含指向Go分配内存的指针，规定C在调用完成后不会仍持有指针）。该规则会在运行时作检查，若运行时检查到违规情况，会打印堆栈并结束程序。  一个小点：区别于Go的complex64与complex128，增加了C.complexfloat与C.complexdouble类型。\n Go命令 我们知道，Go在1.5引入了对vendoring的试验性支持，1.5要使用该功能，需将GO15VENDOREXPERIMENT环境变量置为1。而Go在1.6会默认开启该功能，但仍可手动将GO15VENDOREXPERIMENT置为0来关闭该功能。从Go 1.7起，会关闭该变量，成为默认开启的稳定功能。在Go引入vendoring前，包含vendor文件夹的工程需手动修改。  2 性能方面\n一如既往，Go 1.6改动较广，性能方面，有的程序可能会快一点，有的可能会慢一点。总体讲，基于Go 1基准套件测试，Go 1.6的运行速度会比Go1.5快一些。特别对于使用大量内存的程序，Go 1.6的垃圾收集器停顿时间会较1.5缩短很多。\n3 核心库方面\n  HTTP/2 Go 1.6对新的HTTP/2协议在net/http包增加了透明化支持。当使用HTTPS时，Go客户端及服务端将会适时自动使用HTTP/2。\n  运行时 运行时增加了轻量的、尽力的map并发误用检测。若一个goroutine正在对map作写操作，其它任何goroutine不应对该map进行并发读写。若运行时检测到该误用情况，会打印错误信息并退出程序。便于找出问题的最好方式是使用race探测器，会取得很多有用的详细信息。\n  下面看一段代码：\npackage main import ( \u0026#34;sync\u0026#34; ) func main() { m := make(map[int]int) var wg sync.WaitGroup for i := 0; i \u0026lt; 10; i++ { wg.Add(1) go func(i int) { m[i] = i wg.Done() }(i) } wg.Wait() } 该代码启动10个goroutine对map进行并发写操作。\n$ go run -race test.go go run时使用race检测，可以打印出并发读写map以致程序退出的具体原因。\n================== WARNING: DATA RACE Write at 0x00c000080000 by goroutine 6: runtime.mapassign_fast64() /usr/local/go/src/runtime/map_fast64.go:92 +0x0 main.main.func1() /Users/larry/Documents/workspace/go/project/src/github.com/olzhy/test/test.go:13 +0x52 Previous write at 0x00c000080000 by goroutine 5: runtime.mapassign_fast64() /usr/local/go/src/runtime/map_fast64.go:92 +0x0 main.main.func1() /Users/larry/Documents/workspace/go/project/src/github.com/olzhy/test/test.go:13 +0x52 Goroutine 6 (running) created at: main.main() /Users/larry/Documents/workspace/go/project/src/github.com/olzhy/test/test.go:12 +0xcd Goroutine 5 (finished) created at: main.main() /Users/larry/Documents/workspace/go/project/src/github.com/olzhy/test/test.go:12 +0xcd ================== Found 1 data race(s) exit status 66 此外，对结束程序的panic异常，运行时目前默认仅打印运行中goroutine而非所有存在的goroutine的堆栈。\n因为，通常仅当前goroutine与panic相关，略去其它goroutine的堆栈信息可以减少大量不相关信息的打印。想看程序退出时所有goroutine的堆栈信息，可以设置GOTRACEBACK环境变量为all，或者在程序退出前调用debug.SetTraceback。\n 模板 text/template包可以使用'-\u0026lsquo;号来移除模板动作前边或后边的的空格，将\u0026rsquo;-\u0026lsquo;号置于动作前边可以移除动作前边的空格，将\u0026rsquo;-\u0026lsquo;号置于动作后边可以移除动作后边的空格。  参看如下代码：\npackage main import ( \u0026#34;html/template\u0026#34; \u0026#34;os\u0026#34; ) type Person struct { Name string Age int } func main() { tpl, err := template.New(\u0026#34;test\u0026#34;).Parse(\u0026#34;my name is {{.Name -}} , age is {{.Age -}} .\u0026#34;) if nil != err { panic(err) } p := \u0026amp;Person{\u0026#34;Larry\u0026#34;, 28} err = tpl.Execute(os.Stdout, p) if nil != err { panic(err) } } 运行结果trim掉了\u0026rsquo;,\u0026lsquo;号之前，以及\u0026rsquo;.\u0026lsquo;之前的空格。\nmy name is Larry, age is 28.  参考资料\n[1] https://golang.org/doc/go1.6\n ","permalink":"https://olzhy.github.io/posts/go1dot6-release-notes.html","tags":["Golang"],"title":"Go 1.6 Release Notes 要点整理"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个链表，向右旋转k位，k为非负数。\n例子1：\n输入：1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5-\u0026gt;NULL, k = 2\n输出：4-\u0026gt;5-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;NULL\n释义：\n向右旋转1步：5-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;NULL\n向右旋转2步：4-\u0026gt;5-\u0026gt;1-\u0026gt;2-\u0026gt;3-\u0026gt;NULL\n例子2：\n输入：0-\u0026gt;1-\u0026gt;2-\u0026gt;NULL, k = 4\n输出：2-\u0026gt;0-\u0026gt;1-\u0026gt;NULL\n释义：\n向右旋转1步：2-\u0026gt;0-\u0026gt;1-\u0026gt;NULL\n向右旋转2步：1-\u0026gt;2-\u0026gt;0-\u0026gt;NULL\n向右旋转3步：0-\u0026gt;1-\u0026gt;2-\u0026gt;NULL\n向右旋转4步：2-\u0026gt;0-\u0026gt;1-\u0026gt;NULL\n题目出处：LeetCode\n2 解决思路\n1）首先，p、q两个指针都指向head；\n2）然后，p先走k步；\n2.1）若到达尾节点时，正好走了k步，则无需处理，直接返回head即可；\n2.1）若k步未走完即到达尾节点，则说明k比链表长度大，可以将k模除链表长度后，再返回1）开始计算；\n3）然后，p与q一起走，直至p抵达尾节点，这时，即找到了旋转的分割点。需要将这两段连接起来，即将p的下一个节点指向原头节点，q的下一个节点即为新的头节点，q即为新的尾节点，然后返回新的头节点即可。\n3 Golang实现代码\n注意：Golang循环中，break Label与goto Label的区别，break的Label仅可用于循环，且需放在for循环前面，且跳到Label后不会再执行for循环里的代码；而goto的Label可用于循环，也可用于非循环，可以放在for循环前面，也可以放在for循环后面，当Label放在循环前面时，跳到Label后，还会继续执行Label后的代码。\nhttps://github.com/olzhy/\nfunc rotateRight(head *ListNode, k int) *ListNode { if nil == head || nil == head.Next { return head } p, q := head, head len := 0 // firstly, p move right k steps Loop: for k \u0026gt; 0 { k-- len++ if nil == p.Next { // k is equals to len, do not move, return immediatly  if 0 == k { return head } // k is larger than len, k mod len, then go back to the beginning  k %= len p = head len = 0 goto Loop } p = p.Next } // then, p/q move right together util p arriving at tail  for nil != p.Next { p = p.Next q = q.Next } // re-build relations  p.Next = head head = q.Next q.Next = nil return head } ","permalink":"https://olzhy.github.io/posts/leetcode-rotate-list.html","tags":["Golang","算法"],"title":"LeetCode 61 旋转链表"},{"categories":["计算机"],"contents":"1 速览\n在正式了解Golang Modules之前，我们先速览一下其使用方式。\n在$GOPATH之外的任意地方，创建一个文件夹：\n$ mkdir -p /tmp/hello $ cd /tmp/hello 然后初始化一个新的Module：\n$ go mod init github.com/olzhy/hello 输出：\ngo: creating new go.mod: module github.com/olzhy/hello go.mod内容为：\nmodule github.com/olzhy/hello go 1.12 然后写一段代码：\n$ cat \u0026lt; hello.go package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/olzhy/quote\u0026quot; ) func main() { fmt.Println(quote.Hello()) } EOF build一下：\n$ go build go: finding github.com/olzhy/quote latest go: downloading github.com/olzhy/quote v0.0.0-20190510033103-5cb7d4598cfa go: extracting github.com/olzhy/quote v0.0.0-20190510033103-5cb7d4598cfa go.mod内容为：\nmodule github.com/olzhy/hello go 1.12 require github.com/olzhy/quote v0.0.0-20190510033103-5cb7d4598cfa 可以看到，其会从https://github.com/olzhy/quote的master分支拉取最新提交5cb7d4598cfa。\n该依赖工程非Module管理模式，其仅有两个文件：\nhello.go README.md 现在给依赖工程打一个TAG，名为v1.0.0。然后hello工程更新依赖：\n$ go get -u go: finding github.com/olzhy/quote v1.0.0 go: downloading github.com/olzhy/quote v1.0.0 go: extracting github.com/olzhy/quote v1.0.0 查看go.mod内容为：\nmodule github.com/olzhy/hello go 1.12 require github.com/olzhy/quote v1.0.0 如下为使用Golang Module后的日常工作流。\n每日工作流：\n  a）源码根据需要加入包引入语句；\n  b）标准命令，如go build及go test等会自动更新go.mod并下载依赖包；\n  c）当需要特定版本时，可以使用诸如go get foo@v1.2.3，go get foo@master，go get foo@e3702bed2命令或直接编辑go.mod文件。\n  其它通用命令：\n  a）go list -m all 查看一次构建使用的直接及间接依赖的最终版本；\n  b）go list -u -m all 查看直接及间接依赖的可用的小版本或补丁版本更新；\n  c）go get -u 或 go get -u=patch 将直接与间接依赖更新为最新小版本或补丁版本；\n  d）go build ./... 或 go test ./... 构建或测试模块中的所有包；\n  e）go mod tidy 从go.mod清理不再使用的包；\n  f）go mod edit -replace foo@v1.2=../foo 替换依赖为本地复制或指定版本；\n  g）go mod vendor 转换为vendor依赖方式。\n  2 概念\n 2.1 模块 Module是一组相关Go package的集合，其作为一个单独的单元来版本化。Module记录精确的依赖项，提供可重复的构建。通常，一个版本控制仓库仅包含一个Module（支持单仓库多Module，但其会比单仓库单Module复杂很多）。  仓库、模块与包的关系：\n  a）一个仓库包含一个或多个模块；\n  b）一个模块包含一个或多个包；\n  c）一个包在一个文件夹包含一个或多个.go文件。\n  模块必须以语义学版本命名，格式为v(主版本).(小版本).(补丁)，诸如v0.1.0、v1.2.3，v1.5.0-rc.1等。\n模块由一组源文件树在根目录定义一个go.mod文件，模块源码可以位于GOPATH之外，有如下原语module，require，replace，exclude。\n如下为github.com/olzhy/hello模块的go.mod文件示例内容：\nmodule github.com/olzhy/hello require ( github.com/some/dependency v1.2.3 github.com/another/dependency/v4 v4.0.0 ) 可以看到，一个模块通过module原语声明模块ID，其标识模块路径。该模块下某一个包的被引用路径由该模块路径与自go.mod所在路径起一直到包的路径止的相对路径共同决定。如，一个模块在go.mod声明其ID为example.com/my/module，那么引用该模块下mypkg包的代码为：\nimport \u0026quot;example.com/my/module/mypkg\u0026quot;  2.2 版本选择 若源码中增加了go.mod中未require的新的依赖包，绝大多数诸如go build，go test命令会自动找到对应的包并在go.mod中采用require原语加入该直接依赖的 最高版本。  例如，您依赖的模块M的带标签的发布版本为v1.2.3，那么您的go.mod会新加入require M v1.2.3这一行语句，意味着依赖模块M所允许的版本\u0026gt;= v1.2.3并\u0026lt; v2（v2被认为与v1不兼容）。 最小版本选择算法用于对一次构建的所有模块选择版本，对于每个模块，采用该算法选择的版本为语义学最高版本。\n下面举个例子：\n若您依赖的模块A依赖D（require D v1.0.0），而您依赖的模块B同样依赖D（require D v1.1.1），然后，最小版本选择（选择最高的版本）将会选择v1.1.1版本的D。而选择v1.1.1版本的D是一致的，即使将来发布了v1.2.0版本的D。这样即可保持100%可重复构建。当然您也可以手动升级D为最新可用版本或者指定其为其它版本。查看所选模块版本列表（包括间接依赖），可以使用：\ngo list -m all  2.3 语义学版本引用 Go多年来推荐的包版本化方式：  开放使用的包在演进时应保持向后兼容的准则，Go 1兼容性准则即是一个好的参考。不要移除已导出的名称，若要加一个新功能，需加一个新接口，不要改动老接口名。实在需要推倒之前的，请创建一个新包，以新的路径而被引用。\n最后一句很重要，若破坏了兼容性，需更改包的引用路径。对Go 1.11模块而言，引用兼容性准则可以概述为：\n若新包沿用旧包的引用路径，新包必须向后兼容旧包。\n参考语义学版本命名规则，当一个原始为v1或v1以上的包发生了不兼容变更，该包需要更改主版本。所以，根据引用兼容性准则及语义学版本命名规则（合称为语义学版本引用），主版本需要包含在引用路径内。这样即可保障不兼容的主版本升级时，引用路径即会改变。根据语义学版本引用规则，选用Go Module的代码必须遵守如下规则：\n  a）语义学版本命名；\n  b）若一个Module的版本为v2及以上，模块的主版本必须包含在模块路径及引用路径中（例如，声明方：module github.com/my/mod/v2，引用方：require github.com/my/mod/v2 v2.0.0，包引用处：import \u0026quot;github.com/my/mod/v2/mypkg\u0026quot;）；\n  c）例外，若模块主版本为v0或v1，模块路径及引用路径无须包含主版本。\n  通常来讲，引用路径不同的包是两个全然不同的包（如math/rand和crypto/rand是两个不同的包）。同样，包含不同主版本的引用路径所标识的包亦是两个不同的包。因此，example.com/my/mod/mypkg与example.com/my/mod/v2/mypkg是不同的包，且可能会在一次构建中同时引用。因有些模块还未转换为Module方式，过度期，会支持如下几个例外：\n a）gopkg.in  会继续支持gopkg.in/yaml.v1或gopkg.in/yaml.v2等引用方式。\n  b）当引用还未Module化的v2+版本包时，会有'+incompatible\u0026lsquo;后缀。\n  c）当Module模式未开启时，采用最小模块兼容性。\n  即在Go 1.11邻近版本，不开启Module模式时（GO111MODULE=off），引用v2或以上版本，不会将版本加入路径中。\n3 使用\n 3.1 模块支持激活 安装Go 1.11及以上版本，然后可以使用如下两种方式中的任一种激活模块支持。    a）在$GOPATH/src文件夹之外使用go命令，且当前文件夹或其上层文件夹包含go.mod文件，而GO111MODULE``环境变量未设置或设置为了auto；\n  b）设置GO111MODULE=on，然后调用go命令。\n  即在$GOPATH/src之外使用模块支持，无需设置GO111MODULE环境变量，而在$GOPATH/src使用模块支持，需将GO111MODULE设置为on。\n 3.2 定义一个模块   a）进入对应文件夹  $ cd path 该文件夹可以为设置GO111MODULE=on的$GOPATH/src，或该文件夹之外的任意路径。\n b）执行go mod init  $ go mod init github.com/my/repo 若在初始化一个v2+的模块，需要手动更改go.mod文件及.go代码，以在引用路径及模块路径加入版本信息（语义学版本引用）。\n c）构建模块  $ go build ./... “./...”模式匹配了当前模块下的所有包，go build将自动增加缺失的包。\n d）测试模块  $ go test ./... 或者执行如下语句，可以运行模块内的测试及所有直接及间接依赖测试以检查不兼容问题。\n$ go test all  3.3 依赖升降级 可以使用go get命令进行日常依赖升级及降级，其会自动更新go.mod文件，当然您也可以手动编辑go.mod文件。当然，go get也如go build，go test一样，会自动加入缺失的依赖包。查看可用的小版本或补丁更新，可以执行：  $ go list -u -m all 将直接或间接依赖更新为最新的小版本或补丁版本，可以执行：\n$ go get -u 仅更新为补丁版本，可以执行：\n$ go get -u=patch go get foo等同于go get foo@latest，会将foo更新为最新版本。当有语义学版本时，最新版本为语义学最新版本，没有时，为最新的提交。一个通常错误的认为是，go get -u foo仅获取最新版本的foo。其实其还会获取foo的直接或间接依赖的最新版本。更新版本，推荐的做法是先运行go get foo，好使时再运行go get -u foo。\n进行版本升降级时，可以使用@version后缀，如：\n$ go get foo@v1.6.2 或\n$ go get foo@e3702bed2 还支持模块查询，如：\n$ go get foo@'\u0026lt;v1.6.2' 使用分支名称，可以不考虑其是否有语义学版本，而更新为最新的分支提交。\n$ go get foo@master 版本升降级后，需测试是否有不兼容问题：\n$ go test all  3.4 模块版本发布 发布前执行如下命令，以删减未使用的包。  go mod tidy 然后执行如下命令，保证兼容性。\ngo test all 然后发布时，需将go.sum文件与go.mod一起提交。\n发布v2及以上版本时需注意满足语义学版本引用规则，版本需包含在模块路径及引用路径中。创建一个v2及以上的版本，有如下两种方式：\n a）不创建子文件夹  go.mod文件包含vN路径（如：module github.com/my/module/v3），模块内的包引用亦需修改为包含版本的格式（如：import \u0026quot;github.com/my/module/v3/mypkg\u0026quot;）。\n b）创建子文件夹  创建vN子文件夹，且将go.mod放至该文件夹下，模块路径需以/vN结尾，然后将代码拷贝至vN子文件夹下，然后更新模块内的包引用路径（如：import \u0026quot;github.com/my/module/v3/mypkg\u0026quot;）。 最后，创建一个满足语义学版本的tag，推送至仓库即可。 但需注意子模块的情形，该种情形tag需包含前缀。 如，我们有模块example.com/repo/sub/v2，然后想发布版本v2.1.6，仓库为example.com/repo，子模块定义在sub/v2/go.mod，提交时tag需命名为sub/v2.1.6。\n 参考资料\n[1] https://github.com/golang/go/wiki/Modules\n[2] https://research.swtch.com/vgo\n ","permalink":"https://olzhy.github.io/posts/golang-modules.html","tags":["Golang"],"title":"Golang Modules"},{"categories":["随笔"],"contents":"去年的这个时候，我遇见了Jane。她是我遇见的真正豁达，内心自由坦荡的人。她待我真心实意，她叫我一声“磊哥”，即会扫去我内心所有的乌云。\nJane喜欢运动，喜欢听我唱歌，喜欢研究美食。简单的食材，她都会做成一道难忘的美食。她是能把日子过成诗的人。\n她写过一篇《给未来先生的信》。她理想的那个他不需要有多富贵、有多能耐，但需要与她心灵高度契合。他们一起远行，一起做饭，一起游泳，一起做美食，一起过简单的日子。\n她已将世界看清，但不出离。她热爱生活，懂得生活的真谛。她会忙活半天，用心做一份你爱吃的菜。她会为你的出行安排好所有的日用品，她会将家收拾的干干净净的，物品有序摆放，她会记得给家里的芒果树按时浇水，还有，她喜欢花，收到花她会开心的像个孩子。有她在身边，就会感觉，世界上没有难的住的事，所有事情都会迎刃而解。\n她选择了我，她的爱如水一样自然柔和，真心诚意。我知道遇见她有多么的难得，她在我心里有多重。她是那个会陪我到老的人，我是那个坚守承诺的人，我会为她改变，改变自己的所有坏毛病，去帮她一起实现愿望，永远对她好，一起经历这酸甜苦辣。用一生，让她知道“选择你，没有错”。\n2019.03.01于太原\n","permalink":"https://olzhy.github.io/posts/nice-to-meet-you.html","tags":["随笔"],"title":"很开心遇见你"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个链表，对每对相邻节点作交换后返回该链表。\n注：勿改动节点中的值，仅可改动节点顺序。\n例子：\n输入：1-\u0026gt;2-\u0026gt;3-\u0026gt;4\n输出：2-\u0026gt;1-\u0026gt;4-\u0026gt;3\n题目出处：\nhttps://leetcode.com/problems/swap-nodes-in-pairs/\n2 解决思路\n如图所示，使用三个指针p、q、r指向三个相邻的节点，q.Next = p; p.Next = r.Next即完成一次交换。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/24_Swap_Nodes_In_Pairs/test.go\nfunc swapPairs(head *ListNode) *ListNode { if nil == head || nil == head.Next { return head } p := head head = head.Next q := p.Next for nil != q { r := q.Next q.Next = p if nil == r { p.Next = nil break } if nil == r.Next { p.Next = r r.Next = nil break } p.Next = r.Next p = r q = p.Next } return head } ","permalink":"https://olzhy.github.io/posts/leetcode-swap-nodes-in-pairs.html","tags":["Golang","算法"],"title":"LeetCode 24 成对交换节点"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个链表，移除其自末尾起第N个节点后返回该链表。\n例子：\n输入：给定链表1-\u0026gt;2-\u0026gt;3-\u0026gt;4-\u0026gt;5，且n=2\n输出：移除链表末尾起第2个节点4后，链表变为1-\u0026gt;2-\u0026gt;3-\u0026gt;5。\n题目出处：\nhttps://leetcode.com/problems/remove-nth-node-from-end-of-list/\n2 解决思路\n两个指针初始均指向链表头部，然后让第一个指针先走N步；\n这时，第二个指针开始与第一个指针同时走，当第一个指针到达尾部节点时，第二个指针刚好到达要移除节点的上一个节点。\n这样，将第二个指针的下一个节点指向下下个节点即为所求。\n注：特殊情况为，第一个指针走了N步时，所指的是尾节点的下一个节点，即nil，这时说明要移除的节点是头节点，该种情况返回头节点的下一个节点即可。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/19_Remove_Nth_Node_From_End_Of_List/test.go\nfunc removeNthFromEnd(head *ListNode, n int) *ListNode { p, q := head, head for ; n \u0026gt; 0; n-- { p = p.Next } if nil == p { return head.Next } for nil != p.Next { p = p.Next q = q.Next } q.Next = q.Next.Next return head } ","permalink":"https://olzhy.github.io/posts/leetcode-remove-nth-node-from-end-of-list.html","tags":["Golang","算法"],"title":"LeetCode 19 移除链表末尾起第N个节点"},{"categories":["计算机"],"contents":"1 题目描述\n实现atoi函数，以将字符串转换为一个整数。\n该函数首先丢弃尽可能多的空格字符，直至找到第一个非空格字符。然后由该字符开始（可能会有正负标志）找出尽可能多的数字字符，最后将其转换为一个整数。\n在连续数值字符后可能还会有其他字符，请将这些字符略过，并不影响函数行为。\n若字符串第一个非空格字符已非数字字符，或该字符串为空，甚至该字符串为纯空格字符串，其为无效字符串，返回0即可。\n注意：\na）仅认为' \u0026lsquo;为空格字符；\nb）假定运行环境存储整数范围属于[−231, 231−1]，即32位有符号整数范围。若数值超过该表示范围限制，返回INT_MAX(2^31−1)或INT_MIN(−2^31)。\n例子1：\n输入：\u0026ldquo;42\u0026rdquo;\n输出：42\n例子2：\n输入：\u0026quot; -42\u0026quot;\n输出：-42\n释义：第一个非空字符是\u0026rsquo;-'，然后取尽可能最多的数位，得到整数42。\n例子3：\n输入：\u0026ldquo;4193 with words\u0026rdquo;\n输出：4193\n释义：取到3时停止，因后面的字符非数字。\n例子4：\n输入：\u0026ldquo;words and 987\u0026rdquo;\n输出：0\n释义：第一个非空字符为\u0026rsquo;w'，不是数字也不是+/-符号，因此无需进行后续字符判断，直接返回0。\n例子5：\n输入：\u0026quot;-91283472332\u0026quot;\n输出：-2147483648\n释义：\u0026quot;-91283472332\u0026quot;超过了32位有符号整数表示范围，因此返回INT_MIN(−2^31)。\n题目出处：\nhttps://leetcode.com/problems/string-to-integer-atoi/\n2 解决思路\n首先trim掉头部空格字符，找到第一个非空格字符：\n若为'+'，自下一个字符遍历该字符串，叠加所有连续数字字符，直至找到最大的正整数（若扩展过程变为负数，说明越界，返回32位最大正整数）；\n若为'-'，将negtive设为true，自下一个字符遍历该字符串，叠加所有连续数字字符，直至找到最大的负整数（若扩展过程发现小于最小负整数，说明越界，返回32位最大负整数）；\n若为数字字符，自当前字符遍历该字符串，叠加所有连续数字字符，直至找到最大的正整数（若扩展过程变为负数，说明越界，返回32位最大正整数）。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/8_String_To_Integer/test.go\nconst ( MaxInt = 1\u0026lt;\u0026lt;31 - 1 MinInt = -(1 \u0026lt;\u0026lt; 31) ) func myAtoi(str string) int { v := 0 i := 0 negtive := false // trim blank prefix  for ; i \u0026lt; len(str); i++ { if \u0026#39; \u0026#39; != str[i] { break } } if i \u0026gt; len(str)-1 { return v } // first non-blank char  if \u0026#39;+\u0026#39; == str[i] { i++ } else if \u0026#39;-\u0026#39; == str[i] { negtive = true i++ } else if str[i] \u0026lt; \u0026#39;0\u0026#39; || str[i] \u0026gt; \u0026#39;9\u0026#39; { return v } // integer  for ; i \u0026lt; len(str); i++ { if str[i] \u0026lt; \u0026#39;0\u0026#39; || str[i] \u0026gt; \u0026#39;9\u0026#39; { break } v = 10*v + int(str[i]-\u0026#39;0\u0026#39;) if negtive { if -v \u0026lt;= MinInt { return MinInt } continue } if v \u0026lt; 0 || v \u0026gt; MaxInt { return MaxInt } } if negtive { v = -v } return v } ","permalink":"https://olzhy.github.io/posts/leetcode-string-to-integer.html","tags":["Golang","算法"],"title":"LeetCode 8 字符串转整数"},{"categories":["计算机"],"contents":"1 题目描述\n对给定字符串s，找出其最长回文子串（假定s的最大长度为1000）。\n例子1：\n输入：\u0026ldquo;babad\u0026rdquo;\n输出：\u0026ldquo;bab\u0026rdquo;\n释义：\u0026ldquo;aba\u0026quot;同样是一个有效答案\n例子2：\n输入：\u0026ldquo;cbbd\u0026rdquo;\n输出：\u0026ldquo;bb\u0026rdquo;\n例子3：\n输入：\u0026ldquo;cbbc\u0026rdquo;\n输出：\u0026ldquo;cbbc\u0026rdquo;\n题目出处：\nhttps://leetcode.com/problems/longest-palindromic-substring/\n2 解决思路\n有两类回文情况：abba类型与aba类型，即一个轴对称（轴非某个字符），一个关于中间的某个字符对称。\n遍历字符串，分别计算两类情况的最长子串（若满足就一直向两边扩大，直至找到最长子串），遍历完成即找出全局最长的回文子串。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/5_Longest_Palindromic_Substring/test.go\nfunc longestPalindrome(s string) string { if len(s) \u0026lt; 2 { return s } longest := s[0:1] for i := 1; i \u0026lt; len(s); i++ { for rightStep := 0; rightStep \u0026lt; 2; rightStep++ { for p, q := i-1, i+rightStep; p \u0026gt;= 0 \u0026amp;\u0026amp; q \u0026lt; len(s) \u0026amp;\u0026amp; s[p] == s[q]; { if q-p+1 \u0026gt; len(longest) { longest = s[p : q+1] } p-- q++ } } } return longest } ","permalink":"https://olzhy.github.io/posts/leetcode-longest-palindromic-substring.html","tags":["Golang","算法"],"title":"LeetCode 5 最长回文子串"},{"categories":["计算机"],"contents":"1 题目描述\n给定两个代表两个非负整数的非空链表。数字在链表以逆序存储且链表的每个节点均包含一位数字，将两数相加且以链表返回。\n您可以假设，除数字0外，两数都不会以0开头。\n例子：\n输入：(2 -\u0026gt; 4 -\u0026gt; 3) + (5 -\u0026gt; 6 -\u0026gt; 4)\n输出：7 -\u0026gt; 0 -\u0026gt; 8\n释义：342 + 465 = 807\n题目出处：\nhttps://leetcode.com/problems/add-two-numbers/\n2 解决思路\n初始进位为0，由头至尾同时遍历两链表，即由数字地位到高位遍历链表，将两指针所指两链表对应位置的两节点的数字相加，以抹去进位的数字给结果链表的当前节点赋值，然后将进位提供给下一个节点计算时使用。\n直至两个链表均已遍历完成且进位为0时返回结果。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/2_Add_Two_Numbers/test.go\nfunc addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { l := \u0026amp;ListNode{} p, q, r := l1, l2, l carry := 0 for nil != p || nil != q || carry \u0026gt; 0 { v := carry if nil != p { v += p.Val p = p.Next } if nil != q { v += q.Val q = q.Next } carry, r.Val = v/10, v%10 if nil != p || nil != q || carry \u0026gt; 0 { r.Next = \u0026amp;ListNode{} r = r.Next } } return l } ","permalink":"https://olzhy.github.io/posts/leetcode-add-two-numbers.html","tags":["Golang","算法"],"title":"LeetCode 2 两数相加"},{"categories":["计算机"],"contents":"1 题目描述\n罗马数字由7种符号（I，V，X，L，C，D，M）表示。\n与数值对应关系如下表：\n符号 值 I 1 V 5 X 10 L 50 C 100 D 500 M 1000 如，2的罗马数字写作II，即两个1的相加。然而4的罗马数字非IIII，而是写作IV，将1放在5之前，即5-1。同理，9写作IX。该种作减法的情形有如下6种：\na）I放在V(5)或X(10)前，表示4或9；\nb）X放在L(50)或C(100)前，表示40或90；\nc）C放在D(500)或M(1000)前，表示400或900。\n现给定一个整数，将其转换为罗马数（输入整数的区间为[1, 3999]）。\n例子1：\n输入：3\n输出：\u0026ldquo;III\u0026rdquo;\n例子2：\n输入：4\n输出：\u0026ldquo;IV\u0026rdquo;\n例子3：\n输入：9\n输出：\u0026ldquo;IX\u0026rdquo;\n例子4：\n输入：58\n输出：\u0026ldquo;LVIII\u0026rdquo;\n释义：L=50，V=5，III=3\n例子5：\n输入：1994\n输出：\u0026ldquo;MCMXCIV\u0026rdquo;\n释义：M=1000，CM=900，XC=90，IV=4\n题目出处：\nhttps://leetcode.com/problems/integer-to-roman/\n2 解决思路\n首先，建立一个阿拉伯整数与其罗马数表示的对应表；\n然后，将该表的key数组排序；\n最后，遍历该key数组，找到当前数可以减去的最大值，查表返回其罗马数，拼接上减去该最大值后的数的罗马数，递归直至被减数与减数相等时返回结果。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/12_Integer_To_Roman/test.go\nvar ( table = map[int]string{ 1: \u0026#34;I\u0026#34;, 5: \u0026#34;V\u0026#34;, 10: \u0026#34;X\u0026#34;, 50: \u0026#34;L\u0026#34;, 100: \u0026#34;C\u0026#34;, 500: \u0026#34;D\u0026#34;, 1000: \u0026#34;M\u0026#34;, 4: \u0026#34;IV\u0026#34;, 9: \u0026#34;IX\u0026#34;, 40: \u0026#34;XL\u0026#34;, 90: \u0026#34;XC\u0026#34;, 400: \u0026#34;CD\u0026#34;, 900: \u0026#34;CM\u0026#34;, } romans = func() []int { var keys []int for k := range table { keys = append(keys, k) } sort.Ints(keys) return keys }() ) func intToRoman(num int) string { subtrahend := romans[len(romans)-1] for i, v := range romans { if num \u0026lt; v { subtrahend = romans[i-1] break } } if 0 == num-subtrahend { return table[subtrahend] } return table[subtrahend] + intToRoman(num-subtrahend) } ","permalink":"https://olzhy.github.io/posts/leetcode-integer-to-roman.html","tags":["Golang","算法"],"title":"LeetCode 12 整数转罗马数"},{"categories":["计算机"],"contents":"1 题目描述\n一个UTF8编码的字符是满足如下规则的1~4字节长的字符。\na）对单字节字符，第一个bit位为0；\nb）对n字节字符，前n个bit位全为1，第n+1个bit位是0，然后接着n-1个字节的前两个bit位均是10。\n综上，UTF-8编码字符可以参考下表：\n 十进制表示 | 8位一组二进制表示 --------------------+------------------------------------ 0000 0000-0000 007F | 0xxxxxxx 0000 0080-0000 07FF | 110xxxxx 10xxxxxx 0000 0800-0000 FFFF | 1110xxxx 10xxxxxx 10xxxxxx 0001 0000-0010 FFFF | 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx 给定一组表示输入数据的整数，然后输出其是否以UTF-8编码。\n注意：输入为整数数组，仅整数的低8位用来存储数据，即每个整数仅表示一个字节的数据。\n例子1：\n输入：[197, 130, 1]\n输出：true\n释义：输入所表示的8位一组的序列为11000101 10000010 00000001，前两个字节为有效UTF-8两字节字符，后一个字节为有效UTF-8单字节字符。\n例子2：\n输入：[235, 140, 4]\n输出：false\n释义：输入所表示的8位一组的序列为11101011 10001100 00000100，第1个字节表示其是一个3字节字符，第2个字节以10开始满足规则，第3个字节不满足规则，所以该序列不是有效UTF-8字符序列。\n题目出处：\nhttps://leetcode.com/problems/utf-8-validation/\n2 解决思路\n针对满足UTF-8规则的四类情况，制定4个“模”，然后分别将“模”与输入数据进行位运算判断是否满足四类情形的任一种，不满足直接返回false，满足则递归直至数据末尾。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/393_UTF8_Validation/test.go\nfunc validUtf8(data []int) bool { if 0 == len(data) { return true } switch { case 0x00 == 0x80\u0026amp;data[0]: return validUtf8(data[1:]) case 0xC0 == 0xE0\u0026amp;data[0] \u0026amp;\u0026amp; len(data) \u0026gt; 1 \u0026amp;\u0026amp; 0x80 == 0xC0\u0026amp;data[1]: return validUtf8(data[2:]) case 0xE0 == 0xF0\u0026amp;data[0] \u0026amp;\u0026amp; len(data) \u0026gt; 2 \u0026amp;\u0026amp; 0x80 == 0xC0\u0026amp;data[1] \u0026amp;\u0026amp; 0x80 == 0xC0\u0026amp;data[2]: return validUtf8(data[3:]) case 0xF0 == 0xF8\u0026amp;data[0] \u0026amp;\u0026amp; len(data) \u0026gt; 3 \u0026amp;\u0026amp; 0x80 == 0xC0\u0026amp;data[1] \u0026amp;\u0026amp; 0x80 == 0xC0\u0026amp;data[2] \u0026amp;\u0026amp; 0x80 == 0xC0\u0026amp;data[3]: return validUtf8(data[4:]) } return false } ","permalink":"https://olzhy.github.io/posts/leetcode-utf8-validation.html","tags":["Golang","算法"],"title":"LeetCode 393 UTF-8编码校验"},{"categories":["计算机"],"contents":"1 Go 1.5简介\nGo 1.5是一个重要的版本，包括主要实现结构调整。尽管这样，我们期待绝大多数程序可以像之前一样编译、运行（因该版本仍遵守Go 1兼容性承诺）。\n几项大的特性：\na）编译器及运行时完全用Go重写，实现已没有C，构建及发布对C编译器的依赖已一去不复返；\nb）垃圾收集器已并行化，且已显著降低停顿次数，且尽可能与其他go routine一起并行工作；\nc）Go程序运行默认以GOMAXPROCS参数设置可用核数，之前版本其默认置为1；\nd）不仅Go核心代码，所有仓库已支持内部包；\ne）go命令目前对外部“vendoring”依赖提供试验性支持；\nf）新的go tool trace命令对追踪程序执行提供更细粒度的支持；\ng）新的go doc命令（不同于godoc）只为命令行使用。\n如上几条及实现与工具的几项变化将在下面展开讨论。\n同时，该版本包含一项对map迭代的小的语言级变更。\n最后，发布时间未按6个月的发布间隔如期发布是为了有更多时间准备这个较大的版本。此后发布时间会更具弹性。\n2 语言变化\n Map literals 因疏忽，slice中省略元素类型的语法未应用到map的key中，我们在Go 1.5中作了修正。参看一个例子即会明白:  m := map[Point]string{ Point{29.935523, 52.891566}: \u0026#34;Persepolis\u0026#34;, Point{-25.352594, 131.034361}: \u0026#34;Uluru\u0026#34;, Point{37.422455, -122.084306}: \u0026#34;Googleplex\u0026#34;, } 如上代码可以省略Point类型，直接写作：\nm := map[Point]string{ {29.935523, 52.891566}: \u0026#34;Persepolis\u0026#34;, {-25.352594, 131.034361}: \u0026#34;Uluru\u0026#34;, {37.422455, -122.084306}: \u0026#34;Googleplex\u0026#34;, } 3 实现\n No more C 没有C，编译器及运行时目前已使用Go与汇编器实现。剩余的C源码仅与测试或cgo有关。在1.4及早期版本有C的编译器。其用来构建运行时。一个自定义编译器是必要的，部分是保障C代码与goroutine的栈管理正常工作。因目前运行时已使用Go实现，所以已没有使用该C编译器的必要。移除C的过程详情在别处作了讨论。  该项转换是在自定义工具的辅助下实现的。更重要的是，编译器的C代码实际上是自动转换为Go代码的。其实际是不同语言的同一段程序。因其不是对编译器的一种新的实现，所以我们期望该项转换没有引入新的编译器bug。该项转换过程的概览请参阅ppt。\n Compiler and tools 不依赖但受移至Go所鼓励，工具名称已发生改变。之前旧的名称如6g、8g已不复存在。取而代之，仅有一个执行命令go tool compile，其可以将Go源码编译为适配由$GOARCH和$GOOS所指定的体系结构或操作系统的二进制。同样，现在仅有一个链接器（go tool link）及一个汇编器（go tool asm）。链接器由旧的C实现自动转换而来，但汇编器是一个全新的原生Go实现（将在后边详细讨论）。  与删除6g、8g等名称相似，目前编译器及汇编器的输出是一个纯.o的后缀，而非.8、.6等。\n Garbage collector 垃圾收集器，作为1.5开发的一部分，已重新设计（设计文档概述）。  通过一组高级算法、更好的调度，以及在用户程序可以并行运行更多的收集器，其预期延迟远低于之前发布的版本。收集器的“stop the world”阶段耗时小于10ms且通常会更少。\n对于从低延迟获益的系统，如用户响应式网站，使用新版收集器带来的预期延迟减少可能会更重要。\n新版收集器详情请参看GopherCon 2015的演讲。\n Runtime 在Go 1.5，多goroutine中的哪个goroutine会被调度的顺序发生了变化。调度器特性从未被语言定义，但依赖调度器顺序的程序可能会受这一变化的影响。我们已看到一些（错误）程序受到该变化的影响。  若您有隐式依赖调度器顺序的程序，请作修改。\n另一个潜在的破坏性变化是，运行时目前已将同时运行的线程数默认值（GOMAXPROCS）设置为CPU可用核数。而在之前的发布版本，默认值为1。不期望以多核运行的程序可能会无意中受到影响。其可以通过移除限制或显式设置GOMAXPROCS来更改程序。对于这一变化的更为详尽的讨论，请参看设计文档。\n Build 目前Go编译器及运行时已使用Go实现，一个Go编译器必须可用于对源码进行版本编译。因此，要构建Go内核，必须已有一个工作的Go版本（不在Go内核进行工作的Go程序员不受影响）。任何Go 1.4或之前的版本都可以提供该能力。详情请参看设计文档。  4 接口\n主要由于工业界对32位x86体系结构的不再支持，在1.5提供的二进制下载包已精简。对OS X操作系统的发布版本，仅提供对amd64体系结构的支持，而非386。同样，因Apple不再维护Snow Leopard（Apple OS X 10.6）操作系统，我们对该操作系统提供的接口仍会工作，但不再发布下载版本也不再维护。同样，因DragonflyBSD不再支持32位386体系结构，我们也不再支持dragonfly/386接口。\n然而，有几个新接口使用源码构建后是可用的。其包含darwin/arm及darwin/arm64。新接口linux/arm64通常已有，但cgo仅使用外部链接支持。\n同样，ppc64及ppc64le（64位PowerPC，大小字节）作为试验可用。这些接口支持使用内部链接的cgo。\n在FreeBSD上，Go 1.5需要FreeBSD 8-STABLE+版本，因其使用了新的SYSCALL指令。\n在NaCl，Go 1.5需要SDK版本pepper-41。旧的pepper因从NaCl运行时移除sRPC子系统而发生了不兼容性。\n在Darwin上，使用系统X.509证书接口会因ios的构建tag而失效。\n只要作一些修改已改进，Solaris接口目前已全部支持cgo及net与crypto/x509包。\n5 工具\n Translating 作为从源码树中移除C过程的一部分，编译器与链接器已从C翻译为Go。其是一个纯粹的（机器辅助下）翻译，所以新的程序实质是旧程序的翻译而不是引入新bug的全新程序。我们相信，如果会有bug，翻译过程也仅会引入很少的bug，而且，事实上还发现了一组之前未知的bug，目前已修复。  而汇编器是一个新的程序，会在下边讨论。\n Renaming 编译器（6g、8g等）、汇编器（6a、8a等）及链接器（6l、8l等）程序套件中的每个都已统一为一个单独的工具（工具通过GOOS与GOARCH环境变量配置）。旧的名字已不复存在。新的工具可以通过go tool compile、go tool asm及go tool link来使用。同样，如.6、.8等中间文件后缀也已不复存在，目前，它们统一为纯.o文件。  如，告别go build，要在基于amd64的Darwin系统上使用工作直接构建及链接一个程序，可以运行：\n$ export GOOS=darwin GOARCH=amd64 $ go tool compile program.go $ go tool link program.o   Moving 因go/types包目前已移至主仓库（参看下边），vet及cover工具也已移走。尽管为了兼容，废弃的源码仍在旧的发布版本，但其不再在外部仓库golang.org/x/tools维护。\n  Compiler 如上已述，Go 1.5的编译器为一个从旧的C源码翻译过来的取代6g、8g等的单独的Go程序。其目标文件可以由GOOS、GOARCH指定。\n  1.5的编译器已极其接近于旧的，但一些内部细节有所变化。一个重要的变化是，目前使用math/big包而非自定义高精度算法实现（未被充分测试）来进行常量的评估。我们不期望该变化会影响结果。\n仅对amd64体系结构，编译器有一个新的选项-dynlink，其通过支持对外部共享库的Go数据类型引用来辅助动态链接。\n Assembler 跟编译器及链接器相似，Go 1.5的汇编器是一个替换汇编器套件（6a、8a等）的单独程序，且使用GOARCH及GOOS环境变量来配置体系结构和操作系统。不同于其他程序，汇编器是使用Go编写的全新程序。  新的汇编器与之前的非常接近，但几项变化可能会影响一些汇编器源文件。参看“汇编器引导”更新文档来查阅关于这些变化的更多确切的信息。\n首先，对用于常量的表达式估算有一点不同。其目前使用64位无符号算法和来自Go（非C）的操作符（+、-、\u0026laquo;等）优先级。我们期望这些变化影响极少的程序，但手动验证可能是需要的。 可能更重要的是，SP或PC仅是机器上编号寄存器的别名，如R13表示栈指针，R15表示ARM上的硬件程序计数器，对该寄存器的不含符号的引用是不合法的。如，SP及4(SP)时不合法的，但sym+4(SP)是合法的。在这些机器上，要指定硬件寄存器需使用其真实的R名称。 一个小的变化是，旧的汇编器允许如下方式来定义一个命名常量。 constant=value\n其总是可能与类C的#define定义方式（汇编器包含一个简单的C预处理器实现，是支持的）相似，该特性目前已移除。\n Linker Go 1.5的链接器是一个替换6l、8l等的新的Go程序。其操作系统及指令集通过GOOS与GOARCH来指定。  有几项其它变化。最重要的是增加了一个扩展链接风格的-buildmode选项。其目前支持诸如构建共享库及允许其它语言调入Go库的场景。这些中的一些已在设计文档概述。\n列出可用的构建模式，可以使用如下命令。\n$ go help buildmode 另一项小变化是，链接器不在于Windows可执行文件头部记录构建时间戳。同样，尽管这个可能是固定的，但Windows cgo可执行文件丢失了一些DWARF信息。最后，-X标记需传入两个参数。\n-X importpath.name value 目前，也接受一个更通用的Go标记样式，及单参数name=value对。\n-X importpath.name=value 尽管旧的语法仍然工作，但推荐在脚本总使用新的标记，希望更新为新的方式。\n Go command go命令的基础操作未变化，但有几项变化值得一提。  之前的版本引入了internal文件夹的概念，包内的internal包不可通过go命令引到。在1.4，其已通过在核心库引入一些internal元素而被测试。正如设计文档建议，该变化目前已对所有仓库生效。规则已在设计文档说明，概括来说即是位于internal文件夹或子文件夹下的包，仅可被根与internal文件夹位于同样子树的包引用。\n使用internal文件夹的已有包可能会无意中被该变化所影响，这也是为什么在上一次发布特别说明的原因。\n另一项关于包如何处理的变化是支持“vendoring”的试验性加入。详情请参看go命令文档及设计文档。\n仍有几项小的变化，参看文档查阅详情。\na）SWIG支持目前已更新，诸如.swig或.swigcxx需要SWIG 3.0.6或之后的版本。\nb）install子命令移除了在源码文件夹由build子命令所创建的二进制文件。若存在，避免树下有两个二进制文件存在而引起问题。\nc）std（标准库）通配符包名称已移除命令相关。新的cmd通配符覆盖了命令相关。\nd）新的-asmflags构建选项设置传给汇编器的标记。然而-ccflags构建选项已被移除，其特指旧的，目前已删除的编译器。\ne）新的-buildmode构建选项用来设置构建模式，已在如上讨论。\nf）新的-pkgdir选项用来设置已安装包的位置，以帮助隔离自定义构建。\ng）新的-toolexec选项替代的一个不同的命令以调用汇编器等。其作为go tool的自定义替换。\nh）test子命令目前有一个-count标记，用来指定运行测试及基准测试多少次，测试包通过-test.count来做该项工作。\ni）generate子命令有几项新特性。-run选项指定一个正则表达式来选择执行哪个命令。该项特性已建议，但未在1.4实现。执行模式目前需访问两个环境变量：$GOLINE返回指令的源码行号，$DOLLAR扩展$符。\nj）get目前有一个-insecure标记，用来获取非安全仓库（不加密连接）时开启。\n  Go vet command go tool vet命令目前通过校验结构体tag来做的更多。\n  Trace command 一个对Go程序动态执行跟踪的新的工具可用。其使用方式与测试覆盖工具的使用相似。跟踪套件已集成至go test，然后执行跟踪工具即可分析结果。\n  $ go test -trace=trace.out path/to/package $ go tool trace [flags] pkg.test trace.out flags可以使输出结果在浏览器访问。更多详情请运行：\nsh go tool trace -help 有一个跟踪能力描述，请参看GopherCon 2015的演讲。\n  Go doc command 几次发布后，go doc命令因已变得没必要而被删除。您仍可运行“godoc .”来代之。1.5版本引入了一个新的go doc命令，其比godoc含有更多便捷的命令行接口。其设计特别用于命令行使用，根据调用链，提供对一个包或其元素的更紧凑更聚焦的文档呈现。其仍提供非大小写敏感匹配，并且支持未导出标记的文档展示。运行“go help doc”来查看详情。\n  Cgo 当处理#cgo行时，${SRCDIR}调用目前已将路劲扩展至源码文件夹。其允许选项传至编译器及链接器，以包含源码文件夹相关的文件路劲。当前工作文件夹变化时，没有扩展路劲是无效的。\n  Solaris目前提供全部的cgo支持。在Windows，cgo目前默认使用外部链接。\n当一个C结构体本身非空值，但以空值字段结尾时，Go代码不能再指向该空值字段。任何此类引用均需改写。\n6 性能\n一如既往，变化如此普遍，以致难以对性能作精确的陈述。此版本变化极广，包含一个新的垃圾收集器以及运行时到Go的转换。有些程序可能会运行更快，有些可能会更慢。鉴于如上所提及，垃圾收集器停顿时间显著缩短，平均来讲，以Go 1基准套件运行的程序在Go 1.5上会比Go 1.4上快几个百分点且总低于10ms。\n以Go 1.5构建大约会慢一半。编译器及链接器从C自动转换为Go，导致不符合语言习惯的Go代码较写的好的Go表现差。分析工具及重构会帮助改进代码，但大多工作仍然待做。后续的分析及优化将在Go 1.6及后续版本继续下去。更多详情请参阅这些ppt及相关视频。\n7 核心库\n Flag flag包的PrintDefaults函数以及FlagSet的方法已作修改，以创建更好的使用信息。格式已变得更加对人友好，且在使用信息中，以`背引号`引用的字会被认为是flag的操作数，以在使用信息中显示。如，使用如下方式创建的flag。  cpuFlag = flag.Int(\u0026#34;cpu\u0026#34;, 1, \u0026#34;run `N` processes in parallel\u0026#34;) 将展示帮助信息。\n-cpu N run N processes in parallel (default 1) 同样，仅当其为类型的非0值时，默认值被展示了出来。\n  Floats in math/big math/big包有一个新的基础数据类型Float，其实现了任意精度的浮点数。一个Float值通过一个布尔标记、一个变长尾数及一个32位固定大小的带符号指数表示。Float（bit位尾数）的精度可以被显式指定，否则会被创建该值的第一个操作数所决定。一旦创建，Float的尾数可以由SetPrec方法来修改。Float支持无限的概念，诸如被溢出创建，但将导致等于IEEE 754 NaN的值出发异常。Float操作数支持所有IEEE-754舍入方式。当精度设置为24 (53)位时，因在这些值上使用IEEE-754算法，在float32 (float64)常规范围的操作数产生同样的结果。\n  Go types go/types包目前已在golang.org/x仓库维护。截止Go 1.5，其已移至主仓库。位于旧位置的代码已废弃。也有一个适当的包API变更，在如下讨论。\n  与该项移动关联，go/constant包也已移至主仓库，其之前是golang.org/x/tools/exact。如如上工具一样，go/importer包也已移至主仓库。\n Net net包的DNS解析器几乎总使用cgo来访问系统接口。Go 1.5的一个变化是，在Unix系统的绝大多数DNS解析将不再需要cgo，因其简化了在这些平台的执行操作。目前，若系统网络配置允许，原生Go解析器将可满足。这一变化的一个重要的结果是，每个DNS解析使用一个goroutine而非一个线程，所以一个需解析多个DNS请求的程序将消耗更少的操作系统资源。  如何来运行解析器的决定应用在运行时而非构建时。尽管其仍会工作，但已用于增强Go解析器使用的netgo构建标签将不再必须。一个新的netcgo标签用于在构建时强制使用cgo解析器。欲强制在运行时使用cgo解析器，需设置环境变量GODEBUG=netdns=cgo。更多debug选项列在了这里。\n该变化仅适用于Unix系统。Windows、Mac OS X及Plan 9系统表现跟之前一样。\n  Reflect reflect包有两个新的函数：ArrayOf和FuncOf。这些函数与当前的SliceOf函数类似，在运行时创建新的类型以描述数组和函数。\n  Hardening 通过使用go-fuzz工具的随机测试，标准库的几十个bug被发现。在如下包的bug被修复。\n  archive/tar、archive/zip、compress/flate、encoding/gob、fmt、html/template、image/gif、image/jpeg、image/png及text/template，这些修复增强了实现以防止不正确的及恶意的输入。\n Minor changes to the library https://golang.org/doc/go1.5#minor_library_changes   参考资料\n[1] https://golang.org/doc/go1.5\n ","permalink":"https://olzhy.github.io/posts/go1dot5-release-notes-reading.html","tags":["Golang"],"title":"Go 1.5 Release Notes 研读"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个仅包含数字的字符串，通过返回所有有效的IP地址组合来还原它。\n例子：\n输入：\u0026ldquo;25525511135\u0026rdquo;\n输出：[\u0026ldquo;255.255.11.135\u0026rdquo;, \u0026ldquo;255.255.111.35\u0026rdquo;]\n题目出处：\nhttps://leetcode.com/problems/restore-ip-addresses/\n2 解决思路\n采用递归算法，require标识所需的数字段。\na）从最左分别取1-3个满足0~255的数字；\nb）递归处理剩余字符串，且所需的数字段变为require-1；\nc）若require为1，判断是否满足ip段内数字要求，满足返回，不满足返回空数组；\nd）将a、b两步所得结果拼接为数组返回。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/93_Restore_IP_Addresses/test.go\nfunc restoreIpAddresses(s string) []string { return restore(s, 4) } func restore(s string, require int) []string { if 1 == require { if len(s) \u0026gt; 1 \u0026amp;\u0026amp; \u0026#39;0\u0026#39; == s[0] { return []string{} } if v, _ := strconv.Atoi(s); v \u0026lt; 256 { return []string{s} } return []string{} } var r []string for i := 1; i \u0026lt; 4 \u0026amp;\u0026amp; i+require-1 \u0026lt;= len(s); i++ { prefix := s[:i] if v, _ := strconv.Atoi(prefix); v \u0026lt; 256 { for _, j := range restore(s[i:], require-1) { r = append(r, prefix+\u0026#34;.\u0026#34;+j) } } if \u0026#39;0\u0026#39; == s[0] { break } } return r } ","permalink":"https://olzhy.github.io/posts/leetcode-restore-ip-addresses.html","tags":["Golang","算法"],"title":"LeetCode 93 还原IP地址"},{"categories":["计算机"],"contents":"1 Go 1.4简介\nGo 1.4在Go 1.3发布6个月后如期而至。\n语言级仅有一个向后兼容的小变化，即for-range循环。还有一个可能会破坏编译器规则的变化，即指针的指针的方法调用。\n本版本重点在实现工作上，如改进垃圾收集器性能，为构建一个完全并发的垃圾收集器（将在后续版本推出）做准备工作。本版本的栈在必要时是邻接的、可再分配的，而非链接到一个新的“段”上，因此规避了“hot stack split”问题。本版本有一组新的工具可用，包括在go命令支持构建时源码生成。同时，本版本增加了对ARM处理器在Android、NaCl上的支持，及对AMD64在Plan 9上的支持。\n一如既往，Go 1.4秉承兼容性承诺，一切不作任何变动即可在Go 1.4上编译运行。\n2 语言级变化\n For-range loops 截至Go 1.3，for-range循环有两种方式：  for i, v := range x { ... } for i := range x { ... } 若仅想使用循环，并不关注循环变量值，range前的变量仍不可省（该种情况可能使用下划线，如for _ = range x），因如下方式语法上不允许：\nfor range x { ... } 针对该种场景，之前的处理方式有点笨拙。所以在Go 1.4，“自由变量式”For-range循环的写法是合法的。\n如下为一个定时任务的样例：\nfor range time.Tick(time.Second) { ... }  Method calls on **T 给定如下声明：  type T int func (T) M() {} var x **T 之前，gc与gccgo接受如下方式的调用：\nx.M() 其是对指针的指针变量x的两次解引用，Go说明书允许一次解引用，非两次，所以根据语言定义，该调用是错误的。因此，该调用在Go 1.4是不允许的。尽管非常少的程序会受影响，该项变化是一个不兼容的变化。\n3 支持的操作系统与体系结构上的变化\n  Android Go 1.4能够为运行Android操作系统的ARM处理器构建二进制文件。Go 1.4也能够构建能被Android应用加载的.so库文件（使用mobile子仓库中支持的包）。详细请参看：https://golang.org/s/go14android。\n  NaCl on ARM 之前版本引入对NaCl在32位x86（GOARCH=386）及在使用32位指针的64位x86（GOARCH=amd64p32）上的支持。Go 1.4增加了对NaCl在ARM（GOARCH=arm）上的支持。\n  Plan9 on AMD64 本版本增加了对Plan 9操作系统在AMD64处理器上的支持。提供kernel支持nsec系统调用，并且使用4K页。\n  4 兼容性准则变化\nunsafe包允许人们利用内部实现细节或机器数据表达从而超越Go类型系统所限来做一些事情。Go兼容性准则从未显示指明unsafe包的何种使用是遵从兼容性准则的。当然我们对作非安全事情的代码不作兼容性承诺。我们已在发布版本包含的文档中阐明该情况。Go兼容性准则及unsafe包文档目前已明确非安全代码不受兼容性保障。\n5 实现及工具级变化\n Changes to the runtime Go 1.4之前，运行时（垃圾收集器，并发支持，接口管理，map，slices，string等）绝大部分是由C写的，结合了一些汇编器支持。在Go 1.4，大部分代码已翻译为Go，便于垃圾收集器可以扫描到运行时程序栈及获取关于哪些变量是活跃的的准确信息。该变动虽很大，但不会有程序语法上的影响。  该项重写让垃圾搜集器更精确，意味着可以观测到程序中所有活跃指针的位置。因不会再有保持空指针存活的误报，这意味着堆会更小。其他相关变化也减少了堆大小，堆相对之前版本小了10%-30%。\n一个结果是栈不再是分段了，避免了“hot split”问题。当达到栈大小，新的更大的栈将会被分配，所有goroutine的活跃帧被拷贝过去，且所有栈上的指针已被更新。某些场景性能会有显著提升且更可具预测性。详细请参阅：https://golang.org/s/contigstacks。\n邻接栈的使用意味着栈可以更小启动且不会触发性能问题，所以在Go 1.4，一个goroutine栈的默认启动大小已从8192个字节减到2048个字节。\n为并发垃圾收集器作准备，计划在1.5版本，在堆上写指针值当前是通过函数调用实现的（陈作写屏障），而不是直接来自函数更新值。在Go 1.5，当堆处于运行中，该项技术允许垃圾收集器在堆上间接写。该变化对1.4程序没有语法影响，但发布版本包含该变化，便于测试编译器及结果性能。\n接口值实现已被修改。在之前的版本，取决于实际对象存储类型，接口包含一个指针或一个单字游标值。该实现对垃圾收集器是有问题的，所以截至1.4，接口值总是持有一个指针。在运行时程序，多数接口值总是指针类型，所以影响较小，但诸如在接口存储整型的程序将会有更多次的内存分配。\n截至Go 1.3，若发现本应包含有效指针的内存字包含的是无效指针（如整型数3），运行时会崩溃。在指针值存储整型值的程序若撞到该检测则会崩溃。在Go 1.4，可以将GODEBUG变量设置invalidptr=0来作为工作区以避免崩溃，但我们无法保证后续版本可以避免崩溃。正确的修复办法是不要将整型作为指针型的别名。\n Assembly 编译器cmd/5a、cmd/6a及cmd/8a接受的语言有几项变化，主要是使得将类型信息传递至运行时更容易。  首先，定义TEXT指令标记文件textflag.h已从链接器源文件夹拷贝至标准位置，以便用更简洁的指令引用。\n#include \u0026#34;textflag.h\u0026#34;更重要的变化是，汇编器源码如何定义必要的类型信息。更多程序能够将数据定义从汇编移至Go文件，以对每个汇编函数写一个Go定义。\n详情请参考汇编文档。\n更新：包含textflag.h旧路径的文件虽仍能工作，但建议更新。对于类型信息，多数汇编routine无需改动，但需要检查。定义数据的汇编源文件、使用非空栈帧的函数及返回指针的函数需要特别注意。\n  Status of gccgo GCC的发布日程与Go项目不一定一致。GCC 4.9版本包含Go 1.2的gccgo，可能在GCC 5会有Go 1.4版本的gccgo。\n  Internal packages Go的包系统易于将程序组织为边界清晰的组件，但仅有两种访问方式：本地（未导出型）与全局（导出型）。有时，人们想有非导出的组件，如避免获取接口的客户端编码，该代码虽属于公共仓库的一部分，但不想被其所属的程序外使用。\n  Go还没有该项能力，但截至Go 1.4，Go命令引入了一种定义“内部”包的机制，其不可被源码树所在位置的其他包引用。\n想创建一个这样的包，可以将其置于internal文件夹或internal子文件夹下。当Go命令遇到某被引用的包的路径中有internal，即会校验引用包的位置是否位于internal文件夹的父文件件（如包.../a/b/c/internal/d/e/f仅可被位于.../a/b/c文件夹的包引用，不可被.../a/b/g文件夹或其他位置的代码引用）。\nGo 1.4，内部包机制已对主要Go仓库实施。自1.5起，其会对所有仓库实施。\n Canonical import paths 代码常由诸如github.com的开放服务托管，意味着包引用路径常包含服务前缀，如github.com/rsc/pdf。人们可以根据“现有机制”自定义包路径，但会给包创建两个有效引用路径。这样，同一个程序可能会引用一个包的两个不同路径，或将包移至一个不同的托管服务会影响到客户端代码。  Go 1.4引入了在源码指定包权威路径的方式。若某包引用使用非权威路径，go命令将拒绝编译。\n语法很简单：\npackage pdf // import \u0026#34;rsc.io/pdf\u0026#34; 若有如上指定，go命令将拒绝诸如 github.com/rsc/pdf的引用。\n因检查在构建期，非下载期，所以若go get失败，说明错误引用的包已下载至本地，需手动移除。\n  Import paths for the subrepositories Go项目子仓库（code.google.com/p/go.tools等）现采用自定义引用路径golang.org/x/（如golang.org/x/tools）取代code.google.com/p/go。我们将在2015.06.01左右对代码加入权威引用注解，届时，Go 1.4及后续版本将不接受旧的路径（code.google.com）引用。\n  The go generate subcommand go命令有了一个新的子命令go generate，以在编译前自动运行工具来生成源码。如，其可用来运行yacc（基于实现语法的.y文件）compiler-compiler生成Go源码。或使用stringer工具（位于golang.org/x/tools子仓库），对类型常量自动生成String方法。详情请参阅：https://golang.org/s/go1.4-generate。\n  Change to file name handling 构建约束，也叫构建tag，通过引入或移除文件来控制编译（参看/go/build文档）。也可使用文件名本身来控制编译（在.go或.s后缀前加下划线与体系结构或操作系统名称）。如gopher_arm.go文件仅当目标处理器是ARM才会编译。\n  Go 1.4之前，叫作arm.go的文件会被简单打了tag，但当新的体系结构加入时，该行为会破坏源码（将文件突然打了tag）。因此，在1.4，只有下划线的形式才会打tag（tag包括体系结构及操作系统名称）。\n Other changes to the go command cmd/go命令有几项小变化：  a）除非使用cgo来构建包，因相关的c编译器（如6c）会在未来版本的安装包移除，go命令不再支持编译c源文件（目前仅用来构建部分运行时）。因其很难在各种情况下正确使用，所以我们将其关闭。\nb）与其它子命令的标记一致，go test引入了-o标记，以设置结果二进制的名称。无用的-file标记已移除。\nc）即使包里没有Test函数，go test也会编译链接包中的所有*_test.go文件（之前会忽略这些没有Test函数的文件）。\nd）对非开发类安装，go build子命令的-a标记的行为已发生改变。对于一个运行已发布版本的安装，-a标记将不再重新构建标准库及命令，以避免重写安装文件。\n  Changes to package source layout 在Go源码仓库中，包源码放在src/pkg，这样说得通，但不同于包括Go子仓库的其他仓库。在Go 1.4，pkg级的源码树不复存在，所以之前放在src/pkg/fmt的fmt包的源码，现在提高一级，放在src/fmt。\n  SWIG 因该版本的运行时变化，Go 1.4需SWIG 3.0.3。\n  Miscellany 标准仓库的顶级misc文件夹用于包含对编辑器及IDE的Go支持：有插件、初始化脚本等。因列出的编辑器中的许多已不再被核心团队中的成员所使用，维护这些变得耗时并需要额外的帮助。而且需要我们决策一个给定编辑器（甚至我们未使用的编辑器）的哪个插件好用。\n  Go社区更合适维护这些信息。因此，在Go 1.4，该项支持已从仓库移除。代之，维护在该wiki页。\n6 性能相关\n多数程序在1.4运行速度与在1.3相同，或比在1.3稍快一点，也有一些可能会稍慢一点。因有多想改动，所以难以确切预测预期。\n如上已提及，大量运行时已从C转换为Go，会在堆大小上有缩减。因Go编译器优化更佳，所以会提升一点性能（由于诸如内联函数等情形，会比使用C编译器构建运行时快一点）。\n垃圾收集器也加速了，对重度垃圾程序会有可测量的改进。然而，新的写屏障又将事情减速，典型情况是总量一定时，某些程序取决于其行为，可能会变得快一点或慢一点。\n影响性能的库变化会列在下面。\n7 标准库变化\n  New packages 该版本无新包。\n  Major changes to the library a）bufio.Scanner\n  buffo包的Scanner类型有一个bug已被修复，其可能需要改动自定义split函数。该bug使其不能在EOF生成空token，该修复改变了split函数的结束条件。之前，若没有更多数据，扫描停止在EOF。鉴于文档说明，截至1.4，在输入耗尽时，split函数将在EOF调用一次，所以split函数会生成一个最终的空token。\n更新：可能需要修改自定义split函数以处理在EOF的空token。\nb）syscall\nsyscall包已被冻结（除了需要维护核心仓库的改动）。特别是，其不再用来扩展支持未被核心库使用的新的或不同的系统调用。原因详细描述在另一个文档。\n一个新的golang.org/x/sys子仓库为用来支持各种内核的系统调用的开发提供位置。其有更好的结构，采用3个包（Unix，Windows与Plan 9），每个包都有系统调用的实现。这些包将会辅助的更通用一些，接受在这些操作系统的所有反映内核接口的有效改动。\n Minor changes to the library 请看链接。   参考资料\n[1] https://golang.org/doc/go1.4\n ","permalink":"https://olzhy.github.io/posts/go1dot4-release-notes-reading.html","tags":["Golang"],"title":"Go 1.4 Release Notes 研读"},{"categories":["计算机"],"contents":"1 指令顺序调整\n对于单goroutine程序代码，编译器和处理器有时会调整源码中的指令顺序来做一些优化。当然，此类调整在当前gorouine程序来看并不会改变其源码指令所指定的行为。但在多个线程共享内存的情形下，某个goroutine内部的指令顺序调整可能会影响到依赖其指令顺序的其他goroutine的行为。\n看一段代码：\npackage main import \u0026#34;fmt\u0026#34; var s string var done bool func setup() { s = \u0026#34;hello world\u0026#34; done = true if done { fmt.Println(s) } } func main() { go setup() for !done { } fmt.Println(s) } 如上代码，main函数等待setup将s赋值成功，期待打印出“hello world”。但main函数打印结果可能与预期不同，有可能打印为空串。原因在于该代码受编译器版本或运行时影响，即当前程序使用不同的编译器版本或在不同体系结构系统上运行时，结果可能不同。\n原因在于如上代码中的setup函数的两行赋值语句指令顺序可能会被编译器或运行时CPU更改，即变为：\nfunc setup() { done = true s = \u0026#34;hello world\u0026#34; if done { fmt.Println(s) } } 更改后，执行setup的goroutine本身的行为未受影响，其打印结果总会是“hello world”。\n但依赖done变量写入的main函数goroutine的行为会受影响，其打印结果不一定是“hello world”。\n2 Golang内存顺序保证\n从如上例子可以看出，并发场景下，为保障程序逻辑正确性，需要想办法保障不同goroutine中的代码执行先后顺序。\n不同的CPU体系结构提供不同的fence指令来防止指令顺序重排。而直接在代码中使用fence来作逻辑控制，抬高了并发编程的门槛。Golang并未内置直接操作CPU fence指令的函数或方法，而是提供了诸多“happens before”（先于）机制来保障程序执行顺序。\n 初始化 初始化顺序保证：  a）当前包所有包级变量初始化先与init函数执行；\nb）依赖包init函数执行先于当前包包级变量初始化；\nc）所有依赖包包级变量初始化与init函数执行均先于main函数执行。\n所以一个包含依赖包的程序的初始化执行顺序为：\n依赖包包级变量初始化 \u0026lt; 依赖包init函数执行 \u0026lt; 当前包包级变量初始化 \u0026lt; 当前包init函数执行（\u0026lt;表示先于）。 下面用一段代码证明上述初始化顺序。 在$GOPATH/src/github.com/p下有这样一段代码p.go：\npackage p import \u0026#34;fmt\u0026#34; var a = func() int { fmt.Println(\u0026#34;variable init in p\u0026#34;); return 1 }() func init() { fmt.Println(\u0026#34;p init\u0026#34;) } 在$GOPATH/src/github.com/test下的main.go依赖了p包，代码如下：\npackage main import ( \u0026#34;fmt\u0026#34; _ \u0026#34;github.com/p\u0026#34; ) var b = func() int { fmt.Println(\u0026#34;variable init in main\u0026#34;); return 2 }() func init() { fmt.Println(\u0026#34;main init\u0026#34;) } func main() { } 运行main.go，输出结果为：\nvariable init in p\rp init\rvariable init in main\rmain init\r goroutine创建与销毁 goroutine创建顺序保证：  a）一个goroutine的创建先于其执行。\n例如，如下代码：\nvar a, b string func f() { a = \u0026#34;hello\u0026#34; go func() { fmt.Println(a) b = \u0026#34;world\u0026#34; go func() { fmt.Println(b) }() }() } f函数中，a的赋值先于fmt.Println(a)；b的赋值先于fmt.Println(b)，其打印结果总是：\nhello\rworld\rgoroutine销毁（无顺序保证）：\ngoroutine的销毁并未有先于程序任何事件点的保障。\n请看如下代码：\nvar a string func f() { go func() { a = \u0026#34;hello\u0026#34; }() fmt.Println(a) } 在未加任何同步的情况下，a在一个goroutine是否赋值成功，对任何其他需要“observe”其值的goroutine是没有保证的。\n所以若一个goroutine需要“observe”另一个goroutine，请使用同步机制（如锁）或使用Channel通信来保证执行顺序。\n Channel通信 Channel通信顺序保证：  a）一个Channel的发送操作先于发送操作完成；\nb）一个Channel的接收操作先于接收操作完成；\nc）不论是Buffered Channel还是Unbuffered Channel，一个Channel的第N个成功发送先于第N个成功接收完成；\nd）一个容量为M的Channel的第N个成功接收先于第N+M个成功发送完成（特别当M=0时，其为Unbuffered Channel，其第N个成功接收先于第N个成功发送完成）；\ne）一个Channel的关闭先于接收完成（Channel关闭时返回“零值”）。\n看一段代码：\npackage main import \u0026#34;fmt\u0026#34; var a string func main() { done := make(chan bool, 3) go func() { a = \u0026#34;hello world\u0026#34; done \u0026lt;- true }() \u0026lt;-done fmt.Println(a) } 这段代码输出“hello world”是有保证的。因a的写入先于done的发送，done的发送先于done的接收完成，done的接收完成先于a的打印。\n若将如上代码稍作改动，将done的发送改为done的close，其仍可以保证打印结果为“hello world”。\npackage main import \u0026#34;fmt\u0026#34; var a string func main() { done := make(chan bool, 3) go func() { a = \u0026#34;hello world\u0026#34; close(done) }() \u0026lt;-done fmt.Println(a) } 原因是，Channel的关闭要先于接收完成。\n再看一段代码：\npackage main import \u0026#34;fmt\u0026#34; var a string func main() { done := make(chan bool) go func() { a = \u0026#34;hello world\u0026#34; \u0026lt;-done }() done \u0026lt;- true fmt.Println(a) } 这段代码将Channel通信中第一段代码的发送与接收互换位置，并改用Unbuffered Channel，仍可以保证打印结果为“hello world”。原因在于，a的写入先于done接收，done接收先于done发送完成，done发送完成先于a的打印。\n上述规则中的规则d）可以使用Buffered Channel的容量作并发限制。\n如下代码，在同一时刻至多有2个work()在执行。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { works := []func(){ func() { fmt.Println(\u0026#34;working 0\u0026#34;) }, func() { fmt.Println(\u0026#34;working 1\u0026#34;) }, func() { fmt.Println(\u0026#34;working 2\u0026#34;) }, func() { fmt.Println(\u0026#34;working 3\u0026#34;) }, func() { fmt.Println(\u0026#34;working 4\u0026#34;) }, func() { fmt.Println(\u0026#34;working 5\u0026#34;) }, func() { fmt.Println(\u0026#34;working 6\u0026#34;) }, func() { fmt.Println(\u0026#34;working 7\u0026#34;) }, } limit := make(chan int, 2) for _, work := range works { go func(func()) { limit \u0026lt;- 1 time.Sleep(time.Second) work() \u0026lt;-limit }(work) } select {} }  锁 锁顺序保证：  a）对于sync.Mutex或sync.RWMutex变量l，第N个l.Unlock()调用先于第N+1个l.Lock()调用返回；\nb）对于sync.RWMutex变量l，第N个l.Unlock()调用先于第N个l.RLock()，l.RUnlock()调用先于第N+M(M\u0026gt;=0)个l.Lock()；\n请看如下代码：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var a string var l sync.Mutex func main() { l.Lock() go func() { a = \u0026#34;hello world\u0026#34; l.Unlock() }() l.Lock() fmt.Println(a) } 可以保证其打印结果为“hello world”，因启动的goroutine中第一次l.Unlock()调用先于main中第二次l.Lock()调用返回。\n接下来将Mutex改为RWMutex，代码如下：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var a string var l sync.RWMutex func main() { l.RLock() go func() { a = \u0026#34;hello world\u0026#34; l.RUnlock() }() l.Lock() fmt.Println(a) } 同样可以保证打印结果为“hello world”，因启动的goroutine中第一次l.RUnlock()调用先于main中第一次l.Lock()。\n同理，若改为如下方式，同样可以保证打印结果。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var a string var l sync.RWMutex func main() { l.Lock() go func() { a = \u0026#34;hello world\u0026#34; l.Unlock() }() l.RLock() fmt.Println(a) } 因启动的goroutine中第一次l.Unlock()调用先于main中第一次l.RLock()。\n Once sync.Once用来对多个goroutine同时调用某个函数时（once.Do(f)），保证仅有一个goroutine可以调用f()，其余goroutine的调用会阻塞直至f()返回。  如下代码，setup函数仅会执行一次。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var a string var once sync.Once func setup() { fmt.Println(\u0026#34;setup calling\u0026#34;) a = \u0026#34;hello world\u0026#34; } func main() { for i := 0; i \u0026lt; 4; i++ { go func(i int) { once.Do(setup) fmt.Println(a, i) }(i) } time.Sleep(time.Second) } 输出结果为：\nsetup calling\rhello world 2\rhello world 3\rhello world 0\rhello world 1\r额外注意：若main函数在新启goroutine时，未将i提取为函数参数，会发生多个goroutine重用最后i值的情况。\n如如下代码所示：\nfunc main() { for i := 0; i \u0026lt; 4; i++ { go func() { once.Do(setup) fmt.Println(a, i) }() } time.Sleep(time.Second) } 打印结果为：\nsetup calling\rhello world 4\rhello world 4\rhello world 4\rhello world 4\r所以新启动程序时，宿主函数的参数使用要注意将其放入新的函数的参数列表或者在goroutine启动前使用原变量值的新实例。如如下代码所示。\nfunc main() { for i := 0; i \u0026lt; 4; i++ { i := i go func() { fmt.Println(i) }() } ... }  参考资料\n[1] https://golang.org/ref/mem\n[2] http://nil.csail.mit.edu/6.824/2016/notes/gomem.pdf\n[3] https://go101.org/article/memory-model.html\n[4] https://medium.com/@edwardpie/understanding-the-memory-model-of-golang-part-1-9814f95621b4\n[5] https://medium.com/@edwardpie/understanding-the-memory-model-of-golang-part-2-972fe74372ba\n ","permalink":"https://olzhy.github.io/posts/golang-memory-model.html","tags":["Golang"],"title":"Golang 内存模型"},{"categories":["计算机"],"contents":"1 Go 1.3简介\nGo 1.3在Go 1.2发布6个月后如约而至。本版没有语言级变化，重点放在了实现工作上。提供更精确的垃圾回收、编译器工具链重构等以实现对特别是对大项目的加速构建及全局性能提升，此外还支持了DragonFly BSD、Solaris，Plan 9及谷歌Native Client体系结构(NaCl)，且在同步内存模型上有重点改进。\n2 支持的操作系统及体系结构的变化\n  Removal of support for Windows 2000 因微软于2010年不再支持Windows2000，且该系统在异常处理（Unix术语叫信号量）上实现起来有较多难点，Go截至1.3也不再支持该系统。\n  Support for DragonFly BSD Go 1.3引入了对DragonFly BSD在amd64（64-bit x86）及386（32-bit x86）体系结构上的试验性实现。\n  Support for FreeBSD 自Go 1.2起，支持Go的FreeBSD需在8或8以上。截至Go 1.3，支持Go的FreeBSD需内核使用COMPAT_FREEBSD32标记编译。\n  Support for Native Client Go 1.3已支持Native Client虚拟机体系结构。\n  Support for NetBSD NetBSD 6.0或以上版本支持Go 1.3。\n  Support for OpenBSD OpenBSD 5.5或以上版本支持Go 1.3。\n  Support for Plan 9 Go 1.3对在386（32-bit x86）体系结构上的Plan 9提供试验性支持。\n  Support for Solaris Go 1.3对在amd64（64-bit x86）体系结构上的Solaris提供试验性支持。\n  3 内存模型变化\nGo 1.3内存模型新加了关于在buffered channels上发送或接收消息的新规则，以可用作一个简单的信号量。\n4 实现及工具级变化\n  Stack Go 1.3在goroutine栈实现上摒弃之前的分段模型，更改为邻接模型。当goroutine需要更多的栈空间时，其栈转换至更大的单块内存上。当一个计算重复访问段边界时，如上转换可以很好的规避“hot spot”问题。\n  Changes to the garbage collector 我们知道，当对堆中的值进行检查时，垃圾收集器已经可以很精确。Go 1.3对栈中值检查加入了同等的精确度。诸如一个整型空指针值绝不会再被误当作一个指针，从而防止未使用内存的回收。\n  该假定是进行精确栈扩展及垃圾回收的基础。使用unsafe包将整型值存储为指针类型值的程序是非法的，若被运行时检测到，程序将会中断。同样，使用unsafe包将指针类型值存储为整型值也是非法的，只是在执行中较难被检测到。\n此类情况的指针对运行时是隐藏的，所以栈扩展或垃圾回收可能会回收其所指的内存，从而形成悬空指针。\n更新点：使用unsafe.Pointer将整型值（持有内存）转换为指针类型值的代码是非法的，需重写。可以使用go vet找出此类代码。\n  Map iteration 为规避有人依赖map的迭代顺序写错误的程序，Go 1.3重新对小Map引入了随机序访问的机制。\n  The linker 编译器及链接器已重构。链接器仍是C程序，但原属于链接器的指令选择部分已移至通过新建一个新的liblink库实现的编译器部分。当包第一次编译时，仅进行一次指令选择，这样可以加快对大项目的编译。\n  Status of gccgo GCC 4.9版本将包含Go 1.2的gccgo。\n  Changes to the go command cmd/go有几项新特性。go run及go test新加一个-exec选项以指定运行二进制的可选方式。其直接目的是支持NaCl。\n  当竞争检测开启时，go test测试覆盖率命令自动将模式设置为-atomic，以避免在非安全访问下的测试覆盖率误报。\n目前，不论有没有测试文件，go test都会对包进行构建。\ngo build新加一个-i选项以对所指定目标安装依赖（非目标本身）。\n当前已支持使用cgo进行交叉编译。\n当前，go命令已支持在cgo引用Objective-C文件的包。\n  Changes to cgo 用来处理Go包中import \u0026ldquo;C\u0026quot;声明的cmd/cgo命令修复了对可能引起某些包编译停止的诸多bug。\n  Changes to godoc 目前，使用godoc -analysis可用来对代码作复杂静态分析。\n  5 性能相关\na）运行时处理defer更效率，每个处理defer的goroutine约节省2KB的内存；\nb）垃圾收集器使用并发清除算法，以及更好的并行性与更大的页可累计可节省50-70%的收集器悬停时间；\nc）竞争检测器提速40%；\nd）regexp包使用“a second, one-pass”执行引擎实现，对简单表达式有显著提速。\n6 标准库变化\n Major changes to the library Go 1.3修复了crypto/tls略过认证的bug。  标准库新加了sync.Pool，对实现各类缓存提供有效机制，其内存可被系统自动回收。\ntesting.B新加了RunParallel方法，以实现在并行系统执行基准测试。\n Minor changes to the library https://golang.org/doc/go1.3#minor_library_changes   参考资料\n[1] https://golang.org/doc/go1.3\n ","permalink":"https://olzhy.github.io/posts/go1dot3-release-notes-reading.html","tags":["Golang"],"title":"Go 1.3 Release Notes 研读"},{"categories":["计算机"],"contents":"1 提出问题\n我们知道，一个工程稍大一点，通常会依赖各种各样的包。而Go使用统一的GOPATH管理依赖包，且每个包仅保留一个版本。而不同的依赖包由各自的版本工具独立管理，所以当所依赖的包在新版本发生接口变更或删除时，会面临很多问题。\n为避免此类问题，我们可能会为不同的工程设置不同的GOPATH，或者更改依赖包路径名称。这样手动维护起来也很头疼。\n2 解决问题\nGo 1.5引入了vendor文件夹，其对语言使用，go命令没有任何影响。若某个路径下边包含vendor文件夹，则在某处引用包时，会优先搜索vendor文件夹下的包。\n在Go 1.5开启该项特性需设置GO15VENDOREXPERIMENT=1，而从Go 1.6开始，该项特性默认开启。\n3 使用方式\n3.1 vendor搜索方式\nvendor包的搜索方式为：自包引用处，从其所在文件夹查询是否有vendor文件夹包含所引用包；若没有，然后从其所在文件夹的上层文件夹寻找是否有vendor文件夹包含所引用包，若没有，则再搜索上层文件夹的上层文件夹\u0026hellip;，直至搜索至$GOPATH/src并搜索完成时止。\n例如，如下代码中，$GOPATH/src/x/y/z/main.go引用了包\u0026quot;v\u0026quot;，则不论vendor/v/v.go置于src/，src/x/，src/x/y/，src/x/y/z/中任意一个文件夹下，均可以找到。\n$ cat $GOPATH/src/x/y/z/main.go package main import ( \u0026quot;v\u0026quot; ) func main() { v.V() } $ cat vendor/v/v.go package v import \u0026quot;fmt\u0026quot; func V() { fmt.Println(\u0026quot;I'm a vendor test\u0026quot;) } $ go run main.go I'm a vendor test 当vendor存在嵌套时，若不同的vendor文件夹包含相同的包，且该包在某处被引用，寻找策略仍遵循如上规则。即从包引用处起，逐层向上层文件夹搜索，首先找到的包即为所引，也就是从$GOPATH/src来看，哪个vendor包的路径最长，使用哪个。\n如下代码中，$GOPATH/src/x/y/z/main.go所在工程有两个vendor文件夹（分别位于$GOPATH/src/x/vendor/v/，$GOPATH/src/x/y/z/vendor/v/）包含相同的包\u0026quot;v\u0026quot;，目录树为：\n$ tree $GOPATH/src src └ x ├ vendor │ └ v │ └ v.go └ y └ z ├ vendor │ └ v │ └ v.go └ main.go $ cat $GOPATH/src/x/vendor/v/v.go package v import \u0026quot;fmt\u0026quot; func V() { fmt.Println(\u0026quot;I'm a vendor test, My path is x/vendor/v/\u0026quot;) } $ cat $GOPATH/src/x/y/z/vendor/v/v.go package v import \u0026quot;fmt\u0026quot; func V() { fmt.Println(\u0026quot;I'm a vendor test, My path is x/y/z/vendor/v/\u0026quot;) } 输出为：\n$ go run main.go I'm a vendor test, My path is x/y/z/vendor/v/ 可以看到，真正调用的是$GOPATH/src/x/y/z/vendor/v/v.go。\n3.2 vendor使用规约\n使用vendor时，建议遵循如下两条规约。\na) 当欲将某包vendor时，可能想将所有依赖包均vendor；\nb) 尽量将vendor依赖包结构扁平化，不要vendor套vendor。\n如下示例代码演示vendor扁平化使用。\nmain.go位于$GOPATH/src/github.com/olzhy/test下。\npackage main import ( \u0026quot;strings\u0026quot; \u0026quot;sync\u0026quot; \u0026quot;time\u0026quot; \u0026quot;github.com/z\u0026quot; \u0026quot;github.com/y\u0026quot; \u0026quot;golang.org/z\u0026quot; ) ... $GOPATH/src/github.com/olzhy/test目录树。\n├─ main.go └─ vendor ├─ github.com │ ├─ x │ └─ y └─ golang.org └─ z  参考资料\n[1] https://go.googlesource.com/proposal/+/master/design/25719-go15vendor.md\n[2] https://blog.gopheracademy.com/advent-2015/vendor-folder/\n[3] https://tonybai.com/2015/07/31/understand-go15-vendor/\n ","permalink":"https://olzhy.github.io/posts/golang-vendoring.html","tags":["Golang"],"title":"Golang vendor文件夹使用"},{"categories":["计算机"],"contents":"1 Go 1.2 简介\n自Go 1.2起，版本发布已被缩短为大约每半年一次。\n2 语言级变化\n Use of nil Go 1.2中，空指针使用将会触发panic错误。数组、接口、struct、slice等的空指针使用将会产生panic错误或返回一个安全的非空值。如下例子中，x.Field将会产生空指针错误。  type T struct { X [1\u0026lt;\u0026lt;24]byte Field int32 } func main() { var x *T ... }  Three-index slices Go 1.2新加了对在已有array或slice上做切分操作时指定容量及长度的能力。例如，如下代码在对一个array做切分操作。  var array [10]int slice := array[2:4] 容量为该slice可以拿到的元素的最大个数。其反映所指array的大小，该例子中，slice的容量为8。\nGo 1.2新加了可以同时指定slice大小与容量的新语法。例如，在如下例子中，第2个冒号后的值指定容量，其值需小于等于原始slice或array的容量。\nslice = array[2:4:7] 该代码相较于上一段，len未变，但容量变为了5(7-2)。所以该slice不可以访问原始数组的最后三个元素。\n该三目索引可以省略第一个数值([:i:j]代表从0开始)，但不可以省略后两个数值。\n3 实现及工具级变化\n  Pre-emption in the scheduler 之前版本，当GOMAXPROCS仅提供一个用户线程时，若同一个线程的一个goroutine开启无限循环，那么其他的goroutine将得不到执行。Go 1.2修整了调度器机制，一个包含函数调用（非内敛）的任意循环可以被预先停止，从而允许同一线程中的其他goroutine得到执行。\n  Limit on the number of threads 为避免资源耗尽问题，Go 1.2引入了单一程序可在其地址空间拥有的最大线程数限制（默认为10000）。因goroutine对线程多路复用，所以其未直接限制goroutine的个数，而仅是系统调用中可能同时阻塞的goroutine数。\n  runtime/debug包的SetMaxThreads新函数可以控制线程限定值。\n Stack size 对旧的栈大小设置，对性能要求高的场景可能会引起过度栈碎片切换从而面临性能问题。Go 1.2对goroutine创建时所需的最小栈大小由4KB提至8KB。  此外，runtime/debug包的SetMaxStack新函数可以控制单goroutine的最大栈大小（默认在64位系统为1GB，32位系统为250MB）。\n  Cgo and C++ cgo将可以调用C++编译器来构建以C++编写的链接库片段。\n  Godoc and vet moved to the go.tools subrepository godoc及vet命令的二进制仍包含在发布版本中，但其源码已移至go.tools子仓库。\n  Status of gccgo 期待后续包含gccgo的GCC 4.9版本可以支持Go 1.2。\n  Changes to the gc compiler and linker https://golang.org/doc/go1.2#gc_changes\n  Test coverage Go 1.2使用go tool cover可以查看测试覆盖率。\n  使用go test -cover可以查看基础报告（其会自动插入测试语句并重写源码）。\n$ go test -cover fmt 使用go tool cover可以查看详细报告。如：\n$ go test -coverprofile=cover.out $ go tool cover -func=cover.out 使用help查询具体怎么使用。\n$ go help testflag $ go tool cover -help   The go doc command is deleted go doc命令已删，请使用godoc。\n  Changes to the go command 之前，使用go get不会下载测试依赖。Go 1.2，使用go get -t可以下载测试依赖。\n  4 性能相关\n下面列出Go 1.2性能提升项中的几项。\na）compress/bzip2解压速率提升约30%；\nb）crypto/des包较之前性能提升约5倍；\nc）encoding/json包加密提升约30%；\nd）通过在运行时使用集成网络轮训器，Windows及BSD系统的网络性能提升约30%。\n5 标准库变化\n  The archive/tar and archive/zip packages archive/tar与archive/zip包对os.FileInfo的实现未遵循之前的接口定义。特别是，Name方法要求返回最短限定名，而实现返回的是全路径名。\n  The new encoding package Go 1.2引入的新包encoding，包含诸如encoding/xml、encoding/json及encoding/binary，提供一组标准加密接口，可用于实现自定义编排或反编排。\n  The fmt package Go 1.2，fmt包的格式化打印函数Printf可以使用从1开始的索引来标记打印顺序。如下代码打印结果为c a b。\n  fmt.Sprintf(\u0026#34;%[3]c %[1]c %c\\n\u0026#34;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;)  The text/template and html/template packages Go 1.2引入比较函数，可用于对基础类型作比较。  eq == ne != lt \u0026lt; le \u0026lt;= gt \u0026gt; ge \u0026gt;= 如下为使用示例。\n{{if eq .A 1}} X {{else if eq .A 2}} Y {{end}}  Minor changes to the library https://golang.org/doc/go1.2#minor_library_changes   参考资料\n[1] https://golang.org/doc/go1.2\n ","permalink":"https://olzhy.github.io/posts/go1dot2-release-notes-reading.html","tags":["Golang"],"title":"Go 1.2 Release Notes 研读"},{"categories":["计算机"],"contents":"1 题目描述\n若一个数列至少有三个元素，且任意相邻两元素的差相等，则该数列为一个等差数列。\n例如，如下三个数列即为等差数列：\n1, 3, 5, 7, 9 7, 7, 7, 7 3, -1, -5, -9 如下数列不是等差数列：\n1, 1, 2, 5, 7 现给定一个以0为起始索引，包含N个数的数组A。数组的切片(P, Q)为满足规则(0 \u0026lt;= P \u0026lt; Q \u0026lt; N)的任意整数组合。\n若数组A的切片(P, Q)满足如下规则，则称该数组切片是一个等差数列：\nA[P], A[p + 1], ..., A[Q - 1], A[Q]是一个等差数列，且P + 1 \u0026lt; Q。 代码函数需返回数组A的等差数列的个数。\n例子：\n输入：A = [1, 2, 3, 4]\n输出：3\n释义：A中有3个等差数列切片：[1, 2, 3]，[2, 3, 4]与[1, 2, 3, 4]。\n题目出处：\nhttps://leetcode.com/problems/arithmetic-slices/\n2 解决思路\n首先从A中找出有几个最长等差数列。\na）首先定义slices用来存储所有最长的等差slice，slice初始为2，初始间隔preInterval为a[1]-a[0]；\nb）从第3个元素开始遍历A，若当前元素与前一个元素的差interval与preInterval相等，则slice+1；若interval与preInterval不等，则判断是否将当前slice合入slices，并将slice赋值为2，preInterval赋值为interval，遍历下一个元素；\nc）直至遍历到最后一个元素，若interval与preInterval相等，则判断是否将当前slice合入slices。\n对其中一个满足规则的最长等差数列，计算其中所有满足等差数列规则的切片个数的计算函数为。\nfunc(slice int) int { if slice \u0026lt; 3 { return 0 } num := 0 for i := 3; i \u0026lt;= slice; i++ { num += (slice - i) + 1 } return num } 3 golang实现代码\n综上，整个逻辑的实现代码为：\nhttps://github.com/olzhy/leetcode/blob/master/413_Arithmetic_Slices/test.go\nfunc numberOfArithmeticSlices(a []int) int { if len(a) \u0026lt; 3 { return 0 } slices := []int{} slice := 2 preInterval := a[1] - a[0] for i := 2; i \u0026lt; len(a); i++ { interval := a[i] - a[i-1] if interval == preInterval { slice++ if len(a)-1 == i \u0026amp;\u0026amp; slice \u0026gt; 2 { slices = append(slices, slice) } continue } if slice \u0026gt; 2 { slices = append(slices, slice) } slice = 2 preInterval = interval } f := func(slice int) int { if slice \u0026lt; 3 { return 0 } num := 0 for i := 3; i \u0026lt;= slice; i++ { num += (slice - i) + 1 } return num } sum := 0 for _, slice := range slices { sum += f(slice) } return sum } ","permalink":"https://olzhy.github.io/posts/leetcode-arithmetic-slices.html","tags":["Golang","算法"],"title":"LeetCode 413 等差数列切片"},{"categories":["计算机"],"contents":"1 Go 1.1 简介\nGo 1.1在编译器、核心库，运行时方面做了很多工作，重点在性能上作了改进。\n2 语言级变化\n Integer division by zero 在Go 1，除0是一个运行时panic错误，在Go 1.1，是一个编译器错误。  func f(x int) int { return x/0 }  Method values Go 1.1引入了方法值，即一个需与指定接收值绑定的函数。  func (p []byte) (n int, err error) { return w.Write(p) }  Return requirements Go 1.1之前，对带返回值的函数在函数尾部必须有显式的return语句或panic调用。Go 1.1引入了结束语句的概念，https://golang.org/ref/spec#Terminating_statements，如无限循环、分支语句在每个分支都返回了结果等，去掉了必须在尾部加return的限制。  func loop() int { for { fmt.Println(1) time.Sleep(time.Second) } } 3 实现及工具级变化\n  Status of gccgo GCC的发布日程可能较Go的发布日程会有些许延后。我们期望4.8.2版本的GCC会对Go 1.1提供完整的实现。\n  Command-line flag parsing gc工具链与传统Unix标记解析作了分离，编译器与链接器使用相同的命令行标记解析规则作为Go的标记包。\n  Size of int on 64-bit platforms 之前的Go实现使int与uint在所有系统均为32位。当前gc与gccgo使int与uint在64位系统均为64位。因Go不允许数值类型的隐式转换，所以常规程序不受影响，但之前将int、uint假定为32位程序的运行结果可能会受影响。\n  x := ^uint32(0) // x is 0xffffffff i := int(x) // i is -1 on 32-bit systems, 0xffffffff on 64-bit fmt.Println(i)   Heap size on 64-bit architectures 64位体系结构的堆大小已由几GB扩展到了几十GB，32位体系结构的堆大小未有变化。\n  Unicode 为使UTF-16表示的代码点大于65535，Unicode定义了surrogate halves。\n  Race detector 本版go tool增加了竞争检测器，以便找出程序在对变量并发访问时引起的bug（至少有一个访问为写）。构建或测试时使用-race标记，如go test -race。\n  The gc assemblers 因int在64位体系结构变为了64位（representation of functions），在gc工具链中，函数参数在栈上的位置有了变化，以汇编语言编写的函数至少需修正下帧的指针offset。\n  Changes to the go command 为方便使用Go的新手，go命令有几项优化。\n  a）在测试、编译，运行时，当所需的包未找到时，go命令会给出包括搜索路径列表等更详尽的错误提示。\n$ go build foo/quxx can't load package: package foo/quxx: cannot find package \u0026quot;foo/quxx\u0026quot; in any of: /home/you/go/src/pkg/foo/quxx (from $GOROOT) /home/you/src/foo/quxx (from $GOPATH) b）使用go get命令时，不再允许将$GOROOT作为默认的包下载路径，必须指定合法的$GOPATH。\n$ GOPATH= go get code.google.com/p/foo/quxx package code.google.com/p/foo/quxx: cannot download, $GOPATH not set. For more details see: go help gopath c）若$GOPATH设置与$GOROOT相同，使用go get也会报错。\n$ GOPATH=$GOROOT go get code.google.com/p/foo/quxx warning: GOPATH set to GOROOT (/home/you/go) has no effect package code.google.com/p/foo/quxx: cannot download, $GOPATH must not be set to $GOROOT. For more details see: go help gopath  Changes to the go test command 为了便于profile信息的分析，go test命令运行时若开启profile搜集，生成的二进制文件不再被删除（如：执行如下命令会生成mypackage.test文件）。目前go test支持搜集profile信息以便找出goroutine阻塞的地方。  $ go test -cpuprofile cpuprof.out mypackage   Changes to the go fix command go fix不再应用于修正Go 1之前的代码，请使用Go 1.0工具链（go tool fix）来转换Go 1.0之前的代码。\n  Build constraints Build Constraints\n  Additional platforms Go 1.1工具链对freebsd/arm、netbsd/386、netbsd/amd64、netbsd/arm、openbsd/386及openbsd/amd64等平台增加了试验性支持。\n  Cross compilation 交叉编译时，默认关闭cgo支持，若需开启，请设置CGO_ENABLED=1。\n  4 性能相关\n使用Go 1.1 gc工具链编译的代码会有显著提升。下面列出一些显著的性能提升点。\na）gc编译器在32位intel体系结构的浮点运算有显著提升。\nb）gc编译器在诸如append及接口转换等运行时操作的性能提升上做了很多工作。\nc）新的map实现在节省内存及CPU上有很大提升。\nd）垃圾搜集器在多核计算机执行更加并行化，降低了延迟。\ne）垃圾搜集器更精确，特别在32位系统上，仅使用很少的CPU总时间即可以显著减少堆大小。\nf）将运行时与网络相关的库更紧密的组合，使得网络操作所需的上下文切换更少了。\n5 标准库的几处变化\n bufio.Scanner 为使诸如逐行读取、按空格分割读取等常规读取操作更便捷，Go 1.1引入了Scanner。当然，也可以提供SplitFunc来对输入文本自定义分割方式。  scanner := bufio.NewScanner(os.Stdin) for scanner.Scan() { fmt.Println(scanner.Text()) // Println will add back the final \u0026#39;\\n\u0026#39; } if err := scanner.Err(); err != nil { fmt.Fprintln(os.Stderr, \u0026#34;reading standard input:\u0026#34;, err) }   net https://golang.org/doc/go1.1#net\n  reflect Go 1.1可以对reflect包使用select语句。使用Value.Convert或Type.ConvertibleTo方法可以对Value进行断言或类型转换。方便调用可以使用包装函数MakeFunc。还有一些适用的函数ChanOf、MapOf及SliceOf可以对已有类型进行构造。\n  time Go 1.1目前时间可以达到纳秒级精度。此外增加了YearDay方法，Timer增加了Reset方法等。\n  New packages Go 1.1引入了三个新包。\n  a）go/format包便于程序使用go fmt命令的格式化能力。\nb）net/http/cookiejar包提供了对HTTP cookie的基础管理能力。\nc）runtime/race提供了对数据竞争检测的底层工具。\n Minor changes to the library a）bytes包新加了TrimPrefix及TrimSuffi函数。  b）database/sql新加了测试连接状态的Ping方法。\nc）encoding/json包的Decoder新加了获取其缓存区剩余数据的Buffered方法。\nd）go/ast包新加了CommentMap类型及相关方法以便抓取及处理Go语言的注释。\ne）os/signal包新加了Stop函数，以停止后续信号量对channel的传入。\nf）regexp包新加了Regexp.Split，便于根据正则将字符串分割为slice。\ng）sort包新加了Reverse函数。\nh）strings包新加了TrimPrefix与TrimSuffix函数。\n详细请参看：https://golang.org/doc/go1.1#minor_library_changes\n 参考资料\n[1] https://golang.org/doc/go1.1\n ","permalink":"https://olzhy.github.io/posts/go1dot1-release-notes-reading.html","tags":["Golang"],"title":"Go 1.1 Release Notes 研读"},{"categories":["计算机"],"contents":"1 Go 1简介\nGo 1对语言及核心库作了标准化定义。并声明之后发布的版本需遵守向后兼容的原则。\n2 语言级变化\n Append 用来对slice进行append操作，如下代码中，string无需转换，可以直接append到[]byte。  bytes := []byte{} bytes = append(bytes, []byte(\u0026#34;hello\u0026#34;)...) bytes = append(bytes, \u0026#34; world\u0026#34;...)  Close close是channel的发送者告知消息已发送完毕的一种机制。为避免竞争条件发生，仅允许消息发送方goroutine调用close，不允许消息接收方调用close，Go 1为保障使用正确，增加了编译期检查。如下代码，对只用来接消息的通道进行close会报编译器错误。  var c chan int var csend chan\u0026lt;- int = c var crecv \u0026lt;-chan int = c close(c) // legal close(csend) // legal close(crecv) // illegal  Composite literals Go 1中，组合类型如array, slice, map可以对struct类型元素省略构造器。如下代码中，四种初始化方式均是合法的。  type Date struct { month string day int } // Struct values, fully qualified; always legal. holiday1 := []Date{ Date{\u0026#34;Feb\u0026#34;, 14}, Date{\u0026#34;Nov\u0026#34;, 11}, Date{\u0026#34;Dec\u0026#34;, 25}, } // Struct values, type name elided; always legal. holiday2 := []Date{ {\u0026#34;Feb\u0026#34;, 14}, {\u0026#34;Nov\u0026#34;, 11}, {\u0026#34;Dec\u0026#34;, 25}, } // Pointers, fully qualified, always legal. holiday3 := []*Date{ \u0026amp;Date{\u0026#34;Feb\u0026#34;, 14}, \u0026amp;Date{\u0026#34;Nov\u0026#34;, 11}, \u0026amp;Date{\u0026#34;Dec\u0026#34;, 25}, } // Pointers, type name elided; legal in Go 1. holiday4 := []*Date{ {\u0026#34;Feb\u0026#34;, 14}, {\u0026#34;Nov\u0026#34;, 11}, {\u0026#34;Dec\u0026#34;, 25}, } 使用如下命令可以简化构造代码gofmt -s x.go\n Goroutines during init 在Go 1之前，若在init函数中使用go语句，虽会创建goroutine，但会一直等待，直至init函数中所有程序执行完成，其逻辑才会开始执行。Go 1取消了这个限制，init函数可以正常启动goroutine，还可对全局变量赋值，而不会有死锁发生。  var PackageGlobal int func init() { c := make(chan int) go func() { c \u0026lt;- 1 }() PackageGlobal = \u0026lt;-c }   The rune type 在Go 1之前，代码点是使用int类型来表示的。而因int类型在32位系统为32位位宽，在64位系统为64位位宽，这样，若int从32位变为64位，每个代码点会浪费32位的存储。为使转换至64位int更节省空间，Go 1引入一个新的基础类型rune来表示Unicode字符，与byte是uint8的别名相同，rune是int32的别名。如\u0026rsquo;a', \u0026lsquo;中\u0026rsquo;, \u0026lsquo;\\u0345\u0026rsquo;此类字符目前默认类型为rune。\n  The error type Go 1引入了一个新的内置类型，error，定义如下。\n  type error interface { Error() string }  Deleting from maps 在Go 1中，采用内置delete函数删除map中的元素。无返回值，删除不存在的key，不会报错。  delete(m, k)  Iterating in maps 在Go 1中，采用for range语句遍历map时，元素的访问顺序是不确定的。  m := map[string]int{\u0026#34;Sunday\u0026#34;: 0, \u0026#34;Monday\u0026#34;: 1} for k, v := range m { // This loop should not assume Sunday will be print first  fmt.Println(k, v) }  Multiple assignment  Go 1采用从左至右对表达式或变量先估值，再从左至右进行赋值。\nsa := []int{1, 2, 3} i := 0 i, sa[i] = 1, 2 // sets i = 1, sa[0] = 2  sb := []int{1, 2, 3} j := 0 sb[j], j = 2, 1 // sets sb[0] = 2, j = 1  sc := []int{1, 2, 3} sc[0], sc[0] = 1, 2 // sets sc[0] = 1, then sc[0] = 2 (so sc[0] = 2 at end)  Returns and shadowed variables 在named return中，一个常犯的错误是将头声明的返回变量再次声明为一个新变量并赋值，Go 1编译期检查不允许诸类情况，如下例子即会引起编译期错误。  func Bug() (i, j, k int) { for i = 0; i \u0026lt; 5; i++ { for j := 0; j \u0026lt; 5; j++ { // Redeclares j.  k += i * j if k \u0026gt; 100 { return // Rejected: j is shadowed here.  } } } return // OK: j is not shadowed here. }  Copying structs with unexported fields Go 1允许一个包中未有未导出字段的struct值被另一包进行拷贝。如下例子中，p包中定义了Struct。  type Struct struct { Public int secret int } func NewStruct(a int) Struct { // Note: not a pointer.  return Struct{a, f(a)} } func (s Struct) String() string { return fmt.Sprintf(\u0026#34;{%d (secret %d)}\u0026#34;, s.Public, s.secret) } 在另一包对p.Struct进行赋值并拷贝，可以看到未导出的字段也会被赋值并拷贝。\nimport \u0026#34;p\u0026#34; myStruct := p.NewStruct(23) copyOfMyStruct := myStruct fmt.Println(myStruct, copyOfMyStruct)  Equality Go 1中，struct与array可以分别比较相等或不相等（不包括\u0026gt;=, \u0026lt;=, \u0026gt;, \u0026lt;），因此可以用作map的key，如下代码是可行的。Go 1中不允许函数、map、slice分别比较，其仅可以判断是否等于nil。  type Day struct { long string short string } Christmas := Day{\u0026#34;Christmas\u0026#34;, \u0026#34;XMas\u0026#34;} Thanksgiving := Day{\u0026#34;Thanksgiving\u0026#34;, \u0026#34;Turkey\u0026#34;} holiday := map[Day]bool{ Christmas: true, Thanksgiving: true, } fmt.Printf(\u0026#34;Christmas is a holiday: %t\\n\u0026#34;, holiday[Christmas]) 3 包层次结构\nGo 1对包作了梳理，删减与重命名，使其变得更紧凑简洁。\n The package hierarchy Go 1对包作了整理与重新编排，有些包作了删减，还有些不常用的包移到了子仓库code.google.com/p/go中，如下为包路径变更表。  旧路径\t新路径 asn1\tencoding/asn1 csv\tencoding/csv gob\tencoding/gob json\tencoding/json xml\tencoding/xml exp/template/html html/template big\tmath/big cmath\tmath/cmplx rand\tmath/rand http\tnet/http http/cgi\tnet/http/cgi http/fcgi\tnet/http/fcgi http/httptest\tnet/http/httptest http/pprof\tnet/http/pprof mail\tnet/mail rpc\tnet/rpc rpc/jsonrpc\tnet/rpc/jsonrpc smtp\tnet/smtp url\tnet/url exec\tos/exec scanner\ttext/scanner tabwriter\ttext/tabwriter template\ttext/template template/parse\ttext/template/parse utf8\tunicode/utf8 utf16\tunicode/utf16   The package tree exp 包ebnf、html†（除EscapeString与UnescapeString仍在html包内），go/types已移到exp/下。\n  The package tree old old文件夹下的包不随Go 1发布版本发布，使用该包内容可从源码中找到。\n  Deleted packages Go 1已将如下包彻底删除，gotry命令也已删除。\n  container/vector exp/datafmt go/typechecker old/regexp old/template try  Packages moving to subrepositories 移到了子仓库的包，https://golang.org/doc/go1#subrepo  4 核心库的几处大改动\n The error type and errors package 之前因写os时最先用到了error，那时以为error会是系统级别的，所以就将os.Error放到了os包中。后来越来越清楚error是通用的，并不是系统相关的。Go 1解决了这个问题，内置了接口类型的error，附加一个error的工具类errors（与bytes和strings工具类类似）。error定义如下。  type error interface { Error() string } errors工具类包含一个创建error的函数。\nfunc New(text string) error   System call errors Go 1中，对系统调用错误会返回error。\n  Time 之前Go time包未对绝对时间与时间段作区分。Go 1重新设计了time，用time.Time表示时刻，用time.Duration表示时间段，都含有最高纳秒级的精度。Time可以表示一个从远古至未来的任意一刻，而Duration的跨度仅为加上或减去290年。如下代码示例了使用方式。\n  // sleepUntil sleeps until the specified time. It returns immediately if it\u0026#39;s too late. // sleepUntil sleeps until the specified time. It returns immediately if it\u0026#39;s too late. func sleepUntil(wakeup time.Time) { now := time.Now() // A Time.  if !wakeup.After(now) { return } delta := wakeup.Sub(now) // A Duration.  fmt.Printf(\u0026#34;Sleeping for %.3fs\\n\u0026#34;, delta.Seconds()) time.Sleep(delta) } 5 核心库的几处小改动\nhttps://golang.org/doc/go1#minor\n6 go命令\nGo 1引入了一组支持拉取、构建，包安装等的go命令，从此无需再用makefile来构建go应用了。\n7 cgo命令\nGo 1中，cgo命令对使用//export注释的包生成了不同的文件_cgo_export.h。会对前序语言编译多次，所以使用//export语句的包，切勿将函数定义与变量初始化放在C前序语言中。\n 参考资料\n[1] https://golang.org/doc/go1\n ","permalink":"https://olzhy.github.io/posts/go1-release-notes-reading.html","tags":["Golang"],"title":"Go 1 Release Notes 研读"},{"categories":["计算机"],"contents":"golang Regexp主要提供如下正则所表示的16个方法：\nFind(All)?(String)?(Submatch)?(Index)?\r若带All，该方法返回一个所有递进匹配结果的slice；该方法需要额外传一个整数n，若n\u0026gt;=0，至多返回n个匹配或子匹配，若x\u0026lt;0，返回全部。\n若带String，该方法传入的参数需是string，否则为字节slice，返回结果也为对应的string。\n若带Submatch，该方法返回表达式递进的子匹配slice（子匹配匹配以括号扩起的表达式，也称作匹配组），该slice以左括号从左到右的顺序返回匹配结果，即第0个为匹配整个表达式的结果，第1个为匹配第一个左括号所表示表达式的结果，以此类推。\n若带Index，匹配与子匹配使用字节位置索引对来标识，result[2n:2n+1]标识第n个子匹配的索引。\n也有一些方法不属上述正则所表示的范围。\n1 基础用法\n接下来看一个简单的例子。如下代码，在使用前首先将正则表达式编译，然后对多组字符串判断是否匹配。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) var ( p = regexp.MustCompile(`^[a-z]+\\[\\d+\\]$`) ) func main() { fmt.Println(p.MatchString(\u0026#34;larry[12]\u0026#34;)) fmt.Println(p.MatchString(\u0026#34;jacky[12]\u0026#34;)) fmt.Println(p.MatchString(\u0026#34;linda[a12]\u0026#34;)) } 若仅一次性简单判断字符串是否匹配，也可以不创建Regexp，直接调用regexp包函数。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) func main() { fmt.Println(regexp.Match(`\\w+`, []byte(\u0026#34;hello\u0026#34;))) fmt.Println(regexp.MatchString(`\\d+`, \u0026#34;hello\u0026#34;)) } 常用方法示例：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) func main() { p := regexp.MustCompile(`a.`) fmt.Println(p.Find([]byte(\u0026#34;ababab\u0026#34;))) fmt.Println(p.FindString(\u0026#34;ababab\u0026#34;)) fmt.Println(p.FindAllString(\u0026#34;ababab\u0026#34;, -1)) fmt.Println(p.FindAllStringIndex(\u0026#34;ababab\u0026#34;, -1)) q, _ := regexp.Compile(`^a(.*)b$`) fmt.Println(q.FindAllSubmatch([]byte(\u0026#34;ababab\u0026#34;), -1)) fmt.Println(q.FindAllStringSubmatch(\u0026#34;ababab\u0026#34;, -1)) fmt.Println(q.FindAllStringSubmatchIndex(\u0026#34;ababab\u0026#34;, -1)) r := regexp.MustCompile(`(?m)(key\\d+):\\s+(value\\d+)`) content := []byte(` # comment line key1: value1 key2: value2 key3: value3 `) fmt.Println(string(r.Find(content))) for _, matched := range r.FindAll(content, -1) { fmt.Println(string(matched)) } for _, mutiMatched := range r.FindAllSubmatch(content, -1) { for _, matched := range mutiMatched { fmt.Println(string(matched)) } } } 2 进阶用法\n2.1 Split\nSplit方法返回对传入字符串以表达式为分割符的子串slice，第二个参数n指定最多返回的子串数，负数表示返回所有子串。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) func main() { for _, sub := range regexp.MustCompile(`a+`).Split(\u0026#34;heaallo woarld\u0026#34;, -1) { fmt.Println(sub) } } 2.2 Replace\n如下代码，ReplaceAllString返回源字符串将匹配部分替换为字符串模板的拷贝，替换模板采用$符标识第几个替换组，如$1标识1第一个子匹配组。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) func main() { p := regexp.MustCompile(`(?P\\w+)\\s+(?P\\w+)`) names := p.SubexpNames() fmt.Println(p.ReplaceAllString(\u0026#34;hello world\u0026#34;, fmt.Sprintf(\u0026#34;$%s $%s\u0026#34;, names[2], names[1]))) } 2.3 Expand\nExpand将匹配模板所匹配部分叠加至dst尾部并返回。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;regexp\u0026#34; ) func main() { content := []byte(` # json fragment \u0026#34;id\u0026#34;: \u0026#34;dbsuye23sd83d8dasf7\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Larry\u0026#34;, \u0026#34;birth_year\u0026#34;: 2000 `) p := regexp.MustCompile(`(?m)\u0026#34;(?P\\w+)\u0026#34;:\\s+\u0026#34;?(?P[a-zA-Z0-9]+)\u0026#34;?`) var dst []byte tpl := []byte(\u0026#34;$key=$value\\n\u0026#34;) for _, submatches := range p.FindAllSubmatchIndex(content, -1) { dst = p.Expand(dst, tpl, content, submatches) } fmt.Println(string(dst)) }  参考资料\n[1] https://godoc.org/regexp\n[2] https://godoc.org/regexp/syntax\n ","permalink":"https://olzhy.github.io/posts/golang-regexp.html","tags":["Golang"],"title":"Golang 正则表达式使用小结"},{"categories":["计算机"],"contents":"worker pool 的设计常用来加速处理执行较耗时的重任务，且为了避免 goroutine 的过度创建，需要指定工作池的大小。使用 golang 的 goroutine 与 chan，数行代码即可实现一个简单的工作池。\n1 简单 worker pool\n如下代码中，新建两个 channel，一个是 works chan，一个是 results chan，然后调用 startWorkerPool 启动指定 goroutine 个数的工作池。放入 5 个 work 到 works 后关闭通道，然后从 results 中等待结果即可。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func do(work int, goroutine int) int { time.Sleep(time.Second) fmt.Printf(\u0026#34;goroutine %d done work %d\\n\u0026#34;, goroutine, work) return work } func worker(works \u0026lt;-chan int, results chan\u0026lt;- int, goroutine int) { for work := range works { results \u0026lt;- do(work, goroutine) } } func startWorkerPool(works \u0026lt;-chan int, results chan\u0026lt;- int, size int) { for i := 0; i \u0026lt; size; i++ { go worker(works, results, i) } } func main() { works := make(chan int, 10) results := make(chan int, 10) startWorkerPool(works, results, 2) for i := 0; i \u0026lt; 5; i++ { works \u0026lt;- i } close(works) // waiting for results  for i := 0; i \u0026lt; 5; i++ { \u0026lt;-results } } 运行结果为：\ngoroutine 0 done work 1\rgoroutine 1 done work 0\rgoroutine 0 done work 2\rgoroutine 1 done work 3\rgoroutine 0 done work 4\r2 worker pool 封装\n1 中所示的代码难以满足真实业务场景需求，我们需要对 worker pool 作一层抽象，封装的更通用一点。如下代码封装了一个 worker pool，WorkerPool 的创建需要传入要处理的任务列表及指定 pool 的大小，Task 为任务的封装，需提供该任务的实现。创建完 worker pool，调用 pool.Start()即进入多 goroutine 处理，调用 pool.Results()即会阻塞等待所有任务的执行结果。\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type Task struct { Id int Err error f func() error } func (task *Task) Do() error { return task.f() } type WorkerPool struct { PoolSize int tasksSize int tasksChan chan Task resultsChan chan Task Results func() []Task } func NewWorkerPool(tasks []Task, size int) *WorkerPool { tasksChan := make(chan Task, len(tasks)) resultsChan := make(chan Task, len(tasks)) for _, task := range tasks { tasksChan \u0026lt;- task } close(tasksChan) pool := \u0026amp;WorkerPool{PoolSize: size, tasksSize: len(tasks), tasksChan: tasksChan, resultsChan: resultsChan} pool.Results = pool.results return pool } func (pool *WorkerPool) Start() { for i := 0; i \u0026lt; pool.PoolSize; i++ { go pool.worker() } } func (pool *WorkerPool) worker() { for task := range pool.tasksChan { task.Err = task.Do() pool.resultsChan \u0026lt;- task } } func (pool *WorkerPool) results() []Task { tasks := make([]Task, pool.tasksSize) for i := 0; i \u0026lt; pool.tasksSize; i++ { tasks[i] = \u0026lt;-pool.resultsChan } return tasks } func main() { t := time.Now() tasks := []Task{ {Id: 0, f: func() error { time.Sleep(2 * time.Second); fmt.Println(0); return nil }}, {Id: 1, f: func() error { time.Sleep(time.Second); fmt.Println(1); return errors.New(\u0026#34;error\u0026#34;) }}, {Id: 2, f: func() error { fmt.Println(2); return errors.New(\u0026#34;error\u0026#34;) }}, } pool := NewWorkerPool(tasks, 2) pool.Start() tasks = pool.Results() fmt.Printf(\u0026#34;all tasks finished, timeElapsed: %f s\\n\u0026#34;, time.Now().Sub(t).Seconds()) for _, task := range tasks { fmt.Printf(\u0026#34;result of task %d is %v\\n\u0026#34;, task.Id, task.Err) } } 运行结果为：\n1\r2\r0\rall tasks finished, timeElapsed: 2.006011 s\rresult of task 1 is error\rresult of task 2 is error\rresult of task 0 is nil\r本文代码托管地址：https://github.com/olzhy/go-exercises/tree/master/worker_pool\n 参考资料\n[1] https://gobyexample.com/worker-pools\n[2] https://brandur.org/go-worker-pool\n ","permalink":"https://olzhy.github.io/posts/golang-worker-pool.html","tags":["Golang"],"title":"Golang worker pool 实现"},{"categories":["计算机"],"contents":"业务中，会有对某段逻辑在未来某一时刻执行或以一定时间间隔周期性执行的需求。golang 使用 timer 及 ticker 来满足该需求场景。\n1 Timers\nTimer 表示在未来某一刻执行仅一次的事件。如下代码中，第一个 timer 表示 1s 后执行，\u0026lt;-timer.C 会一直阻塞，直至预定时间到达。第二个 timer 表示 2s 后执行，新启一个 goroutine 等待时间到达，主 routine 在时间未到达前即调用了 Stop()，这样，新启的 goroutine 中的逻辑即不会被执行。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { timer := time.NewTimer(time.Second) \u0026lt;-timer.C fmt.Println(\u0026#34;hello\u0026#34;) timer = time.NewTimer(2 * time.Second) go func() { \u0026lt;-timer.C fmt.Println(\u0026#34;world\u0026#34;) }() if timer.Stop() { fmt.Println(\u0026#34;timer stoped\u0026#34;) } } time.AfterFunc 亦可以创建一个 timer，func 参数可以是时间到达后自定义的执行函数。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { done := make(chan bool) time.AfterFunc(time.Second, func() { fmt.Println(\u0026#34;hello\u0026#34;) done \u0026lt;- true }) \u0026lt;-done } 一个通常错误的认为是，创建 N 个 timer（无论是以 time.NewTimer 方式还是以 time.AfterFunc 方式）会伴随创建出 N 个 goroutine 来跟踪对应的指定时间，以在时间到达时执行。如下代码中，开始时，为了可以查询到系统级 goroutine 堆栈，加了一行代码debug.SetTraceback(\u0026quot;system\u0026quot;)。不传任何参数会打印创建 timer 前的堆栈信息，传入一个参数，会打印创建完 10000 个 timer 后的堆栈。\n创建 timer 前的 goroutine 数：\n$ go run test.go 2\u0026gt;\u0026amp;1 | grep \u0026quot;^goroutine\u0026quot; | wc -l 4 大量创建 timer 后的 goroutine 数：\n$ go run test.go hello 2\u0026gt;\u0026amp;1 | grep \u0026quot;^goroutine\u0026quot; | wc -l 5 可以发现创建 10000 个 timer 仅创建了一个监听 goroutine。这是由于 runtime/time.go 内部使用堆统一管理 timer，新建或停止 timer 仅是在对堆节点作增删，堆将要执行的 timer 排序，最近一个节点到达执行时间，即执行，有 timer 停止即从堆中移除，所以多个 timer 仅统一使用一个 goroutine 作调度即可。\nhttps://github.com/golang/go/blob/master/src/runtime/time.go\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;runtime/debug\u0026#34; \u0026#34;time\u0026#34; ) func main() { debug.SetTraceback(\u0026#34;system\u0026#34;) if len(os.Args) \u0026lt;= 1 { panic(\u0026#34;before\u0026#34;) } for i := 0; i \u0026lt; 10000; i++ { time.NewTimer(time.Second) // time.AfterFunc(time.Second, func() {})  } panic(\u0026#34;after\u0026#34;) } 2 Tickers\nTicker 表示一个按一定时间间隔周期性执行的事件。其创建与 Timer 类似。如下代码中，创建一个每隔 1s 即触发执行的 ticker，新启一个 goroutine 遍历其时钟 chan 打印时间，主 routine 等待 5s 后停止该 ticker，新启的 goroutine 即不会再收到消息。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { ticker := time.NewTicker(time.Second) go func() { for t := range ticker.C { fmt.Println(t) } }() time.Sleep(5 * time.Second) ticker.Stop() } 本文代码托管地址：https://github.com/olzhy/go-exercises/tree/master/timers_and_tickers\n 参考资料\n[1] https://golang.org/pkg/time\n[2] https://gobyexample.com/timers\n[3] https://gobyexample.com/tickers\n[4] https://blog.gopheracademy.com/advent-2016/go-timers\n[5] https://programming.guide/go/time-reset-wait-stop-timeout-cancel-interval.html\n ","permalink":"https://olzhy.github.io/posts/golang-timers-and-tickers.html","tags":["Golang"],"title":"Golang Timers Tickers 使用小结"},{"categories":["计算机"],"contents":"golang 中可以使用 Buffered channel 作为信号量来对服务的并发访问作吞吐量限制。\n如下代码中，Serve 函数遍历请求队列，对每次请求，启动一个 goroutine 来进行 handle，sem 的缓冲大小限制了同时调用 handle 函数的数量，Serve 函数虽可保障每一刻最多有 MaxOutstanding 个 goroutine 正在调用 handle 函数，但在请求过频与过多的情况下无法保证 goroutine 的过度创建以造成资源耗尽的风险。\nServeWithThroughputLimit 函数对 Serve 作了改进，即对给 sem 发送消息提到了 goroutine 创建之前，以对 goroutine 的创建作限制。这样，同一时刻最多有 MaxOutstanding 个 goroutine 对请求进行 handle。\ngolang 中可以使用 Buffered channel 作为信号量来对服务的并发访问作吞吐量限制。\n如下代码中，Serve 函数遍历请求队列，对每次请求，启动一个 goroutine 来进行 handle，sem 的缓冲大小限制了同时调用 handle 函数的数量，Serve 函数虽可保障每一刻最多有 MaxOutstanding 个 goroutine 正在调用 handle 函数，但在请求过频与过多的情况下无法保证 goroutine 的过度创建以造成资源耗尽的风险。\nServeWithThroughputLimit 函数对 Serve 作了改进，即对给 sem 发送消息提到了 goroutine 创建之前，以对 goroutine 的创建作限制。这样，同一时刻最多有 MaxOutstanding 个 goroutine 对请求进行 handle。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) const MaxOutstanding = 2 type Req struct { id int } func handle(req *Req) { time.Sleep(time.Second) fmt.Println(\u0026#34;handle req\u0026#34;, req.id) } func Serve(queue chan *Req) { var wg sync.WaitGroup sem := make(chan int, MaxOutstanding) for req := range queue { wg.Add(1) go func(req *Req) { fmt.Println(\u0026#34;a goroutine launched\u0026#34;) defer wg.Done() sem \u0026lt;- 1 handle(req) \u0026lt;-sem }(req) } wg.Wait() } func ServeWithThroughputLimit(queue chan *Req) { var wg sync.WaitGroup sem := make(chan int, MaxOutstanding) for req := range queue { wg.Add(1) sem \u0026lt;- 1 go func(req *Req) { fmt.Println(\u0026#34;a goroutine launched\u0026#34;) defer wg.Done() handle(req) \u0026lt;-sem }(req) } wg.Wait() } func main() { queue := make(chan *Req, 5) // requests  go func() { for i := 0; i \u0026lt; 5; i++ { queue \u0026lt;- \u0026amp;Req{i} } close(queue) }() // server  // Serve(queue)  ServeWithThroughputLimit(queue) } 调用 Serve 函数的输出为：\na goroutine launched a goroutine launched a goroutine launched a goroutine launched a goroutine launched handle req 4 handle req 3 handle req 1 handle req 2 handle req 0 调用 ServeWithThroughputLimit 函数的输出为：\na goroutine launched a goroutine launched handle req 0 a goroutine launched handle req 1 a goroutine launched handle req 2 a goroutine launched handle req 3 handle req 4 本文代码托管地址：https://github.com/olzhy/go-exercises/tree/master/throughput_limit\n 参考资料\n[1] https://golang.org/doc/effective_go.html#channels\n ","permalink":"https://olzhy.github.io/posts/golang-throughput-limit.html","tags":["Golang"],"title":"Golang 使用channel作并发访问吞吐量限制"},{"categories":["计算机"],"contents":"以常规方式编写并发程序，需要对共享变量作正确的访问控制，处理起来很困难。而 golang 提出一种不同的方式，即共享变量通过 channel 传递，共享变量从不被各个独立运行的线程(goroutine)同时享有，在任一时刻，共享变量仅可被一个 goroutine 访问。所以，不会产生数据竞争。并发编程，golang 鼓励以此种方式进行思考，精简为一句口号——“勿通过共享内存来进行通信，而应通过通信来进行内存共享”。\n1 Unbuffered channels 与 Buffered channels\nUnbuffered channels 的接收者阻塞直至收到消息，发送者阻塞直至接收者接收到消息，该机制可用于两个 goroutine 的状态同步。Buffered channels 在缓冲区未满时，发送者仅在值拷贝到缓冲区之前是阻塞的，而在缓冲区已满时，发送者会阻塞，直至接收者取走了消息，缓冲区有了空余。\n1.1 Unbuffered channels\n如下代码使用 Unbuffered channel 作同步控制。给定一个整型数组，在主 routine 启动另一个 goroutine 将该数组排序，当其完成时，给 done channel 发送完成消息，主 routine 会一直等待直至排序完成，打印结果。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;time\u0026#34; ) func main() { done := make(chan bool) nums := []int{2, 1, 3, 5, 4} go func() { time.Sleep(time.Second) sort.Ints(nums) done \u0026lt;- true }() \u0026lt;-done fmt.Println(nums) } 1.2 Buffered channels\n如下代码中，messages chan 的缓冲区大小为 2，因其为 Buffered channel，所以消息发送与接收无须分开到两个并发的 goroutine 中。\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { messages := make(chan string, 2) messages \u0026lt;- \u0026#34;hello\u0026#34; messages \u0026lt;- \u0026#34;world\u0026#34; fmt.Println(\u0026lt;-messages, \u0026lt;-messages) } 2 配套使用\n2.1 指明 channel direction\n函数封装时，对仅作消息接收或仅作消息发送的 chan 标识 direction 可以借用编译器检查增强类型使用安全。如下代码中，ping 函数中 pings chan 仅用来接收消息，所以参数列表中将其标识为接收者。pong 函数中，pings chan 仅用来发送消息，pongs chan 仅用来接收消息，所以参数列表中二者分别标识为发送者与接收者。\npackage main import \u0026#34;fmt\u0026#34; func ping(pings chan\u0026lt;- string, msg string) { pings \u0026lt;- msg } func pong(pings \u0026lt;-chan string, pongs chan\u0026lt;- string) { pongs \u0026lt;- \u0026lt;-pings } func main() { pings, pongs := make(chan string, 1), make(chan string, 1) ping(pings, \u0026#34;ping\u0026#34;) pong(pings, pongs) fmt.Println(\u0026lt;-pongs) } 2.2 select\n使用 select 可以用来等待多个 channel 的消息，如下代码，创建两个 chan，启动两个 goroutine 耗费不等时间计算结果，主 routine 监听消息，使用两次 select，第一次接收到了 ch2 的消息，第二次接收到了 ch1 的消息，用时 2.000521146s。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func main() { c1, c2 := make(chan int, 1), make(chan int, 1) go func() { time.Sleep(2 * time.Second) c1 \u0026lt;- 1 }() go func() { time.Sleep(time.Second) c2 \u0026lt;- 2 }() for i := 0; i \u0026lt; 2; i++ { select { case msg1 := \u0026lt;-c1: fmt.Println(\u0026#34;received msg from c1\u0026#34;, msg1) case msg2 := \u0026lt;-c2: fmt.Println(\u0026#34;received msg from c2\u0026#34;, msg2) } } } 2.3 select with default\nselect with default 可以用来处理非阻塞式消息发送、接收及多路选择。如下代码中，第一个 select 为非阻塞式消息接收，若收到消息，则落入\u0026lt;-messages case，否则落入 default。第二个 select 为非阻塞式消息发送，与非阻塞式消息接收类似，因 messages chan 为 Unbuffered channel 且无异步消息接收者，因此落入 default case。第三个 select 为多路非阻塞式消息接收。\npackage main import \u0026#34;fmt\u0026#34; func main() { messages := make(chan string) signal := make(chan bool) // receive with default  select { case \u0026lt;-messages: fmt.Println(\u0026#34;message received\u0026#34;) default: fmt.Println(\u0026#34;no message received\u0026#34;) } // send with default  select { case messages \u0026lt;- \u0026#34;message\u0026#34;: fmt.Println(\u0026#34;message sent successfully\u0026#34;) default: fmt.Println(\u0026#34;message sent failed\u0026#34;) } // muti-way select  select { case \u0026lt;-messages: fmt.Println(\u0026#34;message received\u0026#34;) case \u0026lt;-signal: fmt.Println(\u0026#34;signal received\u0026#34;) default: fmt.Println(\u0026#34;no message or signal received\u0026#34;) } } 2.4 close\n当无需再给 channel 发送消息时，可将其 close。如下代码中，创建一个 Buffered channel，首先启动一个异步 goroutine 循环消费消息，然后主 routine 完成消息发送后关闭 chan，消费 goroutine 检测到 chan 关闭后，退出循环。\npackage main import \u0026#34;fmt\u0026#34; func main() { messages := make(chan int, 10) done := make(chan bool) // consumer  go func() { for { msg, more := \u0026lt;-messages if !more { fmt.Println(\u0026#34;no more message\u0026#34;) done \u0026lt;- true break } fmt.Println(\u0026#34;message received\u0026#34;, msg) } }() // producer  for i := 0; i \u0026lt; 5; i++ { messages \u0026lt;- i } close(messages) \u0026lt;-done } 2.5 for range\nfor range 语法不仅可对基础数据结构（slice、map 等）作迭代，还可对 channel 作消息接收迭代。如下代码中，给 messages chan 发送两条消息后将其关闭，然后迭代 messages chan 打印消息。\npackage main import \u0026#34;fmt\u0026#34; func main() { messages := make(chan string, 2) messages \u0026lt;- \u0026#34;hello\u0026#34; messages \u0026lt;- \u0026#34;world\u0026#34; close(messages) for msg := range messages { fmt.Println(msg) } } 3 应用场景\n3.1 超时控制\n资源访问、网络请求等场景作超时控制是非常必要的，可以使用 channel 结合 select 来实现。如下代码，对常规 sum 函数增加超时限制，sumWithTimeout 函数中，select 的 v := \u0026lt;-rlt 在等待计算结果，若在时限范围内计算完成，则正常返回计算结果，若超过时限则落入\u0026lt;-time.After(timeout) case，抛出 timeout error。\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func sum(nums []int) int { rlt := 0 for _, num := range nums { rlt += num } return rlt } func sumWithTimeout(nums []int, timeout time.Duration) (int, error) { rlt := make(chan int) go func() { time.Sleep(2 * time.Second) rlt \u0026lt;- sum(nums) }() select { case v := \u0026lt;-rlt: return v, nil case \u0026lt;-time.After(timeout): return 0, errors.New(\u0026#34;timeout\u0026#34;) } } func main() { nums := []int{1, 2, 3, 4, 5} timeout := 3 * time.Second // time.Second  rlt, err := sumWithTimeout(nums, timeout) if nil != err { fmt.Println(\u0026#34;error\u0026#34;, err) return } fmt.Println(rlt) } 本文代码托管地址：https://github.com/olzhy/go-exercises/tree/master/channels\n 参考资料\n[1] https://golang.org/doc/effective_go.html#channels\n[2] https://gobyexample.com/channel-synchronization\n[3] https://gobyexample.com/channel-buffering\n[4] https://gobyexample.com/channel-directions\n[5] https://gobyexample.com/select\n[6] https://gobyexample.com/non-blocking-channel-operations\n[7] https://gobyexample.com/closing-channels\n[8] https://gobyexample.com/range-over-channels\n[9] https://gobyexample.com/timeouts\n ","permalink":"https://olzhy.github.io/posts/golang-channels.html","tags":["Golang"],"title":"Golang channel 使用小结"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个m x n矩阵，若一个元素为0，将其所在行与列全部元素置为0，请使用原地计算。\n例子1：\n输入：\n[\n[1,1,1],\n[1,0,1],\n[1,1,1]\n]\n输出：\n[\n[1,0,1],\n[0,0,0],\n[1,0,1]\n]\n例子2：\n输入：\n[\n[0,1,2,0],\n[3,4,5,2],\n[1,3,1,5]\n]\n输出：\n[\n[0,0,0,0],\n[0,4,5,0],\n[0,3,1,0]\n]\n进阶：\na) 一个直接的解决思路是使用O(mn)的额外空间，该解决思路较差；\nb) 一个改进算法是使用O(m + n)的额外空间，仍不是最优算法。\nc) 你可以设计一个常数空间的解决方案吗？\n题目出处：\nhttps://leetcode.com/problems/set-matrix-zeroes/\n2 解决思路\n  自左向右，自上到下遍历矩阵；\n  若发现当前行，当前列元素为0，则记录该列，并将该行标识为含0行，若截止目前包含0的列中含有当前列，则将当前行以上所有行在该列的记录置为0；\n  遍历完该行，若该行为包含0的行，则统一将该整行元素置为0；\n  重复2、3步遍历完成，即为所得。\n  整体算法仅最多使用O(n)的额外空间，记录包含0的列。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/73_Set_Matrix_Zeroes/test.go\nfunc setZeroes(matrix [][]int) { zeroCols := make(map[int]int) for i := range matrix { rowHasZero := false for j := range matrix[i] { if 0 == matrix[i][j] { rowHasZero = true zeroCols[j] = 1 } if _, ok := zeroCols[j]; ok { for k := 0; k \u0026lt;= i; k++ { matrix[k][j] = 0 } } } if rowHasZero { for j := range matrix[i] { matrix[i][j] = 0 } } } } ","permalink":"https://olzhy.github.io/posts/leetcode-set-matrix-zeroes.html","tags":["Golang","算法"],"title":"LeetCode 73 矩阵置零"},{"categories":["计算机"],"contents":"1 题目描述\n给定字符串，以Z字形显示。如\u0026quot;PAYPALISHIRING\u0026quot;以给定行数为3的Z字形显示为：\n然后从左到右一行一行拼起来为：\u0026ldquo;PAHNAPLSIIGYIR\u0026rdquo;。\n现在，传入一个字符串及行数，请编写代码求该字符串的Z字形变换。\n例子1：\n输入：s = \u0026ldquo;PAYPALISHIRING\u0026rdquo;, numRows = 3\n输出：\u0026ldquo;PAHNAPLSIIGYIR\u0026rdquo;\n例子2：\n输入：s = \u0026ldquo;PAYPALISHIRING\u0026rdquo;, numRows = 4\n输出：\u0026ldquo;PINALSIGYAHRPI\u0026rdquo;\n释义：\n题目出处：\nhttps://leetcode.com/problems/zigzag-conversion/\n2 解决思路\n如下图所示：\na）最顶部和最底部水平方向两字母之间最大间隔maxInterval为2 * (numRows-1)；\nb）i从0开始，第i行，自左向右，奇数序号水平方向两字母之间距离interval为2 * (numRows-1) - i*2，偶数序号水平方向两字母之间距离为maxInterval - interval；\nc) 根据此规律，可以构造字符串的Z字形显示结果。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/6_ZigZag_Conversion/test.go\nfunc convert(s string, numRows int) string { if numRows \u0026lt; 2 { return s } maxInterval := (numRows - 1) \u0026lt;\u0026lt; 1 interval := maxInterval after := \u0026#34;\u0026#34; for i := 0; i \u0026lt; numRows; i++ { if numRows-1 == i { interval = maxInterval } for j, no := i, 0; j \u0026lt; len(s); no++ { after += string(s[j]) if i \u0026gt; 0 \u0026amp;\u0026amp; i \u0026lt; numRows-1 \u0026amp;\u0026amp; 1 == no\u0026amp;1 { j += maxInterval - interval continue } j += interval } interval -= 2 } return after } ","permalink":"https://olzhy.github.io/posts/leetcode-zigzag-conversion.html","tags":["Golang","算法"],"title":"LeetCode 6 Z字形变换"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个正整数n，生成一个由1到n^2元素以螺旋顺序填充的n x n矩阵。\n例子：\n输入：3\n输出：\n[\n[ 1, 2, 3 ],\n[ 8, 9, 4 ],\n[ 7, 6, 5 ]\n]\n题目出处：\nhttps://leetcode.com/problems/spiral-matrix-ii/\n2 解决思路\n由顶、右、底、左外边界逐步向里遍历矩阵，将元素置入结果矩阵返回即可。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/59_Spiral_Matrix_II/test.go\nfunc generateMatrix(n int) [][]int { matrix := make([][]int, n) for row := 0; row \u0026lt; len(matrix); row++ { matrix[row] = make([]int, n) } v := 1 topBoundary, rightBoundary, bottomBoundary, leftBoundary := 0, len(matrix[0]), len(matrix), 0 for topBoundary \u0026lt; bottomBoundary { // top  for col := leftBoundary; col \u0026lt; rightBoundary; col++ { matrix[topBoundary][col] = v v++ } topBoundary++ // right  for row := topBoundary; row \u0026lt; bottomBoundary; row++ { matrix[row][rightBoundary-1] = v v++ } rightBoundary-- // bottom  for col := rightBoundary - 1; col \u0026gt;= leftBoundary; col-- { matrix[bottomBoundary-1][col] = v v++ } bottomBoundary-- // left  for row := bottomBoundary - 1; row \u0026gt;= topBoundary; row-- { matrix[row][leftBoundary] = v v++ } leftBoundary++ } return matrix } ","permalink":"https://olzhy.github.io/posts/leetcode-spiral-matrix-ii.html","tags":["Golang","算法"],"title":"LeetCode 59 螺旋矩阵 II"},{"categories":["计算机"],"contents":"1 题目描述\n两个整数之间的汉明距离是指两数的二进制数中各对应比特位不同的个数。现给定一组整数，计算该组整数中所有两数组合的汉明距离总和。\n例子：\n输入：4, 14, 2\n输出：6\n释义：\n4的二进制是0100，14的二进制是1110，2的二进制是0010（该例子仅展示出4个比特位），所以按题目要求，答案是HammingDistance(4, 14) + HammingDistance(4, 2) + HammingDistance(14, 2) = 2 + 2 + 2 = 6。\n题目出处：\nhttps://leetcode.com/problems/total-hamming-distance/\n2 常规算法\n2.1 思路描述\n对数组中整数两两求异或后计算其中1的个数并累加。\n2.2 golang代码实现\nfunc totalHammingDistance(nums []int) int { distance := 0 for i := 0; i \u0026lt; len(nums); i++ { for j := i + 1; j \u0026lt; len(nums); j++ { v := nums[i] ^ nums[j] ones := 0 for v \u0026gt; 0 { if 1 == v\u0026amp;1 { ones++ } v = v \u0026gt;\u0026gt; 1 } distance += ones } } return distance } 3 改进算法\n3.1 思路描述\n该问题应避免对数组中所有整数组合对两两计算，可对数组中的二进制数由低位到高位统一计算。该数组在指定bit位的汉明距离和即为数组中二进制整数对应在该bit位值为1的总和与值为0的总和的乘积（该bit位的有效组合数）。例如，对如上例子中的输入，0100、1110，0010在由低位起，第0位汉明距离和为03，第1位汉明距离和为21，第2位汉明距离和为21，第3位为12，总和为0 + 2 + 2 + 2 = 6。\n3.2 golang代码实现\nhttps://github.com/olzhy/leetcode/blob/master/477_Total_Hamming_Distance/test.go\nfunc totalHammingDistance(nums []int) int { max := 0 for _, num := range nums { if num \u0026gt; max { max = num } } distance := 0 for i := 0; max \u0026gt; 0; i++ { binaryOnes := 0 for _, num := range nums { if 1 == (num \u0026gt;\u0026gt; uint(i) \u0026amp; 1) { binaryOnes++ } } distance += (len(nums) - binaryOnes) * binaryOnes max \u0026gt;\u0026gt;= 1 } return distance } ","permalink":"https://olzhy.github.io/posts/leetcode-total-hamming-distance.html","tags":["Golang","算法"],"title":"LeetCode 477 汉明距离总和"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个m x n矩阵（m行，n列），按顺时针螺旋顺序返回矩阵的所有元素。\n例子1：\n输入：\n[\n[ 1, 2, 3 ],\n[ 4, 5, 6 ],\n[ 7, 8, 9 ]\n]\n输出：\n[1,2,3,6,9,8,7,4,5]\n例子2：\n输入：\n[\n[1, 2, 3, 4],\n[5, 6, 7, 8],\n[9,10,11,12]\n]\n输出：\n[1,2,3,4,8,12,11,10,9,5,6,7]\n题目出处：\nhttps://leetcode.com/problems/spiral-matrix/\n2 解决思路\n由顶、右、底、左外边界逐步向里遍历矩阵，将元素置入结果矩阵返回即可。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/54_Spiral_Matrix/test.go\nfunc spiralOrder(matrix [][]int) []int { // special  if 0 == len(matrix) { return []int{} } // standard  var elements []int leftBoundary, rightBundary := 0, len(matrix[0])-1 topBoundary, bottomBoundary := 0, len(matrix)-1 for leftBoundary \u0026lt; rightBundary \u0026amp;\u0026amp; topBoundary \u0026lt; bottomBoundary { // top  for j := leftBoundary; j \u0026lt; rightBundary; j++ { elements = append(elements, matrix[topBoundary][j]) } // right  for i := topBoundary; i \u0026lt; bottomBoundary; i++ { elements = append(elements, matrix[i][rightBundary]) } // bottom  for j := rightBundary; j \u0026gt; leftBoundary; j-- { elements = append(elements, matrix[bottomBoundary][j]) } // left  for i := bottomBoundary; i \u0026gt; topBoundary; i-- { elements = append(elements, matrix[i][leftBoundary]) } leftBoundary++ rightBundary-- topBoundary++ bottomBoundary-- } if leftBoundary == rightBundary \u0026amp;\u0026amp; topBoundary \u0026lt;= bottomBoundary { for i := topBoundary; i \u0026lt;= bottomBoundary; i++ { elements = append(elements, matrix[i][leftBoundary]) } return elements } if topBoundary == bottomBoundary \u0026amp;\u0026amp; leftBoundary \u0026lt;= rightBundary { for j := leftBoundary; j \u0026lt;= rightBundary; j++ { elements = append(elements, matrix[topBoundary][j]) } } return elements } ","permalink":"https://olzhy.github.io/posts/leetcode-spiral-matrix.html","tags":["Golang","算法"],"title":"LeetCode 54 螺旋矩阵"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个单链表L： L0 → L1 → … → Ln-1 → Ln，将其重排为L0 → Ln → L1 → Ln-1 → L2 → Ln-2→…。请勿更改节点中的值。\n例子1：\n给定1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4，重排为1 -\u0026gt; 4 -\u0026gt; 2 -\u0026gt; 3；\n例子2：\n给定1 -\u0026gt; 2 -\u0026gt; 3 -\u0026gt; 4 -\u0026gt; 5，将其重排为1 -\u0026gt; 5 -\u0026gt; 2 -\u0026gt; 4 -\u0026gt; 3。\n题目出处：\nhttps://leetcode.com/problems/reorder-list/\n2 解决思路\n2.1）从头至尾遍历原链表节点构建一个双向链表；\n2.2）两个指针分别从头至尾、从尾至头同时遍历该双向链表来建立所要求的顺序关系，直至汇合即完成重排。重排过程如下图所示。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/143_Reorder_List/test.go\ntype ListNode struct { Val int Next *ListNode } type TwoWayListNode struct { *ListNode Pre *TwoWayListNode } func reorderList(head *ListNode) { // validation  if nil == head { return } // build two-way list  tail := \u0026amp;TwoWayListNode{head, nil} for node := head.Next; nil != node; node = node.Next { currNode := \u0026amp;TwoWayListNode{node, tail} tail = currNode } // re order  for p, q := head, tail; ; { if p == q.ListNode { p.Next = nil break } if p.Next == q.ListNode { p.Next.Next = nil break } x, y := p, q p, q = p.Next, q.Pre x.Next = y.ListNode y.Next = p } } ","permalink":"https://olzhy.github.io/posts/leetcode-reorder-linked-list.html","tags":["Golang","算法"],"title":"LeetCode 143 重排链表"},{"categories":null,"contents":"谢谢您的打赏，我会努力写更多有价值的文章！\n2021\n 02.18微信用户A*e打赏  2020\n 03.21微信用户*建打赏  2019\n 01.15微信用户*?打赏 03.08微信用户*汤打赏  2018\n 11.04微信用户*珍打赏 11.12微信用户A*e打赏 11.12微信用户*n打赏 12.21微信用户Z*U打赏 12.22微信用户*子打赏 12.22微信用户*※打赏 12.22微信用户*狸打赏 12.22微信用户顾*i打赏  ","permalink":"https://olzhy.github.io/thanks/","tags":null,"title":"Thanks"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个包含字母\u0026rsquo;O\u0026rsquo;与\u0026rsquo;X\u0026rsquo;的二维平面，占领所有被\u0026rsquo;X\u0026rsquo;包围的区域（将被\u0026rsquo;X\u0026rsquo;包围的区域中的\u0026rsquo;O\u0026rsquo;填充为\u0026rsquo;X\u0026rsquo;即为占领）。\n例子：\n输入：\nX X X X\nX O O X\nX X O X\nX O X X\n执行程序后应为：\nX X X X\nX X X X\nX X X X\nX O X X\n注：\n包围的区域不应在边界上，即任何在边界上的\u0026rsquo;O\u0026rsquo;不应被替换为\u0026rsquo;X'；若两个含\u0026rsquo;O\u0026rsquo;的单元水平方向或垂直方向连接即认为两部分是连接的；因此，该题目需将任何不在边界上并且不与边界上的\u0026rsquo;O\u0026rsquo;连接的\u0026rsquo;O\u0026rsquo;替换为\u0026rsquo;X'。\n题目出处：\nhttps://leetcode.com/problems/surrounded-regions/\n2 解决思路\n找出该平面左、下、右、上边界上所有的\u0026rsquo;O\u0026rsquo;及其坐标，遍历这些边界上的\u0026rsquo;O'，由其出发寻找所有与其直接以及间接连通的\u0026rsquo;O\u0026rsquo;并记录下来，最后除这些连通的\u0026rsquo;O\u0026rsquo;以外，将平面上所有其他的‘O’置为‘X’。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/130_Surrounded_Regions/test.go\n// findOBoundaries return coordinates that value is \u0026#39;O\u0026#39; // and mark the coordinates visited func findOBoundaries(board [][]byte, oVisited *[][]int) [][]int { var boundaries [][]int if 1 == len(board) { if \u0026#39;O\u0026#39; == board[0][0] { p := []int{0, 0} boundaries = append(boundaries, p) *oVisited = append(*oVisited, p) } return boundaries } // left boundary  for i := 0; i \u0026lt; len(board)-1; i++ { if \u0026#39;O\u0026#39; == board[i][0] { p := []int{i, 0} boundaries = append(boundaries, p) *oVisited = append(*oVisited, p) } } // bottom boundary  for j := 0; j \u0026lt; len(board[0])-1; j++ { if \u0026#39;O\u0026#39; == board[len(board)-1][j] { p := []int{len(board) - 1, j} boundaries = append(boundaries, p) *oVisited = append(*oVisited, p) } } // right boundary  for i := len(board) - 1; i \u0026gt; 0; i-- { if \u0026#39;O\u0026#39; == board[i][len(board[0])-1] { p := []int{i, len(board[0]) - 1} boundaries = append(boundaries, p) *oVisited = append(*oVisited, p) } } // top boundary  for j := len(board[0]) - 1; j \u0026gt; 0; j-- { if \u0026#39;O\u0026#39; == board[0][j] { p := []int{0, j} boundaries = append(boundaries, p) *oVisited = append(*oVisited, p) } } return boundaries } func isVisited(i, j int, oVisited *[][]int) bool { for _, v := range *oVisited { if v[0] == i \u0026amp;\u0026amp; v[1] == j { return true } } return false } func findOsAround(board [][]byte, i, j int, oVisited *[][]int) [][]int { var osAround [][]int // left  if j \u0026gt; 0 { left := board[i][j-1] if !isVisited(i, j-1, oVisited) \u0026amp;\u0026amp; \u0026#39;O\u0026#39; == left { p := []int{i, j - 1} osAround = append(osAround, p) *oVisited = append(*oVisited, p) } } // down  if i \u0026lt; len(board)-1 { down := board[i+1][j] if !isVisited(i+1, j, oVisited) \u0026amp;\u0026amp; \u0026#39;O\u0026#39; == down { p := []int{i + 1, j} osAround = append(osAround, p) *oVisited = append(*oVisited, p) } } // right  if j \u0026lt; len(board[0])-1 { right := board[i][j+1] if !isVisited(i, j+1, oVisited) \u0026amp;\u0026amp; \u0026#39;O\u0026#39; == right { p := []int{i, j + 1} osAround = append(osAround, p) *oVisited = append(*oVisited, p) } } // up  if i \u0026gt; 0 { up := board[i-1][j] if !isVisited(i-1, j, oVisited) \u0026amp;\u0026amp; \u0026#39;O\u0026#39; == up { p := []int{i - 1, j} osAround = append(osAround, p) *oVisited = append(*oVisited, p) } } return osAround } func findOs(board [][]byte, points [][]int, oVisited *[][]int) [][]int { var os [][]int for _, p := range points { os = append(os, findOsAround(board, p[0], p[1], oVisited)...) } return os } func solve(board [][]byte) { if 0 == len(board) { return } var oVisited [][]int // visit all \u0026#39;O\u0026#39;s from boundaries that is \u0026#39;O\u0026#39;  os := findOBoundaries(board, \u0026amp;oVisited) for { os = findOs(board, os, \u0026amp;oVisited) if len(os) == 0 { break } } // alter the not visited \u0026#39;O\u0026#39; to \u0026#39;X\u0026#39;  for i := range board { for j := range board[i] { if \u0026#39;O\u0026#39; == board[i][j] \u0026amp;\u0026amp; !isVisited(i, j, \u0026amp;oVisited) { board[i][j] = \u0026#39;X\u0026#39; } } } } ","permalink":"https://olzhy.github.io/posts/leetcode-surrounded-regions.html","tags":["Golang","算法"],"title":"LeetCode 130 围起的区域"},{"categories":["计算机"],"contents":"1 题目描述\n我们知道集合[1,2,3,\u0026hellip;,n]共包含n!个排列。以n=3为例，其有序全排列如下。\n\u0026ldquo;123\u0026rdquo;\n\u0026ldquo;132\u0026rdquo;\n\u0026ldquo;213\u0026rdquo;\n\u0026ldquo;231\u0026rdquo;\n\u0026ldquo;312\u0026rdquo;\n\u0026ldquo;321\u0026rdquo;\n本题给定n，求其有序全排列中的第k个。\n注：n介于区间[1,9]，k介于区间[1,n!]。\n例子1：\n输入：n = 3, k = 3\n输出：\u0026ldquo;213\u0026rdquo;\n例子2：\n输入：n = 4, k = 9\n输出：\u0026ldquo;2314\u0026rdquo;\n题目出处：\nhttps://leetcode.com/problems/permutation-sequence/\n2 解决思路\n首先根据k找到需要计算的最小子序列，假定找到的该子序列的长度为i，针对该序列分别将第[0,i-1]个元素置于头部的序列共有i*(i-1)!个全排列。所以根据该规律，对于给定的k，即可计算出第几个元素需至于头部，然后将k重置为余数，再对其子序列递归计算结果。\n3 golang实现代码\nfunc factorial(i int) int { f := 1 for ; i \u0026gt;= 1; i-- { f *= i } return f } func getPermutation(s string, k int) string { i := len(s) if 1 == i { return s } factorial := factorial(i) nextFactorial := factorial / i if k \u0026lt;= nextFactorial { return s[:1] + getPermutation(s[1:], k) } c, k := (k-1)/nextFactorial, (k-1)%nextFactorial+1 if c \u0026gt; 0 { s = string(s[c]) + s[:c] + s[c+1:] } return getPermutation(s, k) } func getPermutation(n int, k int) string { s := \u0026#34;\u0026#34; for i := 1; i \u0026lt;= n; i++ { s += strconv.Itoa(i) } return getPermutation(s, k) } ","permalink":"https://olzhy.github.io/posts/leetcode-permutation-sequence.html","tags":["Golang","算法"],"title":"LeetCode 60 求第k个排列"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个包含数字[2-9]的字符串（闭区间），求数字可以代表的所有可能的字母组合。数字与字母的映射关系与手机号码簿相同（如下图），注意数字1未映射任何字母。\n例子：\n输入：\u0026ldquo;23\u0026rdquo;\n输出：[\u0026ldquo;ad\u0026rdquo;, \u0026ldquo;ae\u0026rdquo;, \u0026ldquo;af\u0026rdquo;, \u0026ldquo;bd\u0026rdquo;, \u0026ldquo;be\u0026rdquo;, \u0026ldquo;bf\u0026rdquo;, \u0026ldquo;cd\u0026rdquo;, \u0026ldquo;ce\u0026rdquo;, \u0026ldquo;cf\u0026rdquo;]\n说明：虽如上例子所得组合为字母序，本题不对组合中字母顺序作要求。\n题目出处：\nhttps://leetcode.com/problems/letter-combinations-of-a-phone-number/\n2 解决思路\n从第一个数字起，遍历其代表的所有字母，分别与子串形成的组合数组连接即为所求，所以采用递归，即可算得结果。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/17_Letter_Combinations_Of_A_Phone_Number/test.go\npackage main import \u0026#34;fmt\u0026#34; var ( mapping = map[byte][]string{ \u0026#39;2\u0026#39;: {\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;}, \u0026#39;3\u0026#39;: {\u0026#34;d\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;f\u0026#34;}, \u0026#39;4\u0026#39;: {\u0026#34;g\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;i\u0026#34;}, \u0026#39;5\u0026#39;: {\u0026#34;j\u0026#34;, \u0026#34;k\u0026#34;, \u0026#34;l\u0026#34;}, \u0026#39;6\u0026#39;: {\u0026#34;m\u0026#34;, \u0026#34;n\u0026#34;, \u0026#34;o\u0026#34;}, \u0026#39;7\u0026#39;: {\u0026#34;p\u0026#34;, \u0026#34;q\u0026#34;, \u0026#34;r\u0026#34;, \u0026#34;s\u0026#34;}, \u0026#39;8\u0026#39;: {\u0026#34;t\u0026#34;, \u0026#34;u\u0026#34;, \u0026#34;v\u0026#34;}, \u0026#39;9\u0026#39;: {\u0026#34;w\u0026#34;, \u0026#34;x\u0026#34;, \u0026#34;y\u0026#34;, \u0026#34;z\u0026#34;}, } ) func letterCombinations(digits string) []string { var combinations []string if 0 == len(digits) { return combinations } if 1 == len(digits) { return mapping[digits[0]] } for _, letter := range mapping[digits[0]] { for _, suffix := range letterCombinations(digits[1:]) { combination := string(letter) + suffix combinations = append(combinations, combination) } } return combinations } func main() { fmt.Println(letterCombinations(\u0026#34;234\u0026#34;)) } ","permalink":"https://olzhy.github.io/posts/leetcode-letter-combinations-of-a-phone-number.html","tags":["Golang","算法"],"title":"LeetCode 17 求电话号码的字母组合"},{"categories":["计算机"],"contents":"1 题目描述\n我们知道，集合[1,2,3,\u0026hellip;,n]共有n!种全排列，现给定n (1\u0026lt;=n\u0026lt;=9)，求其有序全排列。\n例子1：\n输入：2\n输出：[\u0026ldquo;12\u0026rdquo;, \u0026ldquo;21\u0026rdquo;]\n例子2：\n输入：3\n输出：[\u0026ldquo;123\u0026rdquo;, \u0026ldquo;132\u0026rdquo;, \u0026ldquo;213\u0026rdquo;, \u0026ldquo;231\u0026rdquo;, \u0026ldquo;312\u0026rdquo;, \u0026ldquo;321\u0026rdquo;]\n2 解决思路\n因字符串初始是有序的，欲求其有序全排列，可对字符串依次自左向右将各元素排在最左边，然后分别拼接其子串形成的排列，递归至子串仅剩一个元素，即求得结果。\n3 golang实现\n3.1）golang代码\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; ) func permutation(s string) []string { if 1 == len(s) { return []string{s} } var results []string for i := 0; i \u0026lt; len(s); i++ { rightPart := s[:i] + s[i+1:] for _, j := range permutation(rightPart) { result := string(s[i]) + j results = append(results, result) } } return results } func getPermutation(n int) []string { s := \u0026#34;\u0026#34; for i := 1; i \u0026lt;= n; i++ { s += strconv.Itoa(i) } return permutation(s) } func main() { fmt.Println(getPermutation(4)) } 3.2）结果输出\n[1234 1243 1324 1342 1423 1432 2134 2143 2314 2341 2413 2431 3124 3142 3214 3241 3412 3421 4123 4132 4213 4231 4312 4321] 4 基准测试\n对n=9，做基准测试，结果如下。\ngoos: darwin goarch: amd64 pkg: github.com/olzhy/test BenchmarkGetPermutation-4 3 428508739 ns/op 180295040 B/op 4094906 allocs/op PASS ok github.com/olzhy/test 2.602s ","permalink":"https://olzhy.github.io/posts/algorithm-permutation.html","tags":["Golang","算法"],"title":"求字符串的有序全排列"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个 n x n 的二维矩阵表征一幅图像，求顺时针旋转90度后的图像。\n注：须使用现有内存空间原地旋转该图像，勿分配额外的二维矩阵空间来做旋转。\n例子1：\n输入矩阵：\n[\n[1,2,3],\n[4,5,6],\n[7,8,9]\n]\n输出：\n[\n[7,4,1],\n[8,5,2],\n[9,6,3]\n]\n例子2：\n输入矩阵：\n[\n[ 5, 1, 9,11],\n[ 2, 4, 8,10],\n[13, 3, 6, 7],\n[15,14,12,16]\n],\n输出：\n[\n[15,13, 2, 5],\n[14, 3, 4, 1],\n[12, 6, 8, 9],\n[16, 7,10,11]\n]\n题目出处：\nhttps://leetcode.com/problems/rotate-image/\n2 解决思路\n1）如图所示，原始图如下图所示，要不分配额外空间，以最小的空间来作旋转，我们设定每次仅移动一位。\n2）一圈元素顺时针旋转90度的移动过程如下图左图所示，仅申请一个元素的存储单元tmp，作顺时针旋转时，我们每次仅移动一位。步骤如下。\n2.1）左上角元素移入tmp；\n2.1）最左边一列元素均向上移动一位；\n2.2）最底部一行元素均向左移动一位；\n2.3）最右边一列元素均向下移动一位；\n2.4）最顶部一行元素均向右移动一位；\n2.5）tmp元素置入顶部行的最左边第二列；\n2.6）循环移动n-1次，直至下图右图所示，整个最外围一圈的元素即顺时针90度旋转完成。\n3）最外围边界递进一层，开始次外围一圈元素的旋转，移动方式同2-3相同。\n4）直至最里边一圈的元素均旋转完成，即得到整个图像的旋转。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/48_Rotate_Image/test.go\nfunc rotate(matrix [][]int) { for level := 0; level \u0026lt; len(matrix)/2; level++ { topBoundary := level leftBoundary := level bottomBoundary := len(matrix) - 1 - level rightBoundary := len(matrix) - 1 - level for times := 0; times \u0026lt; bottomBoundary-topBoundary; times++ { // left  mostLeftTop := matrix[topBoundary][leftBoundary] for row := topBoundary; row \u0026lt; bottomBoundary; row++ { matrix[row][leftBoundary] = matrix[row+1][leftBoundary] } // bottom  for col := leftBoundary; col \u0026lt; rightBoundary; col++ { matrix[bottomBoundary][col] = matrix[bottomBoundary][col+1] } // right  for row := bottomBoundary; row \u0026gt; topBoundary; row-- { matrix[row][rightBoundary] = matrix[row-1][rightBoundary] } // top  for col := rightBoundary; col \u0026gt; leftBoundary+1; col-- { matrix[topBoundary][col] = matrix[topBoundary][col-1] } matrix[topBoundary][leftBoundary+1] = mostLeftTop } } } 4 基准测试\npackage main import \u0026#34;testing\u0026#34; func BenchmarkRotate(b *testing.B) { matrix := [][]int{ {5, 1, 9, 11}, {2, 4, 8, 10}, {13, 3, 6, 7}, {15, 14, 12, 16}, } for i := 0; i \u0026lt; b.N; i++ { rotate(matrix) } } goos: darwin goarch: amd64 pkg: github.com/olzhy/test BenchmarkRotate-4 20000000 65.2 ns/op 0 B/op 0 allocs/op PASS ok github.com/olzhy/test 1.383s ","permalink":"https://olzhy.github.io/posts/leetcode-rotate-image.html","tags":["Golang","算法"],"title":"LeetCode 48 旋转图像"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个字符串，求不含重复字符的最长子串的长度。\n例子1：\n输入：\u0026ldquo;abcabcbb\u0026rdquo;\n输出：3\n说明：最长子串为\u0026quot;abc\u0026quot;，长度为3。\n例子2：\n输入：\u0026ldquo;bbbbb\u0026rdquo;\n输出：1\n说明：最长子串为\u0026quot;b\u0026quot;，长度为1。\n例子3：\n输入：\u0026ldquo;pwwkew\u0026rdquo;\n输出：3\n说明：最长子串为\u0026quot;wke\u0026quot;，长度为3，注意答案需是子串，该例子中\u0026quot;pwke\u0026quot;为子序列，非子串。\n题目出处：\nhttps://leetcode.com/problems/longest-substring-without-repeating-characters/\n2 解决思路\n1）初始最长子串为空；\n2）遍历字符数组，若当前字符不在当前最长子串中，则将其添加到当前最长子串尾部，并更新当前最长子串长度；\n否则，记录当前最长子串长度，并将当前最长子串自当前字符起（不包含该字符）直至尾部截取的新子串并添加当前字符作为新的最长子串，重复2）直至遍历至最后一个字符；\n3）返回最长子串长度。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/3_Longest_Substring/test.go\nfunc lengthOfLongestSubstring(s string) int { currLen, maxLen := 0, 0 preStr := \u0026#34;\u0026#34; for _, c := range []rune(s) { if strings.Contains(preStr, string(c)) { i := strings.LastIndex(preStr, string(c)) if len(preStr)-1 == i { preStr = string(c) currLen = 1 } else { preStr = string(preStr[i+1:]) + string(c) currLen = len(preStr) } continue } preStr += string(c) currLen++ if currLen \u0026gt; maxLen { maxLen = currLen } } return maxLen } ","permalink":"https://olzhy.github.io/posts/leetcode-longest-substring-without-repeating-characters.html","tags":["Golang","算法"],"title":"LeetCode 3 求不含重复字符的最长子串"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个以括号组成的平衡字符串表达式，基于如下规则计算括号表达式的分值。\n1）()的分值为1；\n2）AB的分值为A+B，其中A与B均是平衡字符串；\n3）(A)的分值为2*A，其中A是平衡字符串。\n例子1：\n输入：\u0026quot;()\u0026quot;\n输出：1\n例子2：\n输入：\u0026quot;(())\u0026quot;\n输出：2\n例子3：\n输入：\u0026quot;()()\u0026quot;\n输出：2\n例子4：\n输入：\u0026quot;(()(()))\u0026quot;\n输出：6\n注：\n1）字符串仅由'(\u0026lsquo;或\u0026rsquo;)\u0026lsquo;组成；\n2）2 \u0026lt;= len(s) \u0026lt;= 50\n题目出处：\nhttps://leetcode.com/problems/score-of-parentheses/\n2 解决思路\n1）遍历字符数组；\n1.1）若当前字符与下个字符是\u0026rsquo;(('，则值为“2*(扩起的字符串值)+扩起字符串后面字符串的值”；\n1.2）若当前字符与下个字符是'()'，则值为“1+后面字符串的值”。\n2）至字符数组最后一个字符结束。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/856_Score_Of_Parentheses/test.go\nfunc scoreOfParentheses(s string) int { chars := []rune(s) r := 0 for i := 0; i \u0026lt; len(chars)-1; i++ { c, next := chars[i], chars[i+1] if \u0026#39;(\u0026#39; == c \u0026amp;\u0026amp; \u0026#39;(\u0026#39; == next { sub := \u0026#34;\u0026#34; depth := 1 for i = i + 1; i \u0026lt; len(chars); i++ { if \u0026#39;(\u0026#39; == chars[i] { depth++ } else { depth-- } if 0 == depth { break } sub += string(chars[i]) } if len(chars)-1 == i { return 2 * scoreOfParentheses(sub) } return 2*scoreOfParentheses(sub) + scoreOfParentheses(s[i+1:]) } else if \u0026#39;(\u0026#39; == c \u0026amp;\u0026amp; \u0026#39;)\u0026#39; == next { sub := \u0026#34;\u0026#34; depth := 1 for i = i + 2; i \u0026lt; len(chars); i++ { if \u0026#39;(\u0026#39; == chars[i] { depth++ } else { depth-- } if 0 == depth { break } sub += string(chars[i]) } return 1 + scoreOfParentheses(sub) } } return r } ","permalink":"https://olzhy.github.io/posts/leetcode-score-of-parentheses.html","tags":["Golang","算法"],"title":"LeetCode 856 括号的分值"},{"categories":["计算机"],"contents":"1 题目描述\n实现一个可以对简单字符串表达式进行计算的计算器。该字符串表达式由+，-，(，)及非负整数组成。\n例子1：\n输入：\u0026ldquo;1 + 1\u0026rdquo;\n输出：2\n例子2：\n输入：\u0026quot; 2-1 + 2 \u0026quot;\n输出：3\n例子3：\n输入：\u0026quot;(1+(4+5+2)-3)+(6+8)\u0026quot;\n输出：23\n注：\n1）假定给定的表达式总是有效的；\n2）勿使用内置函数直接求得结果。\n题目出处：\nhttps://leetcode.com/problems/basic-calculator/\n2 解决思路\n1）去除字符串表达式中的所有空格；\n2）遍历字符数组；\n2.1）若当前字符为[0-9]的数值，一直向后找，直至取出整个整数，根据前一个字符是+或-，将当前结果加上或减去该整数；\n2.2）若当前字符是'('，一直向后找，找到跟该左括号匹配的')'，将之间的表达式同样采用2步骤计算结果。根据前一个字符是'+\u0026lsquo;或\u0026rsquo;-'，将当前结果加上或减去该子表达式结果。\n3）至字符数组最后一个字符结束。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/224_Basic_Calculator/test.go\nfunc calc(chars []rune, start, end int) int { r := 0 pre := \u0026#39;x\u0026#39; for i := start; i \u0026lt; end; i++ { c := chars[i] if i \u0026gt; start { pre = chars[i-1] } switch c { case \u0026#39;(\u0026#39;: depth := 1 start = i + 1 for i = start; i \u0026lt; end \u0026amp;\u0026amp; 0 != depth; i++ { switch chars[i] { case \u0026#39;(\u0026#39;: depth++ case \u0026#39;)\u0026#39;: depth-- } } switch pre { case \u0026#39;x\u0026#39;, \u0026#39;+\u0026#39;: r += calc(chars, start, i) case \u0026#39;-\u0026#39;: r -= calc(chars, start, i) } case \u0026#39;+\u0026#39;, \u0026#39;-\u0026#39;: default: v := 0 for ; i \u0026lt; end \u0026amp;\u0026amp; chars[i] \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; chars[i] \u0026lt;= \u0026#39;9\u0026#39;; i++ { v = v*10 + int(chars[i]-\u0026#39;0\u0026#39;) } switch pre { case \u0026#39;x\u0026#39;, \u0026#39;+\u0026#39;: r += v case \u0026#39;-\u0026#39;: r -= v } } } return r } func calculate(s string) int { s = strings.Map(func(r rune) rune { if unicode.IsSpace(r) { return -1 } return r }, s) return calc([]rune(s), 0, len(s)) } ","permalink":"https://olzhy.github.io/posts/leetcode-basic-calculator.html","tags":["Golang","算法"],"title":"LeetCode 224 简单计算器"},{"categories":["计算机"],"contents":"1 题目描述\n给定一个非负整数num，对0 ≤ i ≤ num区间内每个整数，计算其对应的二进制数中1的个数，结果用数组返回。\n例子1：\n输入：2\n输出：[0, 1, 1]\n例子2：\n输入：5\n输出：[0,1,1,2,1,2]\n题目出处：\nhttps://leetcode.com/problems/counting-bits/\n2 解决思路\n2.1 常规算法\nfunc decimal2Binary(n int) string { b := \u0026#34;\u0026#34; for { // remain  r := 0 n, r = n\u0026gt;\u0026gt;1, n%2 b = strconv.Itoa(r) + b if 0 == n { return b } } } func countOne(s string) int { c := 0 for _, i := range []rune(s) { if \u0026#39;1\u0026#39; == i { c++ } } return c } func countBits(num int) []int { s := make([]int, num+1) for i := 0; i \u0026lt;= num; i++ { s[i] = countOne(decimal2Binary(i)) } return s } 2.2 改进思路\n避免对递增数组中的每个数值作计算，将4位看做一个单元，单元内0-15的二进制数中1的个数是确定的。这样采用16进制去计算，给定数值，每除以16所得的余数就是落在该单元内的数值，直至被除数为0，将每个单元中1的个数累加既可。\n3 golang实现代码\nhttps://github.com/olzhy/leetcode/blob/master/338_Couting_Bits/test.go\nfunc countBinaryOneInHexUnit(n int) int { countOne := 0 switch n { case 0: countOne = 0 case 1, 2, 4, 8: countOne = 1 case 3, 5, 6, 9, 10, 12: countOne = 2 case 7, 11, 13, 14: countOne = 3 case 15: countOne = 4 } return countOne } func countBinaryOne(n int) int { // remain  r := 0 countOne := 0 for n \u0026gt; 0 { n, r = n\u0026gt;\u0026gt;4, n%16 countOne += countBinaryOneInHexUnit(r) } return countOne } func countBits(num int) []int { s := make([]int, num+1) for i := 0; i \u0026lt;= num; i++ { s[i] = countBinaryOne(i) } return s } 4 基准测试\n4.1 测试代码\npackage main import ( \u0026#34;testing\u0026#34; ) func BenchmarkCountBits(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { countBits(100000000) } } 4.2 测试结果\n$ go test -test.bench=\u0026quot;.*\u0026quot; goos: darwin goarch: amd64 pkg: github.com/olzhy/test BenchmarkCountBits-4 1 4618146566 ns/op PASS ok github.com/olzhy/test 4.670s ","permalink":"https://olzhy.github.io/posts/leetcode-counting-binary-bits.html","tags":["Golang","算法"],"title":"LeetCode  338 计算二进制数中1的个数"},{"categories":["计算机"],"contents":"1 提供less(i, j int) bool函数\n1.1 golang代码\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) func main() { ps := []struct { name string age int }{ {\u0026#34;larry\u0026#34;, 19}, {\u0026#34;jackey\u0026#34;, 18}, {\u0026#34;lucy\u0026#34;, 20}, } // keeping the original order of equal elements  sort.SliceStable(ps, func(i, j int) bool { return ps[i].age \u0026lt; ps[j].age }) fmt.Println(ps) } 1.2 结果输出\n[{jackey 18} {larry 19} {lucy 20}] 2 实现sort.Interface接口\n2.1 golang代码\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) type person struct { name string age int } type persons []person func (ps persons) Len() int { return len(ps) } func (ps persons) Less(i, j int) bool { return ps[i].age \u0026lt; ps[j].age } func (ps persons) Swap(i, j int) { ps[i], ps[j] = ps[j], ps[i] } func main() { ps := persons{ {\u0026#34;larry\u0026#34;, 19}, {\u0026#34;jackey\u0026#34;, 18}, {\u0026#34;lucy\u0026#34;, 20}, } sort.Sort(ps) fmt.Println(ps) } 2.2 结果输出\n[{jackey 18} {larry 19} {lucy 20}] ","permalink":"https://olzhy.github.io/posts/golang-struct-slice-sorting.html","tags":["Golang"],"title":"Golang struct slice排序"},{"categories":["计算机"],"contents":"1 方法调用采用值拷贝\n1.1 array\ngolang中以array作为参数的方法调用，方法接收的是整个array的值拷贝，所以方法中对array的item重新赋值不起作用。\n如以下代码所示，输出为[1, 2, 3]。\npackage main import \u0026#34;fmt\u0026#34; func modify(a [3]int) { a[0] = 4 } func main() { a := [3]int{1, 2, 3} modify(a) fmt.Println(a) } 1.2 struct\n如下代码传参为struct值拷贝，modify方法或modify函数对person的name属性重新赋值不起作用。\npackage main import \u0026#34;fmt\u0026#34; type person struct { name string } func (p person) modify() { p.name = \u0026#34;jacky\u0026#34; } func modify(p person) { p.name = \u0026#34;jacky\u0026#34; } func main() { p := person{\u0026#34;larry\u0026#34;} p.modify() // modify(p)  fmt.Println(p) } 2 方法调用采用引用拷贝\n2.1 slice\nslice作为底层的数组引用，方法调用采用的是引用的拷贝。\n所以，如下第一段代码，函数的引用拷贝与原始引用指向同一块数组，对slice的item重新赋值是生效的，输出为[4, 2, 3]。\npackage main import \u0026#34;fmt\u0026#34; func modify(s []int) { s[0] = 4 } func main() { s := []int{1, 2, 3} modify(s) fmt.Println(s) } 但第二段代码，输出结果未变化，仍为[1, 2, 3]。是因为对引用的拷贝重新赋值，并不会更改原始引用。\npackage main import \u0026#34;fmt\u0026#34; func modify(s []int) { s = append(s, 4) } func main() { s := []int{1, 2, 3} modify(s) fmt.Println(s) } 所以对slice进行append操作，需要将其作为返回值返回，如以下代码所示，输出为[1 2 3 4]。\npackage main import \u0026#34;fmt\u0026#34; func modify(s []int) []int { s = append(s, 4) return s } func main() { s := []int{1, 2, 3} s = modify(s) fmt.Println(s) } 2.2 struct pointer\n若想改变struct的属性值，传参采用struct pointer。\npackage main import \u0026#34;fmt\u0026#34; type person struct { name string } func (p *person) modify() { p.name = \u0026#34;jacky\u0026#34; } func modify(p *person) { p.name = \u0026#34;jacky\u0026#34; } func main() { p := \u0026amp;person{\u0026#34;larry\u0026#34;} p.modify() // modify(p)  fmt.Println(p) } ","permalink":"https://olzhy.github.io/posts/golang-method-calling-value-copy-or-reference-copy.html","tags":["Golang"],"title":"Golang方法调用值拷贝与引用拷贝"},{"categories":["计算机"],"contents":"1 vscode 安装\n从[1]下载安装 vscode，然后安装[2]插件。\n2 插件安装\nvscode 默认会提示安装所需的插件，安装失败的插件需要设置翻墙代理，手动 go get。\n代理设置\nexport https_proxy=http://ip:port 手动 go get\ngo get github.com/acroca/go-symbols go get github.com/ramya-rao-a/go-outline ... go get golang.org/x/text/unicode/norm go get github.com/golang/tools/refactor/satisfy go get github.com/derekparker/delve/cmd/dlv go install\ncd $GOPATH/src go install all 至此，插件安装完成。\n3 常用快捷键（mac）\n查询快捷键 CMD + K CMD + S 文件内查询 CMD + F 下一处/上一处 CMD + G / CMD + SHIFT + G 文件内查询替换 CMD + ALT + F 符号重命名 FN + F2 格式化 SHIFT + ALT + F 到文件 CMD + P 到某行 CTL + G 新建文件 CMD + N 选中当前行 CMD + I 移动选中行 ALT + ↑ / ALT + ↓ 添加/移除注释 CMD + / 快速修复 CMD + . 调试 FN + F5 打断点 FN + F9 跳过 FN + F10  参考资料\n[1] https://code.visualstudio.com\n[2] https://code.visualstudio.com/docs/languages/go\n[3] https://github.com/derekparker/delve\n[4] https://github.com/Microsoft/vscode-go/wiki/Debugging-Go-code-using-VS-Code\n[5] https://code.visualstudio.com/shortcuts/keyboard-shortcuts-macos.pdf\n ","permalink":"https://olzhy.github.io/posts/vs-code-golang-environment-configuration.html","tags":["Golang"],"title":"VS Code Golang环境搭建"},{"categories":["计算机"],"contents":"1 题目\n找出字符串中的所有整数\n2 golang实现代码\npackage main import \u0026#34;fmt\u0026#34; // find a integer from index i in a byte array // and return the next index func findInt(b []byte, i int) (int, int) { // find first digit index  for ; i \u0026lt; len(b); i++ { if b[i] \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; b[i] \u0026lt;= \u0026#39;9\u0026#39; { break } } v := 0 for ; i \u0026lt; len(b) \u0026amp;\u0026amp; b[i] \u0026gt;= \u0026#39;0\u0026#39; \u0026amp;\u0026amp; b[i] \u0026lt;= \u0026#39;9\u0026#39;; i++ { v = v*10 + int(b[i]-\u0026#39;0\u0026#39;) } return v, i } func main() { s := \u0026#34;cat3234kitty2342monkey897elephant43512panda24\u0026#34; b := []byte(s) v := 0 for i := 0; i \u0026lt; len(b); { v, i = findInt(b, i) fmt.Println(v) } } 3 结果输出\n3234 2342 897 43512 24 ","permalink":"https://olzhy.github.io/posts/find-integers-in-a-string.html","tags":["Golang","算法"],"title":"找出字符串中的所有整数"},{"categories":["随笔"],"contents":"珍珍我错了\n","permalink":"https://olzhy.github.io/posts/jane-i-am-sorry.html","tags":["随笔"],"title":"珍珍我错了"},{"categories":["计算机"],"contents":"1 场景介绍\nweb应用中，常有业务状态需要实时更新的场景。如一个较长的后台任务，从浏览器用户触发执行到执行完成可能需几十秒的时间，这时前端需隔几秒请求一次后台，查询任务执行进度。此种方式是长轮询的方式，是存在一定弊端的，增加了后台服务的负载，若并发操作量太大，后台压力会成倍激增。业界常采用http1.1的websocket扩展协议与浏览器建立长连接来实现实时业务状态更新。\n2 实现方案\n本文采用golang实现一个长连接服务，对外提供两个接口，一个是基于http的rest消息发送接口，一个是基于websocket的cient接入接口，如下图所示。\n为使前端的接入更简单，从建立连接到用户关闭浏览器，中间前端无须发送消息来告知服务器client是否下线。我们将检测放在后台，后台采用定时心跳方式保持对client的监听，若心跳失败，则将该client剔除。如下图所示。\n3 golang实现代码\ncomet服务内有两个模块，http server负责接收消息，comet server负责维护websocket client，每个client启用一个go routine对客户端保持心跳检测。\n3.1 核心模块\npackage comet import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;golang.org/x/net/websocket\u0026#34; ) type HttpServer struct { wsServer *WsServer } type WsServer struct { Clients map[string][]*Client AddCli chan *Client DelCli chan *Client Message chan *Message } type Client struct { UserId string Timestamp int64 conn *websocket.Conn wsServer *WsServer } type Message struct { UserId string `json:\u0026#34;user_id\u0026#34;` Message string `json:\u0026#34;message\u0026#34;` } func NewWsServer() *WsServer { return \u0026amp;WsServer{ make(map[string][]*Client), make(chan *Client), make(chan *Client), make(chan *Message, 1000), } } func NewHttpServer(wsServer *WsServer) *HttpServer { return \u0026amp;HttpServer{wsServer} } func (httpServer *HttpServer) SendMessage(userId, message string) { log.Printf(\u0026#34;message reveived, user_id: %s, message: %s\u0026#34;, userId, message) httpServer.wsServer.Message \u0026lt;- \u0026amp;Message{userId, message} } func (wsServer *WsServer) SendMessage(userId, message string) { clients := wsServer.Clients[userId] if len(clients) \u0026gt; 0 { for _, c := range clients { c.conn.Write([]byte(message)) } log.Printf(\u0026#34;message success sent to client, user_id: %s\u0026#34;, userId) } else { log.Printf(\u0026#34;client not found, user_id: %s\u0026#34;, userId) } } func (wsServer *WsServer) addClient(c *Client) { clients := wsServer.Clients[c.UserId] wsServer.Clients[c.UserId] = append(clients, c) log.Printf(\u0026#34;a client added, userId: %s, timestamp: %d\u0026#34;, c.UserId, c.Timestamp) } func (wsServer *WsServer) delClient(c *Client) { clients := wsServer.Clients[c.UserId] if len(clients) \u0026gt; 0 { for i, client := range clients { if client.Timestamp == c.Timestamp { wsServer.Clients[c.UserId] = append(clients[:i], clients[i+1:]...) break } } } if 0 == len(clients) { delete(wsServer.Clients, c.UserId) } log.Printf(\u0026#34;a client deleted, user_id: %s, timestamp: %d\u0026#34;, c.UserId, c.Timestamp) } func (wsServer *WsServer) Start() { for { select { case msg := \u0026lt;-wsServer.Message: wsServer.SendMessage(msg.UserId, msg.Message) case c := \u0026lt;-wsServer.AddCli: wsServer.addClient(c) case c := \u0026lt;-wsServer.DelCli: wsServer.delClient(c) } } } func (c *Client) heartbeat() error { millis := time.Now().UnixNano() / 1000000 heartbeat := struct { Heartbeat int64 `json:\u0026#34;heartbeat\u0026#34;` }{millis} bytes, _ := json.Marshal(heartbeat) _, err := c.conn.Write(bytes) return err } func (c *Client) Listen() { for { err := c.heartbeat() if nil != err { log.Printf(\u0026#34;client heartbeat error, user_id: %v, timestamp: %d, err: %s\u0026#34;, c.UserId, c.Timestamp, err) c.wsServer.DelCli \u0026lt;- c return } time.Sleep(time.Second * 5) } } 3.2 完整代码\nhttps://github.com/olzhy/comet\n4 一致性哈希包装\n考虑到单服务的同时在线人数支持是有限的，所以在其上层用一致性哈希算法包装。这样同一user_id建立连接会打到同一台后台服务器，给此user_id发送消息也会打到同样的服务器。这样后台部署多个comet服务形成一个集群即可支撑高并发消息推送场景。如下图所示，最外层nginx挂接公网域名，对外提供基于wss的消息接收接口及基于http的消息发送接口。中间采用haproxy对user_id参数作一致性哈希转发，对同一user_id的操作会打到同一台comet server。底层扩展为多台comet server即可构建一个高并发的消息推送服务。\n  2018.09.02\n大连\n","permalink":"https://olzhy.github.io/posts/golang-websocket-combine-consistent-hashing.html","tags":["Golang","算法"],"title":"Golang websocket结合一致性哈希算法构建高并发推送服务"},{"categories":["练字"],"contents":"酒色端能誤國邦，由來美色陷忠良。紂因妲己宗祧失，吳為西施社稷亡。自愛青春行處樂，豈知紅粉笑中槍。武松已殺貪淫婦，莫向東方怨彼蒼。——《水滸傳》第十四回\n大连\n戊戌年二月初一\n2018年3月18日\n","permalink":"https://olzhy.github.io/posts/handwriting-abstinence.html","tags":["练字"],"title":"練字：酒色端能誤國邦"},{"categories":["观影"],"contents":"我叫瑞德，一个在鲨堡监狱已服刑20年的囚徒，我在这里让狱友有求必应，在狱中略显奢侈的物品，如香烟、酒水等，我都能想办法弄到。\n安迪初进来时，很另类，不善言谈，独来独往，活像个局外人。\n他第一次找我，想通过我的渠道获得一个小锤子。\n安迪，初进大牢，看过同期狱友“猪公”被老鸟羞辱，痛哭难熬，被打死在狱中，连个名字都没有留下。\n后被三姐妹欺负，也得忍辱活着。\n汤米，这个光顾过几乎这里所有监狱的常客，述说了亲耳听闻过厄摩讲述射杀安迪妻子的场景。安迪激动的找典狱长诺顿伸冤，诺顿为了利用安迪长期满足自己敛财的欲望，竟将汤米杀害。\n为了满足私欲，典狱长可以杀人封口，可以牺牲一个清白人的一生，他比看管的犯人罪恶深重多了。\n这一刻，安迪对这里彻底心死了。\n一张海报，一枚小锤，铁杵磨针20载，终于逾越桎梏，打通那通向自由之门。\n我本是吃了一惊，但稍作镇定，也就不惊奇了。这才是安迪，这就是安迪，这个陪我度过20载的狱中老友。\n“修缮屋顶工事，讨好警备队长，请我们喝啤酒。”\n“关独囚，却没有煎熬难耐，只因他内心有莫扎特的音乐陪伴。”\n“不畏后果，自由洒脱，擅自广播《费加罗婚礼》，如躺在沙滩沐浴阳光，让全体狱中人心灵重获刹那的自由。”\n“这世上有的鸟是关不住的，只因它的羽翼太光辉了”，安迪的内心一直是自由的，他之所以越狱是因为他没有犯法，不该无辜在这里囚禁一生。他偿还的仅是来自自我内心的愧疚，而这些，早已偿清。\n在我已服刑40年的关头上，又一次假释面谈。\n“你改过自新了吗？”\n那时的犯错少年，早已变成现在的白发老人，物是人非，世事变换，我早已不是原来的那个我，还谈什么改过自新？\n这一次没有被驳回，我被释放。\n一个在狱中度过半生的人，重获自由回归社会时，变得无法适应这个世界。我每日惶恐不安，寸步难行。我开始怀念我的狱中生活，我熟悉那里的“体制”，我可以“呼风唤雨，无所不能”。\n自由不过是一种心境罢了，当你苦苦追寻它时，望洋兴叹；等待的太久，当你早快忘记它时，突然来到身边，这自由却又使人痛苦难耐了。走出有形的牢，又坠入无形的牢。安迪早已看穿这点，人需在“忙着生，忙着死”中二选一。得到救赎的是我，是安迪救赎了我。我不能重蹈老布的覆辙，我需要重整旗鼓，打破这“心中的牢狱”，兑现给安迪的承诺，去寻找那象征“世外桃源”的“芝华塔尼欧”，在那里凤凰涅槃，重塑人生。\n从此，得一片净水，有一位老友，轻舟微漾，盪酒言欢。\n  大连\n2018年3月11日\n","permalink":"https://olzhy.github.io/posts/watching-the-shawshank-redemption.html","tags":["观影"],"title":"观《肖申克的救赎》"},{"categories":["随笔"],"contents":"一\n那天在回家的路上，看着那来来去去的人们，感叹茫茫人海，人与人相识的不易。心中思绪翻滚，就愈加珍惜人与人那凑巧的藤葛，感叹这世事奇妙。\n二\n闭上眼睛本是暗黑的夜，但脑海中的一个人却让这黑夜里洒满繁星。那柔和美丽的光像天使散花一样划向心底的每一个角落。 这夜，哪怕再冷，再暗，也无比温暖， 无所畏惧。\n三\n我会记得那个夏天，一个美好的女子在那里等车，像一朵静静开放的花朵，干净自由。她眸子里流淌的是清澈的山泉，眼睛眨巴眨巴，有种特有的神气，像一泓春水，暖动人心。从她身边走过都怕惊扰到她，让我相信，不管生命中有多少不如意，那些自然的美丽会提醒你永远不要失去对接下来美好事物的憧憬。\n四\n又是一年春节，一个美好的女子，巧笑倩兮。那纤细洁白的手指，贴一个福字，就寓示着最美好的一年。\n五\n我想给她拍很多很多的照片。当彼此不再年轻时，翻看那长长的相册，念起某时那惊鸿一瞥，都会开心的微笑。感叹这时光流长，经历过的事有许多种，但美好的记忆里，有这么一件刻骨铭心。 一个午后，日光洒在一个正当最好年纪的女子身上，璀璨夺目。她的回眸一笑永远定格在那里，不曾老去。这岁月瞬间有了温度。\n戊戌年正月初五于东胜 2018.02.20\n","permalink":"https://olzhy.github.io/posts/beauty.html","tags":["随笔"],"title":"美"},{"categories":["计算机"],"contents":"今日新购了服务器，为方便博客搬家，特编写了 ansible playbook 部署脚本。\n本站采用 Nginx+PHP+Maridb+Wordpress 搭建。需要备份的数据有 nginx 配置文件（nginx.conf），nginx html（html.zip），数据库脚本（wordpress.sql）。部署的目标机操作系统为 CentOS7.2。\n1）该 playbook 目录结构\nplaybook |--- playbook.yml |--- templates | \\--- nginx.conf |--- files | |--- html.zip | \\--- wordpress.sql \\--- HOSTS 2）tasks 细分\nplaybook |--- pre_tasks | |--- 1) make temp dir | |--- 2) install nginx mariadb php-fpm | \\--- 3) install ansible mysql_user module dependencies |--- tasks | |--- 1) unarchive nginx html | |--- 2) import data | \\--- 3) restart nginx php-fpm mariadb \\--- post_tasks |--- 1) remove old filewall |--- 2) install iptables and config access port \\--- 3) clean temp dir 3）playbook.yml 脚本\n--- - hosts: wordpress remote_user: x vars: mysql_root_passwd: x mysql_wordpress_passwd: x pre_tasks: # 1) make temp dir - name: make temp workspace file: path=/tmp/wordpress state=directory # 2) install nginx mariadb php-fpm - name: install nginx yum: name=nginx state=latest - name: install mariadb yum: name={{item}} state=latest with_items: - mariadb - mariadb-server - name: install php-fpm yum: name={{item}} state=latest with_items: - php - php-fpm - php-mysql - php-gd - libjpeg* - php-imap - php-ldap - php-pear - php-xml - php-xmlrpc - php-mbstring - php-mcrypt - php-bcmath - php-mhash - libmcrypt - libmcrypt-devel - php-pdo # 3) install ansible mysql_user module dependencies - name: get pip get_url: url=https://bootstrap.pypa.io/get-pip.py dest=/tmp/wordpress - name: install pip shell: chdir=/tmp/wordpress python get-pip.py - name: install dependencies yum: name={{item}} state=latest with_items: - gcc - mysql-devel - python-devel - name: pip install MySQL-python shell: pip install MySQL-python tasks: # 1) unarchive nginx html - name: cp html.zip copy: src=html.zip dest=/tmp/wordpress - name: remove old nginx html file: path=/usr/share/nginx/html state=absent - name: unarchive html.zip unarchive: src=/tmp/wordpress/html.zip dest=/usr/share/nginx remote_src=yes - name: chown html file: path=/usr/share/nginx/html mode=0755 owner=nginx group=nginx recurse=yes - name: cp nginx.conf template: src=nginx.conf dest=/etc/nginx/nginx.conf - name: nginx restart service: name=nginx state=restarted # 2) import data - name: mariadb start service: name=mariadb state=started - name: cp wordpress.sql copy: src=wordpress.sql dest=/tmp/wordpress - name: create db wordpress mysql_db: name=wordpress state=present encoding=utf8 collation=utf8_general_ci - name: modify root password mysql_user: name=root password={{mysql_root_passwd}} check_implicit_admin=yes state=present - name: add mysql user wordpress mysql_user: name=wordpress password={{mysql_wordpress_passwd}} host=localhost priv='wordpress.*:ALL' login_user=root login_password={{mysql_root_passwd}} state=present - name: import data mysql_db: name=wordpress state=import login_user=root login_password={{mysql_root_passwd}} target=/tmp/wordpress/wordpress.sql # 3) restart nginx php-fpm mariadb - name: restart mariadb php-fpm nginx service: name={{item}} state=restarted with_items: - mariadb - php-fpm - nginx post_tasks: # 1) remove old filewall - name: remove old filewall shell: systemctl stop firewalld \u0026amp;\u0026amp; systemctl mask firewalld # 2) install iptables and config access port - name: install iptables yum: name={{item}} state=latest with_items: - iptables-services - iptables-devel - name: systemctl enable shell: systemctl enable {{item}} with_items: - nginx - mariadb - php-fpm - iptables - name: config iptables shell: iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT \u0026amp;\u0026amp; iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 443 -j ACCEPT \u0026amp;\u0026amp; service iptables save - name: restart iptables service: name=iptables state=restarted # 3) clean temp dir - name: clean temp workspace file: path=/tmp/wordpress state=absent 4）执行 playbook\nansible-playbook -i HOSTS playbook.yml  参考资料\n[1] https://docs.ansible.com/ansible/latest/copy_module.html\n[2] https://docs.ansible.com/ansible/latest/file_module.html\n[3] https://docs.ansible.com/ansible/latest/playbooks_intro.html#playbook-language-example\n[4] https://docs.ansible.com/ansible/latest/mysql_db_module.html\n[5] https://docs.ansible.com/ansible/latest/mysql_user_module.html\n ","permalink":"https://olzhy.github.io/posts/wordpress-ansible-playbook-script.html","tags":["Ansible"],"title":"WordPress站点Ansible Playbook自动化部署脚本"},{"categories":["观影"],"contents":"今日一睹李安导演的《卧虎藏龙》，心中略有感慨，记录如下。\n一个“藏”字，道出了传统中国男人很普遍的一面。男女情深，羞于言；父子情深，讷于口；家国情深，藏于心。\n李慕白，得道之时嘱托师妹俞秀莲将挚爱的青冥宝剑赠予京城的贝勒爷。浪迹江湖逾半生，早已看淡尔虞我诈，打打杀杀，也厌倦了刀光剑影，你死我活。本该云淡风轻，却没有得道的喜悦，反被一种寂灭的悲哀环绕。\n是什么让他难过呢，即是心中隐藏多年的情。\n我愿放弃做一个盖世英雄，一个让天下人敬仰的侠客。刀剑易挡也易破，唯是人心最难识。罢了，罢了，这世间的人情纠葛，爱恨情仇，岂是我能斩得断理得清。师妹啊，去了却这烦扰吧，此剑易主，此身自此可以回归本心，不受束缚，只愿同你共余生。\n玉娇龙，这个“不安分”的姑娘，是传统文化中不受世人待见的角色。自幼得碧眼狐狸私授异类绝学，外加自学武当密籍，练得一身好武功，落寞的是被包办婚姻掣肘，去不了向往的侠客生活。\n玉娇龙与两个男人交过手，也与这两个男人生过情。\n一者，罗小虎，本是倔强女子与劫路盗贼的拼杀，却成全一段爱情。缘何？于剑者而言，舞剑即是谈情，她的嗔怒，别有风韵；他的狂野，亦是风流。你我都是肆意少年，那心底的欲望与情感，早已撒向荒漠的整个星空化作粒粒流沙。\n二者，李慕白，与蒙面黑衣人玉娇龙的过招，分拨的是剑术上的正统与邪道，矫正的却是一个剑者的心性与修为。\n玉娇龙在改变，于剑于心本身，已有改过之意，欲得正统之道；于情而言，剑逢对手，心生仰慕。\n当李慕白安排玉娇龙武当山去，以期他日与罗小虎相会。玉娇龙突然生气的说到：“武当山是酒馆娼寮，我不稀罕”。道出了，那一刻，住在她心里的人不再是罗小虎，却已是李慕白。\n李慕白又何尝不欣赏玉娇龙呢，相较于李慕白对情感的深藏，玉娇龙面对情感却毫不掩饰。她自由洒脱，对李慕白产生强烈的吸引，自此，李慕白心中固守的道开始崩裂了。\n玉娇龙从与包办婚姻的夫婿洞房花烛夜里逃出来，去的不是武当山，却是寻俞秀莲这个姐姐时，俞秀莲何尝未察觉她心里寻的是李慕白。扶柳亦笑姐妹情，棍枪刀锤战青冥。\n人肉之身，为情欲所困，何得人剑合一。剑光冰冷，人有温度，于剑而言终得道，于情而言终是悔。剑身分离了，面对的是两种爱的交织。固守传统，于纷乱中寻得清净；逾越桎梏，于长束中浅尝自由。手牵俞秀莲，于虚无中寻得真实。思慕玉娇龙，于真实里品味虚无。握剑离欲，弃剑欲来。了却这痛苦罢，为玉娇龙挡暗针，成全的亦是自己。仅留一息，不寻道间仙境，只求回归本真。\n玉娇龙自始至终真诚对待本心，用罗小虎的寓言纵身一跃，跳入深崖，以让罗小虎的心好得安放，更以壮美的方式坚定的向伊人飞去了。\n若不以人间生死作度量，这样的结局，他们均已回归了真我。\n  大连\n2018.01.21\n","permalink":"https://olzhy.github.io/posts/see-crouching-tiger-hidden-dragon.html","tags":["观影"],"title":"观《卧虎藏龙》"},{"categories":["计算机"],"contents":"k-d tree即k-dimensional tree，常用来作空间划分及近邻搜索，是二叉空间划分树的一个特例。通常，对于维度为$k$，数据点数为$N$的数据集，k-d tree适用于$N\\gg2^k$的情形。\n1）k-d tree算法原理\nk-d tree是每个节点均为k维数值点的二叉树，其上的每个节点代表一个超平面，该超平面垂直于当前划分维度的坐标轴，并在该维度上将空间划分为两部分，一部分在其左子树，另一部分在其右子树。即若当前节点的划分维度为d，其左子树上所有点在d维的坐标值均小于当前值，右子树上所有点在d维的坐标值均大于等于当前值，本定义对其任意子节点均成立。\n1.1）树的构建\n一个平衡的k-d tree，其所有叶子节点到根节点的距离近似相等。但一个平衡的k-d tree对最近邻搜索、空间搜索等应用场景并非是最优的。\n常规的k-d tree的构建过程为：循环依序取数据点的各维度来作为切分维度，取数据点在该维度的中值作为切分超平面，将中值左侧的数据点挂在其左子树，将中值右侧的数据点挂在其右子树。递归处理其子树，直至所有数据点挂载完毕。\na）切分维度选择优化\n构建开始前，对比数据点在各维度的分布情况，数据点在某一维度坐标值的方差越大分布越分散，方差越小分布越集中。从方差大的维度开始切分可以取得很好的切分效果及平衡性。\nb）中值选择优化\n第一种，算法开始前，对原始数据点在所有维度进行一次排序，存储下来，然后在后续的中值选择中，无须每次都对其子集进行排序，提升了性能。\n第二种，从原始数据点中随机选择固定数目的点，然后对其进行排序，每次从这些样本点中取中值，来作为分割超平面。该方式在实践中被证明可以取得很好性能及很好的平衡性。\n本文采用常规的构建方式，以二维平面点$(x,y)$的集合(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)为例结合下图来说明k-d tree的构建过程。\na） 构建根节点时，此时的切分维度为$x$，如上点集合在$x$维从小到大排序为(2,3)，(4,7)，(5,4)，(7,2)，(8,1)，(9,6)；其中值为(7,2)。（注：2,4,5,7,8,9在数学中的中值为(5 + 7)/2=6，但因该算法的中值需在点集合之内，所以本文中值计算用的是len(points)//2=3, points[3]=(7,2)）\nb） (2,3)，(4,7)，(5,4)挂在(7,2)节点的左子树，(8,1)，(9,6)挂在(7,2)节点的右子树。\nc） 构建(7,2)节点的左子树时，点集合(2,3)，(4,7)，(5,4)此时的切分维度为$y$，中值为(5,4)作为分割平面，(2,3)挂在其左子树，(4,7)挂在其右子树。\nd） 构建(7,2)节点的右子树时，点集合(8,1)，(9,6)此时的切分维度也为$y$，中值为(9,6)作为分割平面，(8,1)挂在其左子树。至此k-d tree构建完成。\n上述的构建过程结合下图可以看出，构建一个k-d tree即是将一个二维平面逐步划分的过程。\n我们还可以结合下图（该图引自维基百科），从三维空间来看一下k-d tree的构建及空间划分过程。\n首先，边框为红色的竖直平面将整个空间划分为两部分，此两部分又分别被边框为绿色的水平平面划分为上下两部分。最后此4个子空间又分别被边框为蓝色的竖直平面分割为两部分，变为8个子空间，此8个子空间即为叶子节点。\n如下为k-d tree的构建代码：\ndef kd_tree(points, depth): if 0 == len(points): return None cutting_dim = depth % len(points[0]) medium_index = len(points) // 2 points.sort(key=itemgetter(cutting_dim)) node = Node(points[medium_index]) node.left = kd_tree(points[:medium_index], depth + 1) node.right = kd_tree(points[medium_index + 1:], depth + 1) return node 1.2）寻找d维最小坐标值点\na）若当前节点的切分维度是d\n因其右子树节点均大于等于当前节点在d维的坐标值，所以可以忽略其右子树，仅在其左子树进行搜索。若无左子树，当前节点即是最小坐标值节点。\nb）若当前节点的切分维度不是d\n需在其左子树与右子树分别进行递归搜索。\n如下为寻找d维最小坐标值点代码：\ndef findmin(n, depth, cutting_dim, min): if min is None: min = n.location if n is None: return min current_cutting_dim = depth % len(min) if n.location[cutting_dim] \u0026lt; min[cutting_dim]: min = n.location if cutting_dim == current_cutting_dim: return findmin(n.left, depth + 1, cutting_dim, min) else: leftmin = findmin(n.left, depth + 1, cutting_dim, min) rightmin = findmin(n.right, depth + 1, cutting_dim, min) if leftmin[cutting_dim] \u0026gt; rightmin[cutting_dim]: return rightmin else: return leftmin 1.3）新增节点\n从根节点出发，若待插入节点在当前节点切分维度的坐标值小于当前节点在该维度的坐标值时，在其左子树插入；若大于等于当前节点在该维度的坐标值时，在其右子树插入。递归遍历，直至叶子节点。\n如下为新增节点代码：\ndef insert(n, point, depth): if n is None: return Node(point) cutting_dim = depth % len(point) if point[cutting_dim] \u0026lt; n.location[cutting_dim]: if n.left is None: n.left = Node(point) else: insert(n.left, point, depth + 1) else: if n.right is None: n.right = Node(point) else: insert(n.right, point, depth + 1) 多次新增节点可能引起树的不平衡。不平衡性超过某一阈值时，需进行再平衡。\n1.4）删除节点\n最简单的方法是将待删节点的所有子节点组成一个新的集合，然后对其进行重新构建。将构建好的子树挂载到被删节点即可。此方法性能不佳，下面考虑优化后的算法。\n假设待删节点T的切分维度为x，下面根据待删节点的几类不同情形进行考虑。\na）无子树\n本身为叶子节点，直接删除。\nb）有右子树\n在T.right寻找x切分维度最小的节点p，然后替换被删节点T；递归处理删除节点p。\nc）无右子树有左子树\n在T.left寻找x切分维度最小的节点p，即p=findmin(T.left, cutting-dim=x)，然后用节点p替换被删节点T；将原T.left作为p.right；递归处理删除节点p。\n（之所以未采用findmax(T.left, cutting-dim=x)节点来替换被删节点，是由于原被删节点的左子树节点存在x维度最大值相等的情形，这样就破坏了左子树在x分割维度的坐标需小于其根节点的定义）\n如下为删除节点代码：\ndef delete(n, point, depth): cutting_dim = depth % len(point) if n.location == point: if n.right is not None: n.location = findmin(n.right, depth + 1, cutting_dim, None) delete(n.right, n.location, depth + 1) elif n.left is not None: n.location = findmin(n.left, depth + 1) delete(n.left, n.location, depth + 1) n.right = n.left n.left = None else: n = None else: if point[cutting_dim] \u0026lt; n.location[cutting_dim]: delete(n.left, point, depth + 1) else: delete(n.right, point, depth + 1) 2）最近邻搜索\n给定点p，查询数据集中与其距离最近点的过程即为最近邻搜索。\n如在上文构建好的k-d tree上搜索(3,5)的最近邻时，本文结合如下左右两图对二维空间的最近邻搜索过程作分析。\na） 首先从根节点(7,2)出发，将当前最近邻设为(7,2)，对该k-d tree作深度优先遍历。以(3,5)为圆心，其到(7,2)的距离为半径画圆（多维空间为超球面），可以看出(8,1)右侧的区域与该圆不相交，所以(8,1)的右子树全部忽略。\nb） 接着走到(7,2)左子树根节点(5,4)，与原最近邻对比距离后，更新当前最近邻为(5,4)。以(3,5)为圆心，其到(5,4)的距离为半径画圆，发现(7,2)右侧的区域与该圆不相交，忽略该侧所有节点，这样(7,2)的整个右子树被标记为已忽略。\nc） 遍历完(5,4)的左右叶子节点，发现与当前最优距离相等，不更新最近邻。所以(3,5)的最近邻为(5,4)。\n如下为最近邻搜索代码：\n3）复杂度分析\n操作\t平均复杂度\t最坏复杂度 新增节点\tO(logn)\tO(n) 删除节点\tO(logn)\tO(n) 最近邻搜索\tO(logn)\tO(n) 4）scikit-learn使用\nscikit-learn是一个实用的机器学习类库，其有KDTree的实现。如下例子为直观展示，仅构建了一个二维空间的k-d tree，然后对其作k近邻搜索及指定半径的范围搜索。多维空间的检索，调用方式与此例相差无多。\n#!/usr/bin/python # -*- coding: UTF-8 -*- import numpy as np from matplotlib import pyplot as plt from matplotlib.patches import Circle from sklearn.neighbors import KDTree np.random.seed(0) points = np.random.random((100, 2)) tree = KDTree(points) point = points[0] # kNN dists, indices = tree.query([point], k=3) print(dists, indices) # query radius indices = tree.query_radius([point], r=0.2) print(indices) fig = plt.figure() ax = fig.add_subplot(111, aspect=\u0026#39;equal\u0026#39;) ax.add_patch(Circle(point, 0.2, color=\u0026#39;r\u0026#39;, fill=False)) X, Y = [p[0] for p in points], [p[1] for p in points] plt.scatter(X, Y) plt.scatter([point[0]], [point[1]], c=\u0026#39;r\u0026#39;) plt.show()  参考资料\n[1] https://en.wikipedia.org/wiki/K-d_tree\n[2] https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/kdtrees.pdf\n[3] https://www.cise.ufl.edu/class/cot5520fa09/CG_RangeKDtrees.pdf\n[4] http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote16.html\n[5] https://rosettacode.org/wiki/K-d_tree\n[6] http://prody.csb.pitt.edu/_modules/prody/kdtree/kdtree.html\n[7] http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html\n[8] http://www.dcc.fc.up.pt/~pribeiro/aulas/taa1516/rangesearch.pdf\n[9] https://courses.cs.washington.edu/courses/cse373/02au/lectures/lecture22l.pdf\n[10] http://www.cs.princeton.edu/courses/archive/spr13/cos226/lectures/99GeometricSearch.pdf\n[11] http://www.cs.umd.edu/class/spring2002/cmsc420-0401/pbasic.pdf\n[12] https://www.ri.cmu.edu/pub_files/pub1/moore_andrew_1991_1/moore_andrew_1991_1.pdf\n ","permalink":"https://olzhy.github.io/posts/kdtree-algorithm-and-implementation.html","tags":["Python","机器学习","算法"],"title":"k-d tree算法原理及实现"},{"categories":["读书"],"contents":"这是阎连科先生在写知识分子自己“丑陋”的一步作品。小说情节看似荒诞，但现实世界正在发生的事情远比小说荒诞多了。\n“我”没有选择青梅竹马，为了地位，为了事业，“我”娶了教授的女儿，赵茹萍。\n然而婚后生活并不幸福，“我”追求的学术在妻子看来一文不值。她深谙世道，能“睡觉”得到的东西，就不用专研学术。“我”更像是时代的遗儿，“我”和这个时代太不合了。面对妻子赵茹萍与校长李广智的通奸，更像是“我”做错了事，“我”活的没有一个男人样，竟给茹萍跪下来苦苦求饶“不要再有下次了好吗”，写出了知识分子的懦弱。\n“我”像一个异类一样被送进了精神病院（当“权威”说你有病，你就是有病，你没有辩驳的权利）。当院长说，你不是教授吗，你去给病人讲课，当他们交头接耳，不愿听，你就可以出院了（将一个人交给“病人”来裁判时，你的认真无法逃脱这个荒诞）。“我”想着方式让病人听不懂，他们却异常认真，这时，“我”的常识失效了，“我”会进入一个死循环，“我”要逃离这里，去哪里？回家。\n乡人知道“我”是“清燕大学”的教授，带着孩子争相找“我”摸头，送吃送喝，为的是孩子能考中。然而未中时，竟似盗贼样争抢我家里的东西。他们正巧撞上我后露出的一丝“羞愧”已将乡人“淳朴”的外衣撕的仅剩一件内裤了（一般作家是不敢“得罪”农民的，阎连科偏敢写出来）。\n这个世界的“知识分子”是虚伪的，“淳朴”的乡人也是“丑陋”的，那还有美好的事情吗？有，那就是玲珍对“我”的爱，和四叔对“我”的恩。\n“我”就没问题了吗？“我”同样有我的问题。“我”作为一个“知识分子”。能看破世俗，却又难以突破世俗的禁锢。“我”没有一个小贩活得真实，没有一个风尘女子过得洒脱。这才是“我”向往天堂街的地方。“我”看到被这个世界鄙夷的人们，竟可以活得如此真实，“我”缺失的东西，他们有（作家们常常以风尘女子作为“知音”，如果将她们的丑陋也揭露了，那作品留给人的最后一线希望都没有了）。“我”是教授啊，怎么能做这样的事。去他的教授，去他的虚伪，那些见不得人的事“我”也想做。“我”与姑娘们一起过年，无所顾忌，放肆自我。“一日不见兮，如隔三秋兮”，当《诗经》中的句子被“我”与姑娘们一起吟唱时，这就是“我”的“黄金时代”。\n同时，“我”的这次放荡，造就了玲珍的死。“我”超出常人预计，同意与玲珍进行衣冠冢。当白雪皑皑的大地上，望见蝴蝶飞来时，这无法解释的一幕，是作家对现实存在的一种混沌（作者后记讲，这一幕是一个真实的情景）。\n自此，“我”照顾起了玲珍的女儿小敏。她一天天长大，当有一天成人时，脸上的红晕，挺拔的乳房，真像玲珍年轻的时候。“我”喜欢上了小敏，可始终不敢对她讲出来，怎么能有这种不伦之恋。“我”怕乡人的唾弃，“我”怕世人的鄙夷。“我”将一切深埋于心，直到那一夜爆发了出来，“我”掐死了小敏的新婚丈夫（最后没有交代死没死，至于死没死已经不重要了）。\n“我”开始了我的“逃犯”生活，去往“我”的精神之国，“我”寻找到了一首首《诗经》之外的古诗，寻找到了“震惊世界”的“诗城”。\n“我”回到了京城。妻子剽窃“我”的书稿，拉拢关系评上了职称，过上了“梦寐以求”的生活。“我”拿着我的新发现去找李广智，李广智在与一群知识分子走“捷径”。你和“我”老婆通奸，“我”忍了。这一次“我”可是发现了一座诗城，对整个名族甚至世界具都有巨大的意义。当“我”看到李广智的与其他知识分子的集体失声，才真真切切对知识分子寄予的最后一丝希望破灭了。知识分子你有肮脏、见不得人的一面，你对名利，对权威卑躬屈膝。但是最后你竟可以对最根本的“专业”都可以亵渎，那么就全完了。\n“我”活在自我的精神世界。小说中对话的“答非所问”，“各言其说”说明“我”与这个时代完全“脱节”了，人与人的交流都有了问题。“我”回到了诗城，建立了一个乌托邦，做着“我”的坚守。这里有合大院校的老先生，名教授，这个专家，那个学者。我们抓阄，与那些风尘姑娘变着法子快乐着。知识分子中少数人的“狂欢”反衬了多数人的“落寞”。\n自此，作家结束了知识分子的这场“自我审视”，哪怕写着自己都犯恶心，但却是我们当今知识分子中真实存在的。\n 大连\n2017年12月10日\n","permalink":"https://olzhy.github.io/posts/reading-feng-ya-song.html","tags":["读书"],"title":"读《风雅颂》"},{"categories":["计算机"],"contents":"假定N为后台服务节点数，当前台携带关键字key发起请求时，我们通常将key进行hash后采用模运算（hash(key)%N）来将请求分发到不同的节点上。\n对前台请求于后台无状态服务节点不敏感的场景而言，只要请求key具有一定的随机性，哪怕节点动态增删，该算法于后台而言已可以达到很好的负载均衡效果。\n但对于分布式缓存，或者分布式数据库等场景而言，上述方式就不合适了。因后台节点的增删会引起几乎所有key的重新映射。这样，于分布式缓存而言，均发生cache miss；于分布式数据库而言发生数据错乱，其影响是灾难性的。\n而一致性哈希算法的目标是，当K个请求key发起请求时。后台增减节点，只会引起K/N的key发生重新映射。即一致性哈希算法，在后台节点稳定时，同一key的每次请求映射到的节点是一样的。而当后台节点增减时，该算法尽量将K个key映射到与之前相同的节点上。\n1）一致性哈希算法原理\n一致性哈希算法是将每个Node节点映射到同一个圆上。将各Node的key采用hash计算，可得到一个整数数组。将该数组排序后，首尾相连即是一个圆。如下图所示，Node的key分布在圆的不同弧段上。同理，若有一请求key，hash后落入该圆的某一弧段（下图三角点所示），顺时针方向寻得离其最近的节点即为其服务节点（下图Node2）。这样每个节点覆盖了圆上从上一节点到其本身的一段弧段区间。如某一节点失效，之前落入其弧段区间的请求即会顺时针移到与其相邻的节点（下图如Node2失效，之前落入Node3至Node2弧段的请求会落入Node1）。而未落入失效弧段区间的节点则不受影响（之前落入Node2至Node3弧段的请求，当Node2失效后不受影响）。增加节点的场景与此类似，新的节点承载一段新区间，这样，落入失效节点至新节点弧段的请求会被新节点所承载。\n在节点固定的情况下，为了增加节点在圆上分布的均匀性与分散性，可以设置节点的replicas（副本数）。下图将replicas设置为2，各节点承载的弧段范围已更加精细且于整体而言分布更加分散。所以适当调节replicas参数可以提高算法的均衡性。\n2）Golang一致性哈希算法实现代码\n本文的hash函数，是对key先做一次md5Sum，然后采用crc32做checkSum得到一个正数。\npackage consistent_hashing import ( \u0026#34;crypto/md5\u0026#34; \u0026#34;hash/crc32\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;sync\u0026#34; ) type Node struct { Id string Address string } type ConsistentHashing struct { mutex sync.RWMutex nodes map[int]Node replicas int } func NewConsistentHashing(nodes []Node, replicas int) *ConsistentHashing { ch := \u0026amp;ConsistentHashing{nodes: make(map[int]Node), replicas: replicas} for _, node := range nodes { ch.AddNode(node) } return ch } func (ch *ConsistentHashing) AddNode(node Node) { ch.mutex.Lock() defer ch.mutex.Unlock() for i := 0; i \u0026lt; ch.replicas; i++ { k := hash(node.Id + \u0026#34;_\u0026#34; + strconv.Itoa(i)) ch.nodes[k] = node } } func (ch *ConsistentHashing) RemoveNode(node Node) { ch.mutex.Lock() defer ch.mutex.Unlock() for i := 0; i \u0026lt; ch.replicas; i++ { k := hash(node.Id + \u0026#34;_\u0026#34; + strconv.Itoa(i)) delete(ch.nodes, k) } } func (ch *ConsistentHashing) GetNode(outerKey string) Node { key := hash(outerKey) nodeKey := ch.findNearestNodeKeyClockwise(key) return ch.nodes[nodeKey] } func (ch *ConsistentHashing) findNearestNodeKeyClockwise(key int) int { ch.mutex.RLock() sortKeys := sortKeys(ch.nodes) ch.mutex.RUnlock() for _, k := range sortKeys { if key \u0026lt;= k { return k } } return sortKeys[0] } func sortKeys(m map[int]Node) []int { var sortedKeys []int for k := range m { sortedKeys = append(sortedKeys, k) } sort.Ints(sortedKeys) return sortedKeys } func hash(key string) int { md5Chan := make(chan []byte, 1) md5Sum := md5.Sum([]byte(key)) md5Chan \u0026lt;- md5Sum[:] return int(crc32.ChecksumIEEE(\u0026lt;-md5Chan)) } 3）均匀性分析\n构建服务节点时，为模拟节点key在圆上的分布，简单采用id（0，1，2）做初始key，replicas为100。根据点间距等比例划分圆后得到其位置，彩色小圆点为其对应的节点（红绿蓝对应0，1，2）；\n三角点代表外部请求的三个字符串（10.10.10.10，10.10.20.11，10.10.30.12）hash后按算法取到的服务节点；\n使用Python matplotlib工具描点绘图如下。\n从图可知，节点虽少（3个），但扩大副本量后，key的分布已具有一定的均匀性与分散性，外部key请求的最终落地节点于整体服务节点而言也是比较均匀的。\n4）Golang高可用集群代理代码\n一致性哈希算法具有很广泛的使用场景。如做请求分流与负载均衡，分布式缓存，分布式存储等。如下代码调用Golang反向代理类库，结合上述一致性哈希算法，根据请求头标记做分发，数行代码，即可构建一个小巧高可用的代理服务器。\npackage main import ( \u0026#34;consistent_hashing\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httputil\u0026#34; \u0026#34;net/url\u0026#34; ) func main() { nodes := []consistent_hashing.Node{ {\u0026#34;0\u0026#34;, \u0026#34;http://10.10.1.10/\u0026#34;}, {\u0026#34;1\u0026#34;, \u0026#34;http://10.10.1.11/\u0026#34;}, {\u0026#34;2\u0026#34;, \u0026#34;http://10.10.1.12/\u0026#34;}, } ch := consistent_hashing.NewConsistentHashing(nodes, 100) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { sign := r.Header.Get(\u0026#34;sign\u0026#34;) node := ch.GetNode(sign) uri, _ := url.Parse(node.Address) httputil.NewSingleHostReverseProxy(uri) }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) }  参考资料\n[1] https://en.m.wikipedia.org/wiki/Consistent_hashing\n[2] http://michaelnielsen.org/blog/consistent-hashing/\n[3] http://www8.org/w8-papers/2a-webserver/caching/paper2.html\n  大连\n2017.12.01\n","permalink":"https://olzhy.github.io/posts/consistent-hashing-and-high-available-cluster-proxy.html","tags":["Golang","算法"],"title":"一致性哈希算法与高可用集群代理"},{"categories":["计算机"],"contents":"当前，各应用平台每天都在产生海量的数据。基于海量数据的深度分析报告越来越有价值。该领域涵盖数学、统计学，计算机科学等众多学科，是一个值得深入研究的方向。本文涉及的是一个简单的数据分析场景，旨在梳理使用Python数据分析涉及的常用类库（pandas、matplotlib等）与入门知识。本文对指定的几家手机品牌，按日期区间，从百度指数网站获取其月度搜索数据，然后绘制出它们的搜索走势对比图。\n1）关键点\na）日期区间（使用pandas的date_range方法）；\nb）对指定日期（年月），获取手机品牌清单中各品牌的搜索量（requests使用）；\nc）构造DataFrame（重点关注data、index和columns参数传值），结合matplotlib绘图。\n2）Python代码\n#!/usr/bin/python3 # -*- coding: UTF-8 -*- import requests import pandas as pd from datetime import datetime import json from pandas import DataFrame from matplotlib import pyplot as plt def get_indices(year, month, brands): uri = \u0026#39;http://index.baidu.com/Interface/Newwordgraph/getTopBrand?i=2\u0026amp;datetype=m\u0026amp;year=\u0026#39; + year + \u0026#39;\u0026amp;no=\u0026#39; + month r = requests.get(uri) if 200 == r.status_code: brand_indices = {data[\u0026#39;name\u0026#39;]: data[\u0026#39;value\u0026#39;] for data in json.loads(r.text)[\u0026#39;data\u0026#39;][\u0026#39;data\u0026#39;]} return [int(brand_indices[brand]) for brand in brands] return [] if \u0026#39;__main__\u0026#39; == __name__: brands = [\u0026#39;IPHONE\u0026#39;, \u0026#39;OPPO\u0026#39;, \u0026#39;LG\u0026#39;, \u0026#39;HTC\u0026#39;, \u0026#39;VIVO\u0026#39;] year_months = [datetime.strftime(date, \u0026#39;%Y-%m\u0026#39;) for date in pd.date_range(start=\u0026#39;20140101\u0026#39;, end=\u0026#39;20171101\u0026#39;, freq=\u0026#39;m\u0026#39;)] data = [] for year_month in year_months: year, month = year_month.split(\u0026#39;-\u0026#39;) indices = get_indices(year, month, brands) data.append(indices) frame = DataFrame(data, index=year_months, columns=brands) frame.plot() plt.title(\u0026#39;Search Trends Of Mobile Phone Brands\u0026#39;) plt.show() 3）结果输出\n 大连\n2017年11月28日\n","permalink":"https://olzhy.github.io/posts/search-trends-of-mobile-phone-brands.html","tags":["Python","数据分析"],"title":"手机品牌搜索走势图"},{"categories":null,"contents":"如下为本站友情链接，以添加先后进行排序。\n Go 语言中文网\nGolang 中文社区，活跃度极高的 Go 语言问答社区。\nTony Bai\n国内知名技术博主，Go 语言布道师。坚持写博十余年，仍孜孜不倦。\nJing blog\n一位旅居海外的新生代软件工程师、管理人。\nXiaobin’s Blog\n一位热爱技术热爱写博的技术人。\n土木坛子\n知名互联网博主，对理财、社会问题等有自己独到的见解。\n有点才艺的孟仔\n全栈工程师、技术管理者。拥有广泛的兴趣爱好与自律的生活方式。\n博友圈\n博客人的朋友圈，博客收录与文章 RSS 聚合网站。\n 想与我交换友情链接？\n 1 友链交换条件\na) 网站建立一年及以上\nb) 内容健康且原创文章不少于十篇\nc) 申请交换友链前，请提前添加好本站友链\n本站信息如下：\n网站标题：磊磊落落 网站首页：https://leileiluoluo.com/ 网站 LOGO：https://leileiluoluo.com/images/favicon.png 网站描述：浩然的个人博客，用于记录生活和分享技术。 2 满足条件，即可按照如下格式给我发送申请邮件\n我的邮箱：leileiluoluo@leileiluoluo.com\n标题格式：[友链交换申请] https://example.com/homepage\n内容格式：\n网站标题：xxxx 网站首页：https://example.com/ 网站建立年份：xxxx 年 网站友链地址：https://example.com/links 收到邮件后，若您的站点满足条件，我会按照您提供的网站标题和网站首页为您添加友链并告知您；若不满足条件，我也会告知您原因！\n 个人网址收藏\n","permalink":"https://olzhy.github.io/links/","tags":null,"title":"友情链接"},{"categories":null,"contents":"关于博主\n「大丈夫行事，当磊磊落落，如日月皎然。」——《晋书》\n大家好，我是浩然，一名软件开发工程师，我的博客「磊磊落落」建立于 2017 年，用于记录我在日常工作中整理的技术知识和日常生活中的点滴感想。目前我在技术上主要关注：Java、Golang、Python、前端开发、云原生、架构设计、DevOps 和 自动化测试。\n许可证\n本站文章及图片采用「知识共享署名 4.0 许可证」进行许可，转载请注明出处。\n我的信箱\nleileiluoluo#leileiluoluo.com （将#替换为@）\n我的 GitHub\nhttps://github.com/olzhy\n","permalink":"https://olzhy.github.io/about/","tags":null,"title":"关于本站"},{"categories":["随笔"],"contents":"行急思缓，静久思动，欲频需节，气堵需疏。\n人浮于世，几乎需对遇见的每个对立命题都得寻得一个平衡点。如游走于太极图谱，凡事不能直接落入黑白，得不断摸索这黑白的交界。而修行即是让心灵不断靠近这条线。\n世事变化，内心却要寻得某种不变，即是一种平衡的状态。\n内心活动看似包裹在驱壳之内，旁人难以察觉，实则心气流动，皆会以某种方式露于形色。若让修道之人，略观其色，听其片语，判断其心念涌动，结果不出其二。\n人在世上游走，于对立的命题间找不到平衡，会被有形的社会规则与无形的内心斗争所折磨，所以需要不断修行。\n修行是寻找自我，改变自我，突破自我，回归自我的过程，也是释放心结，自我豁免的过程。内心自由，是一种恒久坦荡的快乐，也有天人合一的意思。得到宏观与微观，身体与灵魂，物质与精神，存在与虚无的高度统一。\n大连\n2017.10.22\n","permalink":"https://olzhy.github.io/posts/padipata.html","tags":["随笔"],"title":"修行"},{"categories":["计算机"],"contents":"业务中也许会遇到反向构建树的情形，如从外部工具获取到依赖关系、行政区划，组织架构等文本数据时，如何去反向构建树。我们以“获取到了树的深度遍历结果，然后将树结构构建出来，最后用JSON格式输出”，来模拟此类树的反向构建过程。本文采用Ruby作为描述语言。\n1） 已获取的树的遍历结果文本\ncompany +- org-1 +- org-2 | \\- org-2.1 +- org-3 +- org-4 +- org-5 +- org-6 +- org-7 | +- org-7.1 | | \\- org-7.1.1 | +- org-7.2 | \\- org-7.3 +- org-8 | +- org-8.1 | \\- org-8.2 +- org-9 +- org-10 | +- org-10.1 | \\- org-10.2 +- org-11 +- org-12 | \\- org-12.1 +- org-13 +- org-14 | +- org-14.1 | \\- org-14.2 +- org-15 +- org-16 | +- org-16.1 | | +- org-16.1.1 | | | \\- org-16.1.1.1 | | +- org-16.1.2 | | +- org-16.1.3 | | \\- org-16.1.4 | +- org-16.2 | | \\- org-16.2.1 | \\- org-16.3 +- org-17 +- org-18 | \\- org-18.1 +- org-19 +- org-20 +- org-21 | +- org-21.1 | \\- org-21.2 +- org-22 +- org-23 +- org-24 | \\- org-24.1 +- org-25 +- org-26 +- org-27 +- org-28 | \\- org-28.1 +- org-29 | +- org-29.1 | \\- org-29.2 \\- org-30 2）Ruby反向构建树代码\n#!/usr/bin/ruby  #-*- coding: UTF-8 -*-  require \u0026#39;json\u0026#39; class Node def initialize(name, parent) @name = name @parent = parent @children = [] end def add_child(child) @children.push(child) end def name(name) @name = name end def parent() return @parent end def to_json(*options) if @children.length \u0026gt; 0 return {name: @name, children: @children}.to_json(*options) end return {name: @name}.to_json(*options) end end def process(line, last_line) m = $pattern.match(line) if m sign = m[3] depth = m[2].gsub(/\\s/, \u0026#39;\u0026#39;).length + sign.sub(/[+\\\\]/, \u0026#39;\u0026#39;).length name = m[4].strip if 0 == m[1].length $root.name(name) $depth_last += 1 else if depth \u0026lt; $depth_last ($depth_last - depth).times { $parent = $parent.parent(); $depth_last -= 1 } end node = Node.new(name, $parent) $parent.add_child(node) $parent = node $depth_last += 1 end end end $root = $parent = Node.new(\u0026#39;\u0026#39;, nil) $pattern = /(([\\|\\s]*)([+\\\\]?-?).*?)(\\w.*)/ $depth_last = 0 last_line = \u0026#34;\u0026#34; IO.foreach(\u0026#39;depart_tree.txt\u0026#39;) do |line| process(line, last_line) last_line = line end puts JSON.pretty_generate($root) 3）构建成功后，树的JSON输出\n{ \u0026quot;name\u0026quot;: \u0026quot;company\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-1\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-2\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-2.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-3\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-4\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-5\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-6\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-7\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-7.1\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-7.1.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-7.2\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-7.3\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-8\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-8.1\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-8.2\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-9\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-10\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-10.1\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-10.2\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-11\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-12\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-12.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-13\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-14\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-14.1\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-14.2\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-15\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-16\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-16.1\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-16.1.1\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-16.1.1.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-16.1.2\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-16.1.3\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-16.1.4\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-16.2\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-16.2.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-16.3\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-17\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-18\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-18.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-19\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-20\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-21\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-21.1\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-21.2\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-22\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-23\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-24\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-24.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-25\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-26\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-27\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-28\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-28.1\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-29\u0026quot;, \u0026quot;children\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;org-29.1\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;org-29.2\u0026quot; } ] }, { \u0026quot;name\u0026quot;: \u0026quot;org-30\u0026quot; } ] } 大连\n2017.10.22\n","permalink":"https://olzhy.github.io/posts/reverse-build-tree.html","tags":["Ruby","算法"],"title":"根据遍历结果反向构建树"},{"categories":["随笔"],"contents":"每个人是极其独立的个体，很多事情是非常主观的，你说它重要它就重要，你说它不重要它便不重要，你说它有意义它就有意义，你说它没意义也就瞬间没意义了。人与人的共同理解是极少的，即便语言这种描述媒介表达出来甚是相同，仔细去看却有天壤之别。人多数以为的认同感都是一厢情愿的，我们都活在各自的世界里，去完成自己的人生使命，只是在彼此交织构筑的网里，做了一场属于自己的梦。\n2017.02.13\n于DS-BJ火车上\n","permalink":"https://olzhy.github.io/posts/people.html","tags":["随笔"],"title":"人"},{"categories":["随笔"],"contents":"从时间长河来看，每个人都是沧海一粟，每个人的旅程也都是白驹过溪。作家，作为这其中的极少数，不一定是这些人中最敏感，情感或经历最丰富的，却是这当中难得的放弃沉默的记录者。不管是刻画内心独特的波澜，还是描绘时代的声音，都是当中少有的留下印记的人，这也是作家对于时间和时代的意义。于作家自身，也在记录的过程中完成了自己人生的意义。\n2017.02.13\n于DS-BJ火车上\n","permalink":"https://olzhy.github.io/posts/writer-is-chronicler.html","tags":["随笔"],"title":"作家是一个时代的记录者"},{"categories":["随笔"],"contents":"某天，翻开了几年前使用过的旧手机。\n翻看当年的条条短信息，惊讶当时出现过的那些人，发生的那些事，不仔细回想，竟像从没有发生过一样。\n那时让我心跳加速的话语，竟可以平心静气的读出来。\n那些让我欣喜，让我烦恼的人与事，不知何时已从自己的生活中流走了。\n世事交替，人物变换。\n我们都在忙自己的事情，以至于对那些事物的离开没有察觉也没有疑问，继续过着自己的生活。\n欣喜的是，人不会被一件事一直困扰着，总有新的事情出现，抹去彼时的哀愁。\n忧伤的是，人竟可以如此快的淡忘，它们明明在自己的生命中产生过异常强烈的波澜。\n我犹如看过一场马戏，此刻所有的灯光暗下来，看着刚刚在舞台上表演过的演员匆匆退场。\n我与他们握握手，夸赞那精彩的演绎，他们回给我一个微笑，却依然难掩陌生。\n原来出了戏，人们早已将当时的场景轻描淡写。\n我们都很自私，以至于除却自己的生活，对生命中出现过的那些人，仅仅在内心存留一瞬，就忘却了。\n我跟他们挥挥手，感觉这一切似正式演出，又似序幕。已分不清哪个部分才是真正的人生。\n东胜\n2017.02.07\n","permalink":"https://olzhy.github.io/posts/past-like-wind.html","tags":["随笔"],"title":"往事如烟"},{"categories":["随笔"],"contents":"那是去年的事情了。\n秋天的北京依然很闷热。\n我是在工作日的最后一天向H提出爬山邀请的，原以为她怕晒会拒绝我，没想到她竟爽快的答应了。\n我们约好在一个地铁口会面。\n那是我与她的第一次见面，虽然之前已在通信工具上聊过许多次了。\n她比我先到达了相约地点。\n出站，只见一个长发女孩望向这边。我径直走向她，简单几句开场白过后，她已像多年的老友一样开始大大咧咧跟我交谈了。\n她身材修长，个子与我齐眉。一身运动装扮，边走边笑着说着。\n“今天早上，出门的时候，她们还说要下雨，劝我别出来呢”。\n“是啊，我查了天气预报是雨天，没想到现在一扫阴霾，晴空万里，感谢天公作美”。\n走到换乘的公交，她找到一个靠窗的座位，我并排坐在与她相邻的位子。\n“去年和家人去过一次泰国，那里的海好干净，建议你有机会也去看看”。\n“嗯”。\n她述说着那次旅行的种种趣事，我边安静的听着，边近距离打量着眼前这个女孩。\n她眼睛大大的，深黑与亮白界限明晰，干净透明，我惊叹世上竟有如此好看的颜色。\n下了车，我们沿着崎岖的上坡路走了个把小时，靠近一处果园稍作停歇。我递出一张湿巾纸，我们擦擦汗，吃着早上带来的小番茄。\n“没想到你还挺有心的，这个时候还真想吃点水果”，她说。\n“是的，我时常上山，钟爱带些小水果”。\n“从这里往下看，这一会的功夫，我们没少走呢”。\n我望向她指的方向，只见那走过的路已被片片云朵淹没了。\n初秋登高，愈往上，愈神清气爽。\n山路两旁高大的树木，枝叶交错，阳光绕过层层缝隙，洒在岩石上，土地上，斑斑驳驳。\n她手指轻轻拿着那绿中开始泛黄的叶子，安静的看着，又不舍得摘下来。\n“我好喜欢这些不起眼的小叶子”，她说。\n“我也是，那茎条与绿色之间流过了几多岁月”。\n她走在我的前面。\n路边开着颜色鲜亮的小花朵，小秋菊与野玫瑰居多。\n“快看，这一朵好优秀”。\n优秀？我初次听到这样的形容词，但瞬间又觉得恰到好处。\n到达山顶，我们吃了热水泡面，她说此时此刻能吃一碗泡面竟然如此满足。\n吃过午饭，我们几乎同时望向那一处的云朵。\n“你觉得它像什么？”，她问。\n“一个微笑的女子”。\n“嗯”\n她蜷缩着双腿，端坐在一个石头上，望着远处的风景，若有所思。我欣赏着旁边的狗尾巴草，趁不注意，偷偷拍了一张她的背影。\n（后来有一天，她给我发来一张照片，我很惊讶，就是那时候，她也悄悄拍了一张我的照片）\n下山，是一段略有斜坡的公路，我们要沿着这条路走两个多小时，到临近的村子坐回市里的公交。\n她在前面轻快的走着跳着，活像一个随风舞动的天使。\n我们边走边欣赏那两旁的花。\n远远的就听见了她的呼喊声。\n我走过去，只见她真像得了什么至宝。蹲在那里，双手看护着发现的“新物种”，像在守护一个刚出世的生灵，生怕一下子被风吹散了。\n我又看见了那清澈的眸子，像山顶泻下的清泉，这一刻时间被风住了。\n那是一颗洁白无瑕的蒲公英，无论从那个角度看都是一个完美的圆。大自然匠心独具，最简单的颜色竟可生长的如此生动。\n自然总是悄然间将相似的灵魂互相吸引，那是一个动人的生灵与清澈的少女在对话，我安静的看着，生怕打扰了她们。\n我们坐上回城的公交，穿过一段又一段盘山路，她没说几句话就靠着座位睡着了。\n我嗅到那长发飘散着阵阵清爽的香气。倾了倾身子，好想某时她的头能停落在我的肩膀上。\n东胜\n2017.02.06\n","permalink":"https://olzhy.github.io/posts/years-flowing-away.html","tags":["随笔"],"title":"似水流年"},{"categories":["随笔"],"contents":"在一个路口注意到一个人\n我们不知在哪一站相遇的\n我坐在靠窗的位子\n这车上有打牌的 有安静看书的 还有打瞌睡的\n我有时跟他们一样 可现在更喜欢安静\n看着窗外飞翔的鸽子 远处的山 内心会感叹自然的美丽\n看到路边的小贩 喝醉的酒鬼 会想着帮帮他们 但又想 谁没有过窘迫与烦扰 每个人的悲喜都是神圣的\n对了 差点忘了 还是说说偶遇的女孩\n她坐在我前边 喜欢表达 也善于倾听 和你说话时总是微笑着看着你的眼睛 我们聊上几句又看看窗外的风景 伴随着一个个夕阳落下又升起\n期待这个相遇的旅程能够长久一些\n2017.01.09\n","permalink":"https://olzhy.github.io/posts/meet-by-chance.html","tags":["随笔"],"title":"偶遇"},{"categories":["随笔"],"contents":"春\n午后伏案小睡，窗口出来清风。我下意识去关窗户，望见那远处的花由一团团到一簇簇又成一朵朵。须臾，她已将我唤醒了。我索性将下午的事放在一边 ，径直走向那美丽的颜色，那是一种沁人心脾的香。仰面从开近的花轻轻走过，整个身心灵都得到了释放。那一刻，花不语，人自醉。\n夏\n喜欢夏的热烈与不保留。看着那一个个生机勃勃的生命自由绽放，让人心生敬畏。自然有一种奇妙的力量，让人挣脱束缚，变成一个脱缰的快马。\n秋\n偶得闲暇，走近一处庙坛，寻得一处古老城墙，停下脚步。手指轻触那历经几多岁月的砖瓦，惊叹人的命运在它面前竟似一株株花一样，生生灭灭，匆匆走过。\n冬\n在一个暖暖的屋子，喝上半杯咖啡，看着窗外满天飘落的雪花，是多么迷人的场景啊。\n四季变迁，人来人往，使我明白相识的不易。\n自从遇见你，我感到这世界也许存在一种奇妙的维度，让人的眼睛可以穿破所有障碍只看到一个美丽灵动的生命，除却她，尽是流走的云朵。\n我喜欢新芽初露的春，留恋推窗听雨的夏，欣赏红叶铺街的秋，盼望雪落指尖的冬。但更希望能与你一起经历一个个四季变换。\n2017.01.08\n","permalink":"https://olzhy.github.io/posts/letter.html","tags":["随笔"],"title":"信"},{"categories":["随笔"],"contents":"与大多年轻人一样，这几多年，多是为了生存忙碌着。于一生也是一个不可或缺的过程，希望若干年后，可以追随内心，寻得一种平淡明朗的生活，与世界和平相处。相信历经岁月的洗礼，内心自有一份豁达与坦荡。热闹处有宁静，平寂时有欢愉。\nBJ\n2016.10.09\n","permalink":"https://olzhy.github.io/posts/quiet-in-bustling.html","tags":["随笔"],"title":"热闹处有宁静"},{"categories":["随笔"],"contents":"人有多种活法，世界有多种解读方式。那种只遵从内心，跳出常人理解与现有规则过生活的人，需要忍受现实世界多少折磨。那是一种值得敬畏的人生态度，需要倾注生命与灵魂来保持心灵的纯粹，受尽折磨，内心寻得一片空明。由生至死，对隐形力量不顺从，对现实世界不妥协。\nBJ\n2016.10.08\n","permalink":"https://olzhy.github.io/posts/uncompromising-to-real-life.html","tags":["随笔"],"title":"对现实世界不妥协"},{"categories":["随笔"],"contents":"假期结束了，感觉人慢下来才会真正开始想生活， 一想才知道错过些什么，或者突然意识到还有好多重要的事情没有做。人可能要寻得一个属于自己的节奏吧，不急不缓，不忧不扰，不娇纵失望，不迷失自己，也许活着就是在修炼内心，这是一生的事，不能急于求成。\n东胜\n2016.10.06\n","permalink":"https://olzhy.github.io/posts/cultivate-inner-heart.html","tags":["随笔"],"title":"修炼内心"},{"categories":["随笔"],"contents":"去年读《麦田里的守望者》，感叹少年的内心好似雨后的一道彩虹，年少的时光就像夜里洒向麦田的月光。感叹之余又可惜好多那个年纪独有的情感已经从内心忘却了。看到此书时，虽然惊叹作者用别样的文笔勾勒了少年内心那广阔的田野，但自己的童年明明有过相似甚至超脱书籍的异常强烈的情感，但过后了就再也寻不到了。\n年轻时拥有过的爱情，甜的也好，苦的也罢，当时觉得没什么，过后了才会感叹原来那是属于那个年纪独有的一种波澜。\n你我有时可能会出现极其丰富的情感，当时无以名状，或者怯于表达，过后有时会若隐若现，但很难完整的复现那时的心情了。所以同一朵花，在不同的年纪去看，对内心的触动是不一样的。\n相机可以定格女子的美丽，声音也可以留存，唯一难以恒久的是此刻的情感。所以在各个不同的年纪，若有引起内心波澜的人与事，要用最接近真实感受的方式记下来，这是很珍贵的。\nBJ\n2016.08.29\n","permalink":"https://olzhy.github.io/posts/enshrine-emotion.html","tags":["随笔"],"title":"珍藏情感"},{"categories":["随笔"],"contents":"人，都 “懒”的去表达一些深层次的理解。\n每个人都有丰富的独有的内心世界。但语言这种表达媒介，只能传达少之又少的信息。人大量时间由琐事占据，日常交流，要么有不少冗余与失真，要么压根缺少表达的欲望，于别人会觉得“所以你不说，我不知道”。\n话不在多，相处不在久，也许几十年有一个下午和自己的老父亲谈谈心，也是一次难得的经历，会铭记终生，“哦，原来您有这么多有趣的理解”。每个人的内心世界，除了自己，没人能懂，甚至自己也不懂，这也是人的孤独性。\n真正心底的东西，也许此生只会倾心述说一次，也许一次都不会。\n所以我们一直在等待那个听众的出现，遇见你的那个雨天，我愿意表达，而你也在认真听，纵使日后烟消云散，有过这样深度交流的时刻，就是值得的。\nBJ\n2016.08.27\n","permalink":"https://olzhy.github.io/posts/people-lack-of-a-listener.html","tags":["随笔"],"title":"人缺的是一个认真懂你的听众"},{"categories":["随笔"],"contents":"昨日山中巧遇一人，相聊甚欢。今日初醒，突有一些小感，特作记录。\n造物主（姑且这样说），粗暴式主宰人类的三种方式。\n一、人生来就无自我决定性\n人类无法逃离土地，氧气，父体呵护以及依赖的环境而独立生长。这是第一层约束，生物本体存在性制约。所以一开始我们就被有形的，无形的力量左右着。\n二、遗传式知识累积与传递\n人类被生命周期限制，本体无法连续的依托个人知识累积认知世界。前人发明文字语言等文明的传播媒介，而后创立解释性科学借助教育等方式逐代传递与精进，你必须坚定也不得不坚定前人经验总结中每一个节点的正确性，因为人从生物性上缺乏推倒重来的可行性。单点式、断层式，不可推翻式知识累积，无论多么精进，形成的只能是单点科学，难以看清真实的世界。\n三、以人制人\n人类衍生过程中看似自主式实则被自主式形成的社会准则，文化道德宗教等约束。让你无法逃离这些条条框框，所有对约定俗成的质疑、挑战或跃跃欲试会被标准世界当作异类从肉体精神让你受尽折磨。\n最可怕的是这些约束的产生与作用过程人类无法自知。\n就比如开始的那句话，我们沟通的前提必须预设一个节点或者基点的存在性与合理性，比如文字逻辑语义表达的正确性，稍有质疑都会被传统世界唾骂。人类的自由性与独立性微乎甚微。\nBJ\n2016.07.10\n","permalink":"https://olzhy.github.io/posts/freedom-and-independence-of-people.html","tags":["随笔"],"title":"人类的自由性与独立性"},{"categories":["随笔"],"contents":"人、鬼，宇宙； 宗教、物理学； 人类活动、天地气象； 一切看似无关的东西，终会寻得某种契合点。\nBJ\n2016.04.23\n","permalink":"https://olzhy.github.io/posts/fitting-point.html","tags":["随笔"],"title":"契合点"},{"categories":["随笔"],"contents":"甚至一个人的性格，都可看作学识、才智，品性修养的统一表征。\nBJ\n2016.04.23\n","permalink":"https://olzhy.github.io/posts/virtue-and-character.html","tags":["随笔"],"title":"修养与性格"},{"categories":["随笔"],"contents":"一个好的管理者绝不仅仅是一个任务分发者。事要做好做巧，需要在各方面下苦功，对人对团队的了解要像自己的器官一样细致入微，谁能干什么谁不能干什么，做到胸有成竹。不同人的组合会爆发不一样的“化学反应”，这需要很高的情商与智慧。往大了讲，人类活动，社会文化，物理聚变，化学反应，以及种种奇妙之事，均源于人或物质的组合方式。\nBJ\n2016.03.12\n","permalink":"https://olzhy.github.io/posts/a-good-manager.html","tags":["随笔"],"title":"一个好的管理者"},{"categories":["随笔"],"contents":"手指轻触这古老的城墙，感觉到人的渺小；在它面前，人好似一株花一样，匆匆走过，生生灭灭。\nBJ\n2015.12.01\n","permalink":"https://olzhy.github.io/posts/ancient-city-wall.html","tags":["随笔"],"title":"古老城墙"},{"categories":["随笔"],"contents":"这几日正是往常老家秋收繁忙的时候。\n我在的村子不算旱，但可以灌溉的田也没几成，还得是靠天吃饭。人们将几亩好地种菜种瓜，大多的地都来种玉米了。\n清早刚吃过早饭就要去地里干活了。有全家出动的，也有你家我家合伙收割的，一派繁忙景象。\n后面老汉赶着跟他一样上了岁数的老黄牛，驮着老伴，缓慢的向田里行进。\n前面开动的三轮车砰砰作响，后座上淘气的孩子隔着护栏伸手去抓前座男人寥寥无几的头发，几次下来，男人有些恼怒，小孩却嘎嘎笑着。\n到田里了，前面的人掰着玉米几句话功夫已不见身影，后面的男人则挥着镰刀给车开路。孩子们有跟着忙活的，也有在车前抓蚂蚱玩耍的。女人们嘴里话着家常，手里的活却干净利索，上下几轮，地上的玉米已越堆越实。\n半晌的时候肚子饿了，寻一处宽敞地，拿出早上从家带来的干粮，有中秋的月饼，自家种的梨、苹果，喝着略带温热的水稍作停歇。\n到黄昏时，几亩田也就刚好忙活完了。拖拉机走走停停，人们居车两旁低身将玉米抛到车兜里，就这样一堆堆，到地头，车已被装的满满的。\n坐在装满金黄玉米的车顶，握好绳子，开动的车浓烟滚滚，仰头星星点点，耳边阵阵蝉鸣。\nBJ\n2015.10.03\n","permalink":"https://olzhy.github.io/posts/autumn-harvest-in-memory.html","tags":["随笔"],"title":"记忆中的秋收时节"},{"categories":["随笔"],"contents":"对于高考的不平等，有人讲“农村孩子高分低能，北京上海考生综合素质高，所以低分进北大复旦乃情理之中”。我看到一个回复说“既然要求综合素质高，那就应该考查综合素质！凭什么会弹琴就叫综合素质，会种田养猪不叫综合素质？”。\n我看了很有感触，这些年，见过城市的高楼，看过富人的奢华，但都不是那么向往的快乐。想到快乐，也许现在或者将来都难敌小时候在村里捉蚯蚓的那种心情。如果素质不仅是拥有金钱还包括感知生活与获取快乐的能力，那么在那块土地的生活与经历对于我都是难得的至宝。这些年从读书到工作，也接触过不少优秀与值得尊敬的人，见过出身贫寒到学识渊博温文尔雅的蜕变，也见过家境殷实仍奋力追逐的人，对穷人的小瞧与对富人的仇视都是我们应当摒弃的“素质”。\nBJ\n2015.9.19\n","permalink":"https://olzhy.github.io/posts/face-self-and-others.html","tags":["随笔"],"title":"正视自己，正视别人"},{"categories":["随笔"],"contents":"走了一天回来了，去吃类似呷哺的小火锅，不经意看到旁边的女孩带着她的母亲一起吃，看着女孩母亲去坐这种支的高高的凳子很费劲，心里另一种滋味。远方的父母也到这个岁数，每次看到年龄相仿的老人，浮现的总是他们的身影。\n上次户外回来，也在这家店。一个父亲年纪的人来应聘服务员，年轻小伙子指导着说叔叔您这样那样调调火加点水，那个瞬间我看着老人的背影竟觉得跟父亲那么像，我心里酸酸的，当年外面念书也偶尔跟同学出去聚餐吃喝，却难以体会父母在外边打工挣钱的艰辛，心里很自责。\nBJ\n2015.9.19\n","permalink":"https://olzhy.github.io/posts/figure-of-parents.html","tags":["随笔"],"title":"父母的身影"},{"categories":["随笔"],"contents":"买票，一乡村老人翻着被袋子手绢包裹的零钱，跟售票员说去北京火车站。听着说“北京”这个字眼时略带骄傲的声调，体会到那个年代的人对首都的深深向往与浓烈情感。感叹现实是首都内化于我，而我外化于她，但某天这个城市终究会变得包容与热爱每一个爱她的人，所憾的是她再也不能给远去的那个时代的老人一次热烈的拥抱了。\nBJ\n2015.09.02\n","permalink":"https://olzhy.github.io/posts/buy-a-ticket.html","tags":["随笔"],"title":"买票"},{"categories":["随笔"],"contents":"中华文化博大精深，日本人只得分毫，便可振兴民族，改头换面。中国的儒、道、佛、法；中医、星象、风水、伦理等虽门类有别，各成体系，却一脉相承，统一交融。中华文化历经千年积淀，丰富圆满，可谓天地日月之精华。这不仅是老祖宗留给我们的个个锦囊，此生此世此民族遇到的种种，都能在古人的文化瑰宝里得到启迪，找到答案。\nBJ\n2015.09.01\n","permalink":"https://olzhy.github.io/posts/silk-bag-left-behind-by-ancestors.html","tags":["随笔"],"title":"古人留给我们的锦囊"},{"categories":["随笔"],"contents":"这世界，并非所有东西都是确定的，那些偶然的不确定的东西似乎以更微妙的方式影响着我们的生活。\n确定的东西，拥有确定的维度，处于空间某一个角落，你朝那个方向走，就能找到它；不确定的东西，以高维度方式存在，你从自我认知方向去寻找，不一定能抓住它，你就泰然自若，无心插柳，做好本职，它极有可能以不经意的方式出现。\nBJ\n2015.08.28\n","permalink":"https://olzhy.github.io/posts/something-uncertainly.html","tags":["随笔"],"title":"不确定的东西"},{"categories":["随笔"],"contents":"人生的边界就是这个世上的规则，我们所处时代的程式，人生难以逾越的固有思维。\n比如谈恋爱，男孩就应该体面，有眼缘，对你关心呵护，慢慢走近，然后觉得不错，在一起，然后进行后续的。\n如果路上遇到一个陌生人，尾随你，然后当面说我喜欢你，就会觉得好变态，是不是流氓。\n但谁也无法证明一种就比另一种更出自初心。\n就像生来就应该好好赚钱、养家、生子，安享晚年。我们在这个世上生活，总会有一种固有的程式被放进大脑。\n这个程式是那么的顺理成章，稍有质疑都是大逆不道，但我们从没想过为什么就本应是这样，我们大多活在这张网里不肯挣脱。\n有些人摸到了人生的边界，时代的束缚。身体走不出去，就用结束生命的方式让灵魂脱缰。这里有诗人、作家，还有好多，但大多人摸不到，或者摸到了迈不出这一步。\n所以看清这些，并不是要悲观的任命运摆布，而是以更平淡的心去看待这世间正在或即将发生的一切，天高云淡，宠辱不惊，泰然自若，笑看人生。\nYY.BJ\n2015.07.25\n","permalink":"https://olzhy.github.io/posts/boundary-of-our-life.html","tags":["随笔"],"title":"人生的边界"},{"categories":["随笔"],"contents":"一个时代有一个时代“楚门的世界”，我们也许从生到死都不会意识到这种难言的悲哀 。我们生在一个时代，身体血液思维都会受其所限，每个时代都有每个时代的标准活法，而且自我意识会认为我们所走的每一步都理应如此。人们自认为自己的生活拥有超凡的自由度，但永远挣脱不了时代给人的牢笼。\nYY.BJ\n2015.07.25\n","permalink":"https://olzhy.github.io/posts/cage-of-time.html","tags":["随笔"],"title":"时代的牢笼"},{"categories":["随笔"],"contents":"诗人为了保持灵魂的独立，一生都在逃离世俗。如顾城，如海子。他安静的坐在那里，但思想如鸽子一样早已在脑海放飞了，我们很难体会那种脱缰的快感。而鸽子飞累了，终究需要一个地方歇歇脚，但即便寻得一处人间的世外桃源，也终究属于尘世这张网，就算包得再严实，也避免不了某时会落入一颗凡尘，而诗人又有极度的精神洁癖，容不得丝毫世俗之物。到最后，唯有结束生命，才能保持灵魂的洁净。\nYY.BJ\n2015.04.30\n","permalink":"https://olzhy.github.io/posts/soul-of-poet.html","tags":["随笔"],"title":"诗人的灵魂"},{"categories":["随笔"],"contents":"周末与好友相约玉渊潭，穿梭于熙熙攘攘的人群，欣赏那开的正盛的樱花。蹬船游荡于湖中，感受春天的气息。\n北京\n2015.03.29\n","permalink":"https://olzhy.github.io/posts/yinghua.html","tags":["随笔"],"title":"玉渊潭赏樱花"},{"categories":["随笔"],"contents":"爱国从不是不停的歌颂与赞美，也不是无休止的争论。因为动真情的人一般更沉默一些，他们在经历艰难的独立思考，这其中或许会有许多次的痛苦挣扎，也或许会产生各种爱恨交织的复杂情感。但这是真实的，我们不要逃避，也不要认为怀疑就是抹黑。因为爱到深处时往往是爱之俞深 ，责之俞切。当她不完美时，你并没想离开她，相反，你想改造它，改变它，让她变得更好，这或许才是一种真爱国吧。\nBJ\n2015.02.14\n","permalink":"https://olzhy.github.io/posts/about-petriotism.html","tags":["随笔"],"title":"关于爱国"},{"categories":["读书"],"contents":"记得有一次从火车站出来，视野里来来往往的人们为不同的目的地匆匆赶路。我放下行李，驻足去看这最普通不过的情景，心里竟生得一丝曼妙的哲学意味——想象着若运用蒙太奇手法将每个路人真实的一生与这短暂的擦肩剪接到一起同时去看，我们可能会对人生有更加深刻的理解。\n今日读过张晓风的《缘豆儿》，觉得一生的承载就如同这手捧的小小钵子，人世间的情与缘就像这一粒粒各色各样普普通通的豆子。你我初生时，钵子里便有了一粒豆子，那是你我与父母此生注定的缘分。我们拿着这钵子一路跑啊跑，由婴儿长成了少年，由少年长成了成年，钵子里施予的是你我对世界的感恩，收获的是你我与世人的缘分。这一路上有苦，有甜，有乐，有悲；有人，有物，有情，有怨。所有的这些都因缘而生。当持钵子的手变得苍老，人白了头发，这钵子或轻或重；或施予的多，若收获的多；或甜多过苦，或悲多过乐。都不重要了，我们只记得彼时的少年，在奔跑的途中不经意相遇，凝视彼此澈亮的眸子，交换一粒小小的豆子，难掩兴奋。\nYY.BJ\n2015.01.19\n","permalink":"https://olzhy.github.io/posts/read-feat-bean.html","tags":["读书","随笔"],"title":"读《缘豆儿》"},{"categories":["读书"],"contents":"我在农村长大，也深爱那里的土地，那里的人。我记得乡民的勤劳，憨厚，淳朴，却不曾发现我可爱的相亲骨子里的丑陋。余华的《在细雨中呼喊》，虽述说的是我还未出生的那个年代，但一个个栩栩如生的人物，一件件活灵活现的故事，将国人骨子里的那种可悲的共性表现得淋漓尽致，仔细想想，这些人，在我生活过的村子都能找到原型，读几页就得舒一口气，读完，心重的难以释怀。\nSY\n2014年6月8日\n","permalink":"https://olzhy.github.io/posts/read-cries-in-the-drizzle.html","tags":["读书"],"title":"读《在细雨中呼喊》"},{"categories":["随笔"],"contents":"近些天，沈阳下了雨。\n身在城市坚固的建筑里，雨打不到，衣湿不了，马路不会因雨变得泥泞，工作不会因雨而停歇。雨来了，也感受不到雨的气息了。对雨的记忆，还在故乡，还在小时候。那里那时的雨带给我的是另一种思绪。\n那时，我还是上小学的年纪。周末，不用去上学。刚吃完早饭，隔壁院子的老人就来串门了。老人拉着家常，母亲边答话边做着针线活。我坐在炕上挨着玻璃的位置捣鼓着破旧的收音机，这里拧拧，那里拆拆。今天阴天，虽是上午时分，天色显然不像往常那么明亮，准是要下雨的样子。三五句话功夫，雷声一响，就知道雨要来了。母亲吩咐我赶快下炕去收衣服，话音刚落，她已跑去找塑料布去盖晒在院里的农作物了。故乡的雨来的总是那么匆忙，那么毫无防备，收拾中间，雨已从天而降。你奔我跑，以最快的速度收拾完毕，人没被打的很湿，事也还算没被耽误。\n回到屋里，用毛巾擦擦身上的雨水。各自回到炕上，又恢复了原来的安静。屋外却是另一番景象，雨哗哗直下，地面水波涌动，似开了锅的水。隔着墙听到街上有人奔跑的脚步声。有串门的人，有打早去田里干活的人，都被雨夹在了半路上，人们不得不就近躲雨了。这不，街门洞里就听到了躲雨人的嬉笑声。有男人脱了草帽，点根烟蹲在那里说着笑话。也有女人哄着怀里的孩子说着今年的收成差不了。\n又过了些时候，雨小了。我便坐不住了，拿个小桶走出屋子，不为接水，而是倒放着。看着雨滴顺着瓦檐泻下，敲中小桶，发出清脆的滴答声，就会觉得很快乐。玩耍着，周围的几处烟囱开始冒出青烟，已有人家开始做午饭了。母亲喊我到柴房去抱干的玉米秸秆，我们也要烧火做饭了。灶台就在家门口，大部分都被屋檐挡着，只有烟筒伸出外檐。第一锅是烧开水的，等到烧第二锅，母亲已将包子包好了。一排排放在高粱秸秆做的夹层上，似浑圆的灯笼，盖好锅盖，开蒸了。我边烧火边看着滴落的雨将烟筒打湿，燃烧着的秸秆又将烟筒烘干，就这样重复着。约莫一个时辰，饭就熟了，锅盖一揭，热气直冲雨滴现一阵白气，然后便是扑鼻的饭香。\n吃过饭，雨就停了。我穿上我的专属小雨靴，跑出院子，专往水多的地方踩，好有一种“大展宏图”的意味。窄窄的街道上铺满了山坡上冲下来的红胶泥，这种泥是可以烧砖用的。我们那个年纪的小孩最喜欢玩这种泥巴。捏成坦克，圆球，然后等阳光好的时候晒干，变得像石头一样坚硬。淌水淌的没兴致了，就去玩胶泥，对面的人家背朝街，迎街的屋墙刷的白白的。小时候也不知怎么，玩泥巴偏对这种光滑透亮的白墙有种说不出的偏爱。抓胶泥，选位置，托上去。走走跑跑，几个回合，白色墙壁便被托了好多只手印。大功告成，洗洗手上的泥巴，远远看着自己的杰作还格外欣喜。\n“快回来睡午觉”，是母亲喊我的声音。我擦擦手，飞快的跑回去。\n阴天的觉总是特别香，好似过了几个世纪我才醒来。看着窗外放晴的天空，全然分不清这是上午还是下午。\nSY.NS\n2014.05.22\n","permalink":"https://olzhy.github.io/posts/rain.html","tags":["随笔"],"title":"雨"},{"categories":["读书"],"contents":"五一小长假的第二天，图书馆里冷冷清清，我照旧坐在最后一排靠窗的位置，准备用这一下午的时光静静读完《穆》的最后几章。看着窗外落下的点点细雨，读着书中的凄惨故事，不禁唏嘘。当书签夹在最后一页，轻轻将书阖上，心里有种东西在翻滚。\n读到新月告别的那一段，整个心都碎了。人在这世上走一走，酸甜苦辣都得尝一尝。也许只有当我们真正告别的时候才能体会这世间最珍贵的东西是什么。这世间所有的怨都可以忘记，所有的恨都可以原谅，所有的伤都可以释怀，唯有这真情让人恋恋不舍。\n整个故事就像易卜拉欣跟着吐罗耶定巴巴在朝觐的路上看到的幻景，这幻景缘于玉，葬于玉。玉本身是美丽的，和玉留下的故事却是凄惨的。当易卜拉欣清醒之后，掸一掸身上的尘土，这一切都好像没有发生过，继续朝着圣地麦加的方向前行。\n也许你我也是缘于某种东西落入凡尘。我们来到这里，在这里经历我们看到的、想到的、听到的。这里有善、有恶，有怨、有情。我们因这里的人开心难过，为这里的事迷茫彷徨。我们在这里轻唱，我们在这里挣扎，我们在这里怒号。我们厌恶这里，我们怀念这里。梦醒了，我们要和这里告别了，我们也许会留下些什么，也许什么都不会留下。轻轻的离开，好似什么都没有发生过。\n生是一瞬，死是长存。只因人间有爱，才使这瞬间的东西在恒久面前更显珍贵、美丽。\nNEU\n2014.05.02\n","permalink":"https://olzhy.github.io/posts/muslim-funeral-reading.html","tags":["读书"],"title":"读《穆斯林的葬礼》"},{"categories":["读书"],"contents":"上周末刚读到《月清》，想起新月母亲对女儿表现出的漠不关心，以及种种微妙的迹象说明这后面一定有着不一样的故事。刚刚脑海突然闪过一个可怕的猜测“难道新月是韩子奇跟他小姨子所生？”。\nNS\n2014.04.28\n","permalink":"https://olzhy.github.io/posts/muslin-funeral.html","tags":["读书"],"title":"读《穆》"},{"categories":["随笔"],"contents":"午后伏案小睡，窗口吹来轻风，下意识去关窗户，无意间看到那远处的桃花树，我睡眼惺忪，远处的颜色由一团到一簇，再到一朵朵，须臾间花已将我唤醒了。索性将下午的事放在一边，轻快向它走去，伸手轻触那袅娜柔美的花瓣，花蕊轻颤，那是今春的第一次花语。\n幻想曲\nNS\n2014.04.10\n","permalink":"https://olzhy.github.io/posts/spring-coming-again.html","tags":["随笔"],"title":"又是一年春来到"},{"categories":["随笔"],"contents":"周末去公园走了走。\n到一斜坡处，一位小女孩尝试去爬眼前一座看上去很陡峭很危险的石头坡，本以为她身后的父亲会将其制止。没想到父亲没有做声，而是紧随其后，左手发力，右手作保护状，移动笨拙的步子，目不转睛的盯着女儿，提心吊胆爬起坡来。到坡顶了，小女孩开心极了，咯咯的笑着，女孩的父亲出了一头的汗水。\n到一拱桥处，一对年轻恋人在用剪刀石头布来决定移动脚步。男孩连输几局，眼看女友离自己越来越远，焦急的追赶起来。\n现在夜已深了，脑海依然浮现着小女孩天使般的笑容和女孩父亲的闪闪汗珠；年轻恋人的嬉笑追赶和一下午的暖暖阳光。\nNEU\n2014.03.23\n","permalink":"https://olzhy.github.io/posts/warm-sunshine-of-afternoon.html","tags":["随笔"],"title":"一下午的暖暖阳光"},{"categories":["随笔"],"contents":"看到有国人为克里米亚入俄而欢呼雀跃，这欢呼声像在打我们自己的脸一样振聋发聩。我们早已忘记了谁怂恿下的外蒙古独立，而那“公投”的形式又是何等相似。黑龙江以北，乌苏里江以东，天山以西，唐努乌梁海等上百万平方公里的领土不知被谁强占。我们不去延续仇恨，但也不能忘记耻辱，忘记就意味着背叛。\nNEU\n2014.03.19\n","permalink":"https://olzhy.github.io/posts/fogetting-past-means-betrayal.html","tags":["随笔"],"title":"忘记就意味着背叛"},{"categories":["随笔"],"contents":"如今充斥着唯实用至上，金钱至上的价值观，我们重科技而轻人文。使得人文思想滞后科技发展，我们的物质大厦越建越高，道德大厦，精神大厦却一路滑坡。\n科技充实物质生活，人文引领精神追求；科技是看到的，人文是想到的；只有人文领跑科技，才不会出现历史性倒退。\nNS\n2014.02.19\n","permalink":"https://olzhy.github.io/posts/humanity-vs-science-and-technology.html","tags":["随笔"],"title":"人文与科技"},{"categories":["随笔"],"contents":"刚开始，我会觉得精心雕琢过，外壳华丽的文字会引人一震。\n后来觉得这样的文字大多看外表美轮美奂，品内容不知所云。\n现在觉得文字不应以外壳论贵贱，能从中读到真我的就是好文字。\n好文字长满想象的羽翼，源于但超脱本体，如涌泉出水，源远流长。这种由阅读而来的快感无法言喻，它会在脑海无尽絮绕，这美… 无法刻度。它也许还会跨过几多时间，影响时光另一端的少年———让其彻夜难眠。\n也许再过多少年我会觉得：所有功名都是过眼云烟，所有得失都会烟消云散，只有文字能打败时间，唯有思想能穿行千年。\n寝室NEU\n2014.02.18 23点\n","permalink":"https://olzhy.github.io/posts/about-good-writing.html","tags":["随笔"],"title":"关于好文字"},{"categories":["随笔"],"contents":"一生太短，只够嗅几次花，只够听几回雨，只够拾几朵叶，只够看几场雪，只够做几件事， 只够读几首诗，只够陪几年父母，只够活一种人生。\n东胜红泥塔\n2014.01.25\n","permalink":"https://olzhy.github.io/posts/how-much-our-life.html","tags":["随笔"],"title":"人生几何"},{"categories":["随笔"],"contents":"总结2013，我做的不够好，今日深刻反省，彻头彻尾照照丑陋的自己，以此为鉴。目前摆在我面前最大的敌人就是我自己，身上几个致命的毛病像恶魔一样时时阻碍我前行，不及时改正将使我此生一事无成。\n一、爱伪装，对感情不真诚。刘墉说“当你遇到一个女子，只有淡淡喜欢的感觉，却没有原子裂变似的反应，不要去招惹人家”。这一条我没有做到，尤其2013表现的非常可耻，为了恋爱而恋爱，为了取悦对方而说喜欢，真是亵渎了爱情和自己的人格。从此警告自己，以后不暧昧，不伪装。感情的事，慢慢等待，顺其自然。绝不刻意去寻求所谓的爱情。喜欢二字非常神圣，不是发自内心，绝不随便说出口。\n二、没有坚定的目标，执行力不够，办事拖拉。2013仅10月、11月目标坚定，表现良好。其它月份三天打鱼，两天晒网。纯粹忘了自己活着的意义，目标不清晰，活得行尸走肉。“天下古今之庸人，皆以一惰字致败。” 从今以后，每一天都要有追求，有意义的活着，一天都不能虚度。\n三、不够孝顺。为父母付出的远远不够。子欲孝而亲不待。从此刻开始，全心全意去爱自己的家人。\n助我成功之道：每天跑步，每天读书，每天思考，每天反省。\nNS\n2014.01.02\n","permalink":"https://olzhy.github.io/posts/conquer-myself.html","tags":["随笔"],"title":"战胜自己"},{"categories":["随笔"],"contents":"不经意看到一句话，说“有的时候你会觉得整个世界都是你自己的，比如上完晚自习的路上下起了鹅毛大雪”。读完闭上眼睛，想象这美妙的画面，只觉唯有这样温暖的句子才能让这喧嚣的世界静下来。\n我喜欢夜，喜欢雪，也喜欢安静。想起年少时在一个下着大雪的夜晚，坐在炉边烤着红薯，听着炉子里焰火燃烧的声音，等待着红薯烤熟后香味飘出的那一刻是多美的场景啊。\n也不知故乡离我远了，还是我离那个年纪远了，现在身处他乡，却再也寻不得那种感觉了。自哪天起，我开始厌倦城市的灯火通明，这里的夜黑的不彻底。有的时候黑暗并不是可憎的，我想着一个下雪夜，躺在家乡的热炕头，这里没有现代文明的星光灿烂，关上灯便一切皆无，天地都在安睡，静的只剩下这个静谧的小村庄和自己的心跳，这样的夜才是纯粹的。我仍然睁着眼睛，不为寻找光明，只为享受这无暇的黑。这样的夜看不见一切，又仿佛看见了一切。读读心声，想想事情，不知何时已微笑着睡去，那情景又是多么的美妙。\nNEU寝室\n2013.12.01 22点\n","permalink":"https://olzhy.github.io/posts/on-a-quiet-night.html","tags":["随笔"],"title":"静夜思"},{"categories":["随笔"],"contents":"从六月份实习以来，每天都很忙。这次突然放假一周，相较之前的假期，虽短，但也非常欣喜。\n平日里很忙，整天都像在赶早起的班车。待到周末，都有种偷得浮生半日闲的感觉。早上不必早起，中午可以吃顿自己想吃的菜，晌午睡一觉，醒来后泡上衣服不急着洗，待到吃完晚饭，天也凉快了许多。寻得一处小角落，坐下来，拿一瓶冰镇啤酒慢慢喝，当几个小嗝顺着喉咙打上来，那种捎带刺激的气体直通鼻腔，带着啤酒的余味，那个享受，那个畅快，心里油然生得一种快乐，这就是生活。到此情景，想一想，也难怪林清玄先生说看到喝汽水喝到打嗝的人无比的羡慕，他们的脸上都露着幸福的微笑。\n忙起来，脑子里想到的是任务，理想和责任；闲下来想到的是亲朋，自由和生活。忙起来感觉每天都有目标，都在收获；闲下来却可以做几件漫无目的的事，看到的是另一蕃风景。\n日出，日落，花开，花谢，风起，风止。大自然每天都在述说着美丽，每天都是馈赠。睁开双眼，万物尽收眼底；伸开双手，拥抱的就是世界。 其实上天赐予我们的一点都不少，每个人从出生所看到的这世界就是一份厚礼，刚开始我们一样富有，随着生活的推移，有的人物质富有了，心灵却干涸了。走到最后，唯有那个懂得欣赏，懂得感恩的人从没有贫穷过。正如威廉·布莱克所言“一沙一世界，一花一天堂，无限掌中置，刹那成永恒。”或忙或闲，都多看看这个美丽的世界吧。\n我推开窗帘……哇，今晨的第一把阳光就要洒进来。\nNEU寝室\n4:20\n2013.08.12\n","permalink":"https://olzhy.github.io/posts/busy-and-leisure.html","tags":["随笔"],"title":"忙与闲"},{"categories":["随笔"],"contents":"2013辽宁高考作文谈做沙子还是做珍珠，最后讲”一个人，只有做珍珠才能得到别人的认可”。\n个人以为，这样的价值观有待商榷。人关键不在于做沙子还是做珍珠，而在于沙子与珍珠都有用场。沙子就像一个个普通人，他们默默无闻，不引人注目，但可筑高楼做桥梁，依然很重要。珍珠光鲜透亮，但也许只能夺人耳目，却不实用。\nNS\n2013.06.08\n","permalink":"https://olzhy.github.io/posts/be-a-sand-or-pearl.html","tags":["随笔"],"title":"做沙子还是做珍珠"},{"categories":["随笔"],"contents":"从寝室去实验室，途径一条小道。道旁有多颗叫不出名字的树，这个季节，树上开满了花。我不懂花，自然叫不上它们的名字。只觉得那是一种美丽柔和的颜色，让你耳目清爽，心情愉悦。\n路人途经此地，便争相拍照，平日里稍显冷清的小路瞬间热闹了许多。路人冲镜头微笑，而我也会冲路人一笑，无形间花已将我们拉近了。\n途经那个花香最浓的拐角，我总会放慢脚步。穿行的瞬间，闭上双眼，仰面与开近的花朵拂面而过，花不语，人自醉。那种沁人心脾的香，让整个身心灵都得到了释放。这一刻我又似乎很懂花。\n2013.05.12\n","permalink":"https://olzhy.github.io/posts/flower-say-nothing-someone-infatuation.html","tags":["随笔"],"title":"花不语，人自醉"},{"categories":["随笔"],"contents":"你们还正值最美丽的青春年华， 正如笔尖流动的此刻，不知不觉，它正书写着美妙，悄无声息的绽放着。 时间飞火流弦，全然不知间将此刻发生的一切轻描淡写。就是这样的平平淡淡，待到某天，回过头却是轰轰烈烈。恰当时，莫感伤，时间沙漏滴下的每一刻伴随的点滴生活、喜怒哀乐，都将成为青春过后最美的珍藏。\n写给 ZJ 的人人留言\n沈阳\n2013.04.28\n","permalink":"https://olzhy.github.io/posts/about-youth.html","tags":["随笔"],"title":"致青春（写给ZJ的人人留言）"},{"categories":["随笔"],"contents":"刚才无意中登微博看到一则噩耗，我们当年的辅导员，赵老师不幸得了肾衰竭。\n感叹命运无情，悲痛欲绝，学生还在贷款读书，只能尽己所能汇去500元，献出一点绵薄之力。\n只盼赵老师换肾成功，定要活着归来。\n他是我们大学的辅导员，虽四年没有太多交流。给我的印象是那种爱开玩笑，嘻嘻哈哈，热爱生活但不追求事业，知足常乐的那种人。\n四年没有太多接触，只记得毕业后一年，也就是去年暑假去他那帮同学盖章还产生过一点小误会。\n记得那时他在科大门口学车，我帮曹阳过去找他给政审材料盖章。我这个人直来直去，不懂规矩也不愿逢迎守归，先去找的系主任老师，然后他让我去找赵老师。找到他后，记得他从学车的地方赶回来很生气。我心想是不是我这样冒昧找他的领导系主任，在上司面前暴露了他工作时间学车让他生气？ 他气冲冲给我盖完章后一脸怒气。我当时笑了一下还说了声抱歉，回去后心里实在憋屈，改了个签名表达愤怒。我觉得自己没做错什么，虽有他微博，此后也不怎么互动。\n后来偶尔刷刷微博，看到他改了名字。也没怎么注意，后来又有一次看到他改了个带药罐子的名字，只觉得他怎么还这么不成熟，一个名字改来改去，有意思吗，也没太多注意。\n直到今天看到他发一条得病的微博，我震惊了。\n过去的小恩怨在命运无常面前消失全无。\n夏天还活奔乱跳，健健康康的一个人为什么现在会成了这个样子。\n人生怎么这么突然，我盯着屏幕屏住了呼吸，一切都停止了。。。\nNEU.LAB 2013.04.17\n","permalink":"https://olzhy.github.io/posts/destiny.html","tags":["随笔"],"title":"原来命运如此无常"},{"categories":["随笔"],"contents":"我们改变不了大千世界，但可以改变方寸土地，只要每个人从我做起光明磊落，一身正气，明规则便会星火燎原，潜规则便会无地自容。\n2013.03.21\n","permalink":"https://olzhy.github.io/posts/light.html","tags":["随笔"],"title":"光明磊落，一身正气"},{"categories":["随笔"],"contents":"这几个月因为妈妈的身体，家里的事情，心里压力很大，心情很沉重。有好多事情需要承担，有的时候心里着实喘不过气来。 我这个人喜欢安静，不怎么喜欢交朋友。所以这么多年来，自认为处的最真心的朋友就那么几个，当然我也觉得朋友不在于多少， 有个知心的足矣。心情郁闷的时候总是自己藏在心里。至于我家里窘迫和复杂情况，这么多年只跟一个人讲过 。他便是那位我认为最好的朋友。平时我们坐在一起无话不谈，想说心里话第一个想起的是他。 他有什也愿意和我分享。我们是12年的同学，我们是高中同班，上同一个大学，一起考研一起奋斗过。 他心地善良，为人老实，性格内向，不善言语，做事被动。\n感情，学业，平时我们无话不谈。平时遇到什么，看到什么，虽然他的QQ隐身也要留言几句，他看到后也会第一时间回复我。没过几天我就想打个电话看看他过得怎样，追的女生进展怎么样了。开了头便聊的没有尽头。可虽然彼此这么知心要好，但他有个特点就是从不主动联系。总是我先来开第一句，时间久了，觉得老这么主动挺憋心的，调侃一句”你就不能主动给我打个电话”。 他总是用他的专属语气正儿八经回一句:”咱们又不是情侣，总联系有什么说的”。我太了解他的为人了，做朋友没得说，是那种给予半斤换你一两的那种。我也了解他的性格，就是这么直。当然我也相信他也当我是最好的朋友。可我总这么主动，虽不觉得友谊是自己死乞白赖在维系，时间久了也挺受伤。\n这一次因为母亲的身体我真是憔悴无奈。第一个想和说话的人也是他。他最了解我，也最懂我。对他的要求与期许也比别人多了些。我只愿我说出我的心情后得到最知心朋友的安慰。可他还是那么被动。遇到什么事都是我主动去说。朋友啊，这一次哥们心情不美丽，很不美丽，你能主动问候一下吗？我明知道不等对方的反馈，次次主动，说出所有，他定会为我排忧解难。可人遇到挫折了，心情也没有原先那么明朗了，以前那么富裕的”主动”也无力给予了。我只愿说出我的近况后，你能主动去问，主动跟我谈谈心，解解压。可你依旧那么被动。还是我不去说你就不会问，我不给你打电话，你就不会主动给我打。你还是你，可我变得矫情了，这时的我只觉得，我的好朋友，我主动累了，这次你能主动一下吗？ 自我上次给你打电话已经好长时间了，这一次我不主动了，我会去等。朋友啊，你的被动久了也是种伤害啊。\n2013.02.04\n","permalink":"https://olzhy.github.io/posts/friend.html","tags":["随笔"],"title":"朋友啊，你的被动久了让我很受伤害"},{"categories":["随笔"],"contents":"偶然想起几年前在收音机里听到的一个故事，多年过去了，已想不起故事的名字。但清晰记得是讲一个高中时代，男孩每到课间时间出来透风时对面总有一个穿裙子的漂亮女孩同时在对面的楼上做同样的事，他们虽无交集，但深深住刻在彼此的脑海。偶然一次课余活动，他隐约听到了她的名字，多年之后这个名字一直住在他的脑海。他喜欢上了她，他要找到她。不知是上天安排还是缘分注定。他找到了她–一个藏在他脑海多年的人。他们约好回当年读书的地方见面。她告他:”我到了”，他放下电话跑到了她跟前，是的，没错就是她。美丽的裙子，长长的头发，就是当年对面楼上的那个女孩。他叫出了藏在心里多年的名字。转身出现的却是她的闺蜜。过去的一次相见才发展阴差阳错记错了名字。当年的那个女孩站在眼前他叫出的却是另一个名字，却是她好友的名字。一切恍然大悟，全然不知所措。世事弄人，这个结局不禁让人感叹。想起这个故事心底还是跟之前一样的感动。总结几个关键字尽然搜到了这个故事的名字，叫从北楼看你。重新读了一遍，每一个字都是那么的纯真美丽，但结局未免让人遗憾。但仔细想想这个结局也许更接近现实。\n2013.01.14\n","permalink":"https://olzhy.github.io/posts/see-you.html","tags":["随笔"],"title":"从北楼看见你"},{"categories":["读书"],"contents":"简爱读到19章，这时出现的吉普赛人神奇的读心术引人入胜，让人赞叹。与简爱的对话更妙不可言。读到高潮当初揭面纱一刹那，怎么都想不到这个人竟是罗切斯特。这是我读到目前看到最精彩的一幕。\n2013.01.11\n","permalink":"https://olzhy.github.io/posts/jane-eyre.html","tags":["读书"],"title":"读《简爱》"},{"categories":["随笔"],"contents":"对于旅行，也许最重要的不是有很多钱，而是收拾行囊，迈出脚下的第一步。人生不是为了工作，而是为了生活。期待有一天，带着自己所爱的人环游世界！\n2012.11.15\n","permalink":"https://olzhy.github.io/posts/dream.html","tags":["随笔"],"title":"我有一个梦想"},{"categories":["随笔"],"contents":"那天在火车上听到筷子兄弟的这首歌，触到我心灵了。妈妈，您为我付出的太多了，岁月走得慢些吧，儿子一定要让您过上美好的生活。\n2012.11.14\n","permalink":"https://olzhy.github.io/posts/parent.html","tags":["随笔"],"title":"父亲"},{"categories":["随笔"],"contents":"还记得初一那个年少懵懂的年纪，还记得有人陪伴过我们的青葱岁月。自某一刻起，我们不经意作为见证人经历了这份坚定的爱情。走过这么多年， 世间经历多少变迁，有样东西丝毫没有改变。十二年了，真爱经住了风吹雨打与似水流年 。这一刻看着幸福的两个人要结婚的喜讯，心里流淌着的是温暖。你们化作美妙的乐曲装点过我年少的记忆，更让我坚定对爱情的信仰。最美的祝愿送给你们，我看到了好结果，更得到了正能量！\n2012.11.01\n","permalink":"https://olzhy.github.io/posts/wish.html","tags":["随笔"],"title":"写给将要结婚的两位初中同学（二保和袅袅）"},{"categories":["随笔"],"contents":"1、那一年，你们在最美的年纪相识，走过一段难忘的旅程，有了彼此珍藏的回忆，每次翻开都是美好的。以后的以后也许你的生活再没有他的出现，但回忆却是记忆的永恒。\n2、青春太过短暂，来不及在某个地方太过驻足，感觉痛了，微笑着向前也许比等待更显迫切。\n3、美好的东西真的会昙花一现，我们能捕捉到其中最美的画面留着以后珍藏已属不易。要奢求让美成为永恒，也许会逾越了上天所赐的界限。\n4、青春这个东西就是最美好的，但上天太过吝啬，每个人只有一次。我们处在其中，真的来不及在一个地方停留太久。放开些，也许任何美丽的事物都不及青春本身，我们遇到的人或风景，正因发生在此刻才显美妙。\n沈阳\n2012.09.30\n","permalink":"https://olzhy.github.io/posts/qingchuanxiaogan.html","tags":["随笔"],"title":"青春小感"},{"categories":["随笔"],"contents":"在图书馆看到组成原理，想起了在科大念本科时执教此门课程的赵继泽老师。他一心教书，特立独行，坚守原则，慎独自律，不争名夺利，不随波逐流，就这样以自定的原则与标准严格要求着自己。现实中有很多这样的“另类”，我们笑他们“假清高”，笑他们与现实格格不入，笑他们被时代遗弃。\n而我们，为了生存，委曲求全成了金钱的奴隶；看到身边的不平，只愿做一个明哲保身的旁观者。我们评价一个人是否成功只有金钱一个标准，我们没有了年少时的正义感，开始变得冷漠，我们的理想也被金钱绑架，而且为了它卖了命的奔波着，也许走到最后我们终究是个奴隶，出卖的不仅是身体，还有灵魂。\n沈阳\n2012.09.25\n","permalink":"https://olzhy.github.io/posts/xiaogan.html","tags":["随笔"],"title":"小感"},{"categories":["随笔"],"contents":"钓鱼岛事件，许多人们在网上叫喊，被称为愤青。我想说如果日本鬼子在你头上拉屎你都没什么反应，国家的青年都不愤怒，那这个民族是多么的可悲。当今的社会，被扭曲的不仅是人性，还有我们的民族性。\n所谓受过高等教育的人，他们的伟大理想又是什么。拿到美国大学的 offer\u0026mdash;\u0026gt;读完那里的研究生\u0026mdash;\u0026gt;拿到那里的绿卡？你直接喊你想成为美国人就可以了，干吗说的那么高尚！然后呢？你对这个社会做了什么，站在美利坚，站在世界上最民主的国度？然后对你的国人骂街，你想想那时的你又是一个什么嘴脸！\n有人问昂山素季民主什么时候能到来，答到“作为国民，你若没为民族做些什么，没有资格问这个问题”。我们的中国是有很多问题，的确有民族的劣根性，但如果受过高等教育的人都去逃避，都不为民主做的什么？我们拿什么对中国的未来指日可待？\n许多人说在现在的实际情况下，你谈什么民主。我想说不是国家不给你民主，是我们都没有接受民主的意识。你都没有索取的欲望，还谈什么。\n我们这个社会从来不缺精英，但是社会的民主靠的却不仅是精英。而是组成社会的每一个个体，每一个普通人。当所有人都懂得索取民主的时候，就是民主到来的时候。我们需要培养的是国民的主体意识和独立思考能力。个体要能看懂这个社会，有自己评判社会的标准。只有每一个个体都能为这个国家做点什么的时候，那才是民族复兴的时候。\n沈阳\n2012.08.30\n","permalink":"https://olzhy.github.io/posts/diaoyudao.html","tags":["随笔"],"title":"钓鱼岛背后，我们该思考、反省点什么？"},{"categories":["随笔"],"contents":"只要心中有着无比坚定的信念，内心深处的力量就会源源不断涌现；一路上再大的绊脚石，都将被视作成功的基石踩在脚下；眺望前方，竟如此美丽，突感没有了疲惫，朝着前方快步向前。\n考研前三个月 2011.10.02\n","permalink":"https://olzhy.github.io/posts/faith.html","tags":["随笔"],"title":"怀揣信念"},{"categories":["随笔"],"contents":"一个人骑着自行车搬着东西唱着歌，夹着泥土的丝丝小雨划过迎风奔走的脸颊，想想此时的自己，更像一个失落的逃荒者。推开门看到去年结识的老房东，只言片语说明来路，放下东西住到依旧熟悉的小房间，拍拍身上的尘土，冲着镜子对自己傻笑，移动沉重的脚步走到床前躺了下来，再也抑制不住无比杂乱的脑海，这一刻狠狠嘲笑着自己，只为埋葬那改变不了的从前，从下一刻开始我依旧相信明天！\n第一次考研失利，准备二战，租房回来时写\n太原\n2011.05.03\n","permalink":"https://olzhy.github.io/posts/waiting-for-second-round.html","tags":["随笔"],"title":"第一次考研失利，准备二战前的小感"},{"categories":["随笔"],"contents":"也许是因为期末，也许是因为是大学的最后一个学期，突然感觉很快就要离开这块熟悉的地方了。好想把握住现在，珍惜所能拥有的。这时似乎什么都要转瞬之间离我而去，梦境中的自己都想努力抓住些什么。是的，生活就是这样，给你快乐，也给你悲伤。我能做的就是在快乐的时候不悲伤，在悲伤的时候要快乐。现在最想做的就是静下心来，开始自己的旅程。不去想太多，也没必要想太多。努力去改变自己所能控制的，把自己不能控制的交给上帝去安排。唯一期望的就是在毕业时侯，不要有太多遗憾！\n写于大三上学期末\n太原\n2010.07.09\n","permalink":"https://olzhy.github.io/posts/new-journey.html","tags":["随笔"],"title":"静下心来，开始自己的旅程"},{"categories":null,"contents":"","permalink":"https://olzhy.github.io/archives/","tags":null,"title":"文章归档"},{"categories":null,"contents":"传统文化\n 古书房 中國哲學書電子化計劃 易學網  修行人生\n 黃庭書院 上果下空禅师  人文博客\n 赫赫文王 程可八八 无色墨水  正体字学习\n 萌典 韻典網 中文輸入法世界 快意速成輸入法 OpenVanilla 輸入法套件 開放中文轉換  趣站收藏\n 个站商店 趣站志 十年之约  技术博客\n The Old New Thing research!rsc FlowingData Simply Statistics The GitHub Blog Oreilly Data DZone Cloud Native MartinFowler Steve Fracia Christian Posta 王垠的博客 云风的博客 飞雪无情的博客 OneV’s Den Zlatan Eevee Kumu’s Blog Tony Bai 面相信仰编程 峰云就她了 Matrix67 酷壳 BYVoid Algorithms to Go 黑光技术 火丁笔记 鸟窝 Dave Cheney 敖小剑的博客  架构设计相关\n 凤凰架构  机器学习相关\n 刘建平 Pinard Eureka 机器学习读书笔记  Java 相关\n Java 全栈知识体系  Kotlin 相关\n Kotlin Programming Language  Golang 相关\n Golang Golang Spec GopherAcademy Bitfield Consulting Go 101 Go 语言中文网 Golang Bot chai2010 的博客 Go 语言高级编程 mzh/blog No Headback gin Go Revive Go Callvis Gaia Realize Go Tests Herman Schaaf 东隅已逝/桑榆非晚  云原生相关\n 云原生实验室 Huabing Blog  算法相关\n CodeinGame HackerRank CodeForces GeeksforGeeks LeetCode  数据库相关\n PostgreSQL Exercises The pgDash Blog  前端开发相关\n MDN Web Docs Web Fundamentals | Google Developers Learn JavaScript The Modern JavaScript Tutorial Douglas Crockford\u0026rsquo;s JavaScript Frontend Developer Roadmap TypeScript Bootstrap 网道 JavaScript 教程 ES6 入门教程  实用工具相关\n 在线 GIF 图片制作  ","permalink":"https://olzhy.github.io/favorites/","tags":null,"title":"网址收藏"}]